{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mostafa-ja/Adversarials/blob/main/CW_sine5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZ57x18g4b-h"
      },
      "source": [
        "# Load dataset and libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5qu1eexKQ-7v",
        "outputId": "ee5fb878-3694-4662-b8a1-a4e6af0f168b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader,WeightedRandomSampler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy import sparse\n",
        "import gdown\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score,balanced_accuracy_score\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import precision_recall_curve, roc_curve, auc\n",
        "import time\n",
        "import os\n",
        "import math\n",
        "from sklearn.manifold import TSNE\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import tqdm\n",
        "import pickle\n",
        "\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "myj8sjV1eSQc",
        "outputId": "afd59b0e-3f96-436d-c336-ee5eec18be39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://github.com/mostafa-ja/mal_adv3/raw/main/drebin/sparse_matrix_0.npz\n",
            "To: /content/sparse_matrix_0.npz\n",
            "100%|██████████| 461k/461k [00:00<00:00, 26.9MB/s]\n",
            "Downloading...\n",
            "From: https://github.com/mostafa-ja/mal_adv3/raw/main/drebin/sparse_matrix_1.npz\n",
            "To: /content/sparse_matrix_1.npz\n",
            "100%|██████████| 148k/148k [00:00<00:00, 15.8MB/s]\n",
            "Downloading...\n",
            "From: https://github.com/mostafa-ja/mal_adv3/raw/main/drebin/sparse_matrix_2.npz\n",
            "To: /content/sparse_matrix_2.npz\n",
            "100%|██████████| 150k/150k [00:00<00:00, 9.13MB/s]\n",
            "Downloading...\n",
            "From: https://github.com/mostafa-ja/mal_adv3/raw/main/drebin/sparse_matrix_y0.npz\n",
            "To: /content/sparse_matrix_y0.npz\n",
            "100%|██████████| 5.79k/5.79k [00:00<00:00, 4.92MB/s]\n",
            "Downloading...\n",
            "From: https://github.com/mostafa-ja/mal_adv3/raw/main/drebin/sparse_matrix_y1.npz\n",
            "To: /content/sparse_matrix_y1.npz\n",
            "100%|██████████| 2.64k/2.64k [00:00<00:00, 8.43MB/s]\n",
            "Downloading...\n",
            "From: https://github.com/mostafa-ja/mal_adv3/raw/main/drebin/sparse_matrix_y2.npz\n",
            "To: /content/sparse_matrix_y2.npz\n",
            "100%|██████████| 2.71k/2.71k [00:00<00:00, 2.47MB/s]\n",
            "Downloading...\n",
            "From: https://github.com/mostafa-ja/mal_adv4/raw/main/dataset/insertion_array.pkl\n",
            "To: /content/insertion_array.pkl\n",
            "100%|██████████| 80.2k/80.2k [00:00<00:00, 26.8MB/s]\n",
            "Downloading...\n",
            "From: https://github.com/mostafa-ja/mal_adv4/raw/main/dataset/removal_array.pkl\n",
            "To: /content/removal_array.pkl\n",
            "100%|██████████| 80.2k/80.2k [00:00<00:00, 27.4MB/s]\n",
            "Downloading...\n",
            "From: https://github.com/mostafa-ja/mal_adv4/raw/main/dataset/adverserial_attacks_functions.py\n",
            "To: /content/adverserial_attacks_functions.py\n",
            "67.1kB [00:00, 64.2MB/s]                   \n",
            "Downloading...\n",
            "From: https://github.com/mostafa-ja/mal_adv3/raw/main/drebin/model_AT_rFGSM.pth\n",
            "To: /content/model_AT_rFGSM.pth\n",
            "100%|██████████| 8.17M/8.17M [00:00<00:00, 193MB/s]\n",
            "Downloading...\n",
            "From: https://github.com/mostafa-ja/mal_adv3/raw/main/drebin/best_model_AT_imax.pth\n",
            "To: /content/best_model_AT_imax.pth\n",
            "100%|██████████| 8.17M/8.17M [00:00<00:00, 188MB/s]\n",
            "Downloading...\n",
            "From: https://github.com/mostafa-ja/mal_adv3/raw/main/data/model_AT_max%20(4).pth\n",
            "To: /content/model_AT_max%20(4).pth\n",
            "100%|██████████| 8.27M/8.27M [00:00<00:00, 204MB/s]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "download_links = ['https://github.com/mostafa-ja/mal_adv3/raw/main/drebin/sparse_matrix_0.npz',\n",
        "                  'https://github.com/mostafa-ja/mal_adv3/raw/main/drebin/sparse_matrix_1.npz',\n",
        "                  'https://github.com/mostafa-ja/mal_adv3/raw/main/drebin/sparse_matrix_2.npz',\n",
        "                  'https://github.com/mostafa-ja/mal_adv3/raw/main/drebin/sparse_matrix_y0.npz',\n",
        "                  'https://github.com/mostafa-ja/mal_adv3/raw/main/drebin/sparse_matrix_y1.npz',\n",
        "                  'https://github.com/mostafa-ja/mal_adv3/raw/main/drebin/sparse_matrix_y2.npz',\n",
        "                  'https://github.com/mostafa-ja/mal_adv4/raw/main/dataset/insertion_array.pkl',\n",
        "                  'https://github.com/mostafa-ja/mal_adv4/raw/main/dataset/removal_array.pkl',\n",
        "                  'https://github.com/mostafa-ja/mal_adv4/raw/main/dataset/adverserial_attacks_functions.py',\n",
        "                  'https://github.com/mostafa-ja/mal_adv3/raw/main/drebin/model_AT_rFGSM.pth',\n",
        "                  'https://github.com/mostafa-ja/mal_adv3/raw/main/drebin/best_model_AT_imax.pth',\n",
        "                  'https://github.com/mostafa-ja/mal_adv3/raw/main/data/model_AT_max%20(4).pth',\n",
        "]\n",
        "\n",
        "output_filepath = '/content/'\n",
        "for link in download_links:\n",
        "  gdown.download(link, output_filepath)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UyZKBf9rE7Ps",
        "outputId": "b707cd2f-0f1b-40de-dd00-f97372bc14e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "from adverserial_attacks_functions import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OhUqprJUDNPS",
        "outputId": "cd515006-90b3-41f2-dc7e-2afeefc4eba8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000\n",
            "10000\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Open the .pkl file\n",
        "with open('/content/insertion_array.pkl', 'rb') as f:\n",
        "    # Load the object\n",
        "    insertion_array = pickle.load(f)\n",
        "\n",
        "# Close the file\n",
        "f.close()\n",
        "\n",
        "insertion_array = torch.tensor(insertion_array, dtype=torch.uint8).to(device)\n",
        "print(len(insertion_array))\n",
        "\n",
        "# Open the .pkl file\n",
        "with open('/content/removal_array.pkl', 'rb') as f:\n",
        "    # Load the object\n",
        "    removal_array = pickle.load(f)\n",
        "\n",
        "# Close the file\n",
        "f.close()\n",
        "\n",
        "removal_array = torch.tensor(removal_array, dtype=torch.uint8).to(device)\n",
        "print(len(removal_array))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGQPUPlSeXbD",
        "outputId": "a7d9d244-c873-4184-ed87-b7c7ef4f5ba0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shapes:\n",
            "x_train: torch.Size([28683, 10000])\n",
            "x_val: torch.Size([9562, 10000])\n",
            "x_test: torch.Size([9562, 10000])\n",
            "y_train: torch.Size([28683, 1])\n",
            "y_val: torch.Size([9562, 1])\n",
            "y_test: torch.Size([9562, 1])\n"
          ]
        }
      ],
      "source": [
        "# Load dataset\n",
        "X_train = sparse.load_npz(\"/content/sparse_matrix_0.npz\").toarray()\n",
        "X_val = sparse.load_npz(\"/content/sparse_matrix_1.npz\").toarray()\n",
        "X_test = sparse.load_npz(\"/content/sparse_matrix_2.npz\").toarray()\n",
        "\n",
        "X_train = torch.tensor(X_train, dtype=torch.int8)\n",
        "X_val = torch.tensor(X_val, dtype=torch.int8)\n",
        "X_test = torch.tensor(X_test, dtype=torch.int8)\n",
        "\n",
        "\n",
        "y_train = sparse.load_npz(\"/content/sparse_matrix_y0.npz\").toarray().reshape((-1, 1))\n",
        "y_val = sparse.load_npz(\"/content/sparse_matrix_y1.npz\").toarray().reshape((-1, 1))\n",
        "y_test = sparse.load_npz(\"/content/sparse_matrix_y2.npz\").toarray().reshape((-1, 1))\n",
        "\n",
        "y_train = torch.tensor(y_train, dtype=torch.int8)\n",
        "y_val = torch.tensor(y_val, dtype=torch.int8)\n",
        "y_test = torch.tensor(y_test, dtype=torch.int8)\n",
        "\n",
        "\n",
        "print(\"Shapes:\")\n",
        "print(\"x_train:\", X_train.shape)\n",
        "print(\"x_val:\", X_val.shape)\n",
        "print(\"x_test:\", X_test.shape)\n",
        "print(\"y_train:\", y_train.shape)\n",
        "print(\"y_val:\", y_val.shape)\n",
        "print(\"y_test:\", y_test.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ohTwkOtWecT-"
      },
      "outputs": [],
      "source": [
        "\n",
        "# number of benigns and malicious sample in training dataset\n",
        "n_ben = (y_train.squeeze()== 0).sum().item()\n",
        "n_mal = (y_train.squeeze()== 1).sum().item()\n",
        "\n",
        "# Combine features and labels into datasets\n",
        "# we use dtype=torch.int8, for Memory-Efficient here, later we will convert to float\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "val_dataset = TensorDataset(X_val, y_val)\n",
        "test_dataset = TensorDataset(X_test, y_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YM7gwhrB7g7r",
        "outputId": "92d34f8d-4fa9-4adb-e3c6-e1db1b9e9787"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-7d5445839520>:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model_AT_rFGSM.load_state_dict(torch.load('model_AT_rFGSM.pth', map_location=torch.device(device)))\n",
            "<ipython-input-7-7d5445839520>:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model_AT_imax.load_state_dict(torch.load('best_model_AT_imax.pth', map_location=torch.device(device)))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "\n",
        "# Create an instance of your model\n",
        "model_AT_rFGSM = MalwareDetectionModel().to(device)\n",
        "\n",
        "# Load model parameters\n",
        "model_AT_rFGSM.load_state_dict(torch.load('model_AT_rFGSM.pth', map_location=torch.device(device)))\n",
        "\n",
        "# Create an instance of your model\n",
        "model_AT_imax = MalwareDetectionModel().to(device)\n",
        "\n",
        "# Load model parameters\n",
        "model_AT_imax.load_state_dict(torch.load('best_model_AT_imax.pth', map_location=torch.device(device)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4EOIrtwxK11",
        "outputId": "64e3a2a9-6af2-4b5f-fc39-dd65a2702e20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 2])\n",
            "torch.Size([2, 128])\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Define the Linear Classifier(NO NEED)\n",
        "class LinearClassifier(nn.Module):\n",
        "    def __init__(self, dim_in=200, num_classes=2):\n",
        "        super(LinearClassifier, self).__init__()\n",
        "        self.fc = nn.Linear(dim_in, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "\n",
        "# Define the CoreModel\n",
        "class CoreModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CoreModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(10000, 200)\n",
        "        self.dropout1 = nn.Dropout(0.6)\n",
        "        self.fc2 = nn.Linear(200, 200)\n",
        "        self.dropout2 = nn.Dropout(0.6)\n",
        "        self.fc3 = nn.Linear(200, 2)\n",
        "\n",
        "    def forward(self, x, return_z=False):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        #x = self.dropout1(x)\n",
        "        z = F.relu(self.fc2(x))\n",
        "        z2 = self.dropout2(z)\n",
        "        output  = self.fc3(z2)\n",
        "        if return_z:\n",
        "            return output, z\n",
        "\n",
        "        return output\n",
        "\n",
        "    def forward_from_z(self, z):\n",
        "        output = self.fc3(z)\n",
        "        return output\n",
        "\n",
        "# Define the Projection Head\n",
        "class ProjHead(nn.Module):\n",
        "    def __init__(self, dim_in=200, feat_dim=128):\n",
        "        super(ProjHead, self).__init__()\n",
        "        self.head = nn.Linear(dim_in, feat_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #x = F.normalize(x, dim=1)\n",
        "        z_proj = self.head(x)\n",
        "        z_proj = F.normalize(z_proj, dim=1)\n",
        "        return z_proj\n",
        "\n",
        "\n",
        "\n",
        "# Define the Wrapper\n",
        "class Wrapper(nn.Module):\n",
        "    def __init__(self, CoreModel, ProjLayer):\n",
        "        super(Wrapper, self).__init__()\n",
        "        self.CoreModel = CoreModel\n",
        "        self.ProjLayer = ProjLayer\n",
        "        #self.LC = LC\n",
        "\n",
        "    def forward(self, x, return_z=False):\n",
        "        output, z = self.CoreModel(x, return_z=True)\n",
        "        z_proj = self.ProjLayer(z)\n",
        "        #output = self.LC(z)\n",
        "        if return_z:\n",
        "            return output, z_proj\n",
        "        else:\n",
        "            return output\n",
        "\n",
        "# Instantiate the components\n",
        "core_model = CoreModel()\n",
        "proj_head = ProjHead(200, 128)\n",
        "#linear_classifier = LinearClassifier(128, 2)\n",
        "\n",
        "# Combine them in the Wrapper\n",
        "combined_model = Wrapper(core_model, proj_head)\n",
        "\n",
        "# Example usage\n",
        "x = torch.randn(2, 10000)  # Example input\n",
        "output, z_proj = combined_model(x,return_z=True)\n",
        "print(output.shape)\n",
        "print(z_proj.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tRzIXsibxPlA",
        "outputId": "69cc29bb-1be7-4cde-a00d-6834f717dea5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-0291d6a21af7>:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  combined_model.load_state_dict(torch.load('model_AT_max%20(4).pth', map_location=torch.device(device)))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "combined_model = Wrapper(core_model, proj_head).to(device)\n",
        "combined_model.load_state_dict(torch.load('model_AT_max%20(4).pth', map_location=torch.device(device)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "NN07TZGV6p_f"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Q0meyJbEgBK2"
      },
      "outputs": [],
      "source": [
        "def pgd_min22(x, y, model, insertion_array, removal_array, k=25, step_length=0.02, norm='linf', initial_rounding_threshold=0.5, round_threshold=0.5, random=False, is_report_loss_diff=True, is_sample=False):\n",
        "    \"\"\"\n",
        "    Projected Gradient Descent (PGD) adversarial attack (loss based on goal's class, which we have to minimize the loss).\n",
        "    :param y: Ground truth labels\n",
        "    :param x: Feature vector\n",
        "    :param model: Neural network model\n",
        "    :param k: Number of steps\n",
        "    :param step_length: Step size for each iteration\n",
        "    :param norm: Norm used for perturbation ('linf' or 'l2')\n",
        "    :param initial_rounding_threshold: Threshold parameter for rounding the initial x_next\n",
        "    :param round_threshold: Threshold parameter for rounding\n",
        "    :param random: Flag to generate random thresholds\n",
        "    :param is_report_loss_diff: Flag to report loss difference\n",
        "    :param is_sample: Flag to sample randomly from the feasible area\n",
        "    :return: The adversarial version of x (tensor)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Compute natural loss\n",
        "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
        "    loss_natural = criterion(model(x), torch.zeros_like(y.view(-1).long()))\n",
        "\n",
        "    # Initialize starting point\n",
        "    x_next = x.clone()\n",
        "    #x_next = get_x0(x_next, initial_rounding_threshold, is_sample)\n",
        "\n",
        "\n",
        "    # Expand insertion_array and removal_array to match the batch size\n",
        "    expanded_insertion_array = insertion_array.expand(x.shape[0], -1)\n",
        "    expanded_removal_array = removal_array.expand(x.shape[0], -1)\n",
        "\n",
        "    # Update insertion and removal arrays based on input x\n",
        "    insertion_array_updated = torch.bitwise_or(expanded_insertion_array, x.to(torch.uint8))\n",
        "    removal_array_updated = torch.bitwise_or(expanded_removal_array, 1 - x.to(torch.uint8))\n",
        "\n",
        "    # Multi-step PGD\n",
        "    for t in range(k):\n",
        "        #print('*********** ',t)\n",
        "        # Forward pass\n",
        "        x_var = x_next.clone().detach().requires_grad_(True)\n",
        "        y_model = model(x_var)\n",
        "        #loss = criterion(y_model, torch.zeros_like(y.view(-1).long()))\n",
        "        #print(loss)\n",
        "        #print('loss_mal : ',criterion(y_model, torch.zeros_like(y.view(-1).long())))\n",
        "\n",
        "        batch_size = x_var.shape[0]\n",
        "        real = y_model[torch.arange(batch_size), y.view(-1).long()]\n",
        "        other = y_model[torch.arange(batch_size), (y.view(-1).long()- 1)]\n",
        "        loss = torch.clamp((real - other + 0.1), min=0.0)\n",
        "        #loss = (real - other)\n",
        "\n",
        "\n",
        "        # Compute gradient\n",
        "        grad_vars = torch.autograd.grad(loss.mean(), x_var)\n",
        "        gradients = -(grad_vars[0].data)\n",
        "        #print('torch.abs(gradients).sum() : ',torch.abs(gradients).sum(dim=-1))\n",
        "\n",
        "        #grad4insertion = (gradients >= 0) *(x_var < 1.)* insertion_array_updated * gradients\n",
        "        grad4removal = (gradients < 0) * (x_var > 0.) * removal_array_updated * gradients\n",
        "\n",
        "        grad4insertion = (gradients >= 0) *(x_var < 1.)* gradients\n",
        "        #grad4removal = (gradients < 0) * (x_var > 0.) * gradients\n",
        "\n",
        "        gradients = grad4removal + grad4insertion\n",
        "        #print(torch.abs(gradients).sum())\n",
        "\n",
        "        # Norm\n",
        "        if norm == 'linf':\n",
        "            perturbation = torch.sign(gradients).float()\n",
        "\n",
        "        elif norm == 'free':\n",
        "            perturbation = gradients\n",
        "\n",
        "        elif norm == 'max':\n",
        "            max_values, _ = torch.max(torch.abs(gradients), dim=-1, keepdim=True)\n",
        "            perturbation = (gradients / (max_values+ 1e-20))\n",
        "\n",
        "        elif norm == 'l2':\n",
        "            l2norm = torch.linalg.norm(gradients, dim=-1, keepdim=True)\n",
        "            perturbation = (gradients / (l2norm + 1e-20)).float()\n",
        "\n",
        "        elif norm == 'l1':\n",
        "            # consider just features of a sample which are not updated yet(because our update is 0to1 or 1to0 not stepwise)\n",
        "            un_mod = torch.abs(x - x_var) <= 1e-6\n",
        "            gradients = gradients * un_mod\n",
        "            max_grad = torch.topk(torch.abs(gradients).view(gradients.size(0), -1), 1, dim=-1)[0]\n",
        "            #print('max_grad ',max_grad)\n",
        "            perturbation = (torch.abs(gradients) >= max_grad).float() * torch.sign(gradients).float()\n",
        "            done = get_done(x_next, y, model)\n",
        "            if torch.all(done):\n",
        "                break\n",
        "            perturbation[done] = 0.\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Expect 'l1' or 'l2' or 'linf' norm.\")\n",
        "\n",
        "        # Update x_next\n",
        "        x_next = torch.clamp(x_next + perturbation * step_length, min=0., max=1.)\n",
        "\n",
        "    # Rounding step\n",
        "    if random:\n",
        "       round_threshold = torch.rand(x_next.size())\n",
        "    #x_next = round_x(x_next, round_threshold=round_threshold)\n",
        "\n",
        "    # Compute adversarial loss\n",
        "    #loss_adv = criterion(model(x_next), torch.zeros_like(y.view(-1).long())).data\n",
        "\n",
        "    # Replace with natural if adversarial loss is higher\n",
        "    #replace_flag = (loss_adv > loss_natural).squeeze()\n",
        "    #x_next[replace_flag] = x[replace_flag]\n",
        "\n",
        "    diff = torch.abs(x_next - x).sum(dim=-1)\n",
        "\n",
        "    if is_report_loss_diff:\n",
        "        done = get_done(x_next, y, model)\n",
        "        print(f\"PGD {norm}: Attack effectiveness {done.sum().item() / x.size()[0] * 100:.3f}%.\")\n",
        "        print('mean of differnce featres',torch.mean(diff))\n",
        "\n",
        "    return x_next\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AuNerR2pnmqA"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "swUH5UhYnm9g"
      },
      "outputs": [],
      "source": [
        "# Define the DataLoader for training, validation, and test sets\n",
        "batch_size = 10000\n",
        "#sampler = weight_sampler(y_train)\n",
        "#train_loader = DataLoader(train_dataset, batch_size=batch_size,sampler=sampler)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size,shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f_uhx997-RNq"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def final_rounding(oimgs, newimg, model):\n",
        "    # Ensure all tensors are on the GPU\n",
        "    oimgs = oimgs.to(device)\n",
        "    perturbed = newimg.to(device)\n",
        "\n",
        "    # Set the model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Initialize active_mask based on model's initial prediction\n",
        "    active_mask = (model(oimgs).argmax(dim=1) == 1)\n",
        "\n",
        "    # Calculate the absolute difference between original and perturbed images\n",
        "    total_change = (perturbed - oimgs).abs()\n",
        "\n",
        "    # Determine which changes are significant\n",
        "    significant_mask = total_change > 0.1\n",
        "\n",
        "    # Prepare a copy of the original images to apply the final rounding\n",
        "    final = oimgs.clone()\n",
        "\n",
        "    # Determine the direction of changes: 0 for decrease/no change, 1 for increase\n",
        "    changes = (perturbed > oimgs).float()\n",
        "\n",
        "    # Get sorted indices of the total changes for each image in descending order\n",
        "    sorted_idx = total_change.argsort(dim=1, descending=True)\n",
        "\n",
        "    # Get the maximum number of significant changes across all images\n",
        "    max_significant_changes = significant_mask.sum(dim=1).max()\n",
        "\n",
        "    # Iterate over each significant change index\n",
        "    for i in range(max_significant_changes):\n",
        "        # Update active_mask indices\n",
        "        active_indices = torch.nonzero(active_mask).squeeze()\n",
        "\n",
        "        if active_indices.numel() == 0:\n",
        "            break\n",
        "\n",
        "        # Safeguard when active_indices is a single-element tensor\n",
        "        if active_indices.dim() == 0:\n",
        "            active_indices = active_indices.unsqueeze(0)\n",
        "\n",
        "        # Get the relevant indices for current change\n",
        "        indices = sorted_idx[active_indices, i]\n",
        "\n",
        "        # Apply the changes to the final image tensor\n",
        "        final[active_indices, indices] = changes[active_indices, indices]\n",
        "\n",
        "        # Update the active_mask based on the model's output\n",
        "        active_mask[active_indices] = (model(final[active_indices]).argmax(dim=1) == 1)\n",
        "\n",
        "    return final\n"
      ],
      "metadata": {
        "id": "G7HCDPvs-RgR"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "BRaP6iFVnm9h"
      },
      "outputs": [],
      "source": [
        "def adv_predict(test_loader, model, attack, device, **kwargs):\n",
        "\n",
        "    model.eval()\n",
        "    n_samples = 0\n",
        "    cor_test = 0\n",
        "    cor_ad_test = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x_test, y_test in test_loader:\n",
        "            x_test, y_test = x_test.to(torch.float32).to(device), y_test.to(device)\n",
        "            #outputs = model(x_test)\n",
        "            #predicted = outputs.argmax(1).unsqueeze(1)\n",
        "            #acc_test = (predicted == y_test).sum().item() / len(y_test)\n",
        "            #avg_acc_test.append(acc_test)\n",
        "\n",
        "            mal_x_batch, mal_y_batch = x_test[y_test.squeeze() == 1], y_test[y_test.squeeze() == 1]\n",
        "            n_samples += len(mal_y_batch)\n",
        "\n",
        "            outputs = model(mal_x_batch)\n",
        "            predicted = outputs.argmax(1)\n",
        "            cor_test += (predicted == 1).sum().item()\n",
        "\n",
        "            # Generate adversarial examples for test set\n",
        "            with torch.enable_grad():\n",
        "                pertb_mal_x= attack(mal_x_batch, mal_y_batch, model, **kwargs)\n",
        "\n",
        "            outputs = model(pertb_mal_x)\n",
        "            y_pred = outputs.argmax(1)\n",
        "            cor_ad_test += (y_pred == 1).sum().item()\n",
        "\n",
        "            #print(outputs)\n",
        "            print(len(mal_y_batch))\n",
        "            print((y_pred == 0).sum().item())\n",
        "            #print((y_pred == 0))\n",
        "            #print('mean difference ',(torch.abs(pertb_mal_x - mal_x_batch)).sum()/len(mal_y_batch))\n",
        "            #print((torch.abs(pertb_mal_x[y_pred == 0] - mal_x_batch[y_pred == 0])).sum())\n",
        "            #print((torch.abs(pertb_mal_x - mal_x_batch)).sum(1))\n",
        "\n",
        "            #print('***************************')\n",
        "\n",
        "\n",
        "    # Calculate test accuracy\n",
        "    print(f\"Accuracy of just malwares (without attack): {(cor_test / n_samples) * 100:.4}% | Under attack: {(cor_ad_test / n_samples) * 100:.4}%.\")\n",
        "\n",
        "    return pertb_mal_x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTzMhU-t8Fvy"
      },
      "source": [
        "# New function instead of tanh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 568
        },
        "id": "2tMeOBF-Fy9g",
        "outputId": "41fb12e0-2f2d-4e7b-8e1b-3630a1ea56f6"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.optimize import newton\n",
        "\n",
        "# Define the function\n",
        "def f(x):\n",
        "    return 3 * x**2 - 2 * x**3\n",
        "\n",
        "# Generate values for x from 0 to 1\n",
        "x_values = np.linspace(-0.5, 1.5, 500)\n",
        "y_values = f(x_values)\n",
        "\n",
        "# Plot the function\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(x_values, y_values, label=r\"$f(x) = 3x^2 - 2x^3$\", color=\"blue\")\n",
        "\n",
        "# Add labels and title\n",
        "plt.title(\"Plot of the Function $f(x) = 3x^2 - 2x^3$\")\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"f(x)\")\n",
        "plt.axhline(0, color='black',linewidth=0.5)\n",
        "plt.axvline(0, color='black',linewidth=0.5)\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C70tMY-rH3_l"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "sine function"
      ],
      "metadata": {
        "id": "Tyh5V8eBAPe1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ppQXj918IsOV",
        "outputId": "79421ff7-7045-4572-d000-883d2e700ddd"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Define the function and its gradient\n",
        "def sin_func(x):\n",
        "    return (np.sin((x-0.5)*(np.pi))+1)/2.\n",
        "\n",
        "# its derivative\n",
        "def sin_func_prime(x):\n",
        "    return np.pi/2. * np.cos((x-0.5)*(np.pi))\n",
        "\n",
        "# Generate values for x from -2π to 2π\n",
        "x_values = np.linspace(-0.5, 1.5, 1000)\n",
        "sin_values = sin_func(x_values)\n",
        "sin_prime_values = sin_func_prime(x_values)\n",
        "\n",
        "# Plot sin(x)\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(x_values, sin_values, label=r\"$\\sin(x)$\", color=\"blue\")\n",
        "\n",
        "# Add labels and title\n",
        "plt.title(\"Plot of $\\sin(x)$\")\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"$\\sin(x)$\")\n",
        "plt.axhline(0, color='black',linewidth=0.5)\n",
        "plt.axvline(0, color='black',linewidth=0.5)\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot the gradient of sin(x)\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(x_values, sin_prime_values, label=r\"$\\cos(x)$\", color=\"red\")\n",
        "\n",
        "# Add labels and title\n",
        "plt.title(\"Plot of the Gradient of $\\sin(x)$: $\\cos(x)$\")\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"$\\cos(x)$\")\n",
        "plt.axhline(0, color='black',linewidth=0.5)\n",
        "plt.axvline(0, color='black',linewidth=0.5)\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4Smga17MTKi",
        "outputId": "eb8652f9-c119-41a4-d439-08ba9b03137b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0\n",
            "1.0\n"
          ]
        }
      ],
      "source": [
        "print(sin_func(0))\n",
        "print(sin_func(1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BaR8Xzl6M41k",
        "outputId": "7478efa3-bcd8-480d-92c9-c84ba05d5978"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.5707885597057034"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "sin_prime_values.max()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the function\n",
        "def sin_func(x):\n",
        "    return (np.sin((x-0.5)*(np.pi))+1)/2.\n",
        "\n",
        "# Generate values for x including 0 and 1 explicitly\n",
        "x_values = np.linspace(-0.5, 1.5, 10000)\n",
        "\n",
        "# Ensure 0 and 1 are in the list by adding them and sorting the array\n",
        "x_values = np.sort(np.unique(np.concatenate((x_values, [0, 1]))))\n",
        "\n",
        "sin_values = sin_func(x_values)\n",
        "\n",
        "# Calculate the gradient (derivative) numerically using np.gradient\n",
        "sin_prime_values = np.gradient(sin_values, x_values)\n",
        "\n",
        "# Create a figure with two subplots\n",
        "fig, axs = plt.subplots(2, 1, figsize=(10, 10))\n",
        "\n",
        "# Plot sin(x) in the first subplot\n",
        "axs[0].plot(x_values, sin_values, label=r\"$\\sin(x)$\", color=\"blue\")\n",
        "axs[0].set_title(\"Plot of $\\sin(x)$\")\n",
        "axs[0].set_xlabel(\"x\")\n",
        "axs[0].set_ylabel(r\"$\\sin(x)$\")\n",
        "axs[0].axhline(0, color='black', linewidth=0.5)\n",
        "axs[0].axvline(0, color='black', linewidth=0.5)\n",
        "axs[0].grid(True)\n",
        "axs[0].legend()\n",
        "\n",
        "# Plot the gradient of sin(x) in the second subplot\n",
        "axs[1].plot(x_values, sin_prime_values, label=r\"$\\frac{d}{dx}\\sin(x)$\", color=\"red\")\n",
        "axs[1].set_title(\"Plot of Gradient of $\\sin(x)$\")\n",
        "axs[1].set_xlabel(\"x\")\n",
        "axs[1].set_ylabel(r\"$\\frac{d}{dx}\\sin(x)$\")\n",
        "axs[1].axhline(0, color='black', linewidth=0.5)\n",
        "axs[1].axvline(0, color='black', linewidth=0.5)\n",
        "axs[1].grid(True)\n",
        "axs[1].legend()\n",
        "\n",
        "# Show the plots\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 950
        },
        "id": "drLlgSAkBsBs",
        "outputId": "a5604b96-1b1b-4663-98cc-a153a4ef974e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x1000 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**consider that we use \"Numerical Differentiation\"(estimation) not exact values for derivatives**"
      ],
      "metadata": {
        "id": "EwHrhecxEAv0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(sin_prime_values.max())\n",
        "print(sin_prime_values.min())\n",
        "print(abs(sin_prime_values).min())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3R8ukuIEYxM",
        "outputId": "dcae2522-bd91-4674-8282-9015caefe429"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.5707961458890622\n",
            "-1.5707962234202963\n",
            "2.2737367544323206e-13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# at x=0.0\n",
        "print(x_values[2500])\n",
        "print(sin_values[2500])\n",
        "print(sin_prime_values[2500])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aN4STTXKDZzL",
        "outputId": "9509f795-f040-4c90-d73a-2700df4f4d26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0\n",
            "0.0\n",
            "1.294856745697598e-12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# at x=1.0\n",
        "print(x_values[7501])\n",
        "print(sin_values[7501])\n",
        "print(sin_prime_values[7501])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4u186LyjEDu6",
        "outputId": "2e2158f7-3d90-4139-d46f-d7d54844885f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0\n",
            "1.0\n",
            "2.2737367544323206e-13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5WbSALe4-HDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TEST"
      ],
      "metadata": {
        "id": "ur8Xjp9qQdWp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "from torch.optim.lr_scheduler import CyclicLR\n",
        "\n",
        "def compare(output, target, confidence, targeted):\n",
        "    \"\"\"\n",
        "    Compares the output with the target for a targeted or untargeted attack.\n",
        "\n",
        "    For targeted attacks, it checks if the output class matches the target class.\n",
        "    For untargeted attacks, it checks if the output class does not match the target class.\n",
        "    \"\"\"\n",
        "    if not isinstance(output, (float, int, torch.LongTensor, torch.cuda.LongTensor)):\n",
        "        output = torch.clone(output).detach()\n",
        "        if targeted:\n",
        "            output[target] -= confidence  # Slightly reduce the confidence for targeted attacks\n",
        "        else:\n",
        "            output[target] += confidence  # Slightly increase the confidence for untargeted attacks\n",
        "        output = torch.argmax(output, dim=1)\n",
        "    return output == target if targeted else output != target\n",
        "\n",
        "def sin_func_torch(input_tensor):\n",
        "    \"\"\"\n",
        "    A sine-based transformation applied to the input tensor.\n",
        "\n",
        "    The function applies a sine transformation and rescales the output to [0, 1].\n",
        "    \"\"\"\n",
        "    return (torch.sin((input_tensor - 0.5) * torch.pi) + 1) / 2.0\n",
        "\n",
        "# Define the DataLoader for training, validation, and test sets\n",
        "batch_size = 10000\n",
        "#sampler = weight_sampler(y_train)\n",
        "#train_loader = DataLoader(train_dataset, batch_size=batch_size,sampler=sampler)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size,shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "for x_test, y_test in test_loader:\n",
        "    x_test, y_test = x_test.to(torch.float32).to(device), y_test.to(device)\n",
        "    #outputs = model(x_test)\n",
        "    #predicted = outputs.argmax(1).unsqueeze(1)\n",
        "    #acc_test = (predicted == y_test).sum().item() / len(y_test)\n",
        "    #avg_acc_test.append(acc_test)\n",
        "\n",
        "    imgs, y = x_test[y_test.squeeze() == 1], y_test[y_test.squeeze() == 1]\n",
        "    break\n",
        "\n",
        "model = model_AT_rFGSM\n",
        "max_iterations=5000\n",
        "learning_rate=0.01\n",
        "binary_search_steps=1\n",
        "confidence=0.1\n",
        "initial_const=10\n",
        "cons_increase_factor=10\n",
        "\n",
        "labs = torch.zeros_like(y.squeeze(), dtype=torch.int64)\n",
        "batch_size = imgs.shape[0]\n",
        "targeted = True\n",
        "\n",
        "# Expand removal array to match the batch size\n",
        "expanded_removal_array = removal_array.expand(imgs.shape[0], -1)\n",
        "non_fixed_features_mask = torch.bitwise_or(expanded_removal_array, 1 - imgs.to(torch.uint8))\n",
        "\n",
        "mask_0 = non_fixed_features_mask == 0\n",
        "mask_1 = non_fixed_features_mask == 1\n",
        "\n",
        "imgs_tensor = imgs.clone().detach().to(dtype=torch.float32, device=device)\n",
        "labs_tensor = labs.clone().detach().to(dtype=torch.int64, device=device)\n",
        "\n",
        "lower_bound = torch.zeros(batch_size, dtype=torch.float32, device=device)\n",
        "CONST = torch.full((batch_size,), initial_const, dtype=torch.float32, device=device)\n",
        "upper_bound = torch.full((batch_size,), 1e10, dtype=torch.float32, device=device)\n",
        "\n",
        "o_bestl2 = torch.full((batch_size,), 1e10, device=device)\n",
        "o_bestscore = torch.full((batch_size,), -1, device=device)\n",
        "o_bestattack = imgs.clone().detach()\n",
        "\n",
        "o_best_loss1 = torch.full((batch_size,), float('inf'), device=device)\n",
        "o_best_unsuccessfull_attack = imgs.clone().detach()\n",
        "o_success_attack = torch.zeros(batch_size, dtype=torch.bool, device=device)\n",
        "\n",
        "model.eval()\n",
        "\n",
        "for outer_step in range(binary_search_steps):\n",
        "    print(CONST)\n",
        "\n",
        "    modifier = torch.zeros_like(imgs_tensor, requires_grad=True).to(device)\n",
        "    optimizer = torch.optim.Adam([modifier], lr=learning_rate)\n",
        "\n",
        "    def grad_hook(grad):\n",
        "        return grad * non_fixed_features_mask\n",
        "    modifier.register_hook(grad_hook)\n",
        "\n",
        "    scheduler = CyclicLR(optimizer, base_lr=0.0001, max_lr=0.1, step_size_up=4,\n",
        "                          step_size_down=4995, mode='triangular2')\n",
        "\n",
        "    best_loss = torch.full((batch_size,), float('inf'), device=device)\n",
        "    no_improvement_count = torch.zeros(batch_size, dtype=torch.int, device=device)\n",
        "    active_mask = torch.ones(batch_size, dtype=torch.bool, device=device)\n",
        "\n",
        "    bestl2 = torch.full((batch_size,), 1e10, device=device)\n",
        "    bestscore = torch.full((batch_size,), -1, device=device)\n",
        "\n",
        "    for iteration in range(max_iterations):\n",
        "        if (iteration + 1) % 100 == 0:\n",
        "            current_lr = scheduler.get_last_lr()[0]\n",
        "            print(f\"Iteration {iteration + 1}: Current Learning Rate: {current_lr}\")\n",
        "\n",
        "        active_indices = torch.where(active_mask)[0]\n",
        "        active_modifier = modifier[active_mask]\n",
        "        active_imgs_tensor = imgs_tensor[active_mask]\n",
        "        active_labs_tensor = labs_tensor[active_mask]\n",
        "        active_CONST = CONST[active_mask]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        modified_input = active_imgs_tensor + active_modifier\n",
        "        newimg = sin_func_torch(modified_input)\n",
        "        output = model(newimg)\n",
        "\n",
        "        l2dist = torch.sum((newimg - imgs[active_mask]) ** 2, dim=1)\n",
        "\n",
        "        real = output[torch.arange(active_indices.size(0)), active_labs_tensor]\n",
        "        other = output[torch.arange(active_indices.size(0)), active_labs_tensor + 1]\n",
        "\n",
        "        loss1 = torch.clamp((other - real + confidence), min=0.0)\n",
        "        loss2 = l2dist\n",
        "\n",
        "        if outer_step != (binary_search_steps - 1):\n",
        "            loss = (active_CONST * loss1) + loss2\n",
        "        else:\n",
        "            loss = loss1\n",
        "\n",
        "        total_loss = torch.sum(loss)\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            improved_mask = loss < best_loss[active_mask]\n",
        "            best_loss[active_mask] = torch.where(improved_mask, loss, best_loss[active_mask])\n",
        "            no_improvement_count[active_mask] = torch.where(improved_mask,\n",
        "                torch.zeros_like(no_improvement_count[active_mask]),\n",
        "                no_improvement_count[active_mask] + 1)\n",
        "\n",
        "            improved_mask2 = (loss1 < o_best_loss1[active_mask])\n",
        "            o_best_loss1[active_mask] = torch.where(improved_mask2, loss1, o_best_loss1[active_mask])\n",
        "            o_best_unsuccessfull_attack[active_mask] = torch.where(improved_mask2.unsqueeze(-1), newimg, o_best_unsuccessfull_attack[active_mask])\n",
        "\n",
        "            update_mask = (l2dist < bestl2[active_mask]) & compare(output, active_labs_tensor, confidence, targeted)\n",
        "            bestl2[active_mask] = torch.where(update_mask, l2dist, bestl2[active_mask])\n",
        "            bestscore[active_mask] = torch.where(update_mask, torch.argmax(output, dim=1), bestscore[active_mask])\n",
        "\n",
        "            update_mask2 = (l2dist < o_bestl2[active_mask]) & compare(output, active_labs_tensor, confidence, targeted)\n",
        "            o_bestl2[active_mask] = torch.where(update_mask2, l2dist, o_bestl2[active_mask])\n",
        "            o_bestscore[active_mask] = torch.where(update_mask2, torch.argmax(output, dim=1), o_bestscore[active_mask])\n",
        "            o_bestattack[active_mask] = torch.where(update_mask2.unsqueeze(-1), newimg, o_bestattack[active_mask])\n",
        "\n",
        "            # Skip the rest of the loop for samples that haven't improved in 50 iterations\n",
        "            #active_mask[active_mask.clone()] = no_improvement_count[active_mask.clone()] < 50\n",
        "\n",
        "    if outer_step != (binary_search_steps - 1):\n",
        "        success_mask = (bestscore != -1) & compare(bestscore, labs_tensor, confidence, targeted)\n",
        "        o_success_attack  = torch.logical_or(o_success_attack, success_mask)\n",
        "        print('o_success_attack ', o_success_attack.sum())\n",
        "\n",
        "        failure_mask = ~success_mask\n",
        "        upper_bound[success_mask] = torch.minimum(upper_bound[success_mask], CONST[success_mask])\n",
        "        lower_bound[failure_mask] = torch.maximum(lower_bound[failure_mask], CONST[failure_mask])\n",
        "\n",
        "        upper_limit_mask = upper_bound < 1e9\n",
        "        CONST[upper_limit_mask] = (lower_bound[upper_limit_mask] + upper_bound[upper_limit_mask]) / 2\n",
        "        CONST[failure_mask & ~upper_limit_mask] *= cons_increase_factor\n",
        "\n",
        "success_attack_mask = (o_bestl2 != 1e10)\n",
        "final_output = torch.where(success_attack_mask.unsqueeze(-1), o_bestattack, o_best_unsuccessfull_attack)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uvAXtiowPTTx",
        "outputId": "c67a878b-e0a7-44d8-89c4-41dc43460258"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([10., 10., 10.,  ..., 10., 10., 10.], device='cuda:0')\n",
            "Iteration 100: Current Learning Rate: 0.0981\n",
            "Iteration 200: Current Learning Rate: 0.09610000000000002\n",
            "Iteration 300: Current Learning Rate: 0.09410000000000002\n",
            "Iteration 400: Current Learning Rate: 0.09210000000000002\n",
            "Iteration 500: Current Learning Rate: 0.09010000000000001\n",
            "Iteration 600: Current Learning Rate: 0.08810000000000001\n",
            "Iteration 700: Current Learning Rate: 0.0861\n",
            "Iteration 800: Current Learning Rate: 0.08410000000000002\n",
            "Iteration 900: Current Learning Rate: 0.0821\n",
            "Iteration 1000: Current Learning Rate: 0.0801\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def final_rounding(oimgs, newimg, model):\n",
        "    # Ensure all tensors are on the GPU\n",
        "    oimgs = oimgs.to(device)\n",
        "    perturbed = newimg.to(device)\n",
        "\n",
        "    # Calculate the absolute difference between original and perturbed images\n",
        "    total_change = (perturbed - oimgs).abs()\n",
        "\n",
        "    # Create a mask for elements with significant changes\n",
        "    significant_mask = total_change > 0.1\n",
        "\n",
        "    # Prepare a copy of the original images to apply the final rounding\n",
        "    final = oimgs.clone()\n",
        "\n",
        "    # Calculate the direction of changes: 0 for decrease/no change, 1 for increase\n",
        "    changes = torch.where(perturbed > oimgs, torch.tensor(1.0, device=device), torch.tensor(0.0, device=device))\n",
        "\n",
        "    # Set the model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Iterate over each image in the batch\n",
        "    for b in range(total_change.shape[0]):\n",
        "        # Get indices of significant changes, sorted by magnitude\n",
        "        sorted_idx = torch.argsort(total_change[b][significant_mask[b]], descending=True)\n",
        "\n",
        "        # Map sorted indices back to the original image indices\n",
        "        relevant_indices = torch.nonzero(significant_mask[b], as_tuple=True)[0][sorted_idx]\n",
        "\n",
        "        # Apply changes in order of significance\n",
        "        for idx in relevant_indices:\n",
        "            # Update the final image tensor with the change\n",
        "            final[b, idx] = changes[b, idx]\n",
        "\n",
        "            # Check the model's prediction; stop if the desired outcome is achieved\n",
        "            if model(final[b].unsqueeze(0)).argmax() == 0:\n",
        "                break\n",
        "\n",
        "    return final\n"
      ],
      "metadata": {
        "id": "fnUlRhEr5-oh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def final_rounding2(oimgs, newimg, model):\n",
        "    # Ensure all tensors are on the GPU\n",
        "    oimgs = oimgs.to(device)\n",
        "    perturbed = newimg.to(device)\n",
        "\n",
        "    # Set the model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Initialize active_mask based on model's initial prediction\n",
        "    active_mask = (model(oimgs).argmax(dim=1) == 1)\n",
        "\n",
        "    # Calculate the absolute difference between original and perturbed images\n",
        "    total_change = (perturbed - oimgs).abs()\n",
        "\n",
        "    # Determine which changes are significant\n",
        "    significant_mask = total_change > 0.1\n",
        "\n",
        "    # Prepare a copy of the original images to apply the final rounding\n",
        "    final = oimgs.clone()\n",
        "\n",
        "    # Determine the direction of changes: 0 for decrease/no change, 1 for increase\n",
        "    changes = (perturbed > oimgs).float()\n",
        "\n",
        "    # Get sorted indices of the total changes for each image in descending order\n",
        "    sorted_idx = total_change.argsort(dim=1, descending=True)\n",
        "\n",
        "    # Get the maximum number of significant changes across all images\n",
        "    max_significant_changes = significant_mask.sum(dim=1).max()\n",
        "\n",
        "    # Iterate over each significant change index\n",
        "    for i in range(max_significant_changes):\n",
        "        # Update active_mask indices\n",
        "        active_indices = torch.nonzero(active_mask).squeeze()\n",
        "\n",
        "        if active_indices.numel() == 0:\n",
        "            break\n",
        "\n",
        "        # Safeguard when active_indices is a single-element tensor\n",
        "        if active_indices.dim() == 0:\n",
        "            active_indices = active_indices.unsqueeze(0)\n",
        "\n",
        "        # Get the relevant indices for current change\n",
        "        indices = sorted_idx[active_indices, i]\n",
        "\n",
        "        # Apply the changes to the final image tensor\n",
        "        final[active_indices, indices] = changes[active_indices, indices]\n",
        "\n",
        "        # Update the active_mask based on the model's output\n",
        "        active_mask[active_indices] = (model(final[active_indices]).argmax(dim=1) == 1)\n",
        "\n",
        "    return final\n"
      ],
      "metadata": {
        "id": "PK1ZY5fc7_VJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final = final_rounding(imgs, final_output, model)\n",
        "mask = (model(final).argmax(1) == 0)\n",
        "print(((final[mask] - imgs[mask]).abs()).sum())\n",
        "print((mask))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-UpaMrpB9r17",
        "outputId": "a5b7d56c-0471-4cef-885b-e96831c97e45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(20745., device='cuda:0')\n",
            "tensor([ True,  True,  True,  ...,  True, False,  True], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final = final_rounding2(imgs, final_output, model)\n",
        "mask = (model(final).argmax(1) == 0)\n",
        "print(((final[mask] - imgs[mask]).abs()).sum())\n",
        "print((mask))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YIPTOKWv9svP",
        "outputId": "100ac69c-47e8-4bd0-8dfa-3f1993f58b2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(20745., device='cuda:0')\n",
            "tensor([ True,  True,  True,  ...,  True, False,  True], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final = final_rounding(imgs, final_output, model)\n",
        "mask = (model(final).argmax(1) == 0)\n",
        "print(((final[mask] - imgs[mask]).abs()).sum())\n",
        "print((mask))\n",
        "\n",
        "final = final_rounding2(imgs, final_output, model)\n",
        "mask = (model(final).argmax(1) == 0)\n",
        "print(((final[mask] - imgs[mask]).abs()).sum())\n",
        "print((mask))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gjWptqldjJTa",
        "outputId": "4f2a9c20-271e-4033-f361-3618c04ddba1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(220.)\n",
            "tensor([ True,  True,  True, False,  True,  True, False,  True, False,  True,\n",
            "         True,  True,  True,  True,  True])\n",
            "tensor(220.)\n",
            "tensor([ True,  True,  True, False,  True,  True, False,  True, False,  True,\n",
            "         True,  True,  True,  True,  True])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final = final_rounding(imgs, final_output, model)\n",
        "mask = (model(final).argmax(1) == 1)\n",
        "print(((final[mask] - imgs[mask]).abs()).sum())\n",
        "print((mask))\n",
        "\n",
        "final = final_rounding2(imgs, final_output, model)\n",
        "mask = (model(final).argmax(1) == 1)\n",
        "print(((final[mask] - imgs[mask]).abs()).sum())\n",
        "print((mask))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7JtHGjtS5hO3",
        "outputId": "ac374c83-c27c-4d31-96e9-d6f165d94ab7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(275.)\n",
            "tensor([False, False, False,  True, False, False,  True, False,  True, False,\n",
            "        False, False, False, False, False])\n",
            "tensor(348.)\n",
            "tensor([False, False, False,  True, False, False,  True, False,  True, False,\n",
            "        False, False, False, False, False])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k662CfC5nK0p"
      },
      "source": [
        "# all data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G3kMFtJfm-Vf"
      },
      "outputs": [],
      "source": [
        "# Define the DataLoader for training, validation, and test sets\n",
        "batch_size = 10000\n",
        "#sampler = weight_sampler(y_train)\n",
        "#train_loader = DataLoader(train_dataset, batch_size=batch_size,sampler=sampler)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size,shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def final_rounding(oimgs, newimg, model):\n",
        "    # Ensure all tensors are on the GPU\n",
        "    oimgs = oimgs.to(device)\n",
        "    perturbed = newimg.to(device)\n",
        "\n",
        "    # Calculate the absolute difference between original and perturbed images\n",
        "    total_change = (perturbed - oimgs).abs()\n",
        "\n",
        "    # Create a mask for elements with significant changes\n",
        "    significant_mask = total_change > 0.05\n",
        "\n",
        "    # Prepare a copy of the original images to apply the final rounding\n",
        "    final = oimgs.clone()\n",
        "\n",
        "    # Calculate the direction of changes: 0 for decrease/no change, 1 for increase\n",
        "    changes = torch.where(perturbed > oimgs, torch.tensor(1.0, device=device), torch.tensor(0.0, device=device))\n",
        "\n",
        "    # Set the model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Iterate over each image in the batch\n",
        "    for b in range(total_change.shape[0]):\n",
        "        # Get indices of significant changes, sorted by magnitude\n",
        "        sorted_idx = torch.argsort(total_change[b][significant_mask[b]], descending=True)\n",
        "\n",
        "        # Map sorted indices back to the original image indices\n",
        "        relevant_indices = torch.nonzero(significant_mask[b], as_tuple=True)[0][sorted_idx]\n",
        "\n",
        "        # Apply changes in order of significance\n",
        "        for idx in relevant_indices:\n",
        "            # Update the final image tensor with the change\n",
        "            final[b, idx] = changes[b, idx]\n",
        "\n",
        "            # Check the model's prediction; stop if the desired outcome is achieved\n",
        "            if model(final[b].unsqueeze(0)).argmax() == 0:\n",
        "                break\n",
        "\n",
        "    return final\n"
      ],
      "metadata": {
        "id": "iFOBnISyrlMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "from torch.optim.lr_scheduler import CyclicLR\n",
        "\n",
        "def compare(output, target, confidence, targeted):\n",
        "    \"\"\"\n",
        "    Compares the output with the target for a targeted or untargeted attack.\n",
        "\n",
        "    For targeted attacks, it checks if the output class matches the target class.\n",
        "    For untargeted attacks, it checks if the output class does not match the target class.\n",
        "    \"\"\"\n",
        "    if not isinstance(output, (float, int, torch.LongTensor, torch.cuda.LongTensor)):\n",
        "        output = torch.clone(output).detach()\n",
        "        if targeted:\n",
        "            output[target] -= 0.001  # Slightly reduce the confidence for targeted attacks\n",
        "        else:\n",
        "            output[target] += 0.001  # Slightly increase the confidence for untargeted attacks\n",
        "        output = torch.argmax(output, dim=1)\n",
        "    return output == target if targeted else output != target\n",
        "\n",
        "def sin_func_torch(input_tensor):\n",
        "    \"\"\"\n",
        "    A sine-based transformation applied to the input tensor.\n",
        "\n",
        "    The function applies a sine transformation and rescales the output to [0, 1].\n",
        "    \"\"\"\n",
        "    return (torch.sin((input_tensor - 0.5) * torch.pi) + 1) / 2.0\n",
        "\n",
        "def CW(imgs, y, model, removal_array, max_iterations=1000, learning_rate=0.01,\n",
        "       binary_search_steps=3, confidence=0.01, initial_const=0.1,\n",
        "       cons_increase_factor=2, device='cuda'):\n",
        "    \"\"\"\n",
        "    Implements the Carlini & Wagner (CW) L2 attack.\n",
        "\n",
        "    Parameters:\n",
        "    - imgs: The original images to be attacked.\n",
        "    - y: The true labels of the images.\n",
        "    - model: The target model to attack.\n",
        "    - removal_array: A mask array specifying which features are fixed.\n",
        "    - max_iterations: The maximum number of iterations for the attack.\n",
        "    - learning_rate: The learning rate for the optimizer.\n",
        "    - binary_search_steps: The number of steps for the binary search over the constant.\n",
        "    - confidence: Confidence value for the attack.\n",
        "    - initial_const: The initial constant for the attack loss.\n",
        "    - cons_increase_factor: The factor by which the constant is increased during binary search.\n",
        "    - device: The device on which to perform computations.\n",
        "\n",
        "    Returns:\n",
        "    - final_output: The perturbed images after the attack.\n",
        "    \"\"\"\n",
        "\n",
        "    labs = torch.zeros_like(y.squeeze(), dtype=torch.int64)\n",
        "    batch_size = imgs.shape[0]\n",
        "    targeted = True\n",
        "\n",
        "    # Expand removal array to match the batch size\n",
        "    expanded_removal_array = removal_array.expand(imgs.shape[0], -1)\n",
        "    non_fixed_features_mask = torch.bitwise_or(expanded_removal_array, 1 - imgs.to(torch.uint8))\n",
        "\n",
        "    mask_0 = non_fixed_features_mask == 0\n",
        "    mask_1 = non_fixed_features_mask == 1\n",
        "\n",
        "    imgs_tensor = imgs.clone().detach().to(dtype=torch.float32, device=device)\n",
        "    labs_tensor = labs.clone().detach().to(dtype=torch.int64, device=device)\n",
        "\n",
        "    lower_bound = torch.zeros(batch_size, dtype=torch.float32, device=device)\n",
        "    CONST = torch.full((batch_size,), initial_const, dtype=torch.float32, device=device)\n",
        "    upper_bound = torch.full((batch_size,), 1e10, dtype=torch.float32, device=device)\n",
        "\n",
        "    o_bestl2 = torch.full((batch_size,), 1e10, device=device)\n",
        "    o_bestscore = torch.full((batch_size,), -1, device=device)\n",
        "    o_bestattack = imgs.clone().detach()\n",
        "\n",
        "    o_best_loss1 = torch.full((batch_size,), float('inf'), device=device)\n",
        "    o_best_unsuccessfull_attack = imgs.clone().detach()\n",
        "    o_success_attack = torch.zeros(batch_size, dtype=torch.bool, device=device)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    for outer_step in range(binary_search_steps):\n",
        "        print(CONST)\n",
        "\n",
        "        modifier = torch.zeros_like(imgs_tensor, requires_grad=True).to(device)\n",
        "        optimizer = torch.optim.Adam([modifier], lr=learning_rate)\n",
        "\n",
        "        def grad_hook(grad):\n",
        "            return grad * non_fixed_features_mask\n",
        "        modifier.register_hook(grad_hook)\n",
        "\n",
        "        scheduler = CyclicLR(optimizer, base_lr=0.0001, max_lr=0.1, step_size_up=4,\n",
        "                             step_size_down=4995, mode='triangular2')\n",
        "\n",
        "        best_loss = torch.full((batch_size,), float('inf'), device=device)\n",
        "        no_improvement_count = torch.zeros(batch_size, dtype=torch.int, device=device)\n",
        "        active_mask = torch.ones(batch_size, dtype=torch.bool, device=device)\n",
        "\n",
        "        bestl2 = torch.full((batch_size,), 1e10, device=device)\n",
        "        bestscore = torch.full((batch_size,), -1, device=device)\n",
        "\n",
        "        for iteration in range(max_iterations):\n",
        "            if (iteration + 1) % 2000 == 0:\n",
        "                current_lr = scheduler.get_last_lr()[0]\n",
        "                print(f\"Iteration {iteration + 1}: Current Learning Rate: {current_lr}\")\n",
        "\n",
        "            active_indices = torch.where(active_mask)[0]\n",
        "            active_modifier = modifier[active_mask]\n",
        "            active_imgs_tensor = imgs_tensor[active_mask]\n",
        "            active_labs_tensor = labs_tensor[active_mask]\n",
        "            active_CONST = CONST[active_mask]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            modified_input = active_imgs_tensor + active_modifier\n",
        "            newimg = sin_func_torch(modified_input)\n",
        "            output = model(newimg)\n",
        "\n",
        "            l2dist = torch.sum((newimg - imgs[active_mask]) ** 2, dim=1)\n",
        "\n",
        "            real = output[torch.arange(active_indices.size(0)), active_labs_tensor]\n",
        "            other = output[torch.arange(active_indices.size(0)), active_labs_tensor + 1]\n",
        "\n",
        "            loss1 = torch.clamp((other - real + confidence), min=0.0)\n",
        "            loss2 = l2dist\n",
        "\n",
        "            if outer_step != (binary_search_steps - 1):\n",
        "                loss = (active_CONST * loss1) + loss2\n",
        "            else:\n",
        "                loss = loss1\n",
        "\n",
        "            total_loss = torch.sum(loss)\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                improved_mask = loss < best_loss[active_mask]\n",
        "                best_loss[active_mask] = torch.where(improved_mask, loss, best_loss[active_mask])\n",
        "                no_improvement_count[active_mask] = torch.where(improved_mask,\n",
        "                    torch.zeros_like(no_improvement_count[active_mask]),\n",
        "                    no_improvement_count[active_mask] + 1)\n",
        "\n",
        "                improved_mask2 = (loss1 < o_best_loss1[active_mask])\n",
        "                o_best_loss1[active_mask] = torch.where(improved_mask2, loss1, o_best_loss1[active_mask])\n",
        "                o_best_unsuccessfull_attack[active_mask] = torch.where(improved_mask2.unsqueeze(-1), newimg, o_best_unsuccessfull_attack[active_mask])\n",
        "\n",
        "                update_mask = (l2dist < bestl2[active_mask]) & compare(output, active_labs_tensor, confidence, targeted)\n",
        "                bestl2[active_mask] = torch.where(update_mask, l2dist, bestl2[active_mask])\n",
        "                bestscore[active_mask] = torch.where(update_mask, torch.argmax(output, dim=1), bestscore[active_mask])\n",
        "\n",
        "                update_mask2 = (l2dist < o_bestl2[active_mask]) & compare(output, active_labs_tensor, confidence, targeted)\n",
        "                o_bestl2[active_mask] = torch.where(update_mask2, l2dist, o_bestl2[active_mask])\n",
        "                o_bestscore[active_mask] = torch.where(update_mask2, torch.argmax(output, dim=1), o_bestscore[active_mask])\n",
        "                o_bestattack[active_mask] = torch.where(update_mask2.unsqueeze(-1), newimg, o_bestattack[active_mask])\n",
        "\n",
        "                # Skip the rest of the loop for samples that haven't improved in 50 iterations\n",
        "                #active_mask[active_mask.clone()] = no_improvement_count[active_mask.clone()] < 50\n",
        "\n",
        "        if outer_step != (binary_search_steps - 1):\n",
        "            success_mask = (bestscore != -1) & compare(bestscore, labs_tensor, confidence, targeted)\n",
        "            o_success_attack  = torch.logical_or(o_success_attack, success_mask)\n",
        "            print('o_success_attack ', o_success_attack.sum())\n",
        "\n",
        "            failure_mask = ~success_mask\n",
        "            upper_bound[success_mask] = torch.minimum(upper_bound[success_mask], CONST[success_mask])\n",
        "            lower_bound[failure_mask] = torch.maximum(lower_bound[failure_mask], CONST[failure_mask])\n",
        "\n",
        "            upper_limit_mask = upper_bound < 1e9\n",
        "            CONST[upper_limit_mask] = (lower_bound[upper_limit_mask] + upper_bound[upper_limit_mask]) / 2\n",
        "            CONST[failure_mask & ~upper_limit_mask] *= cons_increase_factor\n",
        "\n",
        "    success_attack_mask = (o_bestl2 != 1e10)\n",
        "    final_output = torch.where(success_attack_mask.unsqueeze(-1), o_bestattack, o_best_unsuccessfull_attack)\n",
        "\n",
        "    return final_output\n"
      ],
      "metadata": {
        "id": "zmYWhxhG3rrO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_dfVmweqm2hE"
      },
      "outputs": [],
      "source": [
        "def adv_predict(test_loader, model, attack, device, **kwargs):\n",
        "\n",
        "    model.eval()\n",
        "    n_samples = 0\n",
        "    cor_test = 0\n",
        "    cor_ad_test = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x_test, y_test in test_loader:\n",
        "            x_test, y_test = x_test.to(torch.float32).to(device), y_test.to(device)\n",
        "            #outputs = model(x_test)\n",
        "            #predicted = outputs.argmax(1).unsqueeze(1)\n",
        "            #acc_test = (predicted == y_test).sum().item() / len(y_test)\n",
        "            #avg_acc_test.append(acc_test)\n",
        "\n",
        "            mal_x_batch, mal_y_batch = x_test[y_test.squeeze() == 1], y_test[y_test.squeeze() == 1]\n",
        "            n_samples += len(mal_y_batch)\n",
        "\n",
        "            outputs = model(mal_x_batch)\n",
        "            predicted = outputs.argmax(1)\n",
        "            cor_test += (predicted == 1).sum().item()\n",
        "\n",
        "            # Generate adversarial examples for test set\n",
        "            with torch.enable_grad():\n",
        "                pertb_mal_x= attack(mal_x_batch, mal_y_batch, model, **kwargs)\n",
        "\n",
        "            outputs = model(pertb_mal_x)\n",
        "            y_pred = outputs.argmax(1)\n",
        "            cor_ad_test += (y_pred == 1).sum().item()\n",
        "\n",
        "            #print(outputs)\n",
        "            print(len(mal_y_batch))\n",
        "            print((y_pred == 0).sum().item())\n",
        "            #print((y_pred == 0))\n",
        "            #print('mean difference ',(torch.abs(pertb_mal_x - mal_x_batch)).sum()/len(mal_y_batch))\n",
        "            #print((torch.abs(pertb_mal_x[y_pred == 0] - mal_x_batch[y_pred == 0])).sum())\n",
        "            #print((torch.abs(pertb_mal_x - mal_x_batch)).sum(1))\n",
        "\n",
        "            #print('***************************')\n",
        "\n",
        "\n",
        "    # Calculate test accuracy\n",
        "    print(f\"Accuracy of just malwares (without attack): {(cor_test / n_samples) * 100:.4}% | Under attack: {(cor_ad_test / n_samples) * 100:.4}%.\")\n",
        "\n",
        "    return pertb_mal_x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**`Attack on combined_model`**"
      ],
      "metadata": {
        "id": "Mk8skHEZ6ZhY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6VjwKh1Jv3Xz",
        "outputId": "21303fc2-7a39-4d12-d569-0c5a3994e363"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.1000, 0.1000, 0.1000,  ..., 0.1000, 0.1000, 0.1000], device='cuda:0')\n",
            "Iteration 2000: Current Learning Rate: 0.060100000000000015\n",
            "Iteration 4000: Current Learning Rate: 0.020099999999999996\n",
            "Iteration 6000: Current Learning Rate: 0.04009000000000001\n",
            "Iteration 8000: Current Learning Rate: 0.020089999999999986\n",
            "Iteration 10000: Current Learning Rate: 0.006343750000005557\n",
            "Iteration 12000: Current Learning Rate: 0.01509\n",
            "Iteration 14000: Current Learning Rate: 0.005090000000000002\n",
            "Iteration 16000: Current Learning Rate: 0.0100925\n",
            "Iteration 18000: Current Learning Rate: 0.005092500000000004\n",
            "Iteration 20000: Current Learning Rate: 0.004782812499997237\n",
            "o_success_attack  tensor(18, device='cuda:0')\n",
            "tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0')\n",
            "Iteration 2000: Current Learning Rate: 0.060100000000000015\n",
            "Iteration 4000: Current Learning Rate: 0.020099999999999996\n",
            "Iteration 6000: Current Learning Rate: 0.04009000000000001\n",
            "Iteration 8000: Current Learning Rate: 0.020089999999999986\n",
            "Iteration 10000: Current Learning Rate: 0.006343750000005557\n",
            "Iteration 12000: Current Learning Rate: 0.01509\n",
            "Iteration 14000: Current Learning Rate: 0.005090000000000002\n",
            "Iteration 16000: Current Learning Rate: 0.0100925\n",
            "Iteration 18000: Current Learning Rate: 0.005092500000000004\n",
            "Iteration 20000: Current Learning Rate: 0.004782812499997237\n",
            "o_success_attack  tensor(50, device='cuda:0')\n",
            "tensor([10., 10., 10.,  ..., 10., 10., 10.], device='cuda:0')\n",
            "Iteration 2000: Current Learning Rate: 0.060100000000000015\n",
            "Iteration 4000: Current Learning Rate: 0.020099999999999996\n",
            "Iteration 6000: Current Learning Rate: 0.04009000000000001\n",
            "Iteration 8000: Current Learning Rate: 0.020089999999999986\n",
            "Iteration 10000: Current Learning Rate: 0.006343750000005557\n",
            "Iteration 12000: Current Learning Rate: 0.01509\n",
            "Iteration 14000: Current Learning Rate: 0.005090000000000002\n",
            "Iteration 16000: Current Learning Rate: 0.0100925\n",
            "Iteration 18000: Current Learning Rate: 0.005092500000000004\n",
            "Iteration 20000: Current Learning Rate: 0.004782812499997237\n",
            "o_success_attack  tensor(165, device='cuda:0')\n",
            "tensor([  5.5000, 100.0000, 100.0000,  ..., 100.0000, 100.0000, 100.0000],\n",
            "       device='cuda:0')\n",
            "Iteration 2000: Current Learning Rate: 0.060100000000000015\n",
            "Iteration 4000: Current Learning Rate: 0.020099999999999996\n",
            "Iteration 6000: Current Learning Rate: 0.04009000000000001\n",
            "Iteration 8000: Current Learning Rate: 0.020089999999999986\n",
            "Iteration 10000: Current Learning Rate: 0.006343750000005557\n",
            "Iteration 12000: Current Learning Rate: 0.01509\n",
            "Iteration 14000: Current Learning Rate: 0.005090000000000002\n",
            "Iteration 16000: Current Learning Rate: 0.0100925\n",
            "Iteration 18000: Current Learning Rate: 0.005092500000000004\n",
            "Iteration 20000: Current Learning Rate: 0.004782812499997237\n",
            "o_success_attack  tensor(235, device='cuda:0')\n",
            "tensor([   3.2500, 1000.0000, 1000.0000,  ..., 1000.0000, 1000.0000,\n",
            "        1000.0000], device='cuda:0')\n",
            "Iteration 2000: Current Learning Rate: 0.060100000000000015\n",
            "Iteration 4000: Current Learning Rate: 0.020099999999999996\n",
            "Iteration 6000: Current Learning Rate: 0.04009000000000001\n",
            "Iteration 8000: Current Learning Rate: 0.020089999999999986\n",
            "Iteration 10000: Current Learning Rate: 0.006343750000005557\n",
            "Iteration 12000: Current Learning Rate: 0.01509\n",
            "Iteration 14000: Current Learning Rate: 0.005090000000000002\n",
            "Iteration 16000: Current Learning Rate: 0.0100925\n",
            "Iteration 18000: Current Learning Rate: 0.005092500000000004\n",
            "Iteration 20000: Current Learning Rate: 0.004782812499997237\n",
            "o_success_attack  tensor(246, device='cuda:0')\n",
            "tensor([4.3750e+00, 1.0000e+04, 1.0000e+04,  ..., 1.0000e+04, 1.0000e+04,\n",
            "        1.0000e+04], device='cuda:0')\n",
            "Iteration 2000: Current Learning Rate: 0.060100000000000015\n",
            "Iteration 4000: Current Learning Rate: 0.020099999999999996\n",
            "Iteration 6000: Current Learning Rate: 0.04009000000000001\n",
            "Iteration 8000: Current Learning Rate: 0.020089999999999986\n",
            "Iteration 10000: Current Learning Rate: 0.006343750000005557\n",
            "Iteration 12000: Current Learning Rate: 0.01509\n",
            "Iteration 14000: Current Learning Rate: 0.005090000000000002\n",
            "Iteration 16000: Current Learning Rate: 0.0100925\n",
            "Iteration 18000: Current Learning Rate: 0.005092500000000004\n",
            "Iteration 20000: Current Learning Rate: 0.004782812499997237\n",
            "o_success_attack  tensor(252, device='cuda:0')\n",
            "tensor([4.9375e+00, 1.0000e+05, 1.0000e+05,  ..., 1.0000e+05, 1.0000e+05,\n",
            "        1.0000e+05], device='cuda:0')\n",
            "Iteration 2000: Current Learning Rate: 0.060100000000000015\n",
            "Iteration 4000: Current Learning Rate: 0.020099999999999996\n",
            "Iteration 6000: Current Learning Rate: 0.04009000000000001\n",
            "Iteration 8000: Current Learning Rate: 0.020089999999999986\n",
            "Iteration 10000: Current Learning Rate: 0.006343750000005557\n",
            "Iteration 12000: Current Learning Rate: 0.01509\n",
            "Iteration 14000: Current Learning Rate: 0.005090000000000002\n",
            "Iteration 16000: Current Learning Rate: 0.0100925\n",
            "Iteration 18000: Current Learning Rate: 0.005092500000000004\n",
            "Iteration 20000: Current Learning Rate: 0.004782812499997237\n",
            "o_success_attack  tensor(253, device='cuda:0')\n",
            "tensor([5.2188e+00, 1.0000e+06, 1.0000e+06,  ..., 1.0000e+06, 1.0000e+06,\n",
            "        1.0000e+06], device='cuda:0')\n",
            "Iteration 2000: Current Learning Rate: 0.060100000000000015\n",
            "Iteration 4000: Current Learning Rate: 0.020099999999999996\n",
            "Iteration 6000: Current Learning Rate: 0.04009000000000001\n",
            "Iteration 8000: Current Learning Rate: 0.020089999999999986\n",
            "Iteration 10000: Current Learning Rate: 0.006343750000005557\n",
            "Iteration 12000: Current Learning Rate: 0.01509\n",
            "Iteration 14000: Current Learning Rate: 0.005090000000000002\n",
            "Iteration 16000: Current Learning Rate: 0.0100925\n",
            "Iteration 18000: Current Learning Rate: 0.005092500000000004\n",
            "Iteration 20000: Current Learning Rate: 0.004782812499997237\n",
            "o_success_attack  tensor(255, device='cuda:0')\n",
            "tensor([5.3594e+00, 1.0000e+07, 1.0000e+07,  ..., 1.0000e+07, 1.0000e+07,\n",
            "        1.0000e+07], device='cuda:0')\n",
            "Iteration 2000: Current Learning Rate: 0.060100000000000015\n",
            "Iteration 4000: Current Learning Rate: 0.020099999999999996\n",
            "Iteration 6000: Current Learning Rate: 0.04009000000000001\n",
            "Iteration 8000: Current Learning Rate: 0.020089999999999986\n",
            "Iteration 10000: Current Learning Rate: 0.006343750000005557\n",
            "Iteration 12000: Current Learning Rate: 0.01509\n",
            "Iteration 14000: Current Learning Rate: 0.005090000000000002\n",
            "Iteration 16000: Current Learning Rate: 0.0100925\n",
            "Iteration 18000: Current Learning Rate: 0.005092500000000004\n",
            "Iteration 20000: Current Learning Rate: 0.004782812499997237\n",
            "o_success_attack  tensor(257, device='cuda:0')\n",
            "tensor([5.2891e+00, 1.0000e+08, 1.0000e+08,  ..., 1.0000e+08, 1.0000e+08,\n",
            "        1.0000e+08], device='cuda:0')\n",
            "Iteration 2000: Current Learning Rate: 0.060100000000000015\n",
            "Iteration 4000: Current Learning Rate: 0.020099999999999996\n",
            "Iteration 6000: Current Learning Rate: 0.04009000000000001\n",
            "Iteration 8000: Current Learning Rate: 0.020089999999999986\n",
            "Iteration 10000: Current Learning Rate: 0.006343750000005557\n",
            "Iteration 12000: Current Learning Rate: 0.01509\n",
            "Iteration 14000: Current Learning Rate: 0.005090000000000002\n",
            "Iteration 16000: Current Learning Rate: 0.0100925\n",
            "Iteration 18000: Current Learning Rate: 0.005092500000000004\n",
            "Iteration 20000: Current Learning Rate: 0.004782812499997237\n",
            "o_success_attack  tensor(257, device='cuda:0')\n",
            "tensor([5.2539e+00, 1.0000e+09, 1.0000e+09,  ..., 1.0000e+09, 1.0000e+09,\n",
            "        1.0000e+09], device='cuda:0')\n",
            "Iteration 2000: Current Learning Rate: 0.060100000000000015\n",
            "Iteration 4000: Current Learning Rate: 0.020099999999999996\n",
            "Iteration 6000: Current Learning Rate: 0.04009000000000001\n",
            "Iteration 8000: Current Learning Rate: 0.020089999999999986\n",
            "Iteration 10000: Current Learning Rate: 0.006343750000005557\n",
            "Iteration 12000: Current Learning Rate: 0.01509\n",
            "Iteration 14000: Current Learning Rate: 0.005090000000000002\n",
            "Iteration 16000: Current Learning Rate: 0.0100925\n",
            "Iteration 18000: Current Learning Rate: 0.005092500000000004\n",
            "Iteration 20000: Current Learning Rate: 0.004782812499997237\n",
            "1130\n",
            "263\n",
            "Accuracy of just malwares (without attack): 98.85% | Under attack: 76.73%.\n"
          ]
        }
      ],
      "source": [
        "#CW(mal_x_batch, mal_y_batch, model_AT_rFGSM, removal_array, max_iterations=1000, learning_rate=0.01, binary_search_steps=3, confidence=0.01, initial_const=0.1, device=device)\n",
        "attack_params =  {'device':device, 'confidence':0.1, 'removal_array':removal_array,'max_iterations': 20000, 'learning_rate': 0.3, 'initial_const': 0.1 , 'binary_search_steps' : 11, 'cons_increase_factor' : 10}\n",
        "adv = adv_predict(test_loader, combined_model, CW, **attack_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Sp92NtGv3X0"
      },
      "outputs": [],
      "source": [
        "for x_test, y_test in test_loader:\n",
        "    x_test, y_test = x_test.to(torch.float32).to(device), y_test.to(device)\n",
        "    mal_x_batch, mal_y_batch = x_test[y_test.squeeze() == 1], y_test[y_test.squeeze() == 1]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation on non-rounded adversarial examples\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "combined_model.eval()\n",
        "\n",
        "# Get the model's predictions on adversarial examples\n",
        "outputs = combined_model(adv)\n",
        "y_pred = outputs.argmax(dim=1)\n",
        "\n",
        "# Calculate metrics\n",
        "num_malwares = len(mal_y_batch)\n",
        "num_successful_attacks = (y_pred == 0).sum().item()\n",
        "total_difference = (torch.abs(adv[y_pred == 0] - mal_x_batch[y_pred == 0])).sum().item()\n",
        "accuracy = (y_pred == 1).sum().item() / num_malwares * 100\n",
        "\n",
        "# Print results with clear formatting\n",
        "print(f\"Evaluation Results on NOT_Rounded Adversarial Examples\")\n",
        "print(f\"-----------------------------------------------\")\n",
        "print(f\"Number of malware samples: {num_malwares}\")\n",
        "print(f\"Number of successful attacks: {num_successful_attacks}\")\n",
        "print(f\"Total difference (L1 norm) between successful adversarial examples and original: {total_difference}\")\n",
        "print(f\"Accuracy of the model on malwares under attack: {accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "psvwocSXnwpJ",
        "outputId": "16dac4d8-7091-451d-e01f-1d354eb9627d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Results on NOT_Rounded Adversarial Examples\n",
            "-----------------------------------------------\n",
            "Number of malware samples: 1130\n",
            "Number of successful attacks: 263\n",
            "Total difference (L1 norm) between successful adversarial examples and original: 5421.03466796875\n",
            "Accuracy of the model on malwares under attack: 76.73%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation on rounded adversarial examples(naive technique)\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "combined_model.eval()\n",
        "\n",
        "# Apply the final rounding solution to adversarial examples\n",
        "adv_rounded = round_x(adv, round_threshold=0.5)\n",
        "\n",
        "# Get the model's predictions on the rounded adversarial examples\n",
        "outputs = combined_model(adv_rounded)\n",
        "y_pred = outputs.argmax(dim=1)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "num_malwares = len(mal_y_batch)\n",
        "num_successful_attacks = (y_pred == 0).sum().item()\n",
        "total_difference = (torch.abs(adv_rounded[y_pred == 0] - mal_x_batch[y_pred == 0])).sum().item()\n",
        "accuracy = (y_pred == 1).sum().item() / num_malwares * 100\n",
        "\n",
        "# Display the results with clear formatting\n",
        "print(f\"Evaluation Results on Rounded Adversarial Examples\")\n",
        "print(f\"-----------------------------------------------\")\n",
        "print(f\"Number of malware samples: {num_malwares}\")\n",
        "print(f\"Number of successful attacks: {num_successful_attacks}\")\n",
        "print(f\"Total L1 norm difference (successful adversarial examples vs. original): {total_difference}\")\n",
        "print(f\"Accuracy of the model on malware under attack: {accuracy:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YpxMQ21P2i-H",
        "outputId": "e45a14d5-d812-4e3a-fb5a-fa65b8b8b998"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Results on Rounded Adversarial Examples\n",
            "-----------------------------------------------\n",
            "Number of malware samples: 1130\n",
            "Number of successful attacks: 122\n",
            "Total L1 norm difference (successful adversarial examples vs. original): 2911.0\n",
            "Accuracy of the model on malware under attack: 89.20%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation on rounded adversarial examples\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "combined_model.eval()\n",
        "\n",
        "# Apply the final rounding solution to adversarial examples\n",
        "adv_rounded = final_rounding(mal_x_batch, adv, combined_model)\n",
        "\n",
        "# Get the model's predictions on the rounded adversarial examples\n",
        "outputs = combined_model(adv_rounded)\n",
        "y_pred = outputs.argmax(dim=1)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "num_malwares = len(mal_y_batch)\n",
        "num_successful_attacks = (y_pred == 0).sum().item()\n",
        "total_difference = (torch.abs(adv_rounded[y_pred == 0] - mal_x_batch[y_pred == 0])).sum().item()\n",
        "accuracy = (y_pred == 1).sum().item() / num_malwares * 100\n",
        "\n",
        "# Display the results with clear formatting\n",
        "print(f\"Evaluation Results on Rounded Adversarial Examples\")\n",
        "print(f\"-----------------------------------------------\")\n",
        "print(f\"Number of malware samples: {num_malwares}\")\n",
        "print(f\"Number of successful attacks: {num_successful_attacks}\")\n",
        "print(f\"Total L1 norm difference (successful adversarial examples vs. original): {total_difference}\")\n",
        "print(f\"Accuracy of the model on malware under attack: {accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UkXLe9waz-Sg",
        "outputId": "1dd07edb-aa24-4e1b-d099-97e7d19c29a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Results on Rounded Adversarial Examples\n",
            "-----------------------------------------------\n",
            "Number of malware samples: 1130\n",
            "Number of successful attacks: 249\n",
            "Total L1 norm difference (successful adversarial examples vs. original): 4636.0\n",
            "Accuracy of the model on malware under attack: 77.96%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure that fixed elements are not changed\n",
        "\n",
        "# Ensure `removal_array` is expanded to match the batch size\n",
        "expanded_removal_array = removal_array.expand(mal_x_batch.shape[0], -1)\n",
        "\n",
        "# Create a mask where non-fixed features are set to 1\n",
        "non_fixed_features_mask = torch.bitwise_or(expanded_removal_array, 1 - mal_x_batch.to(torch.uint8))\n",
        "\n",
        "# Calculate indices where the mask is 0 (fixed) or 1 (non-fixed)\n",
        "mask_fixed = non_fixed_features_mask == 0\n",
        "mask_non_fixed = non_fixed_features_mask == 1\n",
        "\n",
        "# Calculate and print the differences\n",
        "diff_adv_fixed = (torch.abs(adv - mal_x_batch) * mask_fixed).sum().item()\n",
        "diff_adv_non_fixed = (torch.abs(adv - mal_x_batch) * mask_non_fixed).sum().item()\n",
        "diff_adv_rounded_fixed = (torch.abs(adv_rounded - mal_x_batch) * mask_fixed).sum().item()\n",
        "diff_adv_rounded_non_fixed = (torch.abs(adv_rounded - mal_x_batch) * mask_non_fixed).sum().item()\n",
        "\n",
        "# Display results with clear formatting\n",
        "print(f\"Differences when mask is 0 (fixed features):\")\n",
        "print(f\"  Adv vs. Original: {diff_adv_fixed}\")\n",
        "print(f\"  Rounded Adv vs. Original: {diff_adv_rounded_fixed}\")\n",
        "print(f\"Differences when mask is 1 (non-fixed features):\")\n",
        "print(f\"  Adv vs. Original: {diff_adv_non_fixed}\")\n",
        "print(f\"  Rounded Adv vs. Original: {diff_adv_rounded_non_fixed}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1zLywnBWpLqp",
        "outputId": "124d79b1-68dd-41da-a1cd-91a191d153cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Differences when mask is 0 (fixed features):\n",
            "  Adv vs. Original: 0.0\n",
            "  Rounded Adv vs. Original: 0.0\n",
            "Differences when mask is 1 (non-fixed features):\n",
            "  Adv vs. Original: 60753.6640625\n",
            "  Rounded Adv vs. Original: 63486.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SEKh_HQn3NBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lvp7SSSD5zSV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**`Attack on model_AT_rFGSM`**"
      ],
      "metadata": {
        "id": "kf-ndaSn6Ebc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fb7a673-bde0-439d-dc82-d44536ab6d53",
        "id": "4wRLzc7_5uka"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100], device='cuda:0')\n",
            "Iteration 2000: Current Learning Rate: 0.060100000000000015\n",
            "Iteration 4000: Current Learning Rate: 0.020099999999999996\n",
            "Iteration 6000: Current Learning Rate: 0.04009000000000001\n",
            "Iteration 8000: Current Learning Rate: 0.020089999999999986\n",
            "Iteration 10000: Current Learning Rate: 0.006343750000005557\n",
            "Iteration 12000: Current Learning Rate: 0.01509\n",
            "Iteration 14000: Current Learning Rate: 0.005090000000000002\n",
            "Iteration 16000: Current Learning Rate: 0.0100925\n",
            "Iteration 18000: Current Learning Rate: 0.005092500000000004\n",
            "Iteration 20000: Current Learning Rate: 0.004782812499997237\n",
            "o_success_attack  tensor(64, device='cuda:0')\n",
            "tensor([0.1000, 0.1000, 0.1000,  ..., 0.1000, 0.1000, 0.1000], device='cuda:0')\n",
            "Iteration 2000: Current Learning Rate: 0.060100000000000015\n",
            "Iteration 4000: Current Learning Rate: 0.020099999999999996\n",
            "Iteration 6000: Current Learning Rate: 0.04009000000000001\n",
            "Iteration 8000: Current Learning Rate: 0.020089999999999986\n",
            "Iteration 10000: Current Learning Rate: 0.006343750000005557\n",
            "Iteration 12000: Current Learning Rate: 0.01509\n",
            "Iteration 14000: Current Learning Rate: 0.005090000000000002\n",
            "Iteration 16000: Current Learning Rate: 0.0100925\n",
            "Iteration 18000: Current Learning Rate: 0.005092500000000004\n",
            "Iteration 20000: Current Learning Rate: 0.004782812499997237\n",
            "o_success_attack  tensor(279, device='cuda:0')\n",
            "tensor([0.0550, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000], device='cuda:0')\n",
            "Iteration 2000: Current Learning Rate: 0.060100000000000015\n",
            "Iteration 4000: Current Learning Rate: 0.020099999999999996\n",
            "Iteration 6000: Current Learning Rate: 0.04009000000000001\n",
            "Iteration 8000: Current Learning Rate: 0.020089999999999986\n",
            "Iteration 10000: Current Learning Rate: 0.006343750000005557\n",
            "Iteration 12000: Current Learning Rate: 0.01509\n",
            "Iteration 14000: Current Learning Rate: 0.005090000000000002\n",
            "Iteration 16000: Current Learning Rate: 0.0100925\n",
            "Iteration 18000: Current Learning Rate: 0.005092500000000004\n",
            "Iteration 20000: Current Learning Rate: 0.004782812499997237\n",
            "o_success_attack  tensor(822, device='cuda:0')\n",
            "tensor([ 0.0325,  0.5500,  0.5500,  ...,  0.5500, 10.0000, 10.0000],\n",
            "       device='cuda:0')\n",
            "Iteration 2000: Current Learning Rate: 0.060100000000000015\n",
            "Iteration 4000: Current Learning Rate: 0.020099999999999996\n",
            "Iteration 6000: Current Learning Rate: 0.04009000000000001\n",
            "Iteration 8000: Current Learning Rate: 0.020089999999999986\n",
            "Iteration 10000: Current Learning Rate: 0.006343750000005557\n",
            "Iteration 12000: Current Learning Rate: 0.01509\n",
            "Iteration 14000: Current Learning Rate: 0.005090000000000002\n",
            "Iteration 16000: Current Learning Rate: 0.0100925\n",
            "Iteration 18000: Current Learning Rate: 0.005092500000000004\n",
            "Iteration 20000: Current Learning Rate: 0.004782812499997237\n",
            "o_success_attack  tensor(945, device='cuda:0')\n",
            "tensor([0.0437, 0.3250, 0.3250,  ..., 0.3250, 5.5000, 5.5000], device='cuda:0')\n",
            "Iteration 2000: Current Learning Rate: 0.060100000000000015\n",
            "Iteration 4000: Current Learning Rate: 0.020099999999999996\n",
            "Iteration 6000: Current Learning Rate: 0.04009000000000001\n",
            "Iteration 8000: Current Learning Rate: 0.020089999999999986\n",
            "Iteration 10000: Current Learning Rate: 0.006343750000005557\n",
            "Iteration 12000: Current Learning Rate: 0.01509\n",
            "Iteration 14000: Current Learning Rate: 0.005090000000000002\n",
            "Iteration 16000: Current Learning Rate: 0.0100925\n",
            "Iteration 18000: Current Learning Rate: 0.005092500000000004\n",
            "Iteration 20000: Current Learning Rate: 0.004782812499997237\n",
            "o_success_attack  tensor(975, device='cuda:0')\n",
            "tensor([0.0494, 0.2125, 0.2125,  ..., 0.2125, 7.7500, 3.2500], device='cuda:0')\n",
            "Iteration 2000: Current Learning Rate: 0.060100000000000015\n",
            "Iteration 4000: Current Learning Rate: 0.020099999999999996\n",
            "Iteration 6000: Current Learning Rate: 0.04009000000000001\n",
            "Iteration 8000: Current Learning Rate: 0.020089999999999986\n",
            "Iteration 10000: Current Learning Rate: 0.006343750000005557\n",
            "Iteration 12000: Current Learning Rate: 0.01509\n",
            "Iteration 14000: Current Learning Rate: 0.005090000000000002\n",
            "Iteration 16000: Current Learning Rate: 0.0100925\n",
            "Iteration 18000: Current Learning Rate: 0.005092500000000004\n",
            "Iteration 20000: Current Learning Rate: 0.004782812499997237\n",
            "o_success_attack  tensor(984, device='cuda:0')\n",
            "tensor([0.0466, 0.1562, 0.2687,  ..., 0.2687, 6.6250, 2.1250], device='cuda:0')\n",
            "Iteration 2000: Current Learning Rate: 0.060100000000000015\n",
            "Iteration 4000: Current Learning Rate: 0.020099999999999996\n",
            "Iteration 6000: Current Learning Rate: 0.04009000000000001\n",
            "Iteration 8000: Current Learning Rate: 0.020089999999999986\n",
            "Iteration 10000: Current Learning Rate: 0.006343750000005557\n",
            "Iteration 12000: Current Learning Rate: 0.01509\n",
            "Iteration 14000: Current Learning Rate: 0.005090000000000002\n",
            "Iteration 16000: Current Learning Rate: 0.0100925\n",
            "Iteration 18000: Current Learning Rate: 0.005092500000000004\n",
            "Iteration 20000: Current Learning Rate: 0.004782812499997237\n",
            "o_success_attack  tensor(987, device='cuda:0')\n",
            "tensor([0.0480, 0.1281, 0.2406,  ..., 0.2406, 7.1875, 1.5625], device='cuda:0')\n",
            "Iteration 2000: Current Learning Rate: 0.060100000000000015\n",
            "Iteration 4000: Current Learning Rate: 0.020099999999999996\n",
            "Iteration 6000: Current Learning Rate: 0.04009000000000001\n",
            "Iteration 8000: Current Learning Rate: 0.020089999999999986\n",
            "Iteration 10000: Current Learning Rate: 0.006343750000005557\n",
            "Iteration 12000: Current Learning Rate: 0.01509\n",
            "Iteration 14000: Current Learning Rate: 0.005090000000000002\n",
            "Iteration 16000: Current Learning Rate: 0.0100925\n",
            "Iteration 18000: Current Learning Rate: 0.005092500000000004\n",
            "Iteration 20000: Current Learning Rate: 0.004782812499997237\n",
            "o_success_attack  tensor(988, device='cuda:0')\n",
            "tensor([0.0473, 0.1422, 0.2266,  ..., 0.2266, 6.9062, 1.2812], device='cuda:0')\n",
            "Iteration 2000: Current Learning Rate: 0.060100000000000015\n",
            "Iteration 4000: Current Learning Rate: 0.020099999999999996\n",
            "Iteration 6000: Current Learning Rate: 0.04009000000000001\n",
            "Iteration 8000: Current Learning Rate: 0.020089999999999986\n",
            "Iteration 10000: Current Learning Rate: 0.006343750000005557\n",
            "Iteration 12000: Current Learning Rate: 0.01509\n",
            "Iteration 14000: Current Learning Rate: 0.005090000000000002\n",
            "Iteration 16000: Current Learning Rate: 0.0100925\n",
            "Iteration 18000: Current Learning Rate: 0.005092500000000004\n",
            "Iteration 20000: Current Learning Rate: 0.004782812499997237\n",
            "o_success_attack  tensor(990, device='cuda:0')\n",
            "tensor([0.0476, 0.1492, 0.2195,  ..., 0.2336, 7.0469, 1.1406], device='cuda:0')\n",
            "Iteration 2000: Current Learning Rate: 0.060100000000000015\n",
            "Iteration 4000: Current Learning Rate: 0.020099999999999996\n",
            "Iteration 6000: Current Learning Rate: 0.04009000000000001\n",
            "Iteration 8000: Current Learning Rate: 0.020089999999999986\n",
            "Iteration 10000: Current Learning Rate: 0.006343750000005557\n",
            "Iteration 12000: Current Learning Rate: 0.01509\n",
            "Iteration 14000: Current Learning Rate: 0.005090000000000002\n",
            "Iteration 16000: Current Learning Rate: 0.0100925\n",
            "Iteration 18000: Current Learning Rate: 0.005092500000000004\n",
            "Iteration 20000: Current Learning Rate: 0.004782812499997237\n",
            "o_success_attack  tensor(991, device='cuda:0')\n",
            "tensor([0.0478, 0.1527, 0.2160,  ..., 0.2301, 7.1172, 1.2109], device='cuda:0')\n",
            "Iteration 2000: Current Learning Rate: 0.060100000000000015\n",
            "Iteration 4000: Current Learning Rate: 0.020099999999999996\n",
            "Iteration 6000: Current Learning Rate: 0.04009000000000001\n",
            "Iteration 8000: Current Learning Rate: 0.020089999999999986\n",
            "Iteration 10000: Current Learning Rate: 0.006343750000005557\n",
            "Iteration 12000: Current Learning Rate: 0.01509\n",
            "Iteration 14000: Current Learning Rate: 0.005090000000000002\n",
            "Iteration 16000: Current Learning Rate: 0.0100925\n",
            "Iteration 18000: Current Learning Rate: 0.005092500000000004\n",
            "Iteration 20000: Current Learning Rate: 0.004782812499997237\n",
            "1130\n",
            "993\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 12.12%.\n"
          ]
        }
      ],
      "source": [
        "#CW(mal_x_batch, mal_y_batch, model_AT_rFGSM, removal_array, max_iterations=1000, learning_rate=0.01, binary_search_steps=3, confidence=0.01, initial_const=0.1, device=device)\n",
        "attack_params =  {'device':device, 'confidence':0.1, 'removal_array':removal_array,'max_iterations': 20000, 'learning_rate': 0.3, 'initial_const': 0.01 , 'binary_search_steps' : 11, 'cons_increase_factor' : 10}\n",
        "adv = adv_predict(test_loader, model_AT_rFGSM, CW, **attack_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aGnQAyXP5ukb"
      },
      "outputs": [],
      "source": [
        "for x_test, y_test in test_loader:\n",
        "    x_test, y_test = x_test.to(torch.float32).to(device), y_test.to(device)\n",
        "    mal_x_batch, mal_y_batch = x_test[y_test.squeeze() == 1], y_test[y_test.squeeze() == 1]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation on non-rounded adversarial examples\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model_AT_rFGSM.eval()\n",
        "\n",
        "# Get the model's predictions on adversarial examples\n",
        "outputs = model_AT_rFGSM(adv)\n",
        "y_pred = outputs.argmax(dim=1)\n",
        "\n",
        "# Calculate metrics\n",
        "num_malwares = len(mal_y_batch)\n",
        "num_successful_attacks = (y_pred == 0).sum().item()\n",
        "total_difference = (torch.abs(adv[y_pred == 0] - mal_x_batch[y_pred == 0])).sum().item()\n",
        "accuracy = (y_pred == 1).sum().item() / num_malwares * 100\n",
        "\n",
        "# Print results with clear formatting\n",
        "print(f\"Evaluation Results on NOT_Rounded Adversarial Examples\")\n",
        "print(f\"-----------------------------------------------\")\n",
        "print(f\"Number of malware samples: {num_malwares}\")\n",
        "print(f\"Number of successful attacks: {num_successful_attacks}\")\n",
        "print(f\"Total difference (L1 norm) between successful adversarial examples and original: {total_difference}\")\n",
        "print(f\"Accuracy of the model on malwares under attack: {accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89bbd210-bc3a-4f72-cd38-eee024227816",
        "id": "72MWQw_d5ukc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Results on NOT_Rounded Adversarial Examples\n",
            "-----------------------------------------------\n",
            "Number of malware samples: 1130\n",
            "Number of successful attacks: 993\n",
            "Total difference (L1 norm) between successful adversarial examples and original: 15776.7734375\n",
            "Accuracy of the model on malwares under attack: 12.12%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation on rounded adversarial examples(naive technique)\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model_AT_rFGSM.eval()\n",
        "\n",
        "# Apply the final rounding solution to adversarial examples\n",
        "adv_rounded = round_x(adv, round_threshold=0.5)\n",
        "\n",
        "# Get the model's predictions on the rounded adversarial examples\n",
        "outputs = model_AT_rFGSM(adv_rounded)\n",
        "y_pred = outputs.argmax(dim=1)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "num_malwares = len(mal_y_batch)\n",
        "num_successful_attacks = (y_pred == 0).sum().item()\n",
        "total_difference = (torch.abs(adv_rounded[y_pred == 0] - mal_x_batch[y_pred == 0])).sum().item()\n",
        "accuracy = (y_pred == 1).sum().item() / num_malwares * 100\n",
        "\n",
        "# Display the results with clear formatting\n",
        "print(f\"Evaluation Results on Rounded Adversarial Examples\")\n",
        "print(f\"-----------------------------------------------\")\n",
        "print(f\"Number of malware samples: {num_malwares}\")\n",
        "print(f\"Number of successful attacks: {num_successful_attacks}\")\n",
        "print(f\"Total L1 norm difference (successful adversarial examples vs. original): {total_difference}\")\n",
        "print(f\"Accuracy of the model on malware under attack: {accuracy:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3157857-b6e0-48a4-8734-cdc400d3c623",
        "id": "-0Sx6JbM5ukc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Results on Rounded Adversarial Examples\n",
            "-----------------------------------------------\n",
            "Number of malware samples: 1130\n",
            "Number of successful attacks: 349\n",
            "Total L1 norm difference (successful adversarial examples vs. original): 2023.0\n",
            "Accuracy of the model on malware under attack: 69.12%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation on rounded adversarial examples\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model_AT_rFGSM.eval()\n",
        "\n",
        "# Apply the final rounding solution to adversarial examples\n",
        "adv_rounded = final_rounding(mal_x_batch, adv, model_AT_rFGSM)\n",
        "\n",
        "# Get the model's predictions on the rounded adversarial examples\n",
        "outputs = model_AT_rFGSM(adv_rounded)\n",
        "y_pred = outputs.argmax(dim=1)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "num_malwares = len(mal_y_batch)\n",
        "num_successful_attacks = (y_pred == 0).sum().item()\n",
        "total_difference = (torch.abs(adv_rounded[y_pred == 0] - mal_x_batch[y_pred == 0])).sum().item()\n",
        "accuracy = (y_pred == 1).sum().item() / num_malwares * 100\n",
        "\n",
        "# Display the results with clear formatting\n",
        "print(f\"Evaluation Results on Rounded Adversarial Examples\")\n",
        "print(f\"-----------------------------------------------\")\n",
        "print(f\"Number of malware samples: {num_malwares}\")\n",
        "print(f\"Number of successful attacks: {num_successful_attacks}\")\n",
        "print(f\"Total L1 norm difference (successful adversarial examples vs. original): {total_difference}\")\n",
        "print(f\"Accuracy of the model on malware under attack: {accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8dfa4933-bc38-4e8a-f4b2-6183b9118049",
        "id": "GFDbUBVF5ukd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Results on Rounded Adversarial Examples\n",
            "-----------------------------------------------\n",
            "Number of malware samples: 1130\n",
            "Number of successful attacks: 922\n",
            "Total L1 norm difference (successful adversarial examples vs. original): 10703.0\n",
            "Accuracy of the model on malware under attack: 18.41%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure that fixed elements are not changed\n",
        "\n",
        "# Ensure `removal_array` is expanded to match the batch size\n",
        "expanded_removal_array = removal_array.expand(mal_x_batch.shape[0], -1)\n",
        "\n",
        "# Create a mask where non-fixed features are set to 1\n",
        "non_fixed_features_mask = torch.bitwise_or(expanded_removal_array, 1 - mal_x_batch.to(torch.uint8))\n",
        "\n",
        "# Calculate indices where the mask is 0 (fixed) or 1 (non-fixed)\n",
        "mask_fixed = non_fixed_features_mask == 0\n",
        "mask_non_fixed = non_fixed_features_mask == 1\n",
        "\n",
        "# Calculate and print the differences\n",
        "diff_adv_fixed = (torch.abs(adv - mal_x_batch) * mask_fixed).sum().item()\n",
        "diff_adv_non_fixed = (torch.abs(adv - mal_x_batch) * mask_non_fixed).sum().item()\n",
        "diff_adv_rounded_fixed = (torch.abs(adv_rounded - mal_x_batch) * mask_fixed).sum().item()\n",
        "diff_adv_rounded_non_fixed = (torch.abs(adv_rounded - mal_x_batch) * mask_non_fixed).sum().item()\n",
        "\n",
        "# Display results with clear formatting\n",
        "print(f\"Differences when mask is 0 (fixed features):\")\n",
        "print(f\"  Adv vs. Original: {diff_adv_fixed}\")\n",
        "print(f\"  Rounded Adv vs. Original: {diff_adv_rounded_fixed}\")\n",
        "print(f\"Differences when mask is 1 (non-fixed features):\")\n",
        "print(f\"  Adv vs. Original: {diff_adv_non_fixed}\")\n",
        "print(f\"  Rounded Adv vs. Original: {diff_adv_rounded_non_fixed}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "124d79b1-68dd-41da-a1cd-91a191d153cf",
        "id": "8PJOqvEq5ukd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Differences when mask is 0 (fixed features):\n",
            "  Adv vs. Original: 0.0\n",
            "  Rounded Adv vs. Original: 0.0\n",
            "Differences when mask is 1 (non-fixed features):\n",
            "  Adv vs. Original: 60753.6640625\n",
            "  Rounded Adv vs. Original: 63486.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sLeJtGNmO0jT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#loss = (real - other)\n",
        "attack_params =  {'insertion_array':insertion_array, 'removal_array':removal_array,'k': 10000, 'step_length': 1., 'norm': 'l2', 'is_report_loss_diff' : False}\n",
        "adv = adv_predict(test_loader, model_AT_rFGSM, pgd_min22, device, **attack_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "slIh7zpRO00S",
        "outputId": "7819cbf6-d79d-43fd-bc17-c383065c6e25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1130\n",
            "924\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 18.23%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation on non-rounded adversarial examples\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model_AT_rFGSM.eval()\n",
        "\n",
        "# Get the model's predictions on adversarial examples\n",
        "outputs = model_AT_rFGSM(adv)\n",
        "y_pred = outputs.argmax(dim=1)\n",
        "\n",
        "# Calculate metrics\n",
        "num_malwares = len(mal_y_batch)\n",
        "num_successful_attacks = (y_pred == 0).sum().item()\n",
        "total_difference = (torch.abs(adv[y_pred == 0] - mal_x_batch[y_pred == 0])).sum().item()\n",
        "accuracy = (y_pred == 1).sum().item() / num_malwares * 100\n",
        "\n",
        "# Print results with clear formatting\n",
        "print(f\"Evaluation Results on NOT_Rounded Adversarial Examples\")\n",
        "print(f\"-----------------------------------------------\")\n",
        "print(f\"Number of malware samples: {num_malwares}\")\n",
        "print(f\"Number of successful attacks: {num_successful_attacks}\")\n",
        "print(f\"Total difference (L1 norm) between successful adversarial examples and original: {total_difference}\")\n",
        "print(f\"Accuracy of the model on malwares under attack: {accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6a64248-2e84-47eb-bb94-2c7912fec2de",
        "id": "BIPYoFMcPOeu"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Results on NOT_Rounded Adversarial Examples\n",
            "-----------------------------------------------\n",
            "Number of malware samples: 1130\n",
            "Number of successful attacks: 924\n",
            "Total difference (L1 norm) between successful adversarial examples and original: 43665.1484375\n",
            "Accuracy of the model on malwares under attack: 18.23%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation on rounded adversarial examples\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model_AT_rFGSM.eval()\n",
        "\n",
        "# Apply the final rounding solution to adversarial examples\n",
        "adv_rounded2 = final_rounding(mal_x_batch, adv, model_AT_rFGSM)\n",
        "\n",
        "# Get the model's predictions on the rounded adversarial examples\n",
        "outputs = model_AT_rFGSM(adv_rounded2)\n",
        "y_pred = outputs.argmax(dim=1)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "num_malwares = len(mal_y_batch)\n",
        "num_successful_attacks = (y_pred == 0).sum().item()\n",
        "total_difference = (torch.abs(adv_rounded2[y_pred == 0] - mal_x_batch[y_pred == 0])).sum().item()\n",
        "accuracy = (y_pred == 1).sum().item() / num_malwares * 100\n",
        "\n",
        "# Display the results with clear formatting\n",
        "print(f\"Evaluation Results on Rounded Adversarial Examples\")\n",
        "print(f\"-----------------------------------------------\")\n",
        "print(f\"Number of malware samples: {num_malwares}\")\n",
        "print(f\"Number of successful attacks: {num_successful_attacks}\")\n",
        "print(f\"Total L1 norm difference (successful adversarial examples vs. original): {total_difference}\")\n",
        "print(f\"Accuracy of the model on malware under attack: {accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYJdIvVjQFBp",
        "outputId": "c0088036-dcf8-4583-eea8-9d7b73830b7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Results on Rounded Adversarial Examples\n",
            "-----------------------------------------------\n",
            "Number of malware samples: 1130\n",
            "Number of successful attacks: 963\n",
            "Total L1 norm difference (successful adversarial examples vs. original): 25367.0\n",
            "Accuracy of the model on malware under attack: 14.78%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the model's predictions on the rounded adversarial examples\n",
        "outputs = model_AT_rFGSM(adv_rounded)\n",
        "y_pred = outputs.argmax(dim=1)"
      ],
      "metadata": {
        "id": "mfM2EdYYQJ4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(y_pred == 0).sum().item()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OU-kWclMQ3Mc",
        "outputId": "3b49a833-7626-4ae8-df0a-8fab95f37cf9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "922"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the model's predictions on the rounded adversarial examples\n",
        "outputs2 = model_AT_rFGSM(adv_rounded2)\n",
        "y_pred2 = outputs2.argmax(dim=1)"
      ],
      "metadata": {
        "id": "S0hP4dTJQfqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(y_pred2 == 0).sum().item()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mKb8rBdbQ6qw",
        "outputId": "7be9761b-0930-4cad-bf00-0114b67c4714"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "963"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.logical_or((y_pred == 0), (y_pred2 == 0)).sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7VxQh0wIQl0W",
        "outputId": "06eb0b61-2c35-41c8-d907-80e34bf60f51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(984, device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ragn4x9YSJxm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vP34KTGWqEyQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "AJmrOho3Bc6A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# loss function (a quarter of circle):\n",
        "\n",
        "\n",
        "```\n",
        "(1 - (perturbation_tensor - 1)**2)**0.5\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "BzDw6mGbO0Kr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "from torch.optim.lr_scheduler import CyclicLR\n",
        "\n",
        "def compare(output, target, confidence, targeted):\n",
        "    \"\"\"\n",
        "    Compares the output with the target for a targeted or untargeted attack.\n",
        "\n",
        "    For targeted attacks, it checks if the output class matches the target class.\n",
        "    For untargeted attacks, it checks if the output class does not match the target class.\n",
        "    \"\"\"\n",
        "    if not isinstance(output, (float, int, torch.LongTensor, torch.cuda.LongTensor)):\n",
        "        output = torch.clone(output).detach()\n",
        "        if targeted:\n",
        "            output[target] -= confidence  # Slightly reduce the confidence for targeted attacks\n",
        "        else:\n",
        "            output[target] += confidence  # Slightly increase the confidence for untargeted attacks\n",
        "        output = torch.argmax(output, dim=1)\n",
        "    return output == target if targeted else output != target\n",
        "\n",
        "def sin_func_torch(input_tensor):\n",
        "    \"\"\"\n",
        "    A sine-based transformation applied to the input tensor.\n",
        "\n",
        "    The function applies a sine transformation and rescales the output to [0, 1].\n",
        "    \"\"\"\n",
        "    return (torch.sin((input_tensor - 0.5) * torch.pi) + 1) / 2.0\n",
        "\n",
        "def CW(imgs, y, model, removal_array, max_iterations=1000, learning_rate=0.01,\n",
        "       binary_search_steps=3, confidence=0.01, initial_const=0.1,\n",
        "       cons_increase_factor=2, device='cuda'):\n",
        "    \"\"\"\n",
        "    Implements the Carlini & Wagner (CW) L2 attack.\n",
        "\n",
        "    Parameters:\n",
        "    - imgs: The original images to be attacked.\n",
        "    - y: The true labels of the images.\n",
        "    - model: The target model to attack.\n",
        "    - removal_array: A mask array specifying which features are fixed.\n",
        "    - max_iterations: The maximum number of iterations for the attack.\n",
        "    - learning_rate: The learning rate for the optimizer.\n",
        "    - binary_search_steps: The number of steps for the binary search over the constant.\n",
        "    - confidence: Confidence value for the attack.\n",
        "    - initial_const: The initial constant for the attack loss.\n",
        "    - cons_increase_factor: The factor by which the constant is increased during binary search.\n",
        "    - device: The device on which to perform computations.\n",
        "\n",
        "    Returns:\n",
        "    - final_output: The perturbed images after the attack.\n",
        "    \"\"\"\n",
        "\n",
        "    labs = torch.zeros_like(y.squeeze(), dtype=torch.int64)\n",
        "    batch_size = imgs.shape[0]\n",
        "    targeted = True\n",
        "\n",
        "    # Expand removal array to match the batch size\n",
        "    expanded_removal_array = removal_array.expand(imgs.shape[0], -1)\n",
        "    non_fixed_features_mask = torch.bitwise_or(expanded_removal_array, 1 - imgs.to(torch.uint8))\n",
        "\n",
        "    mask_0 = non_fixed_features_mask == 0\n",
        "    mask_1 = non_fixed_features_mask == 1\n",
        "\n",
        "    imgs_tensor = imgs.clone().detach().to(dtype=torch.float32, device=device)\n",
        "    labs_tensor = labs.clone().detach().to(dtype=torch.int64, device=device)\n",
        "\n",
        "    lower_bound = torch.zeros(batch_size, dtype=torch.float32, device=device)\n",
        "    CONST = torch.full((batch_size,), initial_const, dtype=torch.float32, device=device)\n",
        "    upper_bound = torch.full((batch_size,), 1e10, dtype=torch.float32, device=device)\n",
        "\n",
        "    o_bestl2 = torch.full((batch_size,), 1e10, device=device)\n",
        "    o_bestscore = torch.full((batch_size,), -1, device=device)\n",
        "    o_bestattack = imgs.clone().detach()\n",
        "\n",
        "    o_best_loss1 = torch.full((batch_size,), float('inf'), device=device)\n",
        "    o_best_unsuccessfull_attack = imgs.clone().detach()\n",
        "    o_success_attack = torch.zeros(batch_size, dtype=torch.bool, device=device)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    for outer_step in range(binary_search_steps):\n",
        "        print(CONST)\n",
        "\n",
        "        modifier = torch.zeros_like(imgs_tensor, requires_grad=True).to(device)\n",
        "        optimizer = torch.optim.Adam([modifier], lr=learning_rate)\n",
        "\n",
        "        def grad_hook(grad):\n",
        "            return grad * non_fixed_features_mask\n",
        "        modifier.register_hook(grad_hook)\n",
        "\n",
        "        scheduler = CyclicLR(optimizer, base_lr=0.0001, max_lr=0.1, step_size_up=4,\n",
        "                             step_size_down=4995, mode='triangular2')\n",
        "\n",
        "        best_loss = torch.full((batch_size,), float('inf'), device=device)\n",
        "        no_improvement_count = torch.zeros(batch_size, dtype=torch.int, device=device)\n",
        "        active_mask = torch.ones(batch_size, dtype=torch.bool, device=device)\n",
        "\n",
        "        bestl2 = torch.full((batch_size,), 1e10, device=device)\n",
        "        bestscore = torch.full((batch_size,), -1, device=device)\n",
        "        success_mask = torch.zeros(batch_size, dtype=torch.bool, device=device)\n",
        "\n",
        "        iteration = 0\n",
        "        while iteration < max_iterations:\n",
        "\n",
        "            if active_mask.sum() == 0:\n",
        "                print(f\"All samples are inactive at iteration {iteration}.\")\n",
        "                best_loss = torch.full((batch_size,), float('inf'), device=device)\n",
        "                no_improvement_count = torch.zeros(batch_size, dtype=torch.int, device=device)\n",
        "                active_mask = torch.ones(batch_size, dtype=torch.bool, device=device)\n",
        "                if iteration < 5000:\n",
        "                    steps_to_skip = 5000 - iteration\n",
        "                    iteration = 5000  # Jump to iteration 5000\n",
        "                elif 5000 < iteration < 10000:\n",
        "                    steps_to_skip = 10000 - iteration\n",
        "                    iteration = 10000  # Jump to iteration 10000\n",
        "                elif 10000 < iteration < 15000:\n",
        "                    steps_to_skip = 15000 - iteration\n",
        "                    iteration = 15000  # Jump to iteration 15000\n",
        "                elif iteration > 15000:\n",
        "                    break  # Exit the loop\n",
        "\n",
        "                # Step the scheduler to the correct point in the cycle\n",
        "                for _ in range(steps_to_skip):\n",
        "                    scheduler.step()\n",
        "\n",
        "            else:\n",
        "                active_indices = torch.where(active_mask)[0]\n",
        "                active_modifier = modifier[active_mask]\n",
        "                active_imgs_tensor = imgs_tensor[active_mask]\n",
        "                active_labs_tensor = labs_tensor[active_mask]\n",
        "                active_CONST = CONST[active_mask]\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                modified_input = active_imgs_tensor + active_modifier\n",
        "                newimg = sin_func_torch(modified_input)\n",
        "                output = model(newimg)\n",
        "\n",
        "                perturbation = torch.where((active_imgs_tensor==1.), (active_imgs_tensor - newimg), (newimg - active_imgs_tensor))\n",
        "                eps = 1e-2\n",
        "                l2dist = torch.sum((1 - (perturbation - 1)**2 + eps)**0.5 - eps**0.5 , dim=1)\n",
        "                #l2dist = torch.sum((newimg - imgs[active_mask]) ** 2, dim=1)\n",
        "\n",
        "                real = output[torch.arange(active_indices.size(0)), active_labs_tensor]\n",
        "                other = output[torch.arange(active_indices.size(0)), active_labs_tensor + 1]\n",
        "\n",
        "                loss1 = torch.clamp((other - real + confidence), min=0.0)\n",
        "\n",
        "                if outer_step != (binary_search_steps - 1):\n",
        "                    loss = (active_CONST * loss1) + l2dist\n",
        "                else:\n",
        "                    loss = loss1\n",
        "\n",
        "                total_loss = torch.sum(loss)\n",
        "                total_loss.backward()\n",
        "                optimizer.step()\n",
        "                scheduler.step()\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    improved_mask = loss < best_loss[active_mask]\n",
        "                    best_loss[active_mask] = torch.where(improved_mask, loss, best_loss[active_mask])\n",
        "                    no_improvement_count[active_mask] = torch.where(improved_mask,\n",
        "                        torch.zeros_like(no_improvement_count[active_mask]),\n",
        "                        no_improvement_count[active_mask] + 1)\n",
        "\n",
        "                    improved_mask2 = (loss1 < o_best_loss1[active_mask])\n",
        "                    o_best_loss1[active_mask] = torch.where(improved_mask2, loss1, o_best_loss1[active_mask])\n",
        "                    o_best_unsuccessfull_attack[active_mask] = torch.where(improved_mask2.unsqueeze(-1), newimg, o_best_unsuccessfull_attack[active_mask])\n",
        "\n",
        "                    success_mask_inloop = compare(output, active_labs_tensor, confidence, targeted)\n",
        "\n",
        "                    update_mask = (l2dist < bestl2[active_mask]) & success_mask_inloop\n",
        "                    bestl2[active_mask] = torch.where(update_mask, l2dist, bestl2[active_mask])\n",
        "                    bestscore[active_mask] = torch.where(update_mask, torch.argmax(output, dim=1), bestscore[active_mask])\n",
        "\n",
        "                    update_mask2 = (l2dist < o_bestl2[active_mask]) & success_mask_inloop\n",
        "                    o_bestl2[active_mask] = torch.where(update_mask2, l2dist, o_bestl2[active_mask])\n",
        "                    o_bestscore[active_mask] = torch.where(update_mask2, torch.argmax(output, dim=1), o_bestscore[active_mask])\n",
        "                    o_bestattack[active_mask] = torch.where(update_mask2.unsqueeze(-1), newimg, o_bestattack[active_mask])\n",
        "\n",
        "                    if (iteration + 1) % 500 == 0:\n",
        "                        success_mask[active_mask]  = torch.logical_or(success_mask_inloop, success_mask[active_mask])\n",
        "                        current_lr = scheduler.get_last_lr()[0]\n",
        "                        print(f\"Iteration {iteration + 1}: Current success : {success_mask_inloop.sum()}, All success : {success_mask.sum()}, mean(l2) : {o_bestl2[success_mask].mean():.6f}, Current Learning Rate: {current_lr:.2f}\")\n",
        "\n",
        "                        #rint(f\"Iteration {iteration + 1}: Current success : {success_mask_inloop.sum()}, All success : {success_mask.sum()}, Current Learning Rate: {current_lr}\")\n",
        "\n",
        "\n",
        "                    # Skip the rest of the loop for samples that haven't improved in 50 iterations\n",
        "                    active_mask[active_mask.clone()] = no_improvement_count[active_mask.clone()] < 100\n",
        "\n",
        "                    iteration += 1  # Increment the iteration manually\n",
        "\n",
        "        if outer_step != (binary_search_steps - 1):\n",
        "            o_success_mask = (bestscore != -1) & compare(bestscore, labs_tensor, confidence, targeted)\n",
        "            o_success_attack  = torch.logical_or(o_success_attack, o_success_mask)\n",
        "            print('o_success_attack ', o_success_attack.sum())\n",
        "            print('-------------------------------------------------')\n",
        "\n",
        "            failure_mask = ~success_mask\n",
        "            upper_bound[success_mask] = torch.minimum(upper_bound[success_mask], CONST[success_mask])\n",
        "            lower_bound[failure_mask] = torch.maximum(lower_bound[failure_mask], CONST[failure_mask])\n",
        "\n",
        "            upper_limit_mask = upper_bound < 1e9\n",
        "            CONST[upper_limit_mask] = (lower_bound[upper_limit_mask] + upper_bound[upper_limit_mask]) / 2\n",
        "            CONST[failure_mask & ~upper_limit_mask] *= cons_increase_factor\n",
        "\n",
        "    success_attack_mask = (o_bestl2 != 1e10)\n",
        "    final_output = torch.where(success_attack_mask.unsqueeze(-1), o_bestattack, o_best_unsuccessfull_attack)\n",
        "\n",
        "    return final_output"
      ],
      "metadata": {
        "id": "l2-tq_ptBdmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UytCx4ETg7fw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "from torch.optim.lr_scheduler import CyclicLR\n",
        "\n",
        "def compare(output, target, confidence, targeted):\n",
        "    \"\"\"\n",
        "    Compares the output with the target for a targeted or untargeted attack.\n",
        "\n",
        "    For targeted attacks, it checks if the output class matches the target class.\n",
        "    For untargeted attacks, it checks if the output class does not match the target class.\n",
        "    \"\"\"\n",
        "    if not isinstance(output, (float, int, torch.LongTensor, torch.cuda.LongTensor)):\n",
        "        output = torch.clone(output).detach()\n",
        "        if targeted:\n",
        "            output[target] -= confidence  # Slightly reduce the confidence for targeted attacks\n",
        "        else:\n",
        "            output[target] += confidence  # Slightly increase the confidence for untargeted attacks\n",
        "        output = torch.argmax(output, dim=1)\n",
        "    return output == target if targeted else output != target\n",
        "\n",
        "def sin_func_torch(input_tensor):\n",
        "    \"\"\"\n",
        "    A sine-based transformation applied to the input tensor.\n",
        "\n",
        "    The function applies a sine transformation and rescales the output to [0, 1].\n",
        "    \"\"\"\n",
        "    return (torch.sin((input_tensor - 0.5) * torch.pi) + 1) / 2.0\n",
        "\n",
        "def CW(imgs, y, model, removal_array, max_iterations=1000, learning_rate=0.01,\n",
        "       binary_search_steps=3, confidence=0.01, initial_const=0.1,\n",
        "       cons_increase_factor=2, device='cuda'):\n",
        "    \"\"\"\n",
        "    Implements the Carlini & Wagner (CW) L2 attack.\n",
        "\n",
        "    Parameters:\n",
        "    - imgs: The original images to be attacked.\n",
        "    - y: The true labels of the images.\n",
        "    - model: The target model to attack.\n",
        "    - removal_array: A mask array specifying which features are fixed.\n",
        "    - max_iterations: The maximum number of iterations for the attack.\n",
        "    - learning_rate: The learning rate for the optimizer.\n",
        "    - binary_search_steps: The number of steps for the binary search over the constant.\n",
        "    - confidence: Confidence value for the attack.\n",
        "    - initial_const: The initial constant for the attack loss.\n",
        "    - cons_increase_factor: The factor by which the constant is increased during binary search.\n",
        "    - device: The device on which to perform computations.\n",
        "\n",
        "    Returns:\n",
        "    - final_output: The perturbed images after the attack.\n",
        "    \"\"\"\n",
        "\n",
        "    labs = torch.zeros_like(y.squeeze(), dtype=torch.int64)\n",
        "    batch_size = imgs.shape[0]\n",
        "    targeted = True\n",
        "\n",
        "    # Expand removal array to match the batch size\n",
        "    expanded_removal_array = removal_array.expand(imgs.shape[0], -1)\n",
        "    non_fixed_features_mask = torch.bitwise_or(expanded_removal_array, 1 - imgs.to(torch.uint8))\n",
        "\n",
        "    mask_0 = non_fixed_features_mask == 0\n",
        "    mask_1 = non_fixed_features_mask == 1\n",
        "\n",
        "    imgs_tensor = imgs.clone().detach().to(dtype=torch.float32, device=device)\n",
        "    labs_tensor = labs.clone().detach().to(dtype=torch.int64, device=device)\n",
        "\n",
        "    lower_bound = torch.zeros(batch_size, dtype=torch.float32, device=device)\n",
        "    CONST = torch.full((batch_size,), initial_const, dtype=torch.float32, device=device)\n",
        "    upper_bound = torch.full((batch_size,), 1e10, dtype=torch.float32, device=device)\n",
        "\n",
        "    o_bestl2 = torch.full((batch_size,), 1e10, device=device)\n",
        "    o_bestscore = torch.full((batch_size,), -1, device=device)\n",
        "    o_bestattack = imgs.clone().detach()\n",
        "\n",
        "    o_best_loss1 = torch.full((batch_size,), float('inf'), device=device)\n",
        "    o_best_unsuccessfull_attack = imgs.clone().detach()\n",
        "    o_success_attack = torch.zeros(batch_size, dtype=torch.bool, device=device)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    for outer_step in range(binary_search_steps):\n",
        "        print(CONST)\n",
        "\n",
        "        modifier = torch.zeros_like(imgs_tensor, requires_grad=True).to(device)\n",
        "        optimizer = torch.optim.Adam([modifier], lr=learning_rate)\n",
        "\n",
        "        def grad_hook(grad):\n",
        "            return grad * non_fixed_features_mask\n",
        "        modifier.register_hook(grad_hook)\n",
        "\n",
        "        scheduler = CyclicLR(optimizer, base_lr=0.0001, max_lr=0.1, step_size_up=5,\n",
        "                             step_size_down=4995, mode='triangular2')\n",
        "\n",
        "        best_loss = torch.full((batch_size,), float('inf'), device=device)\n",
        "        no_improvement_count = torch.zeros(batch_size, dtype=torch.int, device=device)\n",
        "        active_mask = torch.ones(batch_size, dtype=torch.bool, device=device)\n",
        "\n",
        "        bestl2 = torch.full((batch_size,), 1e10, device=device)\n",
        "        bestscore = torch.full((batch_size,), -1, device=device)\n",
        "        success_mask = torch.zeros(batch_size, dtype=torch.bool, device=device)\n",
        "\n",
        "        for iteration in range(max_iterations):\n",
        "\n",
        "            active_indices = torch.where(active_mask)[0]\n",
        "            active_modifier = modifier[active_mask]\n",
        "            active_imgs_tensor = imgs_tensor[active_mask]\n",
        "            active_labs_tensor = labs_tensor[active_mask]\n",
        "            active_CONST = CONST[active_mask]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            modified_input = active_imgs_tensor + active_modifier\n",
        "            newimg = sin_func_torch(modified_input)\n",
        "            output = model(newimg)\n",
        "\n",
        "            perturbation = torch.where((active_imgs_tensor==1.), (active_imgs_tensor - newimg), (newimg - active_imgs_tensor))\n",
        "            eps = 1e-2\n",
        "            l2dist = torch.sum((1 - (perturbation - 1)**2 + eps)**0.5 - eps**0.5 , dim=1)\n",
        "            #l2dist = torch.sum((newimg - imgs[active_mask]) ** 2, dim=1)\n",
        "\n",
        "            real = output[torch.arange(active_indices.size(0)), active_labs_tensor]\n",
        "            other = output[torch.arange(active_indices.size(0)), active_labs_tensor + 1]\n",
        "\n",
        "            loss1 = torch.clamp((other - real + confidence), min=0.0)\n",
        "\n",
        "            if outer_step != (binary_search_steps - 1):\n",
        "                loss = (active_CONST * loss1) + l2dist\n",
        "            else:\n",
        "                loss = loss1\n",
        "\n",
        "            total_loss = torch.sum(loss)\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                improved_mask = loss < best_loss[active_mask]\n",
        "                best_loss[active_mask] = torch.where(improved_mask, loss, best_loss[active_mask])\n",
        "                no_improvement_count[active_mask] = torch.where(improved_mask,\n",
        "                    torch.zeros_like(no_improvement_count[active_mask]),\n",
        "                    no_improvement_count[active_mask] + 1)\n",
        "\n",
        "                improved_mask2 = (loss1 < o_best_loss1[active_mask])\n",
        "                o_best_loss1[active_mask] = torch.where(improved_mask2, loss1, o_best_loss1[active_mask])\n",
        "                o_best_unsuccessfull_attack[active_mask] = torch.where(improved_mask2.unsqueeze(-1), newimg, o_best_unsuccessfull_attack[active_mask])\n",
        "\n",
        "                success_mask_inloop = compare(output, active_labs_tensor, confidence, targeted)\n",
        "\n",
        "                update_mask = (l2dist < bestl2[active_mask]) & success_mask_inloop\n",
        "                bestl2[active_mask] = torch.where(update_mask, l2dist, bestl2[active_mask])\n",
        "                bestscore[active_mask] = torch.where(update_mask, torch.argmax(output, dim=1), bestscore[active_mask])\n",
        "\n",
        "                update_mask2 = (l2dist < o_bestl2[active_mask]) & success_mask_inloop\n",
        "                o_bestl2[active_mask] = torch.where(update_mask2, l2dist, o_bestl2[active_mask])\n",
        "                o_bestscore[active_mask] = torch.where(update_mask2, torch.argmax(output, dim=1), o_bestscore[active_mask])\n",
        "                o_bestattack[active_mask] = torch.where(update_mask2.unsqueeze(-1), newimg, o_bestattack[active_mask])\n",
        "\n",
        "                if (iteration + 1) % 500 == 0:\n",
        "                    success_mask  = torch.logical_or(success_mask_inloop, success_mask)\n",
        "                    current_lr = scheduler.get_last_lr()[0]\n",
        "                    print(f\"Iteration {iteration + 1}: Current success : {success_mask_inloop.sum()} \\t All success : {success_mask.sum()} \\t mean(l2) : {o_bestl2[success_mask].mean():.6f} \\t Current Learning Rate: {current_lr:.3f}\")\n",
        "                    #print(f\"Iteration {iteration + 1}: Current success : {success_mask_inloop.sum()}, All success : {success_mask.sum()}, Current Learning Rate: {current_lr}\")\n",
        "\n",
        "\n",
        "                # Skip the rest of the loop for samples that haven't improved in 50 iterations\n",
        "                #active_mask[active_mask.clone()] = no_improvement_count[active_mask.clone()] < 50\n",
        "\n",
        "        if outer_step != (binary_search_steps - 1):\n",
        "            o_success_mask = (bestscore != -1) & compare(bestscore, labs_tensor, confidence, targeted)\n",
        "            o_success_attack  = torch.logical_or(o_success_attack, o_success_mask)\n",
        "            print('o_success_attack ', o_success_attack.sum())\n",
        "            print('-------------------------------------------------')\n",
        "\n",
        "            failure_mask = ~success_mask\n",
        "            upper_bound[success_mask] = torch.minimum(upper_bound[success_mask], CONST[success_mask])\n",
        "            lower_bound[failure_mask] = torch.maximum(lower_bound[failure_mask], CONST[failure_mask])\n",
        "\n",
        "            upper_limit_mask = upper_bound < 1e9\n",
        "            CONST[upper_limit_mask] = (lower_bound[upper_limit_mask] + upper_bound[upper_limit_mask]) / 2\n",
        "            CONST[failure_mask & ~upper_limit_mask] *= cons_increase_factor\n",
        "\n",
        "    success_attack_mask = (o_bestl2 != 1e10)\n",
        "    final_output = torch.where(success_attack_mask.unsqueeze(-1), o_bestattack, o_best_unsuccessfull_attack)\n",
        "\n",
        "    return final_output"
      ],
      "metadata": {
        "id": "u19eyqj3g7t9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93a3f9a4-5890-403f-934f-bb7252141c4a",
        "id": "JxxENvECBdmM"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0')\n",
            "Iteration 500: Current success : 18 \t All success : 18 \t mean(l2) : 0.764894 \t Current Learning Rate: 0.09\n",
            "Iteration 1000: Current success : 19 \t All success : 19 \t mean(l2) : 0.590904 \t Current Learning Rate: 0.08\n",
            "Iteration 1500: Current success : 20 \t All success : 20 \t mean(l2) : 0.649185 \t Current Learning Rate: 0.07\n",
            "Iteration 2000: Current success : 21 \t All success : 21 \t mean(l2) : 0.747859 \t Current Learning Rate: 0.06\n",
            "Iteration 2500: Current success : 21 \t All success : 21 \t mean(l2) : 0.725767 \t Current Learning Rate: 0.05\n",
            "Iteration 3000: Current success : 21 \t All success : 21 \t mean(l2) : 0.719885 \t Current Learning Rate: 0.04\n",
            "Iteration 3500: Current success : 21 \t All success : 21 \t mean(l2) : 0.719885 \t Current Learning Rate: 0.03\n",
            "Iteration 4000: Current success : 21 \t All success : 21 \t mean(l2) : 0.719885 \t Current Learning Rate: 0.02\n",
            "Iteration 4500: Current success : 21 \t All success : 21 \t mean(l2) : 0.719885 \t Current Learning Rate: 0.01\n",
            "Iteration 5000: Current success : 21 \t All success : 21 \t mean(l2) : 0.719885 \t Current Learning Rate: 0.01\n",
            "Iteration 5500: Current success : 31 \t All success : 33 \t mean(l2) : 1.783481 \t Current Learning Rate: 0.05\n",
            "Iteration 6000: Current success : 31 \t All success : 33 \t mean(l2) : 1.754527 \t Current Learning Rate: 0.04\n",
            "Iteration 6500: Current success : 31 \t All success : 33 \t mean(l2) : 1.754527 \t Current Learning Rate: 0.04\n",
            "Iteration 7000: Current success : 31 \t All success : 33 \t mean(l2) : 1.754222 \t Current Learning Rate: 0.03\n",
            "Iteration 7500: Current success : 31 \t All success : 33 \t mean(l2) : 1.753673 \t Current Learning Rate: 0.03\n",
            "Iteration 8000: Current success : 31 \t All success : 33 \t mean(l2) : 1.753602 \t Current Learning Rate: 0.02\n",
            "Iteration 8500: Current success : 31 \t All success : 33 \t mean(l2) : 1.753602 \t Current Learning Rate: 0.02\n",
            "Iteration 9000: Current success : 31 \t All success : 33 \t mean(l2) : 1.753530 \t Current Learning Rate: 0.01\n",
            "Iteration 9500: Current success : 31 \t All success : 33 \t mean(l2) : 1.753098 \t Current Learning Rate: 0.01\n",
            "Iteration 10000: Current success : 31 \t All success : 33 \t mean(l2) : 1.753097 \t Current Learning Rate: 0.01\n",
            "Iteration 10500: Current success : 31 \t All success : 33 \t mean(l2) : 1.726314 \t Current Learning Rate: 0.02\n",
            "Iteration 11000: Current success : 31 \t All success : 33 \t mean(l2) : 1.726213 \t Current Learning Rate: 0.02\n",
            "Iteration 11500: Current success : 31 \t All success : 33 \t mean(l2) : 1.724969 \t Current Learning Rate: 0.02\n",
            "Iteration 12000: Current success : 31 \t All success : 33 \t mean(l2) : 1.724969 \t Current Learning Rate: 0.02\n",
            "Iteration 12500: Current success : 31 \t All success : 33 \t mean(l2) : 1.724968 \t Current Learning Rate: 0.01\n",
            "Iteration 13000: Current success : 31 \t All success : 33 \t mean(l2) : 1.724968 \t Current Learning Rate: 0.01\n",
            "Iteration 13500: Current success : 31 \t All success : 33 \t mean(l2) : 1.723266 \t Current Learning Rate: 0.01\n",
            "Iteration 14000: Current success : 31 \t All success : 33 \t mean(l2) : 1.723266 \t Current Learning Rate: 0.01\n",
            "Iteration 14500: Current success : 31 \t All success : 33 \t mean(l2) : 1.723266 \t Current Learning Rate: 0.00\n",
            "Iteration 15000: Current success : 31 \t All success : 33 \t mean(l2) : 1.723266 \t Current Learning Rate: 0.01\n",
            "Iteration 15500: Current success : 35 \t All success : 35 \t mean(l2) : 1.778988 \t Current Learning Rate: 0.01\n",
            "Iteration 16000: Current success : 35 \t All success : 35 \t mean(l2) : 1.778988 \t Current Learning Rate: 0.01\n",
            "Iteration 16500: Current success : 35 \t All success : 35 \t mean(l2) : 1.778988 \t Current Learning Rate: 0.01\n",
            "Iteration 17000: Current success : 35 \t All success : 35 \t mean(l2) : 1.778988 \t Current Learning Rate: 0.01\n",
            "Iteration 17500: Current success : 35 \t All success : 35 \t mean(l2) : 1.778988 \t Current Learning Rate: 0.01\n",
            "Iteration 18000: Current success : 35 \t All success : 35 \t mean(l2) : 1.778988 \t Current Learning Rate: 0.01\n",
            "Iteration 18500: Current success : 35 \t All success : 35 \t mean(l2) : 1.778988 \t Current Learning Rate: 0.00\n",
            "Iteration 19000: Current success : 35 \t All success : 35 \t mean(l2) : 1.778988 \t Current Learning Rate: 0.00\n",
            "Iteration 19500: Current success : 35 \t All success : 35 \t mean(l2) : 1.778988 \t Current Learning Rate: 0.00\n",
            "Iteration 20000: Current success : 35 \t All success : 35 \t mean(l2) : 1.778988 \t Current Learning Rate: 0.01\n",
            "o_success_attack  tensor(38, device='cuda:0')\n",
            "-------------------------------------------------\n",
            "tensor([10., 10., 10.,  ..., 10., 10., 10.], device='cuda:0')\n",
            "Iteration 500: Current success : 42 \t All success : 42 \t mean(l2) : 5.735398 \t Current Learning Rate: 0.09\n",
            "Iteration 1000: Current success : 45 \t All success : 48 \t mean(l2) : 6.083510 \t Current Learning Rate: 0.08\n",
            "Iteration 1500: Current success : 49 \t All success : 51 \t mean(l2) : 5.886533 \t Current Learning Rate: 0.07\n",
            "Iteration 2000: Current success : 50 \t All success : 52 \t mean(l2) : 5.915701 \t Current Learning Rate: 0.06\n",
            "Iteration 2500: Current success : 50 \t All success : 52 \t mean(l2) : 5.892773 \t Current Learning Rate: 0.05\n",
            "Iteration 3000: Current success : 50 \t All success : 52 \t mean(l2) : 5.873840 \t Current Learning Rate: 0.04\n",
            "Iteration 3500: Current success : 52 \t All success : 54 \t mean(l2) : 6.093308 \t Current Learning Rate: 0.03\n",
            "Iteration 4000: Current success : 52 \t All success : 54 \t mean(l2) : 6.092801 \t Current Learning Rate: 0.02\n",
            "Iteration 4500: Current success : 52 \t All success : 54 \t mean(l2) : 6.092801 \t Current Learning Rate: 0.01\n",
            "Iteration 5000: Current success : 52 \t All success : 54 \t mean(l2) : 6.076042 \t Current Learning Rate: 0.01\n",
            "Iteration 5500: Current success : 47 \t All success : 62 \t mean(l2) : 5.986812 \t Current Learning Rate: 0.05\n",
            "Iteration 6000: Current success : 45 \t All success : 62 \t mean(l2) : 5.959766 \t Current Learning Rate: 0.04\n",
            "Iteration 6500: Current success : 46 \t All success : 62 \t mean(l2) : 5.939398 \t Current Learning Rate: 0.04\n",
            "Iteration 7000: Current success : 47 \t All success : 62 \t mean(l2) : 5.895590 \t Current Learning Rate: 0.03\n",
            "Iteration 7500: Current success : 47 \t All success : 62 \t mean(l2) : 5.894130 \t Current Learning Rate: 0.03\n",
            "Iteration 8000: Current success : 47 \t All success : 62 \t mean(l2) : 5.894130 \t Current Learning Rate: 0.02\n",
            "Iteration 8500: Current success : 47 \t All success : 62 \t mean(l2) : 5.894130 \t Current Learning Rate: 0.02\n",
            "Iteration 9000: Current success : 47 \t All success : 62 \t mean(l2) : 5.894130 \t Current Learning Rate: 0.01\n",
            "Iteration 9500: Current success : 47 \t All success : 62 \t mean(l2) : 5.894130 \t Current Learning Rate: 0.01\n",
            "Iteration 10000: Current success : 47 \t All success : 62 \t mean(l2) : 5.894130 \t Current Learning Rate: 0.01\n",
            "Iteration 10500: Current success : 47 \t All success : 62 \t mean(l2) : 5.864903 \t Current Learning Rate: 0.02\n",
            "Iteration 11000: Current success : 47 \t All success : 62 \t mean(l2) : 5.864341 \t Current Learning Rate: 0.02\n",
            "Iteration 11500: Current success : 47 \t All success : 62 \t mean(l2) : 5.863865 \t Current Learning Rate: 0.02\n",
            "Iteration 12000: Current success : 47 \t All success : 62 \t mean(l2) : 5.862664 \t Current Learning Rate: 0.02\n",
            "Iteration 12500: Current success : 47 \t All success : 62 \t mean(l2) : 5.862517 \t Current Learning Rate: 0.01\n",
            "Iteration 13000: Current success : 47 \t All success : 62 \t mean(l2) : 5.862025 \t Current Learning Rate: 0.01\n",
            "Iteration 13500: Current success : 47 \t All success : 62 \t mean(l2) : 5.862025 \t Current Learning Rate: 0.01\n",
            "Iteration 14000: Current success : 47 \t All success : 62 \t mean(l2) : 5.862025 \t Current Learning Rate: 0.01\n",
            "Iteration 14500: Current success : 47 \t All success : 62 \t mean(l2) : 5.862025 \t Current Learning Rate: 0.00\n",
            "Iteration 15000: Current success : 47 \t All success : 62 \t mean(l2) : 5.862025 \t Current Learning Rate: 0.01\n",
            "Iteration 15500: Current success : 52 \t All success : 65 \t mean(l2) : 6.354328 \t Current Learning Rate: 0.01\n",
            "Iteration 16000: Current success : 52 \t All success : 65 \t mean(l2) : 6.320949 \t Current Learning Rate: 0.01\n",
            "Iteration 16500: Current success : 52 \t All success : 65 \t mean(l2) : 6.320169 \t Current Learning Rate: 0.01\n",
            "Iteration 17000: Current success : 52 \t All success : 65 \t mean(l2) : 6.320106 \t Current Learning Rate: 0.01\n",
            "Iteration 17500: Current success : 52 \t All success : 65 \t mean(l2) : 6.285196 \t Current Learning Rate: 0.01\n",
            "Iteration 18000: Current success : 52 \t All success : 65 \t mean(l2) : 6.280586 \t Current Learning Rate: 0.01\n",
            "Iteration 18500: Current success : 52 \t All success : 65 \t mean(l2) : 6.280586 \t Current Learning Rate: 0.00\n",
            "Iteration 19000: Current success : 52 \t All success : 65 \t mean(l2) : 6.280586 \t Current Learning Rate: 0.00\n",
            "Iteration 19500: Current success : 52 \t All success : 65 \t mean(l2) : 6.280586 \t Current Learning Rate: 0.00\n",
            "Iteration 20000: Current success : 52 \t All success : 65 \t mean(l2) : 6.280586 \t Current Learning Rate: 0.01\n",
            "o_success_attack  tensor(89, device='cuda:0')\n",
            "-------------------------------------------------\n",
            "tensor([100., 100., 100.,  ..., 100., 100., 100.], device='cuda:0')\n",
            "Iteration 500: Current success : 49 \t All success : 49 \t mean(l2) : 15.792155 \t Current Learning Rate: 0.09\n",
            "Iteration 1000: Current success : 58 \t All success : 65 \t mean(l2) : 17.745901 \t Current Learning Rate: 0.08\n",
            "Iteration 1500: Current success : 64 \t All success : 70 \t mean(l2) : 18.300821 \t Current Learning Rate: 0.07\n",
            "Iteration 2000: Current success : 69 \t All success : 78 \t mean(l2) : 18.288267 \t Current Learning Rate: 0.06\n",
            "Iteration 2500: Current success : 73 \t All success : 79 \t mean(l2) : 18.113010 \t Current Learning Rate: 0.05\n",
            "Iteration 3000: Current success : 72 \t All success : 79 \t mean(l2) : 18.088236 \t Current Learning Rate: 0.04\n",
            "Iteration 3500: Current success : 71 \t All success : 79 \t mean(l2) : 18.030426 \t Current Learning Rate: 0.03\n",
            "Iteration 4000: Current success : 71 \t All success : 79 \t mean(l2) : 18.019220 \t Current Learning Rate: 0.02\n",
            "Iteration 4500: Current success : 71 \t All success : 79 \t mean(l2) : 18.019205 \t Current Learning Rate: 0.01\n",
            "Iteration 5000: Current success : 71 \t All success : 79 \t mean(l2) : 18.019188 \t Current Learning Rate: 0.01\n",
            "Iteration 5500: Current success : 130 \t All success : 150 \t mean(l2) : 27.729828 \t Current Learning Rate: 0.05\n",
            "Iteration 6000: Current success : 133 \t All success : 153 \t mean(l2) : 27.690552 \t Current Learning Rate: 0.04\n",
            "Iteration 6500: Current success : 134 \t All success : 153 \t mean(l2) : 27.364389 \t Current Learning Rate: 0.04\n",
            "Iteration 7000: Current success : 133 \t All success : 153 \t mean(l2) : 27.334059 \t Current Learning Rate: 0.03\n",
            "Iteration 7500: Current success : 134 \t All success : 153 \t mean(l2) : 27.321957 \t Current Learning Rate: 0.03\n",
            "Iteration 8000: Current success : 134 \t All success : 153 \t mean(l2) : 27.307446 \t Current Learning Rate: 0.02\n",
            "Iteration 8500: Current success : 134 \t All success : 153 \t mean(l2) : 27.306072 \t Current Learning Rate: 0.02\n",
            "Iteration 9000: Current success : 134 \t All success : 153 \t mean(l2) : 27.267525 \t Current Learning Rate: 0.01\n",
            "Iteration 9500: Current success : 134 \t All success : 153 \t mean(l2) : 27.235071 \t Current Learning Rate: 0.01\n",
            "Iteration 10000: Current success : 134 \t All success : 153 \t mean(l2) : 27.229420 \t Current Learning Rate: 0.01\n",
            "Iteration 10500: Current success : 140 \t All success : 162 \t mean(l2) : 28.517349 \t Current Learning Rate: 0.02\n",
            "Iteration 11000: Current success : 139 \t All success : 162 \t mean(l2) : 28.454626 \t Current Learning Rate: 0.02\n",
            "Iteration 11500: Current success : 138 \t All success : 162 \t mean(l2) : 28.448658 \t Current Learning Rate: 0.02\n",
            "Iteration 12000: Current success : 141 \t All success : 162 \t mean(l2) : 28.381248 \t Current Learning Rate: 0.02\n",
            "Iteration 12500: Current success : 141 \t All success : 162 \t mean(l2) : 28.348467 \t Current Learning Rate: 0.01\n",
            "Iteration 13000: Current success : 141 \t All success : 162 \t mean(l2) : 28.290739 \t Current Learning Rate: 0.01\n",
            "Iteration 13500: Current success : 141 \t All success : 162 \t mean(l2) : 28.246660 \t Current Learning Rate: 0.01\n",
            "Iteration 14000: Current success : 141 \t All success : 162 \t mean(l2) : 28.037079 \t Current Learning Rate: 0.01\n",
            "Iteration 14500: Current success : 141 \t All success : 162 \t mean(l2) : 27.991796 \t Current Learning Rate: 0.00\n",
            "Iteration 15000: Current success : 141 \t All success : 162 \t mean(l2) : 27.991407 \t Current Learning Rate: 0.01\n",
            "Iteration 15500: Current success : 146 \t All success : 166 \t mean(l2) : 27.406261 \t Current Learning Rate: 0.01\n",
            "Iteration 16000: Current success : 147 \t All success : 167 \t mean(l2) : 27.583658 \t Current Learning Rate: 0.01\n",
            "Iteration 16500: Current success : 147 \t All success : 167 \t mean(l2) : 27.479084 \t Current Learning Rate: 0.01\n",
            "Iteration 17000: Current success : 147 \t All success : 167 \t mean(l2) : 27.459423 \t Current Learning Rate: 0.01\n",
            "Iteration 17500: Current success : 147 \t All success : 167 \t mean(l2) : 27.435925 \t Current Learning Rate: 0.01\n",
            "Iteration 18000: Current success : 147 \t All success : 167 \t mean(l2) : 27.432747 \t Current Learning Rate: 0.01\n",
            "Iteration 18500: Current success : 147 \t All success : 167 \t mean(l2) : 27.394632 \t Current Learning Rate: 0.00\n",
            "Iteration 19000: Current success : 147 \t All success : 167 \t mean(l2) : 27.394144 \t Current Learning Rate: 0.00\n",
            "Iteration 19500: Current success : 147 \t All success : 167 \t mean(l2) : 27.378632 \t Current Learning Rate: 0.00\n",
            "Iteration 20000: Current success : 147 \t All success : 167 \t mean(l2) : 27.377943 \t Current Learning Rate: 0.01\n",
            "o_success_attack  tensor(196, device='cuda:0')\n",
            "-------------------------------------------------\n",
            "tensor([  55., 1000., 1000.,  ..., 1000., 1000., 1000.], device='cuda:0')\n",
            "Iteration 500: Current success : 60 \t All success : 60 \t mean(l2) : 22.262638 \t Current Learning Rate: 0.09\n",
            "Iteration 1000: Current success : 67 \t All success : 70 \t mean(l2) : 25.577765 \t Current Learning Rate: 0.08\n",
            "Iteration 1500: Current success : 99 \t All success : 105 \t mean(l2) : 22.398844 \t Current Learning Rate: 0.07\n",
            "Iteration 2000: Current success : 104 \t All success : 111 \t mean(l2) : 21.309786 \t Current Learning Rate: 0.06\n",
            "Iteration 2500: Current success : 106 \t All success : 112 \t mean(l2) : 21.713398 \t Current Learning Rate: 0.05\n",
            "Iteration 3000: Current success : 108 \t All success : 113 \t mean(l2) : 22.121820 \t Current Learning Rate: 0.04\n",
            "Iteration 3500: Current success : 103 \t All success : 113 \t mean(l2) : 22.091118 \t Current Learning Rate: 0.03\n",
            "Iteration 4000: Current success : 108 \t All success : 113 \t mean(l2) : 22.084869 \t Current Learning Rate: 0.02\n",
            "Iteration 4500: Current success : 108 \t All success : 113 \t mean(l2) : 22.075191 \t Current Learning Rate: 0.01\n",
            "Iteration 5000: Current success : 108 \t All success : 113 \t mean(l2) : 22.068968 \t Current Learning Rate: 0.01\n",
            "Iteration 5500: Current success : 143 \t All success : 158 \t mean(l2) : 21.079317 \t Current Learning Rate: 0.05\n",
            "Iteration 6000: Current success : 137 \t All success : 159 \t mean(l2) : 21.279762 \t Current Learning Rate: 0.04\n",
            "Iteration 6500: Current success : 137 \t All success : 159 \t mean(l2) : 21.220301 \t Current Learning Rate: 0.04\n",
            "Iteration 7000: Current success : 138 \t All success : 159 \t mean(l2) : 21.189800 \t Current Learning Rate: 0.03\n",
            "Iteration 7500: Current success : 138 \t All success : 159 \t mean(l2) : 21.158758 \t Current Learning Rate: 0.03\n",
            "Iteration 8000: Current success : 138 \t All success : 159 \t mean(l2) : 21.146461 \t Current Learning Rate: 0.02\n",
            "Iteration 8500: Current success : 138 \t All success : 159 \t mean(l2) : 21.144335 \t Current Learning Rate: 0.02\n",
            "Iteration 9000: Current success : 138 \t All success : 159 \t mean(l2) : 21.143574 \t Current Learning Rate: 0.01\n",
            "Iteration 9500: Current success : 138 \t All success : 159 \t mean(l2) : 21.142529 \t Current Learning Rate: 0.01\n",
            "Iteration 10000: Current success : 138 \t All success : 159 \t mean(l2) : 21.141884 \t Current Learning Rate: 0.01\n",
            "Iteration 10500: Current success : 142 \t All success : 165 \t mean(l2) : 22.169477 \t Current Learning Rate: 0.02\n",
            "Iteration 11000: Current success : 144 \t All success : 165 \t mean(l2) : 22.065342 \t Current Learning Rate: 0.02\n",
            "Iteration 11500: Current success : 144 \t All success : 165 \t mean(l2) : 22.056305 \t Current Learning Rate: 0.02\n",
            "Iteration 12000: Current success : 144 \t All success : 165 \t mean(l2) : 22.049629 \t Current Learning Rate: 0.02\n",
            "Iteration 12500: Current success : 144 \t All success : 165 \t mean(l2) : 22.047218 \t Current Learning Rate: 0.01\n",
            "Iteration 13000: Current success : 144 \t All success : 165 \t mean(l2) : 22.035334 \t Current Learning Rate: 0.01\n",
            "Iteration 13500: Current success : 144 \t All success : 165 \t mean(l2) : 22.026016 \t Current Learning Rate: 0.01\n",
            "Iteration 14000: Current success : 144 \t All success : 165 \t mean(l2) : 22.016121 \t Current Learning Rate: 0.01\n",
            "Iteration 14500: Current success : 144 \t All success : 165 \t mean(l2) : 22.015810 \t Current Learning Rate: 0.00\n",
            "Iteration 15000: Current success : 144 \t All success : 165 \t mean(l2) : 22.015018 \t Current Learning Rate: 0.01\n",
            "Iteration 15500: Current success : 147 \t All success : 168 \t mean(l2) : 21.991083 \t Current Learning Rate: 0.01\n",
            "Iteration 16000: Current success : 147 \t All success : 168 \t mean(l2) : 21.967257 \t Current Learning Rate: 0.01\n",
            "Iteration 16500: Current success : 147 \t All success : 168 \t mean(l2) : 21.911512 \t Current Learning Rate: 0.01\n",
            "Iteration 17000: Current success : 147 \t All success : 168 \t mean(l2) : 21.870224 \t Current Learning Rate: 0.01\n",
            "Iteration 17500: Current success : 147 \t All success : 168 \t mean(l2) : 21.858208 \t Current Learning Rate: 0.01\n",
            "Iteration 18000: Current success : 147 \t All success : 168 \t mean(l2) : 21.857697 \t Current Learning Rate: 0.01\n",
            "Iteration 18500: Current success : 147 \t All success : 168 \t mean(l2) : 21.852222 \t Current Learning Rate: 0.00\n",
            "Iteration 19000: Current success : 147 \t All success : 168 \t mean(l2) : 21.846849 \t Current Learning Rate: 0.00\n",
            "Iteration 19500: Current success : 147 \t All success : 168 \t mean(l2) : 21.839239 \t Current Learning Rate: 0.00\n",
            "Iteration 20000: Current success : 147 \t All success : 168 \t mean(l2) : 21.835991 \t Current Learning Rate: 0.01\n",
            "o_success_attack  tensor(231, device='cuda:0')\n",
            "-------------------------------------------------\n",
            "tensor([   32.5000, 10000.0000, 10000.0000,  ..., 10000.0000, 10000.0000,\n",
            "        10000.0000], device='cuda:0')\n",
            "Iteration 500: Current success : 55 \t All success : 55 \t mean(l2) : 22.359041 \t Current Learning Rate: 0.09\n",
            "Iteration 1000: Current success : 68 \t All success : 69 \t mean(l2) : 25.394737 \t Current Learning Rate: 0.08\n",
            "Iteration 1500: Current success : 66 \t All success : 69 \t mean(l2) : 25.256355 \t Current Learning Rate: 0.07\n",
            "Iteration 2000: Current success : 69 \t All success : 72 \t mean(l2) : 25.178623 \t Current Learning Rate: 0.06\n",
            "Iteration 2500: Current success : 69 \t All success : 73 \t mean(l2) : 24.971676 \t Current Learning Rate: 0.05\n",
            "Iteration 3000: Current success : 69 \t All success : 74 \t mean(l2) : 25.551083 \t Current Learning Rate: 0.04\n",
            "Iteration 3500: Current success : 71 \t All success : 74 \t mean(l2) : 25.492273 \t Current Learning Rate: 0.03\n",
            "Iteration 4000: Current success : 71 \t All success : 74 \t mean(l2) : 25.461803 \t Current Learning Rate: 0.02\n",
            "Iteration 4500: Current success : 70 \t All success : 74 \t mean(l2) : 25.450903 \t Current Learning Rate: 0.01\n",
            "Iteration 5000: Current success : 71 \t All success : 74 \t mean(l2) : 25.448051 \t Current Learning Rate: 0.01\n",
            "Iteration 5500: Current success : 130 \t All success : 143 \t mean(l2) : 20.200289 \t Current Learning Rate: 0.05\n",
            "Iteration 6000: Current success : 130 \t All success : 143 \t mean(l2) : 20.176764 \t Current Learning Rate: 0.04\n",
            "Iteration 6500: Current success : 129 \t All success : 143 \t mean(l2) : 20.161324 \t Current Learning Rate: 0.04\n",
            "Iteration 7000: Current success : 127 \t All success : 143 \t mean(l2) : 20.112165 \t Current Learning Rate: 0.03\n",
            "Iteration 7500: Current success : 127 \t All success : 143 \t mean(l2) : 20.091030 \t Current Learning Rate: 0.03\n",
            "Iteration 8000: Current success : 126 \t All success : 143 \t mean(l2) : 20.077690 \t Current Learning Rate: 0.02\n",
            "Iteration 8500: Current success : 126 \t All success : 143 \t mean(l2) : 20.070902 \t Current Learning Rate: 0.02\n",
            "Iteration 9000: Current success : 126 \t All success : 143 \t mean(l2) : 20.070051 \t Current Learning Rate: 0.01\n",
            "Iteration 9500: Current success : 126 \t All success : 143 \t mean(l2) : 20.068750 \t Current Learning Rate: 0.01\n",
            "Iteration 10000: Current success : 126 \t All success : 143 \t mean(l2) : 20.067862 \t Current Learning Rate: 0.01\n",
            "Iteration 10500: Current success : 126 \t All success : 146 \t mean(l2) : 20.266594 \t Current Learning Rate: 0.02\n",
            "Iteration 11000: Current success : 126 \t All success : 146 \t mean(l2) : 20.260395 \t Current Learning Rate: 0.02\n",
            "Iteration 11500: Current success : 126 \t All success : 146 \t mean(l2) : 20.253695 \t Current Learning Rate: 0.02\n",
            "Iteration 12000: Current success : 126 \t All success : 146 \t mean(l2) : 20.242315 \t Current Learning Rate: 0.02\n",
            "Iteration 12500: Current success : 126 \t All success : 146 \t mean(l2) : 20.235762 \t Current Learning Rate: 0.01\n",
            "Iteration 13000: Current success : 126 \t All success : 146 \t mean(l2) : 20.235161 \t Current Learning Rate: 0.01\n",
            "Iteration 13500: Current success : 126 \t All success : 146 \t mean(l2) : 20.220247 \t Current Learning Rate: 0.01\n",
            "Iteration 14000: Current success : 122 \t All success : 146 \t mean(l2) : 20.163128 \t Current Learning Rate: 0.01\n",
            "Iteration 14500: Current success : 126 \t All success : 146 \t mean(l2) : 20.162952 \t Current Learning Rate: 0.00\n",
            "Iteration 15000: Current success : 126 \t All success : 146 \t mean(l2) : 20.155766 \t Current Learning Rate: 0.01\n",
            "Iteration 15500: Current success : 134 \t All success : 154 \t mean(l2) : 21.645086 \t Current Learning Rate: 0.01\n",
            "Iteration 16000: Current success : 134 \t All success : 155 \t mean(l2) : 21.578651 \t Current Learning Rate: 0.01\n",
            "Iteration 16500: Current success : 135 \t All success : 155 \t mean(l2) : 21.578148 \t Current Learning Rate: 0.01\n",
            "Iteration 17000: Current success : 135 \t All success : 155 \t mean(l2) : 21.570724 \t Current Learning Rate: 0.01\n",
            "Iteration 17500: Current success : 135 \t All success : 155 \t mean(l2) : 21.560839 \t Current Learning Rate: 0.01\n",
            "Iteration 18000: Current success : 135 \t All success : 155 \t mean(l2) : 21.533262 \t Current Learning Rate: 0.01\n",
            "Iteration 18500: Current success : 135 \t All success : 155 \t mean(l2) : 21.532846 \t Current Learning Rate: 0.00\n",
            "Iteration 19000: Current success : 135 \t All success : 155 \t mean(l2) : 21.532845 \t Current Learning Rate: 0.00\n",
            "Iteration 19500: Current success : 135 \t All success : 155 \t mean(l2) : 21.532764 \t Current Learning Rate: 0.00\n",
            "Iteration 20000: Current success : 135 \t All success : 155 \t mean(l2) : 21.532555 \t Current Learning Rate: 0.01\n",
            "o_success_attack  tensor(246, device='cuda:0')\n",
            "-------------------------------------------------\n",
            "tensor([4.3750e+01, 1.0000e+05, 1.0000e+05,  ..., 1.0000e+05, 1.0000e+05,\n",
            "        1.0000e+05], device='cuda:0')\n",
            "Iteration 500: Current success : 46 \t All success : 46 \t mean(l2) : 17.724682 \t Current Learning Rate: 0.09\n",
            "Iteration 1000: Current success : 58 \t All success : 63 \t mean(l2) : 17.286282 \t Current Learning Rate: 0.08\n",
            "Iteration 1500: Current success : 60 \t All success : 64 \t mean(l2) : 17.546368 \t Current Learning Rate: 0.07\n",
            "Iteration 2000: Current success : 68 \t All success : 70 \t mean(l2) : 16.651114 \t Current Learning Rate: 0.06\n",
            "Iteration 2500: Current success : 69 \t All success : 72 \t mean(l2) : 17.758774 \t Current Learning Rate: 0.05\n",
            "Iteration 3000: Current success : 71 \t All success : 72 \t mean(l2) : 17.751478 \t Current Learning Rate: 0.04\n",
            "Iteration 3500: Current success : 70 \t All success : 72 \t mean(l2) : 17.718864 \t Current Learning Rate: 0.03\n",
            "Iteration 4000: Current success : 70 \t All success : 72 \t mean(l2) : 17.700283 \t Current Learning Rate: 0.02\n",
            "Iteration 4500: Current success : 70 \t All success : 72 \t mean(l2) : 17.700274 \t Current Learning Rate: 0.01\n",
            "Iteration 5000: Current success : 70 \t All success : 72 \t mean(l2) : 17.700266 \t Current Learning Rate: 0.01\n",
            "Iteration 5500: Current success : 101 \t All success : 122 \t mean(l2) : 19.292980 \t Current Learning Rate: 0.05\n",
            "Iteration 6000: Current success : 101 \t All success : 125 \t mean(l2) : 19.758698 \t Current Learning Rate: 0.04\n",
            "Iteration 6500: Current success : 102 \t All success : 125 \t mean(l2) : 19.724602 \t Current Learning Rate: 0.04\n",
            "Iteration 7000: Current success : 102 \t All success : 125 \t mean(l2) : 19.702354 \t Current Learning Rate: 0.03\n",
            "Iteration 7500: Current success : 102 \t All success : 125 \t mean(l2) : 19.678822 \t Current Learning Rate: 0.03\n",
            "Iteration 8000: Current success : 102 \t All success : 125 \t mean(l2) : 19.663269 \t Current Learning Rate: 0.02\n",
            "Iteration 8500: Current success : 102 \t All success : 125 \t mean(l2) : 19.650110 \t Current Learning Rate: 0.02\n",
            "Iteration 9000: Current success : 102 \t All success : 125 \t mean(l2) : 19.639845 \t Current Learning Rate: 0.01\n",
            "Iteration 9500: Current success : 102 \t All success : 125 \t mean(l2) : 19.632994 \t Current Learning Rate: 0.01\n",
            "Iteration 10000: Current success : 102 \t All success : 125 \t mean(l2) : 19.630337 \t Current Learning Rate: 0.01\n",
            "Iteration 10500: Current success : 103 \t All success : 133 \t mean(l2) : 21.206205 \t Current Learning Rate: 0.02\n",
            "Iteration 11000: Current success : 104 \t All success : 138 \t mean(l2) : 22.190731 \t Current Learning Rate: 0.02\n",
            "Iteration 11500: Current success : 105 \t All success : 138 \t mean(l2) : 22.180920 \t Current Learning Rate: 0.02\n",
            "Iteration 12000: Current success : 105 \t All success : 138 \t mean(l2) : 22.158907 \t Current Learning Rate: 0.02\n",
            "Iteration 12500: Current success : 105 \t All success : 138 \t mean(l2) : 22.132828 \t Current Learning Rate: 0.01\n",
            "Iteration 13000: Current success : 105 \t All success : 138 \t mean(l2) : 22.092794 \t Current Learning Rate: 0.01\n",
            "Iteration 13500: Current success : 105 \t All success : 138 \t mean(l2) : 22.081293 \t Current Learning Rate: 0.01\n",
            "Iteration 14000: Current success : 105 \t All success : 138 \t mean(l2) : 22.070822 \t Current Learning Rate: 0.01\n",
            "Iteration 14500: Current success : 105 \t All success : 138 \t mean(l2) : 22.067108 \t Current Learning Rate: 0.00\n",
            "Iteration 15000: Current success : 105 \t All success : 138 \t mean(l2) : 22.065479 \t Current Learning Rate: 0.01\n",
            "Iteration 15500: Current success : 106 \t All success : 139 \t mean(l2) : 21.919779 \t Current Learning Rate: 0.01\n",
            "Iteration 16000: Current success : 106 \t All success : 139 \t mean(l2) : 21.855333 \t Current Learning Rate: 0.01\n",
            "Iteration 16500: Current success : 107 \t All success : 139 \t mean(l2) : 21.797625 \t Current Learning Rate: 0.01\n",
            "Iteration 17000: Current success : 107 \t All success : 139 \t mean(l2) : 21.787264 \t Current Learning Rate: 0.01\n",
            "Iteration 17500: Current success : 107 \t All success : 139 \t mean(l2) : 21.760088 \t Current Learning Rate: 0.01\n",
            "Iteration 18000: Current success : 107 \t All success : 139 \t mean(l2) : 21.758091 \t Current Learning Rate: 0.01\n",
            "Iteration 18500: Current success : 107 \t All success : 139 \t mean(l2) : 21.752329 \t Current Learning Rate: 0.00\n",
            "Iteration 19000: Current success : 107 \t All success : 139 \t mean(l2) : 21.737667 \t Current Learning Rate: 0.00\n",
            "Iteration 19500: Current success : 107 \t All success : 139 \t mean(l2) : 21.725439 \t Current Learning Rate: 0.00\n",
            "Iteration 20000: Current success : 107 \t All success : 139 \t mean(l2) : 21.716942 \t Current Learning Rate: 0.01\n",
            "o_success_attack  tensor(252, device='cuda:0')\n",
            "-------------------------------------------------\n",
            "tensor([3.8125e+01, 1.0000e+06, 1.0000e+06,  ..., 1.0000e+06, 1.0000e+06,\n",
            "        1.0000e+06], device='cuda:0')\n",
            "Iteration 500: Current success : 55 \t All success : 55 \t mean(l2) : 17.300795 \t Current Learning Rate: 0.09\n",
            "Iteration 1000: Current success : 70 \t All success : 76 \t mean(l2) : 23.467468 \t Current Learning Rate: 0.08\n",
            "Iteration 1500: Current success : 68 \t All success : 79 \t mean(l2) : 23.494131 \t Current Learning Rate: 0.07\n",
            "Iteration 2000: Current success : 74 \t All success : 83 \t mean(l2) : 23.347437 \t Current Learning Rate: 0.06\n",
            "Iteration 2500: Current success : 75 \t All success : 85 \t mean(l2) : 23.283062 \t Current Learning Rate: 0.05\n",
            "Iteration 3000: Current success : 73 \t All success : 86 \t mean(l2) : 23.219336 \t Current Learning Rate: 0.04\n",
            "Iteration 3500: Current success : 72 \t All success : 86 \t mean(l2) : 23.217710 \t Current Learning Rate: 0.03\n",
            "Iteration 4000: Current success : 73 \t All success : 86 \t mean(l2) : 23.216873 \t Current Learning Rate: 0.02\n",
            "Iteration 4500: Current success : 73 \t All success : 86 \t mean(l2) : 23.215162 \t Current Learning Rate: 0.01\n",
            "Iteration 5000: Current success : 73 \t All success : 86 \t mean(l2) : 23.214956 \t Current Learning Rate: 0.01\n",
            "Iteration 5500: Current success : 124 \t All success : 158 \t mean(l2) : 20.630981 \t Current Learning Rate: 0.05\n",
            "Iteration 6000: Current success : 125 \t All success : 158 \t mean(l2) : 20.625385 \t Current Learning Rate: 0.04\n",
            "Iteration 6500: Current success : 124 \t All success : 158 \t mean(l2) : 20.613951 \t Current Learning Rate: 0.04\n",
            "Iteration 7000: Current success : 125 \t All success : 158 \t mean(l2) : 20.613941 \t Current Learning Rate: 0.03\n",
            "Iteration 7500: Current success : 125 \t All success : 158 \t mean(l2) : 20.613932 \t Current Learning Rate: 0.03\n",
            "Iteration 8000: Current success : 124 \t All success : 158 \t mean(l2) : 20.613932 \t Current Learning Rate: 0.02\n",
            "Iteration 8500: Current success : 125 \t All success : 158 \t mean(l2) : 20.613932 \t Current Learning Rate: 0.02\n",
            "Iteration 9000: Current success : 125 \t All success : 158 \t mean(l2) : 20.613672 \t Current Learning Rate: 0.01\n",
            "Iteration 9500: Current success : 125 \t All success : 158 \t mean(l2) : 20.608202 \t Current Learning Rate: 0.01\n",
            "Iteration 10000: Current success : 125 \t All success : 158 \t mean(l2) : 20.608202 \t Current Learning Rate: 0.01\n",
            "Iteration 10500: Current success : 133 \t All success : 164 \t mean(l2) : 20.987440 \t Current Learning Rate: 0.02\n",
            "Iteration 11000: Current success : 133 \t All success : 164 \t mean(l2) : 20.987440 \t Current Learning Rate: 0.02\n",
            "Iteration 11500: Current success : 133 \t All success : 164 \t mean(l2) : 20.987440 \t Current Learning Rate: 0.02\n",
            "Iteration 12000: Current success : 133 \t All success : 164 \t mean(l2) : 20.987440 \t Current Learning Rate: 0.02\n",
            "Iteration 12500: Current success : 133 \t All success : 164 \t mean(l2) : 20.981642 \t Current Learning Rate: 0.01\n",
            "Iteration 13000: Current success : 133 \t All success : 164 \t mean(l2) : 20.969822 \t Current Learning Rate: 0.01\n",
            "Iteration 13500: Current success : 133 \t All success : 164 \t mean(l2) : 20.964251 \t Current Learning Rate: 0.01\n",
            "Iteration 14000: Current success : 133 \t All success : 164 \t mean(l2) : 20.964184 \t Current Learning Rate: 0.01\n",
            "Iteration 14500: Current success : 133 \t All success : 164 \t mean(l2) : 20.964182 \t Current Learning Rate: 0.00\n",
            "Iteration 15000: Current success : 133 \t All success : 164 \t mean(l2) : 20.964178 \t Current Learning Rate: 0.01\n",
            "Iteration 15500: Current success : 137 \t All success : 166 \t mean(l2) : 20.938128 \t Current Learning Rate: 0.01\n",
            "Iteration 16000: Current success : 137 \t All success : 166 \t mean(l2) : 20.824911 \t Current Learning Rate: 0.01\n",
            "Iteration 16500: Current success : 137 \t All success : 166 \t mean(l2) : 20.782125 \t Current Learning Rate: 0.01\n",
            "Iteration 17000: Current success : 137 \t All success : 166 \t mean(l2) : 20.766039 \t Current Learning Rate: 0.01\n",
            "Iteration 17500: Current success : 137 \t All success : 166 \t mean(l2) : 20.744757 \t Current Learning Rate: 0.01\n",
            "Iteration 18000: Current success : 138 \t All success : 166 \t mean(l2) : 20.725777 \t Current Learning Rate: 0.01\n",
            "Iteration 18500: Current success : 138 \t All success : 166 \t mean(l2) : 20.721066 \t Current Learning Rate: 0.00\n",
            "Iteration 19000: Current success : 138 \t All success : 166 \t mean(l2) : 20.715519 \t Current Learning Rate: 0.00\n",
            "Iteration 19500: Current success : 138 \t All success : 166 \t mean(l2) : 20.712540 \t Current Learning Rate: 0.00\n",
            "Iteration 20000: Current success : 138 \t All success : 166 \t mean(l2) : 20.711208 \t Current Learning Rate: 0.01\n",
            "o_success_attack  tensor(252, device='cuda:0')\n",
            "-------------------------------------------------\n",
            "tensor([4.0938e+01, 1.0000e+07, 1.0000e+07,  ..., 1.0000e+07, 1.0000e+07,\n",
            "        1.0000e+07], device='cuda:0')\n",
            "Iteration 500: Current success : 89 \t All success : 89 \t mean(l2) : 16.192955 \t Current Learning Rate: 0.09\n",
            "Iteration 1000: Current success : 111 \t All success : 119 \t mean(l2) : 15.897742 \t Current Learning Rate: 0.08\n",
            "Iteration 1500: Current success : 121 \t All success : 131 \t mean(l2) : 17.366274 \t Current Learning Rate: 0.07\n",
            "Iteration 2000: Current success : 134 \t All success : 142 \t mean(l2) : 18.059774 \t Current Learning Rate: 0.06\n",
            "Iteration 2500: Current success : 136 \t All success : 145 \t mean(l2) : 18.507635 \t Current Learning Rate: 0.05\n",
            "Iteration 3000: Current success : 140 \t All success : 147 \t mean(l2) : 18.501608 \t Current Learning Rate: 0.04\n",
            "Iteration 3500: Current success : 140 \t All success : 147 \t mean(l2) : 18.501608 \t Current Learning Rate: 0.03\n",
            "Iteration 4000: Current success : 143 \t All success : 147 \t mean(l2) : 18.501608 \t Current Learning Rate: 0.02\n",
            "Iteration 4500: Current success : 144 \t All success : 147 \t mean(l2) : 18.501608 \t Current Learning Rate: 0.01\n",
            "Iteration 5000: Current success : 144 \t All success : 147 \t mean(l2) : 18.501608 \t Current Learning Rate: 0.01\n",
            "Iteration 5500: Current success : 153 \t All success : 157 \t mean(l2) : 18.273565 \t Current Learning Rate: 0.05\n",
            "Iteration 6000: Current success : 184 \t All success : 187 \t mean(l2) : 17.138977 \t Current Learning Rate: 0.04\n",
            "Iteration 6500: Current success : 186 \t All success : 192 \t mean(l2) : 18.066666 \t Current Learning Rate: 0.04\n",
            "Iteration 7000: Current success : 183 \t All success : 193 \t mean(l2) : 18.324955 \t Current Learning Rate: 0.03\n",
            "Iteration 7500: Current success : 188 \t All success : 193 \t mean(l2) : 18.324955 \t Current Learning Rate: 0.03\n",
            "Iteration 8000: Current success : 189 \t All success : 193 \t mean(l2) : 18.324955 \t Current Learning Rate: 0.02\n",
            "Iteration 8500: Current success : 189 \t All success : 193 \t mean(l2) : 18.324955 \t Current Learning Rate: 0.02\n",
            "Iteration 9000: Current success : 190 \t All success : 193 \t mean(l2) : 18.324955 \t Current Learning Rate: 0.01\n",
            "Iteration 9500: Current success : 190 \t All success : 193 \t mean(l2) : 18.324955 \t Current Learning Rate: 0.01\n",
            "Iteration 10000: Current success : 190 \t All success : 193 \t mean(l2) : 18.324955 \t Current Learning Rate: 0.01\n",
            "Iteration 10500: Current success : 192 \t All success : 195 \t mean(l2) : 18.890453 \t Current Learning Rate: 0.02\n"
          ]
        }
      ],
      "source": [
        "#CW(mal_x_batch, mal_y_batch, model_AT_rFGSM, removal_array, max_iterations=1000, learning_rate=0.01, binary_search_steps=3, confidence=0.01, initial_const=0.1, device=device)\n",
        "attack_params =  {'device':device, 'confidence':0.1, 'removal_array':removal_array,'max_iterations': 20000, 'learning_rate': 0.3, 'initial_const': 1 , 'binary_search_steps' : 8, 'cons_increase_factor' : 10}\n",
        "adv = adv_predict(test_loader, combined_model, CW, **attack_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aUkmoy1rBdmM"
      },
      "outputs": [],
      "source": [
        "for x_test, y_test in test_loader:\n",
        "    x_test, y_test = x_test.to(torch.float32).to(device), y_test.to(device)\n",
        "    mal_x_batch, mal_y_batch = x_test[y_test.squeeze() == 1], y_test[y_test.squeeze() == 1]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation on non-rounded adversarial examples\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "combined_model.eval()\n",
        "\n",
        "# Get the model's predictions on adversarial examples\n",
        "outputs = combined_model(adv)\n",
        "y_pred = outputs.argmax(dim=1)\n",
        "\n",
        "# Calculate metrics\n",
        "num_malwares = len(mal_y_batch)\n",
        "num_successful_attacks = (y_pred == 0).sum().item()\n",
        "total_difference = (torch.abs(adv[y_pred == 0] - mal_x_batch[y_pred == 0])).sum().item()\n",
        "accuracy = (y_pred == 1).sum().item() / num_malwares * 100\n",
        "\n",
        "# Print results with clear formatting\n",
        "print(f\"Evaluation Results on NOT_Rounded Adversarial Examples\")\n",
        "print(f\"-----------------------------------------------\")\n",
        "print(f\"Number of malware samples: {num_malwares}\")\n",
        "print(f\"Number of successful attacks: {num_successful_attacks}\")\n",
        "print(f\"Total difference (L1 norm) between successful adversarial examples and original: {total_difference}\")\n",
        "print(f\"Accuracy of the model on malwares under attack: {accuracy:.2f}%\")"
      ],
      "metadata": {
        "id": "vR5w9AovBdmN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation on rounded adversarial examples\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "combined_model.eval()\n",
        "\n",
        "# Apply the final rounding solution to adversarial examples\n",
        "adv_rounded = final_rounding(mal_x_batch, adv, combined_model)\n",
        "\n",
        "# Get the model's predictions on the rounded adversarial examples\n",
        "outputs = combined_model(adv_rounded)\n",
        "y_pred = outputs.argmax(dim=1)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "nj = len(mal_y_batch)\n",
        "num_successful_attacks = (y_pred == 0).sum().item()\n",
        "total_difference = (torch.abs(adv_rounded[y_pred == 0] - mal_x_batch[y_pred == 0])).sum().item()\n",
        "accuracy = (y_pred == 1).sum().item() / num_malwares * 100\n",
        "\n",
        "# Display the results with clear formatting\n",
        "print(f\"Evaluation Results on Rounded Adversarial Examples\")\n",
        "print(f\"-----------------------------------------------\")\n",
        "print(f\"Number of malware samples: {num_malwares}\")\n",
        "print(f\"Number of successful attacks: {num_successful_attacks}\")\n",
        "print(f\"Total L1 norm difference (successful adversarial examples vs. original): {total_difference}\")\n",
        "print(f\"Accuracy of the model on malware under attack: {accuracy:.2f}%\")"
      ],
      "metadata": {
        "id": "BNaNX_I1BdmN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure that fixed elements are not changed\n",
        "\n",
        "# Ensure `removal_array` is expanded to match the batch size\n",
        "expanded_removal_array = removal_array.expand(mal_x_batch.shape[0], -1)\n",
        "\n",
        "# Create a mask where non-fixed features are set to 1\n",
        "non_fixed_features_mask = torch.bitwise_or(expanded_removal_array, 1 - mal_x_batch.to(torch.uint8))\n",
        "\n",
        "# Calculate indices where the mask is 0 (fixed) or 1 (non-fixed)\n",
        "mask_fixed = non_fixed_features_mask == 0\n",
        "mask_non_fixed = non_fixed_features_mask == 1\n",
        "\n",
        "# Calculate and print the differences\n",
        "diff_adv_fixed = (torch.abs(adv - mal_x_batch) * mask_fixed).sum().item()\n",
        "diff_adv_non_fixed = (torch.abs(adv - mal_x_batch) * mask_non_fixed).sum().item()\n",
        "diff_adv_rounded_fixed = (torch.abs(adv_rounded - mal_x_batch) * mask_fixed).sum().item()\n",
        "diff_adv_rounded_non_fixed = (torch.abs(adv_rounded - mal_x_batch) * mask_non_fixed).sum().item()\n",
        "\n",
        "# Display results with clear formatting\n",
        "print(f\"Differences when mask is 0 (fixed features):\")\n",
        "print(f\"  Adv vs. Original: {diff_adv_fixed}\")\n",
        "print(f\"  Rounded Adv vs. Original: {diff_adv_rounded_fixed}\")\n",
        "print(f\"Differences when mask is 1 (non-fixed features):\")\n",
        "print(f\"  Adv vs. Original: {diff_adv_non_fixed}\")\n",
        "print(f\"  Rounded Adv vs. Original: {diff_adv_rounded_non_fixed}\")\n"
      ],
      "metadata": {
        "id": "oosp1TqYBdmO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# L1 NORM"
      ],
      "metadata": {
        "id": "5RWC9JQRs6ZD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "from torch.optim.lr_scheduler import CyclicLR\n",
        "\n",
        "def compare(output, target, confidence, targeted):\n",
        "    \"\"\"\n",
        "    Compares the output with the target for a targeted or untargeted attack.\n",
        "\n",
        "    For targeted attacks, it checks if the output class matches the target class.\n",
        "    For untargeted attacks, it checks if the output class does not match the target class.\n",
        "    \"\"\"\n",
        "    if not isinstance(output, (float, int, torch.LongTensor, torch.cuda.LongTensor)):\n",
        "        output = torch.clone(output).detach()\n",
        "        if targeted:\n",
        "            output[target] -= 0.001  # Slightly reduce the confidence for targeted attacks\n",
        "        else:\n",
        "            output[target] += 0.001  # Slightly increase the confidence for untargeted attacks\n",
        "        output = torch.argmax(output, dim=1)\n",
        "    return output == target if targeted else output != target\n",
        "\n",
        "def sin_func_torch(input_tensor):\n",
        "    \"\"\"\n",
        "    A sine-based transformation applied to the input tensor.\n",
        "\n",
        "    The function applies a sine transformation and rescales the output to [0, 1].\n",
        "    \"\"\"\n",
        "    return (torch.sin((input_tensor - 0.5) * torch.pi) + 1) / 2.0\n",
        "\n",
        "def CW_l1(imgs, y, model, removal_array, max_iterations=1000, learning_rate=0.01,\n",
        "       binary_search_steps=3, confidence=0.01, initial_const=0.1,\n",
        "       cons_increase_factor=2, device='cuda'):\n",
        "    \"\"\"\n",
        "    Implements the Carlini & Wagner (CW) L1 attack.\n",
        "\n",
        "    Parameters:\n",
        "    - imgs: The original images to be attacked.\n",
        "    - y: The true labels of the images.\n",
        "    - model: The target model to attack.\n",
        "    - removal_array: A mask array specifying which features are fixed.\n",
        "    - max_iterations: The maximum number of iterations for the attack.\n",
        "    - learning_rate: The learning rate for the optimizer.\n",
        "    - binary_search_steps: The number of steps for the binary search over the constant.\n",
        "    - confidence: Confidence value for the attack.\n",
        "    - initial_const: The initial constant for the attack loss.\n",
        "    - cons_increase_factor: The factor by which the constant is increased during binary search.\n",
        "    - device: The device on which to perform computations.\n",
        "\n",
        "    Returns:\n",
        "    - final_output: The perturbed images after the attack.\n",
        "    \"\"\"\n",
        "\n",
        "    labs = torch.zeros_like(y.squeeze(), dtype=torch.int64)\n",
        "    batch_size = imgs.shape[0]\n",
        "    targeted = True\n",
        "\n",
        "    # Expand removal array to match the batch size\n",
        "    expanded_removal_array = removal_array.expand(imgs.shape[0], -1)\n",
        "    non_fixed_features_mask = torch.bitwise_or(expanded_removal_array, 1 - imgs.to(torch.uint8))\n",
        "\n",
        "    mask_0 = non_fixed_features_mask == 0\n",
        "    mask_1 = non_fixed_features_mask == 1\n",
        "\n",
        "    imgs_tensor = imgs.clone().detach().to(dtype=torch.float32, device=device)\n",
        "    labs_tensor = labs.clone().detach().to(dtype=torch.int64, device=device)\n",
        "\n",
        "    lower_bound = torch.zeros(batch_size, dtype=torch.float32, device=device)\n",
        "    CONST = torch.full((batch_size,), initial_const, dtype=torch.float32, device=device)\n",
        "    upper_bound = torch.full((batch_size,), 1e10, dtype=torch.float32, device=device)\n",
        "\n",
        "    o_bestl2 = torch.full((batch_size,), 1e10, device=device)\n",
        "    o_bestscore = torch.full((batch_size,), -1, device=device)\n",
        "    o_bestattack = imgs.clone().detach()\n",
        "\n",
        "    o_best_loss1 = torch.full((batch_size,), float('inf'), device=device)\n",
        "    o_best_unsuccessfull_attack = imgs.clone().detach()\n",
        "    o_success_attack = torch.zeros(batch_size, dtype=torch.bool, device=device)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    for outer_step in range(binary_search_steps):\n",
        "        print(CONST)\n",
        "\n",
        "        modifier = torch.zeros_like(imgs_tensor, requires_grad=True).to(device)\n",
        "        optimizer = torch.optim.Adam([modifier], lr=learning_rate)\n",
        "\n",
        "        def grad_hook(grad):\n",
        "            return grad * non_fixed_features_mask\n",
        "        modifier.register_hook(grad_hook)\n",
        "\n",
        "        scheduler = CyclicLR(optimizer, base_lr=0.0001, max_lr=0.1, step_size_up=4,\n",
        "                             step_size_down=4995, mode='triangular2')\n",
        "\n",
        "        best_loss = torch.full((batch_size,), float('inf'), device=device)\n",
        "        no_improvement_count = torch.zeros(batch_size, dtype=torch.int, device=device)\n",
        "        active_mask = torch.ones(batch_size, dtype=torch.bool, device=device)\n",
        "\n",
        "        bestl2 = torch.full((batch_size,), 1e10, device=device)\n",
        "        bestscore = torch.full((batch_size,), -1, device=device)\n",
        "\n",
        "        for iteration in range(max_iterations):\n",
        "            if (iteration + 1) % 2000 == 0:\n",
        "                current_lr = scheduler.get_last_lr()[0]\n",
        "                print(f\"Iteration {iteration + 1}: Current Learning Rate: {current_lr}\")\n",
        "\n",
        "            active_indices = torch.where(active_mask)[0]\n",
        "            active_modifier = modifier[active_mask]\n",
        "            active_imgs_tensor = imgs_tensor[active_mask]\n",
        "            active_labs_tensor = labs_tensor[active_mask]\n",
        "            active_CONST = CONST[active_mask]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            modified_input = active_imgs_tensor + active_modifier\n",
        "            newimg = sin_func_torch(modified_input)\n",
        "            output = model(newimg)\n",
        "\n",
        "            l2dist = torch.sum(torch.abs(newimg - imgs[active_mask]), dim=1)\n",
        "\n",
        "            real = output[torch.arange(active_indices.size(0)), active_labs_tensor]\n",
        "            other = output[torch.arange(active_indices.size(0)), active_labs_tensor + 1]\n",
        "\n",
        "            loss1 = torch.clamp((other - real + confidence), min=0.0)\n",
        "            loss2 = l2dist\n",
        "\n",
        "            if outer_step != (binary_search_steps - 1):\n",
        "                loss = (active_CONST * loss1) + loss2\n",
        "            else:\n",
        "                loss = loss1\n",
        "\n",
        "            total_loss = torch.sum(loss)\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                improved_mask = loss < best_loss[active_mask]\n",
        "                best_loss[active_mask] = torch.where(improved_mask, loss, best_loss[active_mask])\n",
        "                no_improvement_count[active_mask] = torch.where(improved_mask,\n",
        "                    torch.zeros_like(no_improvement_count[active_mask]),\n",
        "                    no_improvement_count[active_mask] + 1)\n",
        "\n",
        "                improved_mask2 = (loss1 < o_best_loss1[active_mask])\n",
        "                o_best_loss1[active_mask] = torch.where(improved_mask2, loss1, o_best_loss1[active_mask])\n",
        "                o_best_unsuccessfull_attack[active_mask] = torch.where(improved_mask2.unsqueeze(-1), newimg, o_best_unsuccessfull_attack[active_mask])\n",
        "\n",
        "                update_mask = (l2dist < bestl2[active_mask]) & compare(output, active_labs_tensor, confidence, targeted)\n",
        "                bestl2[active_mask] = torch.where(update_mask, l2dist, bestl2[active_mask])\n",
        "                bestscore[active_mask] = torch.where(update_mask, torch.argmax(output, dim=1), bestscore[active_mask])\n",
        "\n",
        "                update_mask2 = (l2dist < o_bestl2[active_mask]) & compare(output, active_labs_tensor, confidence, targeted)\n",
        "                o_bestl2[active_mask] = torch.where(update_mask2, l2dist, o_bestl2[active_mask])\n",
        "                o_bestscore[active_mask] = torch.where(update_mask2, torch.argmax(output, dim=1), o_bestscore[active_mask])\n",
        "                o_bestattack[active_mask] = torch.where(update_mask2.unsqueeze(-1), newimg, o_bestattack[active_mask])\n",
        "\n",
        "                # Skip the rest of the loop for samples that haven't improved in 50 iterations\n",
        "                #active_mask[active_mask.clone()] = no_improvement_count[active_mask.clone()] < 50\n",
        "\n",
        "        if outer_step != (binary_search_steps - 1):\n",
        "            success_mask = (bestscore != -1) & compare(bestscore, labs_tensor, confidence, targeted)\n",
        "            o_success_attack  = torch.logical_or(o_success_attack, success_mask)\n",
        "            print('o_success_attack ', o_success_attack.sum())\n",
        "\n",
        "            failure_mask = ~success_mask\n",
        "            upper_bound[success_mask] = torch.minimum(upper_bound[success_mask], CONST[success_mask])\n",
        "            lower_bound[failure_mask] = torch.maximum(lower_bound[failure_mask], CONST[failure_mask])\n",
        "\n",
        "            upper_limit_mask = upper_bound < 1e9\n",
        "            CONST[upper_limit_mask] = (lower_bound[upper_limit_mask] + upper_bound[upper_limit_mask]) / 2\n",
        "            CONST[failure_mask & ~upper_limit_mask] *= cons_increase_factor\n",
        "\n",
        "    success_attack_mask = (o_bestl2 != 1e10)\n",
        "    final_output = torch.where(success_attack_mask.unsqueeze(-1), o_bestattack, o_best_unsuccessfull_attack)\n",
        "\n",
        "    return final_output\n"
      ],
      "metadata": {
        "id": "G8j_WzFAqFFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f171aad3-4a87-458d-da5c-ca28cb5c76fa",
        "id": "YzqV6tDuqxzL"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.1000, 0.1000, 0.1000,  ..., 0.1000, 0.1000, 0.1000], device='cuda:0')\n",
            "Iteration 2000: Current Learning Rate: 0.060100000000000015\n",
            "Iteration 4000: Current Learning Rate: 0.020099999999999996\n",
            "Iteration 6000: Current Learning Rate: 0.04009000000000001\n",
            "Iteration 8000: Current Learning Rate: 0.020089999999999986\n",
            "Iteration 10000: Current Learning Rate: 0.006343750000005557\n",
            "Iteration 12000: Current Learning Rate: 0.01509\n",
            "Iteration 14000: Current Learning Rate: 0.005090000000000002\n",
            "Iteration 16000: Current Learning Rate: 0.0100925\n",
            "Iteration 18000: Current Learning Rate: 0.005092500000000004\n",
            "Iteration 20000: Current Learning Rate: 0.004782812499997237\n",
            "o_success_attack  tensor(17, device='cuda:0')\n",
            "tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0')\n",
            "Iteration 2000: Current Learning Rate: 0.060100000000000015\n",
            "Iteration 4000: Current Learning Rate: 0.020099999999999996\n",
            "Iteration 6000: Current Learning Rate: 0.04009000000000001\n",
            "Iteration 8000: Current Learning Rate: 0.020089999999999986\n",
            "Iteration 10000: Current Learning Rate: 0.006343750000005557\n",
            "Iteration 12000: Current Learning Rate: 0.01509\n",
            "Iteration 14000: Current Learning Rate: 0.005090000000000002\n",
            "Iteration 16000: Current Learning Rate: 0.0100925\n",
            "Iteration 18000: Current Learning Rate: 0.005092500000000004\n",
            "Iteration 20000: Current Learning Rate: 0.004782812499997237\n",
            "o_success_attack  tensor(50, device='cuda:0')\n",
            "tensor([10., 10., 10.,  ..., 10., 10., 10.], device='cuda:0')\n",
            "Iteration 2000: Current Learning Rate: 0.060100000000000015\n",
            "Iteration 4000: Current Learning Rate: 0.020099999999999996\n",
            "Iteration 6000: Current Learning Rate: 0.04009000000000001\n",
            "Iteration 8000: Current Learning Rate: 0.020089999999999986\n",
            "Iteration 10000: Current Learning Rate: 0.006343750000005557\n",
            "Iteration 12000: Current Learning Rate: 0.01509\n",
            "Iteration 14000: Current Learning Rate: 0.005090000000000002\n",
            "Iteration 16000: Current Learning Rate: 0.0100925\n",
            "Iteration 18000: Current Learning Rate: 0.005092500000000004\n",
            "Iteration 20000: Current Learning Rate: 0.004782812499997237\n",
            "o_success_attack  tensor(149, device='cuda:0')\n",
            "tensor([100., 100., 100.,  ..., 100., 100., 100.], device='cuda:0')\n",
            "Iteration 2000: Current Learning Rate: 0.060100000000000015\n",
            "Iteration 4000: Current Learning Rate: 0.020099999999999996\n",
            "Iteration 6000: Current Learning Rate: 0.04009000000000001\n",
            "Iteration 8000: Current Learning Rate: 0.020089999999999986\n",
            "Iteration 10000: Current Learning Rate: 0.006343750000005557\n",
            "Iteration 12000: Current Learning Rate: 0.01509\n",
            "Iteration 14000: Current Learning Rate: 0.005090000000000002\n",
            "Iteration 16000: Current Learning Rate: 0.0100925\n",
            "Iteration 18000: Current Learning Rate: 0.005092500000000004\n",
            "Iteration 20000: Current Learning Rate: 0.004782812499997237\n",
            "o_success_attack  tensor(200, device='cuda:0')\n",
            "tensor([  55., 1000., 1000.,  ..., 1000., 1000., 1000.], device='cuda:0')\n",
            "Iteration 2000: Current Learning Rate: 0.060100000000000015\n",
            "Iteration 4000: Current Learning Rate: 0.020099999999999996\n",
            "Iteration 6000: Current Learning Rate: 0.04009000000000001\n",
            "Iteration 8000: Current Learning Rate: 0.020089999999999986\n",
            "Iteration 10000: Current Learning Rate: 0.006343750000005557\n",
            "Iteration 12000: Current Learning Rate: 0.01509\n",
            "Iteration 14000: Current Learning Rate: 0.005090000000000002\n",
            "Iteration 16000: Current Learning Rate: 0.0100925\n",
            "Iteration 18000: Current Learning Rate: 0.005092500000000004\n",
            "Iteration 20000: Current Learning Rate: 0.004782812499997237\n",
            "o_success_attack  tensor(243, device='cuda:0')\n",
            "tensor([   32.5000, 10000.0000, 10000.0000,  ..., 10000.0000, 10000.0000,\n",
            "        10000.0000], device='cuda:0')\n",
            "Iteration 2000: Current Learning Rate: 0.060100000000000015\n",
            "Iteration 4000: Current Learning Rate: 0.020099999999999996\n",
            "Iteration 6000: Current Learning Rate: 0.04009000000000001\n",
            "Iteration 8000: Current Learning Rate: 0.020089999999999986\n",
            "Iteration 10000: Current Learning Rate: 0.006343750000005557\n",
            "Iteration 12000: Current Learning Rate: 0.01509\n",
            "Iteration 14000: Current Learning Rate: 0.005090000000000002\n",
            "Iteration 16000: Current Learning Rate: 0.0100925\n",
            "Iteration 18000: Current Learning Rate: 0.005092500000000004\n",
            "Iteration 20000: Current Learning Rate: 0.004782812499997237\n",
            "o_success_attack  tensor(257, device='cuda:0')\n",
            "tensor([2.1250e+01, 1.0000e+05, 1.0000e+05,  ..., 1.0000e+05, 1.0000e+05,\n",
            "        1.0000e+05], device='cuda:0')\n",
            "Iteration 2000: Current Learning Rate: 0.060100000000000015\n",
            "Iteration 4000: Current Learning Rate: 0.020099999999999996\n",
            "Iteration 6000: Current Learning Rate: 0.04009000000000001\n",
            "Iteration 8000: Current Learning Rate: 0.020089999999999986\n",
            "Iteration 10000: Current Learning Rate: 0.006343750000005557\n",
            "Iteration 12000: Current Learning Rate: 0.01509\n",
            "Iteration 14000: Current Learning Rate: 0.005090000000000002\n",
            "Iteration 16000: Current Learning Rate: 0.0100925\n",
            "Iteration 18000: Current Learning Rate: 0.005092500000000004\n",
            "Iteration 20000: Current Learning Rate: 0.004782812499997237\n",
            "o_success_attack  tensor(260, device='cuda:0')\n",
            "tensor([1.5625e+01, 1.0000e+06, 1.0000e+06,  ..., 1.0000e+06, 1.0000e+06,\n",
            "        1.0000e+06], device='cuda:0')\n",
            "Iteration 2000: Current Learning Rate: 0.060100000000000015\n",
            "Iteration 4000: Current Learning Rate: 0.020099999999999996\n",
            "Iteration 6000: Current Learning Rate: 0.04009000000000001\n",
            "Iteration 8000: Current Learning Rate: 0.020089999999999986\n",
            "Iteration 10000: Current Learning Rate: 0.006343750000005557\n",
            "Iteration 12000: Current Learning Rate: 0.01509\n",
            "Iteration 14000: Current Learning Rate: 0.005090000000000002\n",
            "Iteration 16000: Current Learning Rate: 0.0100925\n",
            "Iteration 18000: Current Learning Rate: 0.005092500000000004\n",
            "Iteration 20000: Current Learning Rate: 0.004782812499997237\n",
            "o_success_attack  tensor(266, device='cuda:0')\n",
            "tensor([1.2812e+01, 1.0000e+07, 1.0000e+07,  ..., 1.0000e+07, 1.0000e+07,\n",
            "        1.0000e+07], device='cuda:0')\n",
            "Iteration 2000: Current Learning Rate: 0.060100000000000015\n",
            "Iteration 4000: Current Learning Rate: 0.020099999999999996\n",
            "Iteration 6000: Current Learning Rate: 0.04009000000000001\n",
            "Iteration 8000: Current Learning Rate: 0.020089999999999986\n",
            "Iteration 10000: Current Learning Rate: 0.006343750000005557\n",
            "Iteration 12000: Current Learning Rate: 0.01509\n",
            "Iteration 14000: Current Learning Rate: 0.005090000000000002\n",
            "Iteration 16000: Current Learning Rate: 0.0100925\n",
            "Iteration 18000: Current Learning Rate: 0.005092500000000004\n",
            "Iteration 20000: Current Learning Rate: 0.004782812499997237\n",
            "o_success_attack  tensor(267, device='cuda:0')\n",
            "tensor([1.1406e+01, 1.0000e+08, 1.0000e+08,  ..., 1.0000e+08, 1.0000e+08,\n",
            "        1.0000e+08], device='cuda:0')\n",
            "Iteration 2000: Current Learning Rate: 0.060100000000000015\n",
            "Iteration 4000: Current Learning Rate: 0.020099999999999996\n",
            "Iteration 6000: Current Learning Rate: 0.04009000000000001\n",
            "Iteration 8000: Current Learning Rate: 0.020089999999999986\n",
            "Iteration 10000: Current Learning Rate: 0.006343750000005557\n",
            "Iteration 12000: Current Learning Rate: 0.01509\n",
            "Iteration 14000: Current Learning Rate: 0.005090000000000002\n",
            "Iteration 16000: Current Learning Rate: 0.0100925\n",
            "Iteration 18000: Current Learning Rate: 0.005092500000000004\n",
            "Iteration 20000: Current Learning Rate: 0.004782812499997237\n",
            "o_success_attack  tensor(268, device='cuda:0')\n",
            "tensor([1.2109e+01, 1.0000e+09, 1.0000e+09,  ..., 1.0000e+09, 1.0000e+09,\n",
            "        1.0000e+09], device='cuda:0')\n",
            "Iteration 2000: Current Learning Rate: 0.060100000000000015\n",
            "Iteration 4000: Current Learning Rate: 0.020099999999999996\n",
            "Iteration 6000: Current Learning Rate: 0.04009000000000001\n",
            "Iteration 8000: Current Learning Rate: 0.020089999999999986\n",
            "Iteration 10000: Current Learning Rate: 0.006343750000005557\n",
            "Iteration 12000: Current Learning Rate: 0.01509\n",
            "Iteration 14000: Current Learning Rate: 0.005090000000000002\n",
            "Iteration 16000: Current Learning Rate: 0.0100925\n",
            "Iteration 18000: Current Learning Rate: 0.005092500000000004\n",
            "Iteration 20000: Current Learning Rate: 0.004782812499997237\n",
            "1130\n",
            "271\n",
            "Accuracy of just malwares (without attack): 98.85% | Under attack: 76.02%.\n"
          ]
        }
      ],
      "source": [
        "#CW(mal_x_batch, mal_y_batch, model_AT_rFGSM, removal_array, max_iterations=1000, learning_rate=0.01, binary_search_steps=3, confidence=0.01, initial_const=0.1, device=device)\n",
        "attack_params =  {'device':device, 'confidence':0.1, 'removal_array':removal_array,'max_iterations': 20000, 'learning_rate': 0.3, 'initial_const': 0.1 , 'binary_search_steps' : 11, 'cons_increase_factor' : 10}\n",
        "adv = adv_predict(test_loader, combined_model, CW_l1, **attack_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K0stLn0zqxzM"
      },
      "outputs": [],
      "source": [
        "for x_test, y_test in test_loader:\n",
        "    x_test, y_test = x_test.to(torch.float32).to(device), y_test.to(device)\n",
        "    mal_x_batch, mal_y_batch = x_test[y_test.squeeze() == 1], y_test[y_test.squeeze() == 1]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation on non-rounded adversarial examples\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "combined_model.eval()\n",
        "\n",
        "# Get the model's predictions on adversarial examples\n",
        "outputs = combined_model(adv)\n",
        "y_pred = outputs.argmax(dim=1)\n",
        "\n",
        "# Calculate metrics\n",
        "num_malwares = len(mal_y_batch)\n",
        "num_successful_attacks = (y_pred == 0).sum().item()\n",
        "total_difference = (torch.abs(adv[y_pred == 0] - mal_x_batch[y_pred == 0])).sum().item()\n",
        "accuracy = (y_pred == 1).sum().item() / num_malwares * 100\n",
        "\n",
        "# Print results with clear formatting\n",
        "print(f\"Evaluation Results on NOT_Rounded Adversarial Examples\")\n",
        "print(f\"-----------------------------------------------\")\n",
        "print(f\"Number of malware samples: {num_malwares}\")\n",
        "print(f\"Number of successful attacks: {num_successful_attacks}\")\n",
        "print(f\"Total difference (L1 norm) between successful adversarial examples and original: {total_difference}\")\n",
        "print(f\"Accuracy of the model on malwares under attack: {accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b24ec66d-8165-491e-9f59-10678913ee8b",
        "id": "e3pfa6JqqxzM"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Results on NOT_Rounded Adversarial Examples\n",
            "-----------------------------------------------\n",
            "Number of malware samples: 1130\n",
            "Number of successful attacks: 271\n",
            "Total difference (L1 norm) between successful adversarial examples and original: 5706.3466796875\n",
            "Accuracy of the model on malwares under attack: 76.02%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation on rounded adversarial examples(naive technique)\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "combined_model.eval()\n",
        "\n",
        "# Apply the final rounding solution to adversarial examples\n",
        "adv_rounded = round_x(adv, round_threshold=0.5)\n",
        "\n",
        "# Get the model's predictions on the rounded adversarial examples\n",
        "outputs = combined_model(adv_rounded)\n",
        "y_pred = outputs.argmax(dim=1)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "num_malwares = len(mal_y_batch)\n",
        "num_successful_attacks = (y_pred == 0).sum().item()\n",
        "total_difference = (torch.abs(adv_rounded[y_pred == 0] - mal_x_batch[y_pred == 0])).sum().item()\n",
        "accuracy = (y_pred == 1).sum().item() / num_malwares * 100\n",
        "\n",
        "# Display the results with clear formatting\n",
        "print(f\"Evaluation Results on Rounded Adversarial Examples\")\n",
        "print(f\"-----------------------------------------------\")\n",
        "print(f\"Number of malware samples: {num_malwares}\")\n",
        "print(f\"Number of successful attacks: {num_successful_attacks}\")\n",
        "print(f\"Total L1 norm difference (successful adversarial examples vs. original): {total_difference}\")\n",
        "print(f\"Accuracy of the model on malware under attack: {accuracy:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57ff4876-31b5-4288-ef7e-ae7fe36aba79",
        "id": "z8BvhgMKqxzN"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Results on Rounded Adversarial Examples\n",
            "-----------------------------------------------\n",
            "Number of malware samples: 1130\n",
            "Number of successful attacks: 168\n",
            "Total L1 norm difference (successful adversarial examples vs. original): 2929.0\n",
            "Accuracy of the model on malware under attack: 85.13%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation on rounded adversarial examples\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "combined_model.eval()\n",
        "\n",
        "# Apply the final rounding solution to adversarial examples\n",
        "adv_rounded = final_rounding(mal_x_batch, adv, combined_model)\n",
        "\n",
        "# Get the model's predictions on the rounded adversarial examples\n",
        "outputs = combined_model(adv_rounded)\n",
        "y_pred = outputs.argmax(dim=1)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "num_malwares = len(mal_y_batch)\n",
        "num_successful_attacks = (y_pred == 0).sum().item()\n",
        "total_difference = (torch.abs(adv_rounded[y_pred == 0] - mal_x_batch[y_pred == 0])).sum().item()\n",
        "accuracy = (y_pred == 1).sum().item() / num_malwares * 100\n",
        "\n",
        "# Display the results with clear formatting\n",
        "print(f\"Evaluation Results on Rounded Adversarial Examples\")\n",
        "print(f\"-----------------------------------------------\")\n",
        "print(f\"Number of malware samples: {num_malwares}\")\n",
        "print(f\"Number of successful attacks: {num_successful_attacks}\")\n",
        "print(f\"Total L1 norm difference (successful adversarial examples vs. original): {total_difference}\")\n",
        "print(f\"Accuracy of the model on malware under attack: {accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d024d0b-8f05-47db-ffb9-06117329b28d",
        "id": "yfdsrVcuqxzN"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Results on Rounded Adversarial Examples\n",
            "-----------------------------------------------\n",
            "Number of malware samples: 1130\n",
            "Number of successful attacks: 236\n",
            "Total L1 norm difference (successful adversarial examples vs. original): 4326.0\n",
            "Accuracy of the model on malware under attack: 79.12%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure that fixed elements are not changed\n",
        "\n",
        "# Ensure `removal_array` is expanded to match the batch size\n",
        "expanded_removal_array = removal_array.expand(mal_x_batch.shape[0], -1)\n",
        "\n",
        "# Create a mask where non-fixed features are set to 1\n",
        "non_fixed_features_mask = torch.bitwise_or(expanded_removal_array, 1 - mal_x_batch.to(torch.uint8))\n",
        "\n",
        "# Calculate indices where the mask is 0 (fixed) or 1 (non-fixed)\n",
        "mask_fixed = non_fixed_features_mask == 0\n",
        "mask_non_fixed = non_fixed_features_mask == 1\n",
        "\n",
        "# Calculate and print the differences\n",
        "diff_adv_fixed = (torch.abs(adv - mal_x_batch) * mask_fixed).sum().item()\n",
        "diff_adv_non_fixed = (torch.abs(adv - mal_x_batch) * mask_non_fixed).sum().item()\n",
        "diff_adv_rounded_fixed = (torch.abs(adv_rounded - mal_x_batch) * mask_fixed).sum().item()\n",
        "diff_adv_rounded_non_fixed = (torch.abs(adv_rounded - mal_x_batch) * mask_non_fixed).sum().item()\n",
        "\n",
        "# Display results with clear formatting\n",
        "print(f\"Differences when mask is 0 (fixed features):\")\n",
        "print(f\"  Adv vs. Original: {diff_adv_fixed}\")\n",
        "print(f\"  Rounded Adv vs. Original: {diff_adv_rounded_fixed}\")\n",
        "print(f\"Differences when mask is 1 (non-fixed features):\")\n",
        "print(f\"  Adv vs. Original: {diff_adv_non_fixed}\")\n",
        "print(f\"  Rounded Adv vs. Original: {diff_adv_rounded_non_fixed}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44aeed7b-8dfd-430f-8514-45aa5eb6224c",
        "id": "tppRKFXbqxzN"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Differences when mask is 0 (fixed features):\n",
            "  Adv vs. Original: 0.0\n",
            "  Rounded Adv vs. Original: 0.0\n",
            "Differences when mask is 1 (non-fixed features):\n",
            "  Adv vs. Original: 59452.9296875\n",
            "  Rounded Adv vs. Original: 61321.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8u2DhnyWCRkJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Wa5mDeGFCRe7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "from torch.optim.lr_scheduler import CyclicLR\n",
        "\n",
        "def compare(output, target, confidence, targeted):\n",
        "    \"\"\"\n",
        "    Compares the output with the target for a targeted or untargeted attack.\n",
        "\n",
        "    For targeted attacks, it checks if the output class matches the target class.\n",
        "    For untargeted attacks, it checks if the output class does not match the target class.\n",
        "    \"\"\"\n",
        "    if not isinstance(output, (float, int, torch.LongTensor, torch.cuda.LongTensor)):\n",
        "        output = torch.clone(output).detach()\n",
        "        if targeted:\n",
        "            output[target] -= 0.001  # Slightly reduce the confidence for targeted attacks\n",
        "        else:\n",
        "            output[target] += 0.001  # Slightly increase the confidence for untargeted attacks\n",
        "        output = torch.argmax(output, dim=1)\n",
        "    return output == target if targeted else output != target\n",
        "\n",
        "def sin_func_torch(input_tensor):\n",
        "    \"\"\"\n",
        "    A sine-based transformation applied to the input tensor.\n",
        "\n",
        "    The function applies a sine transformation and rescales the output to [0, 1].\n",
        "    \"\"\"\n",
        "    return (torch.sin((input_tensor - 0.5) * torch.pi) + 1) / 2.0\n",
        "\n",
        "def CW_l1(imgs, y, model, removal_array, max_iterations=1000, learning_rate=0.01,\n",
        "       binary_search_steps=3, confidence=0.01, initial_const=0.1,\n",
        "       cons_increase_factor=2, device='cuda'):\n",
        "    \"\"\"\n",
        "    Implements the Carlini & Wagner (CW) L1 attack.\n",
        "\n",
        "    Parameters:\n",
        "    - imgs: The original images to be attacked.\n",
        "    - y: The true labels of the images.\n",
        "    - model: The target model to attack.\n",
        "    - removal_array: A mask array specifying which features are fixed.\n",
        "    - max_iterations: The maximum number of iterations for the attack.\n",
        "    - learning_rate: The learning rate for the optimizer.\n",
        "    - binary_search_steps: The number of steps for the binary search over the constant.\n",
        "    - confidence: Confidence value for the attack.\n",
        "    - initial_const: The initial constant for the attack loss.\n",
        "    - cons_increase_factor: The factor by which the constant is increased during binary search.\n",
        "    - device: The device on which to perform computations.\n",
        "\n",
        "    Returns:\n",
        "    - final_output: The perturbed images after the attack.\n",
        "    \"\"\"\n",
        "\n",
        "    labs = torch.zeros_like(y.squeeze(), dtype=torch.int64)\n",
        "    batch_size = imgs.shape[0]\n",
        "    targeted = True\n",
        "\n",
        "    # Expand removal array to match the batch size\n",
        "    expanded_removal_array = removal_array.expand(imgs.shape[0], -1)\n",
        "    non_fixed_features_mask = torch.bitwise_or(expanded_removal_array, 1 - imgs.to(torch.uint8))\n",
        "\n",
        "    mask_0 = non_fixed_features_mask == 0\n",
        "    mask_1 = non_fixed_features_mask == 1\n",
        "\n",
        "    imgs_tensor = imgs.clone().detach().to(dtype=torch.float32, device=device)\n",
        "    labs_tensor = labs.clone().detach().to(dtype=torch.int64, device=device)\n",
        "\n",
        "    lower_bound = torch.zeros(batch_size, dtype=torch.float32, device=device)\n",
        "    CONST = torch.full((batch_size,), initial_const, dtype=torch.float32, device=device)\n",
        "    upper_bound = torch.full((batch_size,), 1e10, dtype=torch.float32, device=device)\n",
        "\n",
        "    o_bestl2 = torch.full((batch_size,), 1e10, device=device)\n",
        "    o_bestscore = torch.full((batch_size,), -1, device=device)\n",
        "    o_bestattack = imgs.clone().detach()\n",
        "\n",
        "    o_best_loss1 = torch.full((batch_size,), float('inf'), device=device)\n",
        "    o_best_unsuccessfull_attack = imgs.clone().detach()\n",
        "    o_success_attack = torch.zeros(batch_size, dtype=torch.bool, device=device)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    for outer_step in range(binary_search_steps):\n",
        "        print(CONST)\n",
        "\n",
        "        modifier = torch.zeros_like(imgs_tensor, requires_grad=True).to(device)\n",
        "        optimizer = torch.optim.Adam([modifier], lr=learning_rate)\n",
        "\n",
        "        def grad_hook(grad):\n",
        "            return grad * non_fixed_features_mask\n",
        "        modifier.register_hook(grad_hook)\n",
        "\n",
        "        scheduler = CyclicLR(optimizer, base_lr=0.0001, max_lr=0.1, step_size_up=4,\n",
        "                             step_size_down=4995, mode='triangular2')\n",
        "\n",
        "        best_loss = torch.full((batch_size,), float('inf'), device=device)\n",
        "        no_improvement_count = torch.zeros(batch_size, dtype=torch.int, device=device)\n",
        "        active_mask = torch.ones(batch_size, dtype=torch.bool, device=device)\n",
        "\n",
        "        bestl2 = torch.full((batch_size,), 1e10, device=device)\n",
        "        bestscore = torch.full((batch_size,), -1, device=device)\n",
        "\n",
        "        for iteration in range(max_iterations):\n",
        "            if (iteration + 1) % 2000 == 0:\n",
        "                current_lr = scheduler.get_last_lr()[0]\n",
        "                print(f\"Iteration {iteration + 1}: Current Learning Rate: {current_lr}\")\n",
        "\n",
        "            active_indices = torch.where(active_mask)[0]\n",
        "            active_modifier = modifier[active_mask]\n",
        "            active_imgs_tensor = imgs_tensor[active_mask]\n",
        "            active_labs_tensor = labs_tensor[active_mask]\n",
        "            active_CONST = CONST[active_mask]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            modified_input = active_imgs_tensor + active_modifier\n",
        "            newimg = sin_func_torch(modified_input)\n",
        "            output = model(newimg)\n",
        "\n",
        "            epsilon = 1e-1\n",
        "            perturbation = torch.abs(active_modifier)\n",
        "            l2dist = torch.sum(perturbation / (perturbation + epsilon), dim=1)\n",
        "            #torch.sum(torch.abs(newimg - imgs[active_mask]), dim=1)\n",
        "\n",
        "            real = output[torch.arange(active_indices.size(0)), active_labs_tensor]\n",
        "            other = output[torch.arange(active_indices.size(0)), active_labs_tensor + 1]\n",
        "\n",
        "            loss1 = torch.clamp((other - real + confidence), min=0.0)\n",
        "            loss2 = l2dist\n",
        "\n",
        "            if outer_step != (binary_search_steps - 1):\n",
        "                loss = (active_CONST * loss1) + loss2\n",
        "            else:\n",
        "                loss = loss1\n",
        "\n",
        "            total_loss = torch.sum(loss)\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                improved_mask = loss < best_loss[active_mask]\n",
        "                best_loss[active_mask] = torch.where(improved_mask, loss, best_loss[active_mask])\n",
        "                no_improvement_count[active_mask] = torch.where(improved_mask,\n",
        "                    torch.zeros_like(no_improvement_count[active_mask]),\n",
        "                    no_improvement_count[active_mask] + 1)\n",
        "\n",
        "                improved_mask2 = (loss1 < o_best_loss1[active_mask])\n",
        "                o_best_loss1[active_mask] = torch.where(improved_mask2, loss1, o_best_loss1[active_mask])\n",
        "                o_best_unsuccessfull_attack[active_mask] = torch.where(improved_mask2.unsqueeze(-1), newimg, o_best_unsuccessfull_attack[active_mask])\n",
        "\n",
        "                update_mask = (l2dist < bestl2[active_mask]) & compare(output, active_labs_tensor, confidence, targeted)\n",
        "                bestl2[active_mask] = torch.where(update_mask, l2dist, bestl2[active_mask])\n",
        "                bestscore[active_mask] = torch.where(update_mask, torch.argmax(output, dim=1), bestscore[active_mask])\n",
        "\n",
        "                update_mask2 = (l2dist < o_bestl2[active_mask]) & compare(output, active_labs_tensor, confidence, targeted)\n",
        "                o_bestl2[active_mask] = torch.where(update_mask2, l2dist, o_bestl2[active_mask])\n",
        "                o_bestscore[active_mask] = torch.where(update_mask2, torch.argmax(output, dim=1), o_bestscore[active_mask])\n",
        "                o_bestattack[active_mask] = torch.where(update_mask2.unsqueeze(-1), newimg, o_bestattack[active_mask])\n",
        "\n",
        "                # Skip the rest of the loop for samples that haven't improved in 50 iterations\n",
        "                #active_mask[active_mask.clone()] = no_improvement_count[active_mask.clone()] < 50\n",
        "\n",
        "        if outer_step != (binary_search_steps - 1):\n",
        "            success_mask = (bestscore != -1) & compare(bestscore, labs_tensor, confidence, targeted)\n",
        "            o_success_attack  = torch.logical_or(o_success_attack, success_mask)\n",
        "            print('o_success_attack ', o_success_attack.sum())\n",
        "\n",
        "            failure_mask = ~success_mask\n",
        "            upper_bound[success_mask] = torch.minimum(upper_bound[success_mask], CONST[success_mask])\n",
        "            lower_bound[failure_mask] = torch.maximum(lower_bound[failure_mask], CONST[failure_mask])\n",
        "\n",
        "            upper_limit_mask = upper_bound < 1e9\n",
        "            CONST[upper_limit_mask] = (lower_bound[upper_limit_mask] + upper_bound[upper_limit_mask]) / 2\n",
        "            CONST[failure_mask & ~upper_limit_mask] *= cons_increase_factor\n",
        "\n",
        "    success_attack_mask = (o_bestl2 != 1e10)\n",
        "    final_output = torch.where(success_attack_mask.unsqueeze(-1), o_bestattack, o_best_unsuccessfull_attack)\n",
        "\n",
        "    return final_output\n"
      ],
      "metadata": {
        "id": "jGXG6IMbCSLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19e97455-d260-4497-e856-d15d0a27c19d",
        "id": "KD5McQZ7CSLX"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.1000, 0.1000, 0.1000,  ..., 0.1000, 0.1000, 0.1000], device='cuda:0')\n",
            "Iteration 2000: Current Learning Rate: 0.060100000000000015\n",
            "Iteration 4000: Current Learning Rate: 0.020099999999999996\n",
            "Iteration 6000: Current Learning Rate: 0.04009000000000001\n",
            "Iteration 8000: Current Learning Rate: 0.020089999999999986\n",
            "Iteration 10000: Current Learning Rate: 0.006343750000005557\n",
            "Iteration 12000: Current Learning Rate: 0.01509\n",
            "Iteration 14000: Current Learning Rate: 0.005090000000000002\n",
            "Iteration 16000: Current Learning Rate: 0.0100925\n",
            "Iteration 18000: Current Learning Rate: 0.005092500000000004\n",
            "Iteration 20000: Current Learning Rate: 0.004782812499997237\n",
            "o_success_attack  tensor(13, device='cuda:0')\n",
            "tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0')\n",
            "Iteration 2000: Current Learning Rate: 0.060100000000000015\n",
            "Iteration 4000: Current Learning Rate: 0.020099999999999996\n",
            "Iteration 6000: Current Learning Rate: 0.04009000000000001\n",
            "Iteration 8000: Current Learning Rate: 0.020089999999999986\n",
            "Iteration 10000: Current Learning Rate: 0.006343750000005557\n",
            "Iteration 12000: Current Learning Rate: 0.01509\n",
            "Iteration 14000: Current Learning Rate: 0.005090000000000002\n",
            "Iteration 16000: Current Learning Rate: 0.0100925\n",
            "Iteration 18000: Current Learning Rate: 0.005092500000000004\n",
            "Iteration 20000: Current Learning Rate: 0.004782812499997237\n",
            "o_success_attack  tensor(35, device='cuda:0')\n",
            "tensor([10., 10., 10.,  ..., 10., 10., 10.], device='cuda:0')\n",
            "Iteration 2000: Current Learning Rate: 0.060100000000000015\n",
            "Iteration 4000: Current Learning Rate: 0.020099999999999996\n",
            "Iteration 6000: Current Learning Rate: 0.04009000000000001\n",
            "Iteration 8000: Current Learning Rate: 0.020089999999999986\n",
            "Iteration 10000: Current Learning Rate: 0.006343750000005557\n",
            "Iteration 12000: Current Learning Rate: 0.01509\n",
            "Iteration 14000: Current Learning Rate: 0.005090000000000002\n",
            "Iteration 16000: Current Learning Rate: 0.0100925\n",
            "Iteration 18000: Current Learning Rate: 0.005092500000000004\n",
            "Iteration 20000: Current Learning Rate: 0.004782812499997237\n",
            "o_success_attack  tensor(123, device='cuda:0')\n",
            "tensor([100., 100., 100.,  ..., 100., 100., 100.], device='cuda:0')\n",
            "Iteration 2000: Current Learning Rate: 0.060100000000000015\n",
            "Iteration 4000: Current Learning Rate: 0.020099999999999996\n",
            "Iteration 6000: Current Learning Rate: 0.04009000000000001\n",
            "Iteration 8000: Current Learning Rate: 0.020089999999999986\n",
            "Iteration 10000: Current Learning Rate: 0.006343750000005557\n",
            "Iteration 12000: Current Learning Rate: 0.01509\n",
            "Iteration 14000: Current Learning Rate: 0.005090000000000002\n",
            "Iteration 16000: Current Learning Rate: 0.0100925\n",
            "Iteration 18000: Current Learning Rate: 0.005092500000000004\n",
            "Iteration 20000: Current Learning Rate: 0.004782812499997237\n",
            "o_success_attack  tensor(158, device='cuda:0')\n",
            "tensor([1000., 1000., 1000.,  ..., 1000., 1000., 1000.], device='cuda:0')\n",
            "Iteration 2000: Current Learning Rate: 0.060100000000000015\n",
            "Iteration 4000: Current Learning Rate: 0.020099999999999996\n",
            "Iteration 6000: Current Learning Rate: 0.04009000000000001\n",
            "Iteration 8000: Current Learning Rate: 0.020089999999999986\n",
            "Iteration 10000: Current Learning Rate: 0.006343750000005557\n",
            "Iteration 12000: Current Learning Rate: 0.01509\n",
            "Iteration 14000: Current Learning Rate: 0.005090000000000002\n",
            "Iteration 16000: Current Learning Rate: 0.0100925\n",
            "Iteration 18000: Current Learning Rate: 0.005092500000000004\n",
            "Iteration 20000: Current Learning Rate: 0.004782812499997237\n",
            "o_success_attack  tensor(203, device='cuda:0')\n",
            "tensor([10000., 10000., 10000.,  ..., 10000., 10000., 10000.], device='cuda:0')\n",
            "Iteration 2000: Current Learning Rate: 0.060100000000000015\n",
            "Iteration 4000: Current Learning Rate: 0.020099999999999996\n",
            "Iteration 6000: Current Learning Rate: 0.04009000000000001\n",
            "Iteration 8000: Current Learning Rate: 0.020089999999999986\n",
            "Iteration 10000: Current Learning Rate: 0.006343750000005557\n",
            "Iteration 12000: Current Learning Rate: 0.01509\n",
            "Iteration 14000: Current Learning Rate: 0.005090000000000002\n",
            "Iteration 16000: Current Learning Rate: 0.0100925\n",
            "Iteration 18000: Current Learning Rate: 0.005092500000000004\n",
            "Iteration 20000: Current Learning Rate: 0.004782812499997237\n",
            "o_success_attack  tensor(235, device='cuda:0')\n",
            "tensor([  5500., 100000., 100000.,  ..., 100000., 100000., 100000.],\n",
            "       device='cuda:0')\n",
            "Iteration 2000: Current Learning Rate: 0.060100000000000015\n",
            "Iteration 4000: Current Learning Rate: 0.020099999999999996\n",
            "Iteration 6000: Current Learning Rate: 0.04009000000000001\n",
            "Iteration 8000: Current Learning Rate: 0.020089999999999986\n",
            "Iteration 10000: Current Learning Rate: 0.006343750000005557\n",
            "Iteration 12000: Current Learning Rate: 0.01509\n",
            "Iteration 14000: Current Learning Rate: 0.005090000000000002\n",
            "Iteration 16000: Current Learning Rate: 0.0100925\n",
            "Iteration 18000: Current Learning Rate: 0.005092500000000004\n",
            "Iteration 20000: Current Learning Rate: 0.004782812499997237\n",
            "o_success_attack  tensor(248, device='cuda:0')\n",
            "tensor([   3250., 1000000., 1000000.,  ..., 1000000., 1000000., 1000000.],\n",
            "       device='cuda:0')\n",
            "Iteration 2000: Current Learning Rate: 0.060100000000000015\n",
            "Iteration 4000: Current Learning Rate: 0.020099999999999996\n",
            "Iteration 6000: Current Learning Rate: 0.04009000000000001\n",
            "Iteration 8000: Current Learning Rate: 0.020089999999999986\n",
            "Iteration 10000: Current Learning Rate: 0.006343750000005557\n",
            "Iteration 12000: Current Learning Rate: 0.01509\n",
            "Iteration 14000: Current Learning Rate: 0.005090000000000002\n",
            "Iteration 16000: Current Learning Rate: 0.0100925\n",
            "Iteration 18000: Current Learning Rate: 0.005092500000000004\n",
            "Iteration 20000: Current Learning Rate: 0.004782812499997237\n",
            "o_success_attack  tensor(251, device='cuda:0')\n",
            "tensor([2.1250e+03, 1.0000e+07, 1.0000e+07,  ..., 1.0000e+07, 1.0000e+07,\n",
            "        1.0000e+07], device='cuda:0')\n",
            "Iteration 2000: Current Learning Rate: 0.060100000000000015\n",
            "Iteration 4000: Current Learning Rate: 0.020099999999999996\n",
            "Iteration 6000: Current Learning Rate: 0.04009000000000001\n",
            "Iteration 8000: Current Learning Rate: 0.020089999999999986\n",
            "Iteration 10000: Current Learning Rate: 0.006343750000005557\n",
            "Iteration 12000: Current Learning Rate: 0.01509\n",
            "Iteration 14000: Current Learning Rate: 0.005090000000000002\n",
            "Iteration 16000: Current Learning Rate: 0.0100925\n",
            "Iteration 18000: Current Learning Rate: 0.005092500000000004\n",
            "Iteration 20000: Current Learning Rate: 0.004782812499997237\n",
            "o_success_attack  tensor(256, device='cuda:0')\n",
            "tensor([2.6875e+03, 1.0000e+08, 1.0000e+08,  ..., 1.0000e+08, 1.0000e+08,\n",
            "        1.0000e+08], device='cuda:0')\n",
            "Iteration 2000: Current Learning Rate: 0.060100000000000015\n",
            "Iteration 4000: Current Learning Rate: 0.020099999999999996\n",
            "Iteration 6000: Current Learning Rate: 0.04009000000000001\n",
            "Iteration 8000: Current Learning Rate: 0.020089999999999986\n",
            "Iteration 10000: Current Learning Rate: 0.006343750000005557\n",
            "Iteration 12000: Current Learning Rate: 0.01509\n",
            "Iteration 14000: Current Learning Rate: 0.005090000000000002\n",
            "Iteration 16000: Current Learning Rate: 0.0100925\n",
            "Iteration 18000: Current Learning Rate: 0.005092500000000004\n",
            "Iteration 20000: Current Learning Rate: 0.004782812499997237\n",
            "o_success_attack  tensor(257, device='cuda:0')\n",
            "tensor([2.4062e+03, 1.0000e+09, 1.0000e+09,  ..., 1.0000e+09, 1.0000e+09,\n",
            "        1.0000e+09], device='cuda:0')\n",
            "Iteration 2000: Current Learning Rate: 0.060100000000000015\n",
            "Iteration 4000: Current Learning Rate: 0.020099999999999996\n",
            "Iteration 6000: Current Learning Rate: 0.04009000000000001\n",
            "Iteration 8000: Current Learning Rate: 0.020089999999999986\n",
            "Iteration 10000: Current Learning Rate: 0.006343750000005557\n",
            "Iteration 12000: Current Learning Rate: 0.01509\n",
            "Iteration 14000: Current Learning Rate: 0.005090000000000002\n",
            "Iteration 16000: Current Learning Rate: 0.0100925\n",
            "Iteration 18000: Current Learning Rate: 0.005092500000000004\n",
            "Iteration 20000: Current Learning Rate: 0.004782812499997237\n",
            "1130\n",
            "263\n",
            "Accuracy of just malwares (without attack): 98.85% | Under attack: 76.73%.\n"
          ]
        }
      ],
      "source": [
        "#CW(mal_x_batch, mal_y_batch, model_AT_rFGSM, removal_array, max_iterations=1000, learning_rate=0.01, binary_search_steps=3, confidence=0.01, initial_const=0.1, device=device)\n",
        "attack_params =  {'device':device, 'confidence':0.1, 'removal_array':removal_array,'max_iterations': 20000, 'learning_rate': 0.3, 'initial_const': 0.1 , 'binary_search_steps' : 11, 'cons_increase_factor' : 10}\n",
        "adv = adv_predict(test_loader, combined_model, CW_l1, **attack_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VvO6vztoCSLY"
      },
      "outputs": [],
      "source": [
        "for x_test, y_test in test_loader:\n",
        "    x_test, y_test = x_test.to(torch.float32).to(device), y_test.to(device)\n",
        "    mal_x_batch, mal_y_batch = x_test[y_test.squeeze() == 1], y_test[y_test.squeeze() == 1]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation on non-rounded adversarial examples\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "combined_model.eval()\n",
        "\n",
        "# Get the model's predictions on adversarial examples\n",
        "outputs = combined_model(adv)\n",
        "y_pred = outputs.argmax(dim=1)\n",
        "\n",
        "# Calculate metrics\n",
        "num_malwares = len(mal_y_batch)\n",
        "num_successful_attacks = (y_pred == 0).sum().item()\n",
        "total_difference = (torch.abs(adv[y_pred == 0] - mal_x_batch[y_pred == 0])).sum().item()\n",
        "accuracy = (y_pred == 1).sum().item() / num_malwares * 100\n",
        "\n",
        "# Print results with clear formatting\n",
        "print(f\"Evaluation Results on NOT_Rounded Adversarial Examples\")\n",
        "print(f\"-----------------------------------------------\")\n",
        "print(f\"Number of malware samples: {num_malwares}\")\n",
        "print(f\"Number of successful attacks: {num_successful_attacks}\")\n",
        "print(f\"Total difference (L1 norm) between successful adversarial examples and original: {total_difference}\")\n",
        "print(f\"Accuracy of the model on malwares under attack: {accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7eca9ef6-d58b-44a8-b9c5-13d173878ac2",
        "id": "wi-D2XriCSLY"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Results on NOT_Rounded Adversarial Examples\n",
            "-----------------------------------------------\n",
            "Number of malware samples: 1130\n",
            "Number of successful attacks: 263\n",
            "Total difference (L1 norm) between successful adversarial examples and original: 6375.40771484375\n",
            "Accuracy of the model on malwares under attack: 76.73%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def l0_penalty(original_data, adv_data):\n",
        "    perturbation = original_data - adv_data\n",
        "    return torch.sum(perturbation != 0).float()\n",
        "\n",
        "l0_penalty(mal_x_batch[y_pred == 0], adv[y_pred == 0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vCX19BBsWTu3",
        "outputId": "dcc83db7-3a8a-4475-9651-4e879d6cbe8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(33640., device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation on rounded adversarial examples(naive technique)\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "combined_model.eval()\n",
        "\n",
        "# Apply the final rounding solution to adversarial examples\n",
        "adv_rounded = round_x(adv, round_threshold=0.5)\n",
        "\n",
        "# Get the model's predictions on the rounded adversarial examples\n",
        "outputs = combined_model(adv_rounded)\n",
        "y_pred = outputs.argmax(dim=1)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "num_malwares = len(mal_y_batch)\n",
        "num_successful_attacks = (y_pred == 0).sum().item()\n",
        "total_difference = (torch.abs(adv_rounded[y_pred == 0] - mal_x_batch[y_pred == 0])).sum().item()\n",
        "accuracy = (y_pred == 1).sum().item() / num_malwares * 100\n",
        "\n",
        "# Display the results with clear formatting\n",
        "print(f\"Evaluation Results on Rounded Adversarial Examples\")\n",
        "print(f\"-----------------------------------------------\")\n",
        "print(f\"Number of malware samples: {num_malwares}\")\n",
        "print(f\"Number of successful attacks: {num_successful_attacks}\")\n",
        "print(f\"Total L1 norm difference (successful adversarial examples vs. original): {total_difference}\")\n",
        "print(f\"Accuracy of the model on malware under attack: {accuracy:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36077f56-4b64-4a2d-fb23-67496c6304ac",
        "id": "y1BTWtPmCSLZ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Results on Rounded Adversarial Examples\n",
            "-----------------------------------------------\n",
            "Number of malware samples: 1130\n",
            "Number of successful attacks: 219\n",
            "Total L1 norm difference (successful adversarial examples vs. original): 5400.0\n",
            "Accuracy of the model on malware under attack: 80.62%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation on rounded adversarial examples\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "combined_model.eval()\n",
        "\n",
        "# Apply the final rounding solution to adversarial examples\n",
        "adv_rounded = final_rounding(mal_x_batch, adv, combined_model)\n",
        "\n",
        "# Get the model's predictions on the rounded adversarial examples\n",
        "outputs = combined_model(adv_rounded)\n",
        "y_pred = outputs.argmax(dim=1)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "num_malwares = len(mal_y_batch)\n",
        "num_successful_attacks = (y_pred == 0).sum().item()\n",
        "total_difference = (torch.abs(adv_rounded[y_pred == 0] - mal_x_batch[y_pred == 0])).sum().item()\n",
        "accuracy = (y_pred == 1).sum().item() / num_malwares * 100\n",
        "\n",
        "# Display the results with clear formatting\n",
        "print(f\"Evaluation Results on Rounded Adversarial Examples\")\n",
        "print(f\"-----------------------------------------------\")\n",
        "print(f\"Number of malware samples: {num_malwares}\")\n",
        "print(f\"Number of successful attacks: {num_successful_attacks}\")\n",
        "print(f\"Total L1 norm difference (successful adversarial examples vs. original): {total_difference}\")\n",
        "print(f\"Accuracy of the model on malware under attack: {accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d5ae5a3-58d2-4626-e493-015443bf7dd2",
        "id": "MOpGZHKICSLZ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Results on Rounded Adversarial Examples\n",
            "-----------------------------------------------\n",
            "Number of malware samples: 1130\n",
            "Number of successful attacks: 246\n",
            "Total L1 norm difference (successful adversarial examples vs. original): 5149.0\n",
            "Accuracy of the model on malware under attack: 78.23%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure that fixed elements are not changed\n",
        "\n",
        "# Ensure `removal_array` is expanded to match the batch size\n",
        "expanded_removal_array = removal_array.expand(mal_x_batch.shape[0], -1)\n",
        "\n",
        "# Create a mask where non-fixed features are set to 1\n",
        "non_fixed_features_mask = torch.bitwise_or(expanded_removal_array, 1 - mal_x_batch.to(torch.uint8))\n",
        "\n",
        "# Calculate indices where the mask is 0 (fixed) or 1 (non-fixed)\n",
        "mask_fixed = non_fixed_features_mask == 0\n",
        "mask_non_fixed = non_fixed_features_mask == 1\n",
        "\n",
        "# Calculate and print the differences\n",
        "diff_adv_fixed = (torch.abs(adv - mal_x_batch) * mask_fixed).sum().item()\n",
        "diff_adv_non_fixed = (torch.abs(adv - mal_x_batch) * mask_non_fixed).sum().item()\n",
        "diff_adv_rounded_fixed = (torch.abs(adv_rounded - mal_x_batch) * mask_fixed).sum().item()\n",
        "diff_adv_rounded_non_fixed = (torch.abs(adv_rounded - mal_x_batch) * mask_non_fixed).sum().item()\n",
        "\n",
        "# Display results with clear formatting\n",
        "print(f\"Differences when mask is 0 (fixed features):\")\n",
        "print(f\"  Adv vs. Original: {diff_adv_fixed}\")\n",
        "print(f\"  Rounded Adv vs. Original: {diff_adv_rounded_fixed}\")\n",
        "print(f\"Differences when mask is 1 (non-fixed features):\")\n",
        "print(f\"  Adv vs. Original: {diff_adv_non_fixed}\")\n",
        "print(f\"  Rounded Adv vs. Original: {diff_adv_rounded_non_fixed}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb63910b-1a89-433a-ef06-893ae1aa7d05",
        "id": "Exjp48F7CSLa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Differences when mask is 0 (fixed features):\n",
            "  Adv vs. Original: 0.0\n",
            "  Rounded Adv vs. Original: 0.0\n",
            "Differences when mask is 1 (non-fixed features):\n",
            "  Adv vs. Original: 61379.2109375\n",
            "  Rounded Adv vs. Original: 62537.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# custom function for loss\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "sparsity_loss = perturbation**2 / (perturbation**2 + epsilon**2)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "GehIpqUrhVHo"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LaTa9Q6ThSs1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "\n",
        "# Define parameters\n",
        "epsilon = 0.001\n",
        "perturbation = np.linspace(-1, 1, 10001)\n",
        "\n",
        "# Calculate sparsity loss\n",
        "sparsity_loss = perturbation**2 / (perturbation**2 + epsilon)\n",
        "\n",
        "# Plot sparsity loss\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(perturbation, sparsity_loss, label='Sparsity-Inducing Loss')\n",
        "plt.title('Sparsity-Inducing Loss vs. Perturbation')\n",
        "plt.xlabel('Perturbation')\n",
        "plt.ylabel('Sparsity-Inducing Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Convert perturbation to a PyTorch tensor\n",
        "perturbation_tensor = torch.tensor(perturbation, requires_grad=True)\n",
        "\n",
        "# Calculate sparsity loss in PyTorch\n",
        "sparsity_loss_tensor = perturbation_tensor**2 / (perturbation_tensor**2 + epsilon)\n",
        "\n",
        "# Calculate the gradient (numerical derivative) of the sparsity loss\n",
        "sparsity_loss_tensor.backward(torch.ones_like(perturbation_tensor))\n",
        "gradient = perturbation_tensor.grad\n",
        "\n",
        "# Plot the gradient\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(perturbation, gradient.detach().numpy(), label='Numerical Derivative (Gradient)')\n",
        "plt.title('Numerical Derivative of Sparsity-Inducing Loss')\n",
        "plt.xlabel('Perturbation')\n",
        "plt.ylabel('Gradient')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CSl6ZkU2ZLYn",
        "outputId": "5f613bbd-fb52-4a45-a8f1-23fe3559cd5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Perturbation at index 5000 (expected 0): {perturbation[5000]}\")\n",
        "print(f\"Gradient at index 5000 (corresponding to perturbation=0): {gradient[5000]}\")\n",
        "max_grad_index = gradient.argmax()\n",
        "max_grad_value = gradient[max_grad_index]\n",
        "corresponding_perturbation = perturbation[max_grad_index]\n",
        "print(f\"Perturbation corresponding to the maximum gradient: {corresponding_perturbation}\")\n",
        "print(f\"Maximum gradient: {max_grad_value}\")\n"
      ],
      "metadata": {
        "id": "1h12Nyw7mvPH",
        "outputId": "6a85ac16-e500-400f-f354-a82d9dab38b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perturbation at index 5000 (expected 0): 0.0\n",
            "Gradient at index 5000 (corresponding to perturbation=0): 0.0\n",
            "Perturbation corresponding to the maximum gradient: 0.018199999999999994\n",
            "Maximum gradient: 20.53944322404991\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "from torch.optim.lr_scheduler import CyclicLR\n",
        "\n",
        "def compare(output, target, confidence, targeted):\n",
        "    \"\"\"\n",
        "    Compares the output with the target for a targeted or untargeted attack.\n",
        "\n",
        "    For targeted attacks, it checks if the output class matches the target class.\n",
        "    For untargeted attacks, it checks if the output class does not match the target class.\n",
        "    \"\"\"\n",
        "    if not isinstance(output, (float, int, torch.LongTensor, torch.cuda.LongTensor)):\n",
        "        output = torch.clone(output).detach()\n",
        "        if targeted:\n",
        "            output[target] -= 0.001  # Slightly reduce the confidence for targeted attacks\n",
        "        else:\n",
        "            output[target] += 0.001  # Slightly increase the confidence for untargeted attacks\n",
        "        output = torch.argmax(output, dim=1)\n",
        "    return output == target if targeted else output != target\n",
        "\n",
        "def sin_func_torch(input_tensor):\n",
        "    \"\"\"\n",
        "    A sine-based transformation applied to the input tensor.\n",
        "\n",
        "    The function applies a sine transformation and rescales the output to [0, 1].\n",
        "    \"\"\"\n",
        "    return (torch.sin((input_tensor - 0.5) * torch.pi) + 1) / 2.0\n",
        "\n",
        "def CW_l1(imgs, y, model, removal_array, max_iterations=1000, learning_rate=0.01,\n",
        "       binary_search_steps=3, confidence=0.01, initial_const=0.1,\n",
        "       cons_increase_factor=2, device='cuda'):\n",
        "    \"\"\"\n",
        "    Implements the Carlini & Wagner (CW) L1 attack.\n",
        "\n",
        "    Parameters:\n",
        "    - imgs: The original images to be attacked.\n",
        "    - y: The true labels of the images.\n",
        "    - model: The target model to attack.\n",
        "    - removal_array: A mask array specifying which features are fixed.\n",
        "    - max_iterations: The maximum number of iterations for the attack.\n",
        "    - learning_rate: The learning rate for the optimizer.\n",
        "    - binary_search_steps: The number of steps for the binary search over the constant.\n",
        "    - confidence: Confidence value for the attack.\n",
        "    - initial_const: The initial constant for the attack loss.\n",
        "    - cons_increase_factor: The factor by which the constant is increased during binary search.\n",
        "    - device: The device on which to perform computations.\n",
        "\n",
        "    Returns:\n",
        "    - final_output: The perturbed images after the attack.\n",
        "    \"\"\"\n",
        "\n",
        "    labs = torch.zeros_like(y.squeeze(), dtype=torch.int64)\n",
        "    batch_size = imgs.shape[0]\n",
        "    targeted = True\n",
        "\n",
        "    # Expand removal array to match the batch size\n",
        "    expanded_removal_array = removal_array.expand(imgs.shape[0], -1)\n",
        "    non_fixed_features_mask = torch.bitwise_or(expanded_removal_array, 1 - imgs.to(torch.uint8))\n",
        "\n",
        "    mask_0 = non_fixed_features_mask == 0\n",
        "    mask_1 = non_fixed_features_mask == 1\n",
        "\n",
        "    imgs_tensor = imgs.clone().detach().to(dtype=torch.float32, device=device)\n",
        "    labs_tensor = labs.clone().detach().to(dtype=torch.int64, device=device)\n",
        "\n",
        "    lower_bound = torch.zeros(batch_size, dtype=torch.float32, device=device)\n",
        "    CONST = torch.full((batch_size,), initial_const, dtype=torch.float32, device=device)\n",
        "    upper_bound = torch.full((batch_size,), 1e10, dtype=torch.float32, device=device)\n",
        "\n",
        "    o_bestl2 = torch.full((batch_size,), 1e10, device=device)\n",
        "    o_bestscore = torch.full((batch_size,), -1, device=device)\n",
        "    o_bestattack = imgs.clone().detach()\n",
        "\n",
        "    o_best_loss1 = torch.full((batch_size,), float('inf'), device=device)\n",
        "    o_best_unsuccessfull_attack = imgs.clone().detach()\n",
        "    o_success_attack = torch.zeros(batch_size, dtype=torch.bool, device=device)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    for outer_step in range(binary_search_steps):\n",
        "        print(CONST)\n",
        "\n",
        "        modifier = torch.zeros_like(imgs_tensor, requires_grad=True).to(device)\n",
        "        optimizer = torch.optim.Adam([modifier], lr=learning_rate)\n",
        "\n",
        "        def grad_hook(grad):\n",
        "            return grad * non_fixed_features_mask\n",
        "        modifier.register_hook(grad_hook)\n",
        "\n",
        "        scheduler = CyclicLR(optimizer, base_lr=0.0001, max_lr=0.1, step_size_up=4,\n",
        "                             step_size_down=4995, mode='triangular2')\n",
        "\n",
        "        best_loss = torch.full((batch_size,), float('inf'), device=device)\n",
        "        no_improvement_count = torch.zeros(batch_size, dtype=torch.int, device=device)\n",
        "        active_mask = torch.ones(batch_size, dtype=torch.bool, device=device)\n",
        "\n",
        "        bestl2 = torch.full((batch_size,), 1e10, device=device)\n",
        "        bestscore = torch.full((batch_size,), -1, device=device)\n",
        "\n",
        "        for iteration in range(max_iterations):\n",
        "            if (iteration + 1) % 2000 == 0:\n",
        "                current_lr = scheduler.get_last_lr()[0]\n",
        "                print(f\"Iteration {iteration + 1}: Current Learning Rate: {current_lr}\")\n",
        "\n",
        "            active_indices = torch.where(active_mask)[0]\n",
        "            active_modifier = modifier[active_mask]\n",
        "            active_imgs_tensor = imgs_tensor[active_mask]\n",
        "            active_labs_tensor = labs_tensor[active_mask]\n",
        "            active_CONST = CONST[active_mask]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            modified_input = active_imgs_tensor + active_modifier\n",
        "            newimg = sin_func_torch(modified_input)\n",
        "            output = model(newimg)\n",
        "\n",
        "            epsilon = torch.tensor(0.001, device=newimg.device, dtype=newimg.dtype)\n",
        "            perturbation = (newimg - imgs[active_mask]).square()\n",
        "            l2dist = torch.sum(perturbation / (perturbation + epsilon), dim=1)\n",
        "            #torch.sum(perturbation / (perturbation + epsilon), dim=1)\n",
        "            #torch.sum(torch.abs(newimg - imgs[active_mask]), dim=1)\n",
        "\n",
        "            real = output[torch.arange(active_indices.size(0)), active_labs_tensor]\n",
        "            other = output[torch.arange(active_indices.size(0)), active_labs_tensor + 1]\n",
        "\n",
        "            loss1 = torch.clamp((other - real + confidence), min=0.0)\n",
        "            #loss2 = l2dist\n",
        "\n",
        "            if outer_step != (binary_search_steps - 1):\n",
        "                loss = (active_CONST * loss1) + (l2dist / 20.)\n",
        "            else:\n",
        "                loss = loss1\n",
        "\n",
        "            total_loss = torch.sum(loss)\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                improved_mask = loss < best_loss[active_mask]\n",
        "                best_loss[active_mask] = torch.where(improved_mask, loss, best_loss[active_mask])\n",
        "                no_improvement_count[active_mask] = torch.where(improved_mask,\n",
        "                    torch.zeros_like(no_improvement_count[active_mask]),\n",
        "                    no_improvement_count[active_mask] + 1)\n",
        "\n",
        "                improved_mask2 = (loss1 < o_best_loss1[active_mask])\n",
        "                o_best_loss1[active_mask] = torch.where(improved_mask2, loss1, o_best_loss1[active_mask])\n",
        "                o_best_unsuccessfull_attack[active_mask] = torch.where(improved_mask2.unsqueeze(-1), newimg, o_best_unsuccessfull_attack[active_mask])\n",
        "\n",
        "                update_mask = (l2dist < bestl2[active_mask]) & compare(output, active_labs_tensor, confidence, targeted)\n",
        "                bestl2[active_mask] = torch.where(update_mask, l2dist, bestl2[active_mask])\n",
        "                bestscore[active_mask] = torch.where(update_mask, torch.argmax(output, dim=1), bestscore[active_mask])\n",
        "\n",
        "                update_mask2 = (l2dist < o_bestl2[active_mask]) & compare(output, active_labs_tensor, confidence, targeted)\n",
        "                o_bestl2[active_mask] = torch.where(update_mask2, l2dist, o_bestl2[active_mask])\n",
        "                o_bestscore[active_mask] = torch.where(update_mask2, torch.argmax(output, dim=1), o_bestscore[active_mask])\n",
        "                o_bestattack[active_mask] = torch.where(update_mask2.unsqueeze(-1), newimg, o_bestattack[active_mask])\n",
        "\n",
        "                # Skip the rest of the loop for samples that haven't improved in 50 iterations\n",
        "                #active_mask[active_mask.clone()] = no_improvement_count[active_mask.clone()] < 50\n",
        "\n",
        "        if outer_step != (binary_search_steps - 1):\n",
        "            success_mask = (bestscore != -1) & compare(bestscore, labs_tensor, confidence, targeted)\n",
        "            o_success_attack  = torch.logical_or(o_success_attack, success_mask)\n",
        "            print('o_success_attack ', o_success_attack.sum())\n",
        "\n",
        "            failure_mask = ~success_mask\n",
        "            upper_bound[success_mask] = torch.minimum(upper_bound[success_mask], CONST[success_mask])\n",
        "            lower_bound[failure_mask] = torch.maximum(lower_bound[failure_mask], CONST[failure_mask])\n",
        "\n",
        "            upper_limit_mask = upper_bound < 1e10\n",
        "            CONST[upper_limit_mask] = (lower_bound[upper_limit_mask] + upper_bound[upper_limit_mask]) / 2\n",
        "            CONST[failure_mask & ~upper_limit_mask] *= cons_increase_factor\n",
        "\n",
        "    success_attack_mask = (o_bestl2 != 1e10)\n",
        "    final_output = torch.where(success_attack_mask.unsqueeze(-1), o_bestattack, o_best_unsuccessfull_attack)\n",
        "\n",
        "    return final_output\n"
      ],
      "metadata": {
        "id": "X8MmNXeNh0NV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf616249-a01a-479f-bf1e-75e90e3567b1",
        "id": "j8iqZ8deh0Nl"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.1000, 0.1000, 0.1000,  ..., 0.1000, 0.1000, 0.1000], device='cuda:0')\n",
            "Iteration 2000: Current Learning Rate: 0.060100000000000015\n",
            "Iteration 4000: Current Learning Rate: 0.020099999999999996\n",
            "Iteration 6000: Current Learning Rate: 0.04009000000000001\n",
            "Iteration 8000: Current Learning Rate: 0.020089999999999986\n",
            "Iteration 10000: Current Learning Rate: 0.006343750000005557\n",
            "Iteration 12000: Current Learning Rate: 0.01509\n",
            "Iteration 14000: Current Learning Rate: 0.005090000000000002\n",
            "Iteration 16000: Current Learning Rate: 0.0100925\n",
            "Iteration 18000: Current Learning Rate: 0.005092500000000004\n",
            "Iteration 20000: Current Learning Rate: 0.004782812499997237\n",
            "o_success_attack  tensor(34, device='cuda:0')\n",
            "tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0')\n",
            "Iteration 2000: Current Learning Rate: 0.060100000000000015\n",
            "Iteration 4000: Current Learning Rate: 0.020099999999999996\n",
            "Iteration 6000: Current Learning Rate: 0.04009000000000001\n",
            "Iteration 8000: Current Learning Rate: 0.020089999999999986\n",
            "Iteration 10000: Current Learning Rate: 0.006343750000005557\n",
            "Iteration 12000: Current Learning Rate: 0.01509\n",
            "Iteration 14000: Current Learning Rate: 0.005090000000000002\n",
            "Iteration 16000: Current Learning Rate: 0.0100925\n",
            "Iteration 18000: Current Learning Rate: 0.005092500000000004\n",
            "Iteration 20000: Current Learning Rate: 0.004782812499997237\n",
            "o_success_attack  tensor(98, device='cuda:0')\n",
            "tensor([10., 10., 10.,  ..., 10., 10., 10.], device='cuda:0')\n",
            "Iteration 2000: Current Learning Rate: 0.060100000000000015\n",
            "Iteration 4000: Current Learning Rate: 0.020099999999999996\n",
            "Iteration 6000: Current Learning Rate: 0.04009000000000001\n",
            "Iteration 8000: Current Learning Rate: 0.020089999999999986\n",
            "Iteration 10000: Current Learning Rate: 0.006343750000005557\n",
            "Iteration 12000: Current Learning Rate: 0.01509\n",
            "Iteration 14000: Current Learning Rate: 0.005090000000000002\n",
            "Iteration 16000: Current Learning Rate: 0.0100925\n",
            "Iteration 18000: Current Learning Rate: 0.005092500000000004\n",
            "Iteration 20000: Current Learning Rate: 0.004782812499997237\n",
            "o_success_attack  tensor(167, device='cuda:0')\n",
            "tensor([  5.5000, 100.0000, 100.0000,  ..., 100.0000, 100.0000, 100.0000],\n",
            "       device='cuda:0')\n",
            "Iteration 2000: Current Learning Rate: 0.060100000000000015\n",
            "Iteration 4000: Current Learning Rate: 0.020099999999999996\n",
            "Iteration 6000: Current Learning Rate: 0.04009000000000001\n",
            "Iteration 8000: Current Learning Rate: 0.020089999999999986\n",
            "Iteration 10000: Current Learning Rate: 0.006343750000005557\n",
            "Iteration 12000: Current Learning Rate: 0.01509\n",
            "Iteration 14000: Current Learning Rate: 0.005090000000000002\n",
            "Iteration 16000: Current Learning Rate: 0.0100925\n",
            "Iteration 18000: Current Learning Rate: 0.005092500000000004\n",
            "Iteration 20000: Current Learning Rate: 0.004782812499997237\n",
            "o_success_attack  tensor(222, device='cuda:0')\n",
            "tensor([   3.2500, 1000.0000, 1000.0000,  ..., 1000.0000, 1000.0000,\n",
            "        1000.0000], device='cuda:0')\n",
            "Iteration 2000: Current Learning Rate: 0.060100000000000015\n",
            "Iteration 4000: Current Learning Rate: 0.020099999999999996\n",
            "Iteration 6000: Current Learning Rate: 0.04009000000000001\n",
            "Iteration 8000: Current Learning Rate: 0.020089999999999986\n",
            "Iteration 10000: Current Learning Rate: 0.006343750000005557\n",
            "Iteration 12000: Current Learning Rate: 0.01509\n",
            "Iteration 14000: Current Learning Rate: 0.005090000000000002\n",
            "Iteration 16000: Current Learning Rate: 0.0100925\n",
            "Iteration 18000: Current Learning Rate: 0.005092500000000004\n",
            "Iteration 20000: Current Learning Rate: 0.004782812499997237\n",
            "o_success_attack  tensor(240, device='cuda:0')\n",
            "tensor([4.3750e+00, 1.0000e+04, 1.0000e+04,  ..., 1.0000e+04, 1.0000e+04,\n",
            "        1.0000e+04], device='cuda:0')\n",
            "Iteration 2000: Current Learning Rate: 0.060100000000000015\n",
            "Iteration 4000: Current Learning Rate: 0.020099999999999996\n",
            "Iteration 6000: Current Learning Rate: 0.04009000000000001\n",
            "Iteration 8000: Current Learning Rate: 0.020089999999999986\n",
            "Iteration 10000: Current Learning Rate: 0.006343750000005557\n",
            "Iteration 12000: Current Learning Rate: 0.01509\n",
            "Iteration 14000: Current Learning Rate: 0.005090000000000002\n",
            "Iteration 16000: Current Learning Rate: 0.0100925\n",
            "Iteration 18000: Current Learning Rate: 0.005092500000000004\n",
            "Iteration 20000: Current Learning Rate: 0.004782812499997237\n",
            "o_success_attack  tensor(252, device='cuda:0')\n",
            "tensor([3.8125e+00, 1.0000e+05, 1.0000e+05,  ..., 1.0000e+05, 1.0000e+05,\n",
            "        1.0000e+05], device='cuda:0')\n",
            "Iteration 2000: Current Learning Rate: 0.060100000000000015\n",
            "Iteration 4000: Current Learning Rate: 0.020099999999999996\n",
            "Iteration 6000: Current Learning Rate: 0.04009000000000001\n",
            "Iteration 8000: Current Learning Rate: 0.020089999999999986\n",
            "Iteration 10000: Current Learning Rate: 0.006343750000005557\n",
            "Iteration 12000: Current Learning Rate: 0.01509\n",
            "Iteration 14000: Current Learning Rate: 0.005090000000000002\n",
            "Iteration 16000: Current Learning Rate: 0.0100925\n",
            "Iteration 18000: Current Learning Rate: 0.005092500000000004\n",
            "Iteration 20000: Current Learning Rate: 0.004782812499997237\n",
            "o_success_attack  tensor(260, device='cuda:0')\n",
            "tensor([3.5312e+00, 1.0000e+06, 1.0000e+06,  ..., 1.0000e+06, 1.0000e+06,\n",
            "        1.0000e+06], device='cuda:0')\n",
            "Iteration 2000: Current Learning Rate: 0.060100000000000015\n",
            "Iteration 4000: Current Learning Rate: 0.020099999999999996\n",
            "Iteration 6000: Current Learning Rate: 0.04009000000000001\n",
            "Iteration 8000: Current Learning Rate: 0.020089999999999986\n",
            "Iteration 10000: Current Learning Rate: 0.006343750000005557\n",
            "Iteration 12000: Current Learning Rate: 0.01509\n",
            "Iteration 14000: Current Learning Rate: 0.005090000000000002\n",
            "Iteration 16000: Current Learning Rate: 0.0100925\n",
            "Iteration 18000: Current Learning Rate: 0.005092500000000004\n",
            "Iteration 20000: Current Learning Rate: 0.004782812499997237\n",
            "o_success_attack  tensor(263, device='cuda:0')\n",
            "tensor([3.3906e+00, 1.0000e+07, 1.0000e+07,  ..., 1.0000e+07, 1.0000e+07,\n",
            "        1.0000e+07], device='cuda:0')\n",
            "Iteration 2000: Current Learning Rate: 0.060100000000000015\n",
            "Iteration 4000: Current Learning Rate: 0.020099999999999996\n",
            "Iteration 6000: Current Learning Rate: 0.04009000000000001\n",
            "Iteration 8000: Current Learning Rate: 0.020089999999999986\n",
            "Iteration 10000: Current Learning Rate: 0.006343750000005557\n",
            "Iteration 12000: Current Learning Rate: 0.01509\n",
            "Iteration 14000: Current Learning Rate: 0.005090000000000002\n",
            "Iteration 16000: Current Learning Rate: 0.0100925\n",
            "Iteration 18000: Current Learning Rate: 0.005092500000000004\n",
            "Iteration 20000: Current Learning Rate: 0.004782812499997237\n",
            "1130\n",
            "269\n",
            "Accuracy of just malwares (without attack): 98.85% | Under attack: 76.19%.\n"
          ]
        }
      ],
      "source": [
        "#CW(mal_x_batch, mal_y_batch, model_AT_rFGSM, removal_array, max_iterations=1000, learning_rate=0.01, binary_search_steps=3, confidence=0.01, initial_const=0.1, device=device)\n",
        "attack_params =  {'device':device, 'confidence':0.01, 'removal_array':removal_array,'max_iterations': 20000, 'learning_rate': 0.3, 'initial_const': 0.1 , 'binary_search_steps' : 9, 'cons_increase_factor' : 10}\n",
        "adv = adv_predict(test_loader, combined_model, CW_l1, **attack_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gz8dOParh0Nm"
      },
      "outputs": [],
      "source": [
        "for x_test, y_test in test_loader:\n",
        "    x_test, y_test = x_test.to(torch.float32).to(device), y_test.to(device)\n",
        "    mal_x_batch, mal_y_batch = x_test[y_test.squeeze() == 1], y_test[y_test.squeeze() == 1]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation on non-rounded adversarial examples\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "combined_model.eval()\n",
        "\n",
        "# Get the model's predictions on adversarial examples\n",
        "outputs = combined_model(adv)\n",
        "y_pred = outputs.argmax(dim=1)\n",
        "\n",
        "# Calculate metrics\n",
        "num_malwares = len(mal_y_batch)\n",
        "num_successful_attacks = (y_pred == 0).sum().item()\n",
        "total_difference = (torch.abs(adv[y_pred == 0] - mal_x_batch[y_pred == 0])).sum().item()\n",
        "accuracy = (y_pred == 1).sum().item() / num_malwares * 100\n",
        "\n",
        "# Print results with clear formatting\n",
        "print(f\"Evaluation Results on NOT_Rounded Adversarial Examples\")\n",
        "print(f\"-----------------------------------------------\")\n",
        "print(f\"Number of malware samples: {num_malwares}\")\n",
        "print(f\"Number of successful attacks: {num_successful_attacks}\")\n",
        "print(f\"Total difference (L1 norm) between successful adversarial examples and original: {total_difference}\")\n",
        "print(f\"Accuracy of the model on malwares under attack: {accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "259d83e2-4e1f-44e2-818e-00db5565df90",
        "id": "dVPzJe6th0Nm"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Results on NOT_Rounded Adversarial Examples\n",
            "-----------------------------------------------\n",
            "Number of malware samples: 1130\n",
            "Number of successful attacks: 269\n",
            "Total difference (L1 norm) between successful adversarial examples and original: 7212.6298828125\n",
            "Accuracy of the model on malwares under attack: 76.19%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def l0_penalty(original_data, adv_data):\n",
        "    perturbation = original_data - adv_data\n",
        "    return torch.sum(perturbation != 0).float()\n",
        "\n",
        "l0_penalty(mal_x_batch[y_pred == 0], adv[y_pred == 0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fa6ae69-3f67-456f-8a5c-9b6d84573eac",
        "id": "npZh9hmxh0Nn"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(742406., device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation on rounded adversarial examples\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "combined_model.eval()\n",
        "\n",
        "# Apply the final rounding solution to adversarial examples\n",
        "adv_rounded = final_rounding(mal_x_batch, adv, combined_model)\n",
        "\n",
        "# Get the model's predictions on the rounded adversarial examples\n",
        "outputs = combined_model(adv_rounded)\n",
        "y_pred = outputs.argmax(dim=1)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "num_malwares = len(mal_y_batch)\n",
        "num_successful_attacks = (y_pred == 0).sum().item()\n",
        "total_difference = (torch.abs(adv_rounded[y_pred == 0] - mal_x_batch[y_pred == 0])).sum().item()\n",
        "accuracy = (y_pred == 1).sum().item() / num_malwares * 100\n",
        "\n",
        "# Display the results with clear formatting\n",
        "print(f\"Evaluation Results on Rounded Adversarial Examples\")\n",
        "print(f\"-----------------------------------------------\")\n",
        "print(f\"Number of malware samples: {num_malwares}\")\n",
        "print(f\"Number of successful attacks: {num_successful_attacks}\")\n",
        "print(f\"Total L1 norm difference (successful adversarial examples vs. original): {total_difference}\")\n",
        "print(f\"Accuracy of the model on malware under attack: {accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "861763c4-bc6c-495b-c595-93da08d615e7",
        "id": "bpL20p3Ch0No"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Results on Rounded Adversarial Examples\n",
            "-----------------------------------------------\n",
            "Number of malware samples: 1130\n",
            "Number of successful attacks: 249\n",
            "Total L1 norm difference (successful adversarial examples vs. original): 5944.0\n",
            "Accuracy of the model on malware under attack: 77.96%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure that fixed elements are not changed\n",
        "\n",
        "# Ensure `removal_array` is expanded to match the batch size\n",
        "expanded_removal_array = removal_array.expand(mal_x_batch.shape[0], -1)\n",
        "\n",
        "# Create a mask where non-fixed features are set to 1\n",
        "non_fixed_features_mask = torch.bitwise_or(expanded_removal_array, 1 - mal_x_batch.to(torch.uint8))\n",
        "\n",
        "# Calculate indices where the mask is 0 (fixed) or 1 (non-fixed)\n",
        "mask_fixed = non_fixed_features_mask == 0\n",
        "mask_non_fixed = non_fixed_features_mask == 1\n",
        "\n",
        "# Calculate and print the differences\n",
        "diff_adv_fixed = (torch.abs(adv - mal_x_batch) * mask_fixed).sum().item()\n",
        "diff_adv_non_fixed = (torch.abs(adv - mal_x_batch) * mask_non_fixed).sum().item()\n",
        "diff_adv_rounded_fixed = (torch.abs(adv_rounded - mal_x_batch) * mask_fixed).sum().item()\n",
        "diff_adv_rounded_non_fixed = (torch.abs(adv_rounded - mal_x_batch) * mask_non_fixed).sum().item()\n",
        "\n",
        "# Display results with clear formatting\n",
        "print(f\"Differences when mask is 0 (fixed features):\")\n",
        "print(f\"  Adv vs. Original: {diff_adv_fixed}\")\n",
        "print(f\"  Rounded Adv vs. Original: {diff_adv_rounded_fixed}\")\n",
        "print(f\"Differences when mask is 1 (non-fixed features):\")\n",
        "print(f\"  Adv vs. Original: {diff_adv_non_fixed}\")\n",
        "print(f\"  Rounded Adv vs. Original: {diff_adv_rounded_non_fixed}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a6b275f-113c-4ad2-f09c-b7fdbba49e38",
        "id": "2m87oJ8Eh0No"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Differences when mask is 0 (fixed features):\n",
            "  Adv vs. Original: 0.0\n",
            "  Rounded Adv vs. Original: 0.0\n",
            "Differences when mask is 1 (non-fixed features):\n",
            "  Adv vs. Original: 62959.68359375\n",
            "  Rounded Adv vs. Original: 64055.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = combined_model(adv)\n",
        "y_pred = outputs.argmax(dim=1)"
      ],
      "metadata": {
        "id": "ZShyve8sRPl-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adv[y_pred==0][0][:200]- mal_x_batch[y_pred==0][0][:200]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HqZwZW_IREWW",
        "outputId": "8e67e861-ba87-4783-a1ec-9a1d5dc28f14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00, -9.9935e-01,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  2.4378e-05,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  1.0000e+00,  1.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  1.1499e-01,  0.0000e+00,  0.0000e+00,  9.9976e-01,\n",
              "         0.0000e+00,  1.3968e-02,  0.0000e+00,  2.9802e-08,  0.0000e+00,\n",
              "         9.9998e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         2.9802e-08,  0.0000e+00,  0.0000e+00,  0.0000e+00,  3.2783e-06,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  7.1196e-01,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         1.0000e+00,  0.0000e+00,  4.3198e-03,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         5.9605e-08,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  6.5565e-07,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  1.3411e-06,  0.0000e+00,  0.0000e+00,\n",
              "         5.9009e-05,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  2.6786e-03,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  2.9802e-08,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "adv[y_pred==0][1][:200] - mal_x_batch[y_pred==0][1][:200]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXFrsn_XQy-_",
        "outputId": "6794f288-47c8-4280-d2f9-1a9775ce2ce6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.0000e+00,  1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  1.0000e+00,  8.9407e-06,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  2.9802e-08,  3.9041e-06,  1.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  1.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  1.0000e+00, -7.9274e-06,  0.0000e+00,  1.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00, -1.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  9.9927e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         1.0000e+00,  0.0000e+00,  1.0000e+00,  2.1726e-05,  1.0000e+00,\n",
              "         0.0000e+00,  1.0000e+00,  0.0000e+00, -1.1444e-05,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00, -1.0000e+00, -1.0000e+00,\n",
              "         0.0000e+00,  1.0000e+00,  1.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  1.3739e-05,\n",
              "         0.0000e+00,  1.0000e+00,  0.0000e+00,  2.9802e-08,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
              "         1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  3.9160e-05,  1.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  9.9339e-01,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -8.1062e-06,\n",
              "         0.0000e+00,  1.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00,\n",
              "         1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  1.4901e-06,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7zdkR12USbb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# custom function for loss2\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "sparsity_loss = perturbation**2 / (perturbation**2 + epsilon**2)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "UCJdcRAmSbnT"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M3rvkOviSbnT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "\n",
        "# Define parameters\n",
        "epsilon = 0.0001\n",
        "perturbation = np.linspace(-1, 1, 10001)\n",
        "\n",
        "# Calculate sparsity loss\n",
        "sparsity_loss = perturbation**2 / (perturbation**2 + epsilon)\n",
        "\n",
        "# Plot sparsity loss\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(perturbation, sparsity_loss, label='Sparsity-Inducing Loss')\n",
        "plt.title('Sparsity-Inducing Loss vs. Perturbation')\n",
        "plt.xlabel('Perturbation')\n",
        "plt.ylabel('Sparsity-Inducing Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Convert perturbation to a PyTorch tensor\n",
        "perturbation_tensor = torch.tensor(perturbation, requires_grad=True)\n",
        "\n",
        "# Calculate sparsity loss in PyTorch\n",
        "sparsity_loss_tensor = perturbation_tensor**2 / (perturbation_tensor**2 + epsilon)\n",
        "\n",
        "# Calculate the gradient (numerical derivative) of the sparsity loss\n",
        "sparsity_loss_tensor.backward(torch.ones_like(perturbation_tensor))\n",
        "gradient = perturbation_tensor.grad\n",
        "\n",
        "# Plot the gradient\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(perturbation, gradient.detach().numpy(), label='Numerical Derivative (Gradient)')\n",
        "plt.title('Numerical Derivative of Sparsity-Inducing Loss')\n",
        "plt.xlabel('Perturbation')\n",
        "plt.ylabel('Gradient')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0461ea39-e0e6-4d09-e6d8-f3140eda0379",
        "id": "3VwIeC8RSbnU"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Perturbation at index 5000 (expected 0): {perturbation[5000]}\")\n",
        "print(f\"Gradient at index 5000 (corresponding to perturbation=0): {gradient[5000]}\")\n",
        "max_grad_index = gradient.argmax()\n",
        "max_grad_value = gradient[max_grad_index]\n",
        "corresponding_perturbation = perturbation[max_grad_index]\n",
        "print(f\"Perturbation corresponding to the maximum gradient: {corresponding_perturbation}\")\n",
        "print(f\"Maximum gradient: {max_grad_value}\")\n"
      ],
      "metadata": {
        "outputId": "7c1df9c0-3ae5-467b-c2fc-b644bead3336",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_fY2r-ySbnU"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perturbation at index 5000 (expected 0): 0.0\n",
            "Gradient at index 5000 (corresponding to perturbation=0): 0.0\n",
            "Perturbation corresponding to the maximum gradient: 0.005800000000000027\n",
            "Maximum gradient: 64.95088235101768\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "from torch.optim.lr_scheduler import CyclicLR\n",
        "\n",
        "def compare(output, target, confidence, targeted):\n",
        "    \"\"\"\n",
        "    Compares the output with the target for a targeted or untargeted attack.\n",
        "\n",
        "    For targeted attacks, it checks if the output class matches the target class.\n",
        "    For untargeted attacks, it checks if the output class does not match the target class.\n",
        "    \"\"\"\n",
        "    if not isinstance(output, (float, int, torch.LongTensor, torch.cuda.LongTensor)):\n",
        "        output = torch.clone(output).detach()\n",
        "        if targeted:\n",
        "            output[target] -= 0.001  # Slightly reduce the confidence for targeted attacks\n",
        "        else:\n",
        "            output[target] += 0.001  # Slightly increase the confidence for untargeted attacks\n",
        "        output = torch.argmax(output, dim=1)\n",
        "    return output == target if targeted else output != target\n",
        "\n",
        "def sin_func_torch(input_tensor):\n",
        "    \"\"\"\n",
        "    A sine-based transformation applied to the input tensor.\n",
        "\n",
        "    The function applies a sine transformation and rescales the output to [0, 1].\n",
        "    \"\"\"\n",
        "    return (torch.sin((input_tensor - 0.5) * torch.pi) + 1) / 2.0\n",
        "\n",
        "def CW_l1(imgs, y, model, removal_array, max_iterations=1000, learning_rate=0.01,\n",
        "       binary_search_steps=3, confidence=0.01, initial_const=0.1,\n",
        "       cons_increase_factor=2, device='cuda'):\n",
        "    \"\"\"\n",
        "    Implements the Carlini & Wagner (CW) L1 attack.\n",
        "\n",
        "    Parameters:\n",
        "    - imgs: The original images to be attacked.\n",
        "    - y: The true labels of the images.\n",
        "    - model: The target model to attack.\n",
        "    - removal_array: A mask array specifying which features are fixed.\n",
        "    - max_iterations: The maximum number of iterations for the attack.\n",
        "    - learning_rate: The learning rate for the optimizer.\n",
        "    - binary_search_steps: The number of steps for the binary search over the constant.\n",
        "    - confidence: Confidence value for the attack.\n",
        "    - initial_const: The initial constant for the attack loss.\n",
        "    - cons_increase_factor: The factor by which the constant is increased during binary search.\n",
        "    - device: The device on which to perform computations.\n",
        "\n",
        "    Returns:\n",
        "    - final_output: The perturbed images after the attack.\n",
        "    \"\"\"\n",
        "\n",
        "    labs = torch.zeros_like(y.squeeze(), dtype=torch.int64)\n",
        "    batch_size = imgs.shape[0]\n",
        "    targeted = True\n",
        "\n",
        "    # Expand removal array to match the batch size\n",
        "    expanded_removal_array = removal_array.expand(imgs.shape[0], -1)\n",
        "    non_fixed_features_mask = torch.bitwise_or(expanded_removal_array, 1 - imgs.to(torch.uint8))\n",
        "\n",
        "    mask_0 = non_fixed_features_mask == 0\n",
        "    mask_1 = non_fixed_features_mask == 1\n",
        "\n",
        "    imgs_tensor = imgs.clone().detach().to(dtype=torch.float32, device=device)\n",
        "    labs_tensor = labs.clone().detach().to(dtype=torch.int64, device=device)\n",
        "\n",
        "    lower_bound = torch.zeros(batch_size, dtype=torch.float32, device=device)\n",
        "    CONST = torch.full((batch_size,), initial_const, dtype=torch.float32, device=device)\n",
        "    upper_bound = torch.full((batch_size,), 1e10, dtype=torch.float32, device=device)\n",
        "\n",
        "    o_bestl2 = torch.full((batch_size,), 1e10, device=device)\n",
        "    o_bestscore = torch.full((batch_size,), -1, device=device)\n",
        "    o_bestattack = imgs.clone().detach()\n",
        "\n",
        "    o_best_loss1 = torch.full((batch_size,), float('inf'), device=device)\n",
        "    o_best_unsuccessfull_attack = imgs.clone().detach()\n",
        "    o_success_attack = torch.zeros(batch_size, dtype=torch.bool, device=device)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    for outer_step in range(binary_search_steps):\n",
        "        print(CONST)\n",
        "\n",
        "        modifier = torch.zeros_like(imgs_tensor, requires_grad=True).to(device)\n",
        "        optimizer = torch.optim.Adam([modifier], lr=learning_rate)\n",
        "\n",
        "        def grad_hook(grad):\n",
        "            return grad * non_fixed_features_mask\n",
        "        modifier.register_hook(grad_hook)\n",
        "\n",
        "        scheduler = CyclicLR(optimizer, base_lr=0.00001, max_lr=0.1, step_size_up=5,\n",
        "                             step_size_down=4995, mode='triangular2')\n",
        "\n",
        "        best_loss = torch.full((batch_size,), float('inf'), device=device)\n",
        "        no_improvement_count = torch.zeros(batch_size, dtype=torch.int, device=device)\n",
        "        active_mask = torch.ones(batch_size, dtype=torch.bool, device=device)\n",
        "\n",
        "        bestl2 = torch.full((batch_size,), 1e10, device=device)\n",
        "        bestscore = torch.full((batch_size,), -1, device=device)\n",
        "\n",
        "        for iteration in range(max_iterations):\n",
        "            if (iteration + 1) % 2000 == 0:\n",
        "                current_lr = scheduler.get_last_lr()[0]\n",
        "                print(f\"Iteration {iteration + 1}: Current Learning Rate: {current_lr}\")\n",
        "\n",
        "            active_indices = torch.where(active_mask)[0]\n",
        "            active_modifier = modifier[active_mask]\n",
        "            active_imgs_tensor = imgs_tensor[active_mask]\n",
        "            active_labs_tensor = labs_tensor[active_mask]\n",
        "            active_CONST = CONST[active_mask]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            modified_input = active_imgs_tensor + active_modifier\n",
        "            newimg = sin_func_torch(modified_input)\n",
        "            output = model(newimg)\n",
        "\n",
        "            epsilon = torch.tensor(0.0001, device=newimg.device, dtype=newimg.dtype)\n",
        "            perturbation = (newimg - imgs[active_mask]).square()\n",
        "            l2dist = torch.sum(perturbation / (perturbation + epsilon), dim=1)\n",
        "            #torch.sum(perturbation / (perturbation + epsilon), dim=1)\n",
        "            #torch.sum(torch.abs(newimg - imgs[active_mask]), dim=1)\n",
        "\n",
        "            real = output[torch.arange(active_indices.size(0)), active_labs_tensor]\n",
        "            other = output[torch.arange(active_indices.size(0)), active_labs_tensor + 1]\n",
        "\n",
        "            loss1 = torch.clamp((other - real + confidence), min=0.0)\n",
        "            #loss2 = l2dist\n",
        "\n",
        "            if outer_step != (binary_search_steps - 1):\n",
        "                loss = (active_CONST * loss1) + (l2dist / 20.)\n",
        "            else:\n",
        "                loss = loss1\n",
        "\n",
        "            total_loss = torch.sum(loss)\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                improved_mask = loss < best_loss[active_mask]\n",
        "                best_loss[active_mask] = torch.where(improved_mask, loss, best_loss[active_mask])\n",
        "                no_improvement_count[active_mask] = torch.where(improved_mask,\n",
        "                    torch.zeros_like(no_improvement_count[active_mask]),\n",
        "                    no_improvement_count[active_mask] + 1)\n",
        "\n",
        "                improved_mask2 = (loss1 < o_best_loss1[active_mask])\n",
        "                o_best_loss1[active_mask] = torch.where(improved_mask2, loss1, o_best_loss1[active_mask])\n",
        "                o_best_unsuccessfull_attack[active_mask] = torch.where(improved_mask2.unsqueeze(-1), newimg, o_best_unsuccessfull_attack[active_mask])\n",
        "\n",
        "                update_mask = (l2dist < bestl2[active_mask]) & compare(output, active_labs_tensor, confidence, targeted)\n",
        "                bestl2[active_mask] = torch.where(update_mask, l2dist, bestl2[active_mask])\n",
        "                bestscore[active_mask] = torch.where(update_mask, torch.argmax(output, dim=1), bestscore[active_mask])\n",
        "\n",
        "                update_mask2 = (l2dist < o_bestl2[active_mask]) & compare(output, active_labs_tensor, confidence, targeted)\n",
        "                o_bestl2[active_mask] = torch.where(update_mask2, l2dist, o_bestl2[active_mask])\n",
        "                o_bestscore[active_mask] = torch.where(update_mask2, torch.argmax(output, dim=1), o_bestscore[active_mask])\n",
        "                o_bestattack[active_mask] = torch.where(update_mask2.unsqueeze(-1), newimg, o_bestattack[active_mask])\n",
        "\n",
        "                # Skip the rest of the loop for samples that haven't improved in 50 iterations\n",
        "                #active_mask[active_mask.clone()] = no_improvement_count[active_mask.clone()] < 50\n",
        "\n",
        "        if outer_step != (binary_search_steps - 1):\n",
        "            success_mask = (bestscore != -1) & compare(bestscore, labs_tensor, confidence, targeted)\n",
        "            o_success_attack  = torch.logical_or(o_success_attack, success_mask)\n",
        "            print('o_success_attack ', o_success_attack.sum())\n",
        "\n",
        "            failure_mask = ~success_mask\n",
        "            upper_bound[success_mask] = torch.minimum(upper_bound[success_mask], CONST[success_mask])\n",
        "            lower_bound[failure_mask] = torch.maximum(lower_bound[failure_mask], CONST[failure_mask])\n",
        "\n",
        "            upper_limit_mask = upper_bound < 1e10\n",
        "            CONST[upper_limit_mask] = (lower_bound[upper_limit_mask] + upper_bound[upper_limit_mask]) / 2\n",
        "            CONST[failure_mask & ~upper_limit_mask] *= cons_increase_factor\n",
        "\n",
        "    success_attack_mask = (o_bestl2 != 1e10)\n",
        "    final_output = torch.where(success_attack_mask.unsqueeze(-1), o_bestattack, o_best_unsuccessfull_attack)\n",
        "\n",
        "    return final_output\n"
      ],
      "metadata": {
        "id": "GhaBffT0SbnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6016ea95-066b-4a2a-cd39-413ba2b9477c",
        "id": "m1kBT4wuSbnW"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.1000, 0.1000, 0.1000,  ..., 0.1000, 0.1000, 0.1000], device='cuda:0')\n",
            "Iteration 2000: Current Learning Rate: 0.06008407207207209\n",
            "Iteration 4000: Current Learning Rate: 0.020048036036036054\n",
            "Iteration 6000: Current Learning Rate: 0.04005604504504506\n",
            "Iteration 8000: Current Learning Rate: 0.020038027027027023\n",
            "Iteration 10000: Current Learning Rate: 2.0009009009007906e-05\n",
            "Iteration 12000: Current Learning Rate: 0.01502851801801802\n",
            "Iteration 14000: Current Learning Rate: 0.005019509009009013\n",
            "Iteration 16000: Current Learning Rate: 0.010021511261261265\n",
            "Iteration 18000: Current Learning Rate: 0.005017006756756755\n",
            "Iteration 20000: Current Learning Rate: 1.2502252252246421e-05\n",
            "o_success_attack  tensor(46, device='cuda:0')\n",
            "tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0')\n",
            "Iteration 2000: Current Learning Rate: 0.06008407207207209\n",
            "Iteration 4000: Current Learning Rate: 0.020048036036036054\n",
            "Iteration 6000: Current Learning Rate: 0.04005604504504506\n",
            "Iteration 8000: Current Learning Rate: 0.020038027027027023\n",
            "Iteration 10000: Current Learning Rate: 2.0009009009007906e-05\n",
            "Iteration 12000: Current Learning Rate: 0.01502851801801802\n",
            "Iteration 14000: Current Learning Rate: 0.005019509009009013\n",
            "Iteration 16000: Current Learning Rate: 0.010021511261261265\n",
            "Iteration 18000: Current Learning Rate: 0.005017006756756755\n",
            "Iteration 20000: Current Learning Rate: 1.2502252252246421e-05\n",
            "o_success_attack  tensor(142, device='cuda:0')\n",
            "tensor([10., 10., 10.,  ..., 10., 10., 10.], device='cuda:0')\n",
            "Iteration 2000: Current Learning Rate: 0.06008407207207209\n",
            "Iteration 4000: Current Learning Rate: 0.020048036036036054\n",
            "Iteration 6000: Current Learning Rate: 0.04005604504504506\n",
            "Iteration 8000: Current Learning Rate: 0.020038027027027023\n",
            "Iteration 10000: Current Learning Rate: 2.0009009009007906e-05\n",
            "Iteration 12000: Current Learning Rate: 0.01502851801801802\n",
            "Iteration 14000: Current Learning Rate: 0.005019509009009013\n",
            "Iteration 16000: Current Learning Rate: 0.010021511261261265\n",
            "Iteration 18000: Current Learning Rate: 0.005017006756756755\n",
            "Iteration 20000: Current Learning Rate: 1.2502252252246421e-05\n",
            "o_success_attack  tensor(193, device='cuda:0')\n",
            "tensor([  5.5000, 100.0000, 100.0000,  ..., 100.0000, 100.0000, 100.0000],\n",
            "       device='cuda:0')\n",
            "Iteration 2000: Current Learning Rate: 0.06008407207207209\n",
            "Iteration 4000: Current Learning Rate: 0.020048036036036054\n",
            "Iteration 6000: Current Learning Rate: 0.04005604504504506\n",
            "Iteration 8000: Current Learning Rate: 0.020038027027027023\n",
            "Iteration 10000: Current Learning Rate: 2.0009009009007906e-05\n",
            "Iteration 12000: Current Learning Rate: 0.01502851801801802\n",
            "Iteration 14000: Current Learning Rate: 0.005019509009009013\n",
            "Iteration 16000: Current Learning Rate: 0.010021511261261265\n",
            "Iteration 18000: Current Learning Rate: 0.005017006756756755\n",
            "Iteration 20000: Current Learning Rate: 1.2502252252246421e-05\n",
            "o_success_attack  tensor(234, device='cuda:0')\n",
            "tensor([   3.2500, 1000.0000, 1000.0000,  ..., 1000.0000, 1000.0000,\n",
            "        1000.0000], device='cuda:0')\n",
            "Iteration 2000: Current Learning Rate: 0.06008407207207209\n",
            "Iteration 4000: Current Learning Rate: 0.020048036036036054\n",
            "Iteration 6000: Current Learning Rate: 0.04005604504504506\n",
            "Iteration 8000: Current Learning Rate: 0.020038027027027023\n",
            "Iteration 10000: Current Learning Rate: 2.0009009009007906e-05\n",
            "Iteration 12000: Current Learning Rate: 0.01502851801801802\n",
            "Iteration 14000: Current Learning Rate: 0.005019509009009013\n",
            "Iteration 16000: Current Learning Rate: 0.010021511261261265\n",
            "Iteration 18000: Current Learning Rate: 0.005017006756756755\n",
            "Iteration 20000: Current Learning Rate: 1.2502252252246421e-05\n",
            "o_success_attack  tensor(243, device='cuda:0')\n",
            "tensor([2.1250e+00, 1.0000e+04, 1.0000e+04,  ..., 1.0000e+04, 1.0000e+04,\n",
            "        1.0000e+04], device='cuda:0')\n",
            "Iteration 2000: Current Learning Rate: 0.06008407207207209\n",
            "Iteration 4000: Current Learning Rate: 0.020048036036036054\n",
            "Iteration 6000: Current Learning Rate: 0.04005604504504506\n",
            "Iteration 8000: Current Learning Rate: 0.020038027027027023\n",
            "Iteration 10000: Current Learning Rate: 2.0009009009007906e-05\n",
            "Iteration 12000: Current Learning Rate: 0.01502851801801802\n",
            "Iteration 14000: Current Learning Rate: 0.005019509009009013\n",
            "Iteration 16000: Current Learning Rate: 0.010021511261261265\n",
            "Iteration 18000: Current Learning Rate: 0.005017006756756755\n",
            "Iteration 20000: Current Learning Rate: 1.2502252252246421e-05\n",
            "o_success_attack  tensor(250, device='cuda:0')\n",
            "tensor([1.5625e+00, 1.0000e+05, 1.0000e+05,  ..., 1.0000e+05, 1.0000e+05,\n",
            "        1.0000e+05], device='cuda:0')\n",
            "Iteration 2000: Current Learning Rate: 0.06008407207207209\n",
            "Iteration 4000: Current Learning Rate: 0.020048036036036054\n",
            "Iteration 6000: Current Learning Rate: 0.04005604504504506\n",
            "Iteration 8000: Current Learning Rate: 0.020038027027027023\n",
            "Iteration 10000: Current Learning Rate: 2.0009009009007906e-05\n",
            "Iteration 12000: Current Learning Rate: 0.01502851801801802\n",
            "Iteration 14000: Current Learning Rate: 0.005019509009009013\n",
            "Iteration 16000: Current Learning Rate: 0.010021511261261265\n",
            "Iteration 18000: Current Learning Rate: 0.005017006756756755\n",
            "Iteration 20000: Current Learning Rate: 1.2502252252246421e-05\n",
            "o_success_attack  tensor(256, device='cuda:0')\n",
            "tensor([1.8438e+00, 1.0000e+06, 1.0000e+06,  ..., 1.0000e+06, 1.0000e+06,\n",
            "        1.0000e+06], device='cuda:0')\n",
            "Iteration 2000: Current Learning Rate: 0.06008407207207209\n",
            "Iteration 4000: Current Learning Rate: 0.020048036036036054\n",
            "Iteration 6000: Current Learning Rate: 0.04005604504504506\n",
            "Iteration 8000: Current Learning Rate: 0.020038027027027023\n",
            "Iteration 10000: Current Learning Rate: 2.0009009009007906e-05\n",
            "Iteration 12000: Current Learning Rate: 0.01502851801801802\n",
            "Iteration 14000: Current Learning Rate: 0.005019509009009013\n",
            "Iteration 16000: Current Learning Rate: 0.010021511261261265\n",
            "Iteration 18000: Current Learning Rate: 0.005017006756756755\n",
            "Iteration 20000: Current Learning Rate: 1.2502252252246421e-05\n",
            "o_success_attack  tensor(256, device='cuda:0')\n",
            "tensor([1.9844e+00, 1.0000e+07, 1.0000e+07,  ..., 1.0000e+07, 1.0000e+07,\n",
            "        1.0000e+07], device='cuda:0')\n",
            "Iteration 2000: Current Learning Rate: 0.06008407207207209\n",
            "Iteration 4000: Current Learning Rate: 0.020048036036036054\n",
            "Iteration 6000: Current Learning Rate: 0.04005604504504506\n",
            "Iteration 8000: Current Learning Rate: 0.020038027027027023\n",
            "Iteration 10000: Current Learning Rate: 2.0009009009007906e-05\n",
            "Iteration 12000: Current Learning Rate: 0.01502851801801802\n",
            "Iteration 14000: Current Learning Rate: 0.005019509009009013\n",
            "Iteration 16000: Current Learning Rate: 0.010021511261261265\n",
            "Iteration 18000: Current Learning Rate: 0.005017006756756755\n",
            "Iteration 20000: Current Learning Rate: 1.2502252252246421e-05\n",
            "1130\n",
            "260\n",
            "Accuracy of just malwares (without attack): 98.85% | Under attack: 76.99%.\n"
          ]
        }
      ],
      "source": [
        "#CW(mal_x_batch, mal_y_batch, model_AT_rFGSM, removal_array, max_iterations=1000, learning_rate=0.01, binary_search_steps=3, confidence=0.01, initial_const=0.1, device=device)\n",
        "attack_params =  {'device':device, 'confidence':0.01, 'removal_array':removal_array,'max_iterations': 20000, 'learning_rate': 0.3, 'initial_const': 0.1 , 'binary_search_steps' : 9, 'cons_increase_factor' : 10}\n",
        "adv = adv_predict(test_loader, combined_model, CW_l1, **attack_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O1ai7g3ISbnX"
      },
      "outputs": [],
      "source": [
        "for x_test, y_test in test_loader:\n",
        "    x_test, y_test = x_test.to(torch.float32).to(device), y_test.to(device)\n",
        "    mal_x_batch, mal_y_batch = x_test[y_test.squeeze() == 1], y_test[y_test.squeeze() == 1]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation on non-rounded adversarial examples\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "combined_model.eval()\n",
        "\n",
        "# Get the model's predictions on adversarial examples\n",
        "outputs = combined_model(adv)\n",
        "y_pred = outputs.argmax(dim=1)\n",
        "\n",
        "# Calculate metrics\n",
        "num_malwares = len(mal_y_batch)\n",
        "num_successful_attacks = (y_pred == 0).sum().item()\n",
        "total_difference = (torch.abs(adv[y_pred == 0] - mal_x_batch[y_pred == 0])).sum().item()\n",
        "accuracy = (y_pred == 1).sum().item() / num_malwares * 100\n",
        "\n",
        "# Print results with clear formatting\n",
        "print(f\"Evaluation Results on NOT_Rounded Adversarial Examples\")\n",
        "print(f\"-----------------------------------------------\")\n",
        "print(f\"Number of malware samples: {num_malwares}\")\n",
        "print(f\"Number of successful attacks: {num_successful_attacks}\")\n",
        "print(f\"Total difference (L1 norm) between successful adversarial examples and original: {total_difference}\")\n",
        "print(f\"Accuracy of the model on malwares under attack: {accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "000e3fb6-d89c-4a0f-e49b-997fb4cb8c6e",
        "id": "dcY3c8P0SbnX"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Results on NOT_Rounded Adversarial Examples\n",
            "-----------------------------------------------\n",
            "Number of malware samples: 1130\n",
            "Number of successful attacks: 260\n",
            "Total difference (L1 norm) between successful adversarial examples and original: 7149.763671875\n",
            "Accuracy of the model on malwares under attack: 76.99%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def l0_penalty(original_data, adv_data):\n",
        "    perturbation = original_data - adv_data\n",
        "    return torch.sum(perturbation != 0).float()\n",
        "\n",
        "l0_penalty(mal_x_batch[y_pred == 0], adv[y_pred == 0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fa6ae69-3f67-456f-8a5c-9b6d84573eac",
        "id": "yXFJvaA3SbnY"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(742406., device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation on rounded adversarial examples\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "combined_model.eval()\n",
        "\n",
        "# Apply the final rounding solution to adversarial examples\n",
        "adv_rounded = final_rounding(mal_x_batch, adv, combined_model)\n",
        "\n",
        "# Get the model's predictions on the rounded adversarial examples\n",
        "outputs = combined_model(adv_rounded)\n",
        "y_pred = outputs.argmax(dim=1)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "num_malwares = len(mal_y_batch)\n",
        "num_successful_attacks = (y_pred == 0).sum().item()\n",
        "total_difference = (torch.abs(adv_rounded[y_pred == 0] - mal_x_batch[y_pred == 0])).sum().item()\n",
        "accuracy = (y_pred == 1).sum().item() / num_malwares * 100\n",
        "\n",
        "# Display the results with clear formatting\n",
        "print(f\"Evaluation Results on Rounded Adversarial Examples\")\n",
        "print(f\"-----------------------------------------------\")\n",
        "print(f\"Number of malware samples: {num_malwares}\")\n",
        "print(f\"Number of successful attacks: {num_successful_attacks}\")\n",
        "print(f\"Total L1 norm difference (successful adversarial examples vs. original): {total_difference}\")\n",
        "print(f\"Accuracy of the model on malware under attack: {accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72b01f2e-bd29-4308-a3a3-578c6ec7ce58",
        "id": "F_uyV_GtSbnY"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Results on Rounded Adversarial Examples\n",
            "-----------------------------------------------\n",
            "Number of malware samples: 1130\n",
            "Number of successful attacks: 243\n",
            "Total L1 norm difference (successful adversarial examples vs. original): 6184.0\n",
            "Accuracy of the model on malware under attack: 78.50%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure that fixed elements are not changed\n",
        "\n",
        "# Ensure `removal_array` is expanded to match the batch size\n",
        "expanded_removal_array = removal_array.expand(mal_x_batch.shape[0], -1)\n",
        "\n",
        "# Create a mask where non-fixed features are set to 1\n",
        "non_fixed_features_mask = torch.bitwise_or(expanded_removal_array, 1 - mal_x_batch.to(torch.uint8))\n",
        "\n",
        "# Calculate indices where the mask is 0 (fixed) or 1 (non-fixed)\n",
        "mask_fixed = non_fixed_features_mask == 0\n",
        "mask_non_fixed = non_fixed_features_mask == 1\n",
        "\n",
        "# Calculate and print the differences\n",
        "diff_adv_fixed = (torch.abs(adv - mal_x_batch) * mask_fixed).sum().item()\n",
        "diff_adv_non_fixed = (torch.abs(adv - mal_x_batch) * mask_non_fixed).sum().item()\n",
        "diff_adv_rounded_fixed = (torch.abs(adv_rounded - mal_x_batch) * mask_fixed).sum().item()\n",
        "diff_adv_rounded_non_fixed = (torch.abs(adv_rounded - mal_x_batch) * mask_non_fixed).sum().item()\n",
        "\n",
        "# Display results with clear formatting\n",
        "print(f\"Differences when mask is 0 (fixed features):\")\n",
        "print(f\"  Adv vs. Original: {diff_adv_fixed}\")\n",
        "print(f\"  Rounded Adv vs. Original: {diff_adv_rounded_fixed}\")\n",
        "print(f\"Differences when mask is 1 (non-fixed features):\")\n",
        "print(f\"  Adv vs. Original: {diff_adv_non_fixed}\")\n",
        "print(f\"  Rounded Adv vs. Original: {diff_adv_rounded_non_fixed}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99ac4607-2ef1-4bbe-8f07-b446e993e677",
        "id": "LVabQdgkSbnZ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Differences when mask is 0 (fixed features):\n",
            "  Adv vs. Original: 0.0\n",
            "  Rounded Adv vs. Original: 0.0\n",
            "Differences when mask is 1 (non-fixed features):\n",
            "  Adv vs. Original: 60227.328125\n",
            "  Rounded Adv vs. Original: 61537.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = combined_model(adv)\n",
        "y_pred = outputs.argmax(dim=1)"
      ],
      "metadata": {
        "id": "L2omYpN5SbnZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adv[y_pred==0][0][:200]- mal_x_batch[y_pred==0][0][:200]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4O8xi4wJlwLS",
        "outputId": "9882de2d-244e-46db-cf73-c05a2ebf9b5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00, -9.8934e-01,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  8.9407e-08,\n",
              "         0.0000e+00,  0.0000e+00,  1.0000e+00,  2.1040e-05,  0.0000e+00,\n",
              "         0.0000e+00,  1.1970e-03,  0.0000e+00,  0.0000e+00,  9.9940e-01,\n",
              "         0.0000e+00,  6.7932e-01,  1.7881e-07,  0.0000e+00,  5.9605e-08,\n",
              "         9.9957e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  5.3644e-07,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  9.5901e-01,\n",
              "         0.0000e+00,  0.0000e+00,  8.9407e-08,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  2.9802e-08,\n",
              "         1.0000e+00,  0.0000e+00,  3.2783e-07,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  1.0000e+00,  0.0000e+00,  0.0000e+00,  2.9802e-08,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         1.7583e-06,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         2.8908e-06,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  9.8784e-01,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  2.9802e-08,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "adv[y_pred==0][1][:200] - mal_x_batch[y_pred==0][1][:200]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66c23bfb-cb42-44ce-ebe3-eda02e38910a",
        "id": "BbckGAZPSbna"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.0000e+00,  7.9435e-04,  0.0000e+00,  0.0000e+00,  1.0312e-05,\n",
              "         0.0000e+00,  9.9996e-01,  2.6822e-07,  0.0000e+00,  0.0000e+00,\n",
              "         5.9605e-07,  9.9993e-01,  1.4279e-01,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  9.9013e-01,  0.0000e+00,  0.0000e+00, -2.0146e-05,\n",
              "         0.0000e+00,  7.4577e-04,  9.9993e-01,  1.0000e+00,  1.2547e-05,\n",
              "        -5.9605e-07,  2.5928e-06,  0.0000e+00,  0.0000e+00,  9.9994e-01,\n",
              "         2.0862e-07,  0.0000e+00,  1.1921e-07,  5.9605e-08,  1.2293e-01,\n",
              "         4.9949e-05,  1.0000e+00, -4.6511e-03,  8.9407e-07,  2.3246e-06,\n",
              "         0.0000e+00,  8.9407e-08,  2.9802e-08,  5.9605e-08,  3.5763e-07,\n",
              "         5.3644e-07,  0.0000e+00,  3.5763e-07,  5.9605e-07,  1.2883e-02,\n",
              "         5.9605e-08,  2.3842e-07, -1.0000e+00,  2.6822e-07,  0.0000e+00,\n",
              "         1.4901e-07,  7.0333e-06,  1.4901e-07,  2.9802e-08,  2.3842e-07,\n",
              "         0.0000e+00,  2.3842e-07,  5.9605e-08,  0.0000e+00,  5.9605e-08,\n",
              "        -9.9977e-01,  5.9605e-08,  1.4901e-07,  1.7881e-07,  0.0000e+00,\n",
              "         3.5465e-06,  2.3842e-07,  9.9987e-01,  1.1921e-07,  1.1966e-03,\n",
              "         0.0000e+00,  1.8179e-05,  0.0000e+00, -1.3502e-01,  2.9802e-08,\n",
              "         1.3709e-06, -9.9999e-01,  1.2763e-03, -7.9870e-06, -6.6696e-03,\n",
              "         2.3246e-06,  5.2860e-04,  5.2959e-02,  1.3530e-05,  2.3842e-07,\n",
              "         0.0000e+00,  2.1905e-05,  9.9857e-01,  9.7645e-01,  9.9812e-01,\n",
              "         2.9802e-08,  4.8755e-02,  1.1921e-07,  0.0000e+00,  8.0466e-07,\n",
              "         1.3709e-06,  1.1921e-07,  1.1921e-07,  7.1858e-01,  1.0000e+00,\n",
              "         4.1723e-07,  1.0000e+00,  2.5928e-06,  1.8165e-03,  0.0000e+00,\n",
              "         3.8743e-07,  0.0000e+00,  1.0073e-05,  1.0000e+00,  1.4901e-07,\n",
              "         1.1533e-05,  5.0664e-07,  6.1233e-01,  0.0000e+00,  8.9407e-08,\n",
              "         6.6651e-03,  5.9605e-08,  1.6987e-06,  1.1921e-07,  2.9802e-08,\n",
              "         1.0729e-06,  0.0000e+00,  2.9802e-08,  0.0000e+00,  2.9802e-08,\n",
              "         1.7881e-07,  3.7551e-06,  9.1195e-06,  2.8526e-03,  1.7881e-07,\n",
              "         4.7684e-07,  0.0000e+00,  1.8179e-06,  9.9999e-01,  2.0862e-07,\n",
              "         1.4901e-07,  0.0000e+00,  0.0000e+00,  2.9802e-08,  6.5565e-07,\n",
              "         2.9802e-07,  0.0000e+00,  2.9802e-08,  8.9407e-08,  5.9605e-08,\n",
              "         2.9802e-08,  8.9407e-08,  2.6822e-07,  8.9407e-08,  2.2650e-06,\n",
              "         0.0000e+00,  0.0000e+00,  2.0862e-07,  2.9802e-08,  2.2198e-03,\n",
              "         0.0000e+00,  1.1921e-06,  2.1577e-05,  2.3842e-07, -1.9073e-06,\n",
              "         0.0000e+00,  9.9731e-01,  3.2544e-05,  0.0000e+00,  9.9999e-01,\n",
              "         1.4901e-07,  7.4506e-07,  1.4305e-06,  0.0000e+00,  2.9802e-08,\n",
              "         2.9802e-08,  0.0000e+00,  5.9605e-08,  2.0862e-07,  1.4901e-07,\n",
              "         0.0000e+00,  6.8545e-07,  0.0000e+00,  0.0000e+00,  1.1921e-07,\n",
              "         2.9802e-08,  5.1856e-06,  0.0000e+00,  0.0000e+00,  2.9802e-08,\n",
              "         8.9407e-08,  3.7253e-06,  8.9407e-08,  5.9605e-08,  2.9802e-07,\n",
              "         6.5565e-07,  0.0000e+00,  0.0000e+00,  2.9802e-07,  0.0000e+00],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New function"
      ],
      "metadata": {
        "id": "Ncbi4tB0m4EU"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EtiReFmUm4m9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "from torch.optim.lr_scheduler import CyclicLR\n",
        "\n",
        "def compare(output, target, confidence, targeted):\n",
        "    \"\"\"\n",
        "    Compares the output with the target for a targeted or untargeted attack.\n",
        "\n",
        "    For targeted attacks, it checks if the output class matches the target class.\n",
        "    For untargeted attacks, it checks if the output class does not match the target class.\n",
        "    \"\"\"\n",
        "    if not isinstance(output, (float, int, torch.LongTensor, torch.cuda.LongTensor)):\n",
        "        output = torch.clone(output).detach()\n",
        "        if targeted:\n",
        "            output[target] -= 0.001  # Slightly reduce the confidence for targeted attacks\n",
        "        else:\n",
        "            output[target] += 0.001  # Slightly increase the confidence for untargeted attacks\n",
        "        output = torch.argmax(output, dim=1)\n",
        "    return output == target if targeted else output != target\n",
        "\n",
        "def sin_func_torch(input_tensor):\n",
        "    \"\"\"\n",
        "    A sine-based transformation applied to the input tensor.\n",
        "\n",
        "    The function applies a sine transformation and rescales the output to [0, 1].\n",
        "    \"\"\"\n",
        "    return (torch.sin((input_tensor - 0.5) * torch.pi) + 1) / 2.0\n",
        "\n",
        "def CW_l1(imgs, y, model, removal_array, max_iterations=1000, learning_rate=0.01,\n",
        "       binary_search_steps=3, confidence=0.01, initial_const=0.1,\n",
        "       cons_increase_factor=2, device='cuda'):\n",
        "    \"\"\"\n",
        "    Implements the Carlini & Wagner (CW) L1 attack.\n",
        "\n",
        "    Parameters:\n",
        "    - imgs: The original images to be attacked.\n",
        "    - y: The true labels of the images.\n",
        "    - model: The target model to attack.\n",
        "    - removal_array: A mask array specifying which features are fixed.\n",
        "    - max_iterations: The maximum number of iterations for the attack.\n",
        "    - learning_rate: The learning rate for the optimizer.\n",
        "    - binary_search_steps: The number of steps for the binary search over the constant.\n",
        "    - confidence: Confidence value for the attack.\n",
        "    - initial_const: The initial constant for the attack loss.\n",
        "    - cons_increase_factor: The factor by which the constant is increased during binary search.\n",
        "    - device: The device on which to perform computations.\n",
        "\n",
        "    Returns:\n",
        "    - final_output: The perturbed images after the attack.\n",
        "    \"\"\"\n",
        "\n",
        "    labs = torch.zeros_like(y.squeeze(), dtype=torch.int64)\n",
        "    batch_size = imgs.shape[0]\n",
        "    targeted = True\n",
        "\n",
        "    # Expand removal array to match the batch size\n",
        "    expanded_removal_array = removal_array.expand(imgs.shape[0], -1)\n",
        "    non_fixed_features_mask = torch.bitwise_or(expanded_removal_array, 1 - imgs.to(torch.uint8))\n",
        "\n",
        "    mask_0 = non_fixed_features_mask == 0\n",
        "    mask_1 = non_fixed_features_mask == 1\n",
        "\n",
        "    imgs_tensor = imgs.clone().detach().to(dtype=torch.float32, device=device)\n",
        "    labs_tensor = labs.clone().detach().to(dtype=torch.int64, device=device)\n",
        "\n",
        "    lower_bound = torch.zeros(batch_size, dtype=torch.float32, device=device)\n",
        "    CONST = torch.full((batch_size,), initial_const, dtype=torch.float32, device=device)\n",
        "    upper_bound = torch.full((batch_size,), 1e10, dtype=torch.float32, device=device)\n",
        "\n",
        "    o_bestl2 = torch.full((batch_size,), 1e10, device=device)\n",
        "    o_bestscore = torch.full((batch_size,), -1, device=device)\n",
        "    o_bestattack = imgs.clone().detach()\n",
        "\n",
        "    o_best_loss1 = torch.full((batch_size,), float('inf'), device=device)\n",
        "    o_best_unsuccessfull_attack = imgs.clone().detach()\n",
        "    o_success_attack = torch.zeros(batch_size, dtype=torch.bool, device=device)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    for outer_step in range(binary_search_steps):\n",
        "        print(CONST)\n",
        "\n",
        "        modifier = torch.zeros_like(imgs_tensor, requires_grad=True).to(device)\n",
        "        optimizer = torch.optim.Adam([modifier], lr=learning_rate)\n",
        "\n",
        "        def grad_hook(grad):\n",
        "            return grad * non_fixed_features_mask\n",
        "        modifier.register_hook(grad_hook)\n",
        "\n",
        "        scheduler = CyclicLR(optimizer, base_lr=0.00001, max_lr=0.1, step_size_up=5,\n",
        "                             step_size_down=4995, mode='triangular2')\n",
        "\n",
        "        best_loss = torch.full((batch_size,), float('inf'), device=device)\n",
        "        no_improvement_count = torch.zeros(batch_size, dtype=torch.int, device=device)\n",
        "        active_mask = torch.ones(batch_size, dtype=torch.bool, device=device)\n",
        "\n",
        "        bestl2 = torch.full((batch_size,), 1e10, device=device)\n",
        "        bestscore = torch.full((batch_size,), -1, device=device)\n",
        "\n",
        "        for iteration in range(max_iterations):\n",
        "            if (iteration + 1) % 2000 == 0:\n",
        "                current_lr = scheduler.get_last_lr()[0]\n",
        "                print(f\"Iteration {iteration + 1}: Current Learning Rate: {current_lr}\")\n",
        "\n",
        "            active_indices = torch.where(active_mask)[0]\n",
        "            active_modifier = modifier[active_mask]\n",
        "            active_imgs_tensor = imgs_tensor[active_mask]\n",
        "            active_labs_tensor = labs_tensor[active_mask]\n",
        "            active_CONST = CONST[active_mask]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            modified_input = active_imgs_tensor + active_modifier\n",
        "            newimg = sin_func_torch(modified_input)\n",
        "            output = model(newimg)\n",
        "\n",
        "            #perturbation = torch.abs(newimg - imgs[active_mask])\n",
        "            #l2dist = torch.sum(perturbation , dim=1)\n",
        "\n",
        "            l2dist = torch.sum(torch.abs(newimg - imgs[active_mask]), dim=1)\n",
        "            #torch.sum(perturbation / (perturbation + epsilon), dim=1)\n",
        "            #torch.sum(torch.abs(newimg - imgs[active_mask]), dim=1)\n",
        "\n",
        "            real = output[torch.arange(active_indices.size(0)), active_labs_tensor]\n",
        "            other = output[torch.arange(active_indices.size(0)), active_labs_tensor + 1]\n",
        "\n",
        "            loss1 = torch.clamp((other - real + confidence), min=0.0)\n",
        "            #loss2 = l2dist\n",
        "\n",
        "            if outer_step != (binary_search_steps - 1):\n",
        "                loss = (active_CONST * loss1) + l2dist\n",
        "            else:\n",
        "                loss = loss1\n",
        "\n",
        "            total_loss = torch.sum(loss)\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                improved_mask = loss < best_loss[active_mask]\n",
        "                best_loss[active_mask] = torch.where(improved_mask, loss, best_loss[active_mask])\n",
        "                no_improvement_count[active_mask] = torch.where(improved_mask,\n",
        "                    torch.zeros_like(no_improvement_count[active_mask]),\n",
        "                    no_improvement_count[active_mask] + 1)\n",
        "\n",
        "                improved_mask2 = (loss1 < o_best_loss1[active_mask])\n",
        "                o_best_loss1[active_mask] = torch.where(improved_mask2, loss1, o_best_loss1[active_mask])\n",
        "                o_best_unsuccessfull_attack[active_mask] = torch.where(improved_mask2.unsqueeze(-1), newimg, o_best_unsuccessfull_attack[active_mask])\n",
        "\n",
        "                update_mask = (l2dist < bestl2[active_mask]) & compare(output, active_labs_tensor, confidence, targeted)\n",
        "                bestl2[active_mask] = torch.where(update_mask, l2dist, bestl2[active_mask])\n",
        "                bestscore[active_mask] = torch.where(update_mask, torch.argmax(output, dim=1), bestscore[active_mask])\n",
        "\n",
        "                update_mask2 = (l2dist < o_bestl2[active_mask]) & compare(output, active_labs_tensor, confidence, targeted)\n",
        "                o_bestl2[active_mask] = torch.where(update_mask2, l2dist, o_bestl2[active_mask])\n",
        "                o_bestscore[active_mask] = torch.where(update_mask2, torch.argmax(output, dim=1), o_bestscore[active_mask])\n",
        "                o_bestattack[active_mask] = torch.where(update_mask2.unsqueeze(-1), newimg, o_bestattack[active_mask])\n",
        "\n",
        "                # Skip the rest of the loop for samples that haven't improved in 50 iterations\n",
        "                #active_mask[active_mask.clone()] = no_improvement_count[active_mask.clone()] < 50\n",
        "\n",
        "        if outer_step != (binary_search_steps - 1):\n",
        "            success_mask = (bestscore != -1) & compare(bestscore, labs_tensor, confidence, targeted)\n",
        "            o_success_attack  = torch.logical_or(o_success_attack, success_mask)\n",
        "            print('o_success_attack ', o_success_attack.sum())\n",
        "\n",
        "            failure_mask = ~success_mask\n",
        "            upper_bound[success_mask] = torch.minimum(upper_bound[success_mask], CONST[success_mask])\n",
        "            lower_bound[failure_mask] = torch.maximum(lower_bound[failure_mask], CONST[failure_mask])\n",
        "\n",
        "            upper_limit_mask = upper_bound < 1e10\n",
        "            CONST[upper_limit_mask] = (lower_bound[upper_limit_mask] + upper_bound[upper_limit_mask]) / 2\n",
        "            CONST[failure_mask & ~upper_limit_mask] *= cons_increase_factor\n",
        "\n",
        "    success_attack_mask = (o_bestl2 != 1e10)\n",
        "    final_output = torch.where(success_attack_mask.unsqueeze(-1), o_bestattack, o_best_unsuccessfull_attack)\n",
        "\n",
        "    return final_output\n"
      ],
      "metadata": {
        "id": "wWZhAOP5m4_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4972873e-ca7e-44d1-d7af-9c09fb4572c0",
        "id": "4g1G1DPNm4_n"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.1000, 0.1000, 0.1000,  ..., 0.1000, 0.1000, 0.1000], device='cuda:0')\n",
            "Iteration 2000: Current Learning Rate: 0.06008407207207209\n",
            "Iteration 4000: Current Learning Rate: 0.020048036036036054\n",
            "Iteration 6000: Current Learning Rate: 0.04005604504504506\n",
            "Iteration 8000: Current Learning Rate: 0.020038027027027023\n",
            "Iteration 10000: Current Learning Rate: 2.0009009009007906e-05\n",
            "Iteration 12000: Current Learning Rate: 0.01502851801801802\n",
            "Iteration 14000: Current Learning Rate: 0.005019509009009013\n",
            "Iteration 16000: Current Learning Rate: 0.010021511261261265\n",
            "Iteration 18000: Current Learning Rate: 0.005017006756756755\n",
            "Iteration 20000: Current Learning Rate: 1.2502252252246421e-05\n",
            "o_success_attack  tensor(18, device='cuda:0')\n",
            "tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0')\n",
            "Iteration 2000: Current Learning Rate: 0.06008407207207209\n",
            "Iteration 4000: Current Learning Rate: 0.020048036036036054\n",
            "Iteration 6000: Current Learning Rate: 0.04005604504504506\n",
            "Iteration 8000: Current Learning Rate: 0.020038027027027023\n",
            "Iteration 10000: Current Learning Rate: 2.0009009009007906e-05\n",
            "Iteration 12000: Current Learning Rate: 0.01502851801801802\n",
            "Iteration 14000: Current Learning Rate: 0.005019509009009013\n",
            "Iteration 16000: Current Learning Rate: 0.010021511261261265\n",
            "Iteration 18000: Current Learning Rate: 0.005017006756756755\n",
            "Iteration 20000: Current Learning Rate: 1.2502252252246421e-05\n",
            "o_success_attack  tensor(60, device='cuda:0')\n",
            "tensor([10., 10., 10.,  ..., 10., 10., 10.], device='cuda:0')\n",
            "Iteration 2000: Current Learning Rate: 0.06008407207207209\n",
            "Iteration 4000: Current Learning Rate: 0.020048036036036054\n",
            "Iteration 6000: Current Learning Rate: 0.04005604504504506\n",
            "Iteration 8000: Current Learning Rate: 0.020038027027027023\n",
            "Iteration 10000: Current Learning Rate: 2.0009009009007906e-05\n",
            "Iteration 12000: Current Learning Rate: 0.01502851801801802\n",
            "Iteration 14000: Current Learning Rate: 0.005019509009009013\n",
            "Iteration 16000: Current Learning Rate: 0.010021511261261265\n",
            "Iteration 18000: Current Learning Rate: 0.005017006756756755\n",
            "Iteration 20000: Current Learning Rate: 1.2502252252246421e-05\n",
            "o_success_attack  tensor(179, device='cuda:0')\n",
            "tensor([100., 100., 100.,  ..., 100., 100., 100.], device='cuda:0')\n",
            "Iteration 2000: Current Learning Rate: 0.06008407207207209\n",
            "Iteration 4000: Current Learning Rate: 0.020048036036036054\n",
            "Iteration 6000: Current Learning Rate: 0.04005604504504506\n",
            "Iteration 8000: Current Learning Rate: 0.020038027027027023\n",
            "Iteration 10000: Current Learning Rate: 2.0009009009007906e-05\n",
            "Iteration 12000: Current Learning Rate: 0.01502851801801802\n",
            "Iteration 14000: Current Learning Rate: 0.005019509009009013\n",
            "Iteration 16000: Current Learning Rate: 0.010021511261261265\n",
            "Iteration 18000: Current Learning Rate: 0.005017006756756755\n",
            "Iteration 20000: Current Learning Rate: 1.2502252252246421e-05\n",
            "o_success_attack  tensor(236, device='cuda:0')\n",
            "tensor([  55., 1000., 1000.,  ..., 1000., 1000., 1000.], device='cuda:0')\n",
            "Iteration 2000: Current Learning Rate: 0.06008407207207209\n",
            "Iteration 4000: Current Learning Rate: 0.020048036036036054\n",
            "Iteration 6000: Current Learning Rate: 0.04005604504504506\n",
            "Iteration 8000: Current Learning Rate: 0.020038027027027023\n",
            "Iteration 10000: Current Learning Rate: 2.0009009009007906e-05\n",
            "Iteration 12000: Current Learning Rate: 0.01502851801801802\n",
            "Iteration 14000: Current Learning Rate: 0.005019509009009013\n",
            "Iteration 16000: Current Learning Rate: 0.010021511261261265\n",
            "Iteration 18000: Current Learning Rate: 0.005017006756756755\n",
            "Iteration 20000: Current Learning Rate: 1.2502252252246421e-05\n",
            "o_success_attack  tensor(262, device='cuda:0')\n",
            "tensor([   32.5000, 10000.0000, 10000.0000,  ..., 10000.0000, 10000.0000,\n",
            "        10000.0000], device='cuda:0')\n",
            "Iteration 2000: Current Learning Rate: 0.06008407207207209\n",
            "Iteration 4000: Current Learning Rate: 0.020048036036036054\n",
            "Iteration 6000: Current Learning Rate: 0.04005604504504506\n",
            "Iteration 8000: Current Learning Rate: 0.020038027027027023\n",
            "Iteration 10000: Current Learning Rate: 2.0009009009007906e-05\n",
            "Iteration 12000: Current Learning Rate: 0.01502851801801802\n",
            "Iteration 14000: Current Learning Rate: 0.005019509009009013\n",
            "Iteration 16000: Current Learning Rate: 0.010021511261261265\n",
            "Iteration 18000: Current Learning Rate: 0.005017006756756755\n",
            "Iteration 20000: Current Learning Rate: 1.2502252252246421e-05\n",
            "o_success_attack  tensor(270, device='cuda:0')\n",
            "tensor([2.1250e+01, 1.0000e+05, 1.0000e+05,  ..., 1.0000e+05, 1.0000e+05,\n",
            "        1.0000e+05], device='cuda:0')\n",
            "Iteration 2000: Current Learning Rate: 0.06008407207207209\n",
            "Iteration 4000: Current Learning Rate: 0.020048036036036054\n",
            "Iteration 6000: Current Learning Rate: 0.04005604504504506\n",
            "Iteration 8000: Current Learning Rate: 0.020038027027027023\n",
            "Iteration 10000: Current Learning Rate: 2.0009009009007906e-05\n",
            "Iteration 12000: Current Learning Rate: 0.01502851801801802\n",
            "Iteration 14000: Current Learning Rate: 0.005019509009009013\n",
            "Iteration 16000: Current Learning Rate: 0.010021511261261265\n",
            "Iteration 18000: Current Learning Rate: 0.005017006756756755\n",
            "Iteration 20000: Current Learning Rate: 1.2502252252246421e-05\n",
            "o_success_attack  tensor(270, device='cuda:0')\n",
            "tensor([1.5625e+01, 1.0000e+06, 1.0000e+06,  ..., 1.0000e+06, 1.0000e+06,\n",
            "        1.0000e+06], device='cuda:0')\n",
            "Iteration 2000: Current Learning Rate: 0.06008407207207209\n",
            "Iteration 4000: Current Learning Rate: 0.020048036036036054\n",
            "Iteration 6000: Current Learning Rate: 0.04005604504504506\n",
            "Iteration 8000: Current Learning Rate: 0.020038027027027023\n",
            "Iteration 10000: Current Learning Rate: 2.0009009009007906e-05\n",
            "Iteration 12000: Current Learning Rate: 0.01502851801801802\n",
            "Iteration 14000: Current Learning Rate: 0.005019509009009013\n",
            "Iteration 16000: Current Learning Rate: 0.010021511261261265\n",
            "Iteration 18000: Current Learning Rate: 0.005017006756756755\n",
            "Iteration 20000: Current Learning Rate: 1.2502252252246421e-05\n",
            "o_success_attack  tensor(271, device='cuda:0')\n",
            "tensor([1.2812e+01, 1.0000e+07, 1.0000e+07,  ..., 1.0000e+07, 1.0000e+07,\n",
            "        1.0000e+07], device='cuda:0')\n",
            "Iteration 2000: Current Learning Rate: 0.06008407207207209\n",
            "Iteration 4000: Current Learning Rate: 0.020048036036036054\n",
            "Iteration 6000: Current Learning Rate: 0.04005604504504506\n",
            "Iteration 8000: Current Learning Rate: 0.020038027027027023\n",
            "Iteration 10000: Current Learning Rate: 2.0009009009007906e-05\n",
            "Iteration 12000: Current Learning Rate: 0.01502851801801802\n",
            "Iteration 14000: Current Learning Rate: 0.005019509009009013\n",
            "Iteration 16000: Current Learning Rate: 0.010021511261261265\n",
            "Iteration 18000: Current Learning Rate: 0.005017006756756755\n",
            "Iteration 20000: Current Learning Rate: 1.2502252252246421e-05\n",
            "1130\n",
            "274\n",
            "Accuracy of just malwares (without attack): 98.85% | Under attack: 75.75%.\n"
          ]
        }
      ],
      "source": [
        "#CW(mal_x_batch, mal_y_batch, model_AT_rFGSM, removal_array, max_iterations=1000, learning_rate=0.01, binary_search_steps=3, confidence=0.01, initial_const=0.1, device=device)\n",
        "attack_params =  {'device':device, 'confidence':0.1, 'removal_array':removal_array,'max_iterations': 20000, 'learning_rate': 0.3, 'initial_const': 0.1 , 'binary_search_steps' : 9, 'cons_increase_factor' : 10}\n",
        "adv = adv_predict(test_loader, combined_model, CW_l1, **attack_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJKK4Ogdm4_n"
      },
      "outputs": [],
      "source": [
        "for x_test, y_test in test_loader:\n",
        "    x_test, y_test = x_test.to(torch.float32).to(device), y_test.to(device)\n",
        "    mal_x_batch, mal_y_batch = x_test[y_test.squeeze() == 1], y_test[y_test.squeeze() == 1]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation on non-rounded adversarial examples\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "combined_model.eval()\n",
        "\n",
        "# Get the model's predictions on adversarial examples\n",
        "outputs = combined_model(adv)\n",
        "y_pred = outputs.argmax(dim=1)\n",
        "\n",
        "# Calculate metrics\n",
        "num_malwares = len(mal_y_batch)\n",
        "num_successful_attacks = (y_pred == 0).sum().item()\n",
        "total_difference = (torch.abs(adv[y_pred == 0] - mal_x_batch[y_pred == 0])).sum().item()\n",
        "accuracy = (y_pred == 1).sum().item() / num_malwares * 100\n",
        "\n",
        "# Print results with clear formatting\n",
        "print(f\"Evaluation Results on NOT_Rounded Adversarial Examples\")\n",
        "print(f\"-----------------------------------------------\")\n",
        "print(f\"Number of malware samples: {num_malwares}\")\n",
        "print(f\"Number of successful attacks: {num_successful_attacks}\")\n",
        "print(f\"Total difference (L1 norm) between successful adversarial examples and original: {total_difference}\")\n",
        "print(f\"Accuracy of the model on malwares under attack: {accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2843255c-b511-43dd-f13d-1b914c65de3d",
        "id": "9Be6vLz6m4_o"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Results on NOT_Rounded Adversarial Examples\n",
            "-----------------------------------------------\n",
            "Number of malware samples: 1130\n",
            "Number of successful attacks: 274\n",
            "Total difference (L1 norm) between successful adversarial examples and original: 5365.03271484375\n",
            "Accuracy of the model on malwares under attack: 75.75%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation on rounded adversarial examples\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "combined_model.eval()\n",
        "\n",
        "# Apply the final rounding solution to adversarial examples\n",
        "adv_rounded = final_rounding(mal_x_batch, adv, combined_model)\n",
        "\n",
        "# Get the model's predictions on the rounded adversarial examples\n",
        "outputs = combined_model(adv_rounded)\n",
        "y_pred = outputs.argmax(dim=1)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "num_malwares = len(mal_y_batch)\n",
        "num_successful_attacks = (y_pred == 0).sum().item()\n",
        "total_difference = (torch.abs(adv_rounded[y_pred == 0] - mal_x_batch[y_pred == 0])).sum().item()\n",
        "accuracy = (y_pred == 1).sum().item() / num_malwares * 100\n",
        "\n",
        "# Display the results with clear formatting\n",
        "print(f\"Evaluation Results on Rounded Adversarial Examples\")\n",
        "print(f\"-----------------------------------------------\")\n",
        "print(f\"Number of malware samples: {num_malwares}\")\n",
        "print(f\"Number of successful attacks: {num_successful_attacks}\")\n",
        "print(f\"Total L1 norm difference (successful adversarial examples vs. original): {total_difference}\")\n",
        "print(f\"Accuracy of the model on malware under attack: {accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af9181e8-e4da-4bc8-869d-3202a236e322",
        "id": "2wGdR_jOm4_o"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Results on Rounded Adversarial Examples\n",
            "-----------------------------------------------\n",
            "Number of malware samples: 1130\n",
            "Number of successful attacks: 218\n",
            "Total L1 norm difference (successful adversarial examples vs. original): 4297.0\n",
            "Accuracy of the model on malware under attack: 80.71%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure that fixed elements are not changed\n",
        "\n",
        "# Ensure `removal_array` is expanded to match the batch size\n",
        "expanded_removal_array = removal_array.expand(mal_x_batch.shape[0], -1)\n",
        "\n",
        "# Create a mask where non-fixed features are set to 1\n",
        "non_fixed_features_mask = torch.bitwise_or(expanded_removal_array, 1 - mal_x_batch.to(torch.uint8))\n",
        "\n",
        "# Calculate indices where the mask is 0 (fixed) or 1 (non-fixed)\n",
        "mask_fixed = non_fixed_features_mask == 0\n",
        "mask_non_fixed = non_fixed_features_mask == 1\n",
        "\n",
        "# Calculate and print the differences\n",
        "diff_adv_fixed = (torch.abs(adv - mal_x_batch) * mask_fixed).sum().item()\n",
        "diff_adv_non_fixed = (torch.abs(adv - mal_x_batch) * mask_non_fixed).sum().item()\n",
        "diff_adv_rounded_fixed = (torch.abs(adv_rounded - mal_x_batch) * mask_fixed).sum().item()\n",
        "diff_adv_rounded_non_fixed = (torch.abs(adv_rounded - mal_x_batch) * mask_non_fixed).sum().item()\n",
        "\n",
        "# Display results with clear formatting\n",
        "print(f\"Differences when mask is 0 (fixed features):\")\n",
        "print(f\"  Adv vs. Original: {diff_adv_fixed}\")\n",
        "print(f\"  Rounded Adv vs. Original: {diff_adv_rounded_fixed}\")\n",
        "print(f\"Differences when mask is 1 (non-fixed features):\")\n",
        "print(f\"  Adv vs. Original: {diff_adv_non_fixed}\")\n",
        "print(f\"  Rounded Adv vs. Original: {diff_adv_rounded_non_fixed}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "811e9422-a4ca-4cce-b69f-56e0d54881ac",
        "id": "RpTbQsF7m4_o"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Differences when mask is 0 (fixed features):\n",
            "  Adv vs. Original: 0.0\n",
            "  Rounded Adv vs. Original: 0.0\n",
            "Differences when mask is 1 (non-fixed features):\n",
            "  Adv vs. Original: 58572.51953125\n",
            "  Rounded Adv vs. Original: 60531.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = combined_model(adv)\n",
        "y_pred = outputs.argmax(dim=1)"
      ],
      "metadata": {
        "id": "1WW6zX9Em4_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adv[y_pred==0][0][:200]- mal_x_batch[y_pred==0][0][:200]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6f6b70c-7c72-4345-c463-21b1e5634fe0",
        "id": "RqzfSGQ1m4_p"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00, -1.1921e-07,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  1.0000e+00,  4.1911e-01,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  9.9999e-01,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  8.5227e-01,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  6.1245e-01,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "adv[y_pred==0][1][:200] - mal_x_batch[y_pred==0][1][:200]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aee5eec8-3563-4aec-b6ea-8940b1033872",
        "id": "ReTzSz3km4_p"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.0000e+00,  1.7735e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         4.7410e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00, -5.9605e-08,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00, -1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  2.9802e-08,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00,\n",
              "         0.0000e+00,  1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  4.9940e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  2.9802e-08,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EhCpuIwn6W9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New function2"
      ],
      "metadata": {
        "id": "dbMHy9bF6XLW"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hDPpTEaCDaMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "from torch.optim.lr_scheduler import CyclicLR\n",
        "\n",
        "def compare(output, target, confidence, targeted):\n",
        "    \"\"\"\n",
        "    Compares the output with the target for a targeted or untargeted attack.\n",
        "\n",
        "    For targeted attacks, it checks if the output class matches the target class.\n",
        "    For untargeted attacks, it checks if the output class does not match the target class.\n",
        "    \"\"\"\n",
        "    if not isinstance(output, (float, int, torch.LongTensor, torch.cuda.LongTensor)):\n",
        "        output = torch.clone(output).detach()\n",
        "        if targeted:\n",
        "            output[target] -= confidence  # Slightly reduce the confidence for targeted attacks\n",
        "        else:\n",
        "            output[target] += confidence  # Slightly increase the confidence for untargeted attacks\n",
        "        output = torch.argmax(output, dim=1)\n",
        "    return output == target if targeted else output != target\n",
        "\n",
        "def sin_func_torch(input_tensor):\n",
        "    \"\"\"\n",
        "    A sine-based transformation applied to the input tensor.\n",
        "\n",
        "    The function applies a sine transformation and rescales the output to [0, 1].\n",
        "    \"\"\"\n",
        "    return (torch.sin((input_tensor - 0.5) * torch.pi) + 1) / 2.0\n",
        "\n",
        "def compare_round(mal_x_batch,newimg, model):\n",
        "\n",
        "    adv_rounded = final_rounding(mal_x_batch, newimg, model)\n",
        "    logits = model(adv_rounded)\n",
        "\n",
        "    output = torch.argmax(logits, dim=1)\n",
        "    return output == 0\n",
        "\n",
        "def CW_l1(imgs, y, model, removal_array, max_iterations=1000, learning_rate=0.01,\n",
        "       binary_search_steps=3, confidence=0.01, initial_const=0.1,\n",
        "       cons_increase_factor=2, device='cuda'):\n",
        "    \"\"\"\n",
        "    Implements the Carlini & Wagner (CW) L1 attack.\n",
        "\n",
        "    Parameters:\n",
        "    - imgs: The original images to be attacked.\n",
        "    - y: The true labels of the images.\n",
        "    - model: The target model to attack.\n",
        "    - removal_array: A mask array specifying which features are fixed.\n",
        "    - max_iterations: The maximum number of iterations for the attack.\n",
        "    - learning_rate: The learning rate for the optimizer.\n",
        "    - binary_search_steps: The number of steps for the binary search over the constant.\n",
        "    - confidence: Confidence value for the attack.\n",
        "    - initial_const: The initial constant for the attack loss.\n",
        "    - cons_increase_factor: The factor by which the constant is increased during binary search.\n",
        "    - device: The device on which to perform computations.\n",
        "\n",
        "    Returns:\n",
        "    - final_output: The perturbed images after the attack.\n",
        "    \"\"\"\n",
        "\n",
        "    labs = torch.zeros_like(y.squeeze(), dtype=torch.int64)\n",
        "    batch_size = imgs.shape[0]\n",
        "    targeted = True\n",
        "\n",
        "    # Expand removal array to match the batch size\n",
        "    expanded_removal_array = removal_array.expand(imgs.shape[0], -1)\n",
        "    non_fixed_features_mask = torch.bitwise_or(expanded_removal_array, 1 - imgs.to(torch.uint8))\n",
        "\n",
        "    mask_0 = non_fixed_features_mask == 0\n",
        "    mask_1 = non_fixed_features_mask == 1\n",
        "\n",
        "    imgs_tensor = imgs.clone().detach().to(dtype=torch.float32, device=device)\n",
        "    labs_tensor = labs.clone().detach().to(dtype=torch.int64, device=device)\n",
        "\n",
        "    lower_bound = torch.zeros(batch_size, dtype=torch.float32, device=device)\n",
        "    CONST = torch.full((batch_size,), initial_const, dtype=torch.float32, device=device)\n",
        "    upper_bound = torch.full((batch_size,), 1e10, dtype=torch.float32, device=device)\n",
        "\n",
        "    o_bestl2 = torch.full((batch_size,), 1e10, device=device)\n",
        "    o_bestscore = torch.full((batch_size,), -1, device=device)\n",
        "    o_bestattack = imgs.clone().detach()\n",
        "\n",
        "    o_best_loss1 = torch.full((batch_size,), float('inf'), device=device)\n",
        "    o_best_unsuccessfull_attack = imgs.clone().detach()\n",
        "    o_success_attack = torch.zeros(batch_size, dtype=torch.bool, device=device)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    for outer_step in range(binary_search_steps):\n",
        "        print(CONST)\n",
        "\n",
        "        modifier = torch.zeros_like(imgs_tensor, requires_grad=True).to(device)\n",
        "        optimizer = torch.optim.Adam([modifier], lr=learning_rate)\n",
        "\n",
        "        def grad_hook(grad):\n",
        "            return grad * non_fixed_features_mask\n",
        "        modifier.register_hook(grad_hook)\n",
        "\n",
        "        scheduler = CyclicLR(optimizer, base_lr=0.00001, max_lr=0.1, step_size_up=5,\n",
        "                             step_size_down=4995, mode='triangular2')\n",
        "\n",
        "        best_loss = torch.full((batch_size,), float('inf'), device=device)\n",
        "        no_improvement_count = torch.zeros(batch_size, dtype=torch.int, device=device)\n",
        "        active_mask = torch.ones(batch_size, dtype=torch.bool, device=device)\n",
        "\n",
        "        bestl2 = torch.full((batch_size,), 1e10, device=device)\n",
        "        bestscore = torch.full((batch_size,), -1, device=device)\n",
        "\n",
        "        for iteration in range(max_iterations):\n",
        "            if (iteration + 1) % 2000 == 0:\n",
        "                current_lr = scheduler.get_last_lr()[0]\n",
        "                print(f\"Iteration {iteration + 1}: Current Learning Rate: {current_lr}\")\n",
        "\n",
        "            active_indices = torch.where(active_mask)[0]\n",
        "            active_modifier = modifier[active_mask]\n",
        "            active_imgs_tensor = imgs_tensor[active_mask]\n",
        "            active_labs_tensor = labs_tensor[active_mask]\n",
        "            active_CONST = CONST[active_mask]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            modified_input = active_imgs_tensor + active_modifier\n",
        "            newimg = sin_func_torch(modified_input)\n",
        "            output = model(newimg)\n",
        "\n",
        "            perturbation = torch.where((active_imgs_tensor==1.), (active_imgs_tensor - newimg), (newimg - active_imgs_tensor))\n",
        "\n",
        "            l2dist = torch.sum(perturbation, dim=1)\n",
        "\n",
        "            real = output[torch.arange(active_indices.size(0)), active_labs_tensor]\n",
        "            other = output[torch.arange(active_indices.size(0)), active_labs_tensor + 1]\n",
        "\n",
        "            loss1 = torch.clamp((other - real + confidence), min=0.0)\n",
        "\n",
        "            if outer_step != (binary_search_steps - 1):\n",
        "                loss = (active_CONST * loss1) + l2dist\n",
        "            else:\n",
        "                loss = loss1\n",
        "\n",
        "            total_loss = torch.sum(loss)\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                improved_mask = loss < best_loss[active_mask]\n",
        "                best_loss[active_mask] = torch.where(improved_mask, loss, best_loss[active_mask])\n",
        "                no_improvement_count[active_mask] = torch.where(improved_mask,\n",
        "                    torch.zeros_like(no_improvement_count[active_mask]),\n",
        "                    no_improvement_count[active_mask] + 1)\n",
        "\n",
        "                improved_mask2 = (loss1 < o_best_loss1[active_mask])\n",
        "                o_best_loss1[active_mask] = torch.where(improved_mask2, loss1, o_best_loss1[active_mask])\n",
        "                o_best_unsuccessfull_attack[active_mask] = torch.where(improved_mask2.unsqueeze(-1), newimg, o_best_unsuccessfull_attack[active_mask])\n",
        "\n",
        "                update_mask = (l2dist < bestl2[active_mask]) & compare(output, active_labs_tensor, confidence, targeted)\n",
        "                bestl2[active_mask] = torch.where(update_mask, l2dist, bestl2[active_mask])\n",
        "                bestscore[active_mask] = torch.where(update_mask, torch.argmax(output, dim=1), bestscore[active_mask])\n",
        "\n",
        "                update_mask2 = (l2dist < o_bestl2[active_mask]) & compare(output, active_labs_tensor, confidence, targeted)\n",
        "                o_bestl2[active_mask] = torch.where(update_mask2, l2dist, o_bestl2[active_mask])\n",
        "                o_bestscore[active_mask] = torch.where(update_mask2, torch.argmax(output, dim=1), o_bestscore[active_mask])\n",
        "                o_bestattack[active_mask] = torch.where(update_mask2.unsqueeze(-1), newimg, o_bestattack[active_mask])\n",
        "\n",
        "                # Skip the rest of the loop for samples that haven't improved in 50 iterations\n",
        "                #active_mask[active_mask.clone()] = no_improvement_count[active_mask.clone()] < 50\n",
        "\n",
        "        if outer_step != (binary_search_steps - 1):\n",
        "            success_mask = (bestscore != -1) & compare(bestscore, labs_tensor, confidence, targeted)\n",
        "            o_success_attack  = torch.logical_or(o_success_attack, success_mask)\n",
        "            print('o_success_attack ', o_success_attack.sum())\n",
        "\n",
        "            o_success_attack_round = compare_round(imgs_tensor,o_bestattack, model)\n",
        "            print('o_success_attack_round ', o_success_attack_round.sum())\n",
        "            print('----------------------------------------')\n",
        "\n",
        "            failure_mask = ~success_mask\n",
        "            upper_bound[success_mask] = torch.minimum(upper_bound[success_mask], CONST[success_mask])\n",
        "            lower_bound[failure_mask] = torch.maximum(lower_bound[failure_mask], CONST[failure_mask])\n",
        "\n",
        "            upper_limit_mask = upper_bound < 1e10\n",
        "            CONST[upper_limit_mask] = (lower_bound[upper_limit_mask] + upper_bound[upper_limit_mask]) / 2\n",
        "            CONST[failure_mask & ~upper_limit_mask] *= cons_increase_factor\n",
        "\n",
        "    success_attack_mask = (o_bestl2 != 1e10)\n",
        "    final_output = torch.where(success_attack_mask.unsqueeze(-1), o_bestattack, o_best_unsuccessfull_attack)\n",
        "\n",
        "    return final_output\n"
      ],
      "metadata": {
        "id": "SS4cT2_KDabb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00e5d60e-8c0c-4ace-a2b3-a934e4fb0dab",
        "id": "oqtFxCyY6XLX"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.1000, 0.1000, 0.1000,  ..., 0.1000, 0.1000, 0.1000], device='cuda:0')\n",
            "Iteration 2000: Current Learning Rate: 0.06008407207207209\n",
            "Iteration 4000: Current Learning Rate: 0.020048036036036054\n",
            "Iteration 6000: Current Learning Rate: 0.04005604504504506\n",
            "Iteration 8000: Current Learning Rate: 0.020038027027027023\n",
            "Iteration 10000: Current Learning Rate: 2.0009009009007906e-05\n",
            "Iteration 12000: Current Learning Rate: 0.01502851801801802\n",
            "Iteration 14000: Current Learning Rate: 0.005019509009009013\n",
            "Iteration 16000: Current Learning Rate: 0.010021511261261265\n",
            "Iteration 18000: Current Learning Rate: 0.005017006756756755\n",
            "Iteration 20000: Current Learning Rate: 1.2502252252246421e-05\n",
            "o_success_attack  tensor(17, device='cuda:0')\n",
            "o_success_attack_round  tensor(17, device='cuda:0')\n",
            "----------------------------------------\n",
            "tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0')\n",
            "Iteration 2000: Current Learning Rate: 0.06008407207207209\n",
            "Iteration 4000: Current Learning Rate: 0.020048036036036054\n",
            "Iteration 6000: Current Learning Rate: 0.04005604504504506\n",
            "Iteration 8000: Current Learning Rate: 0.020038027027027023\n",
            "Iteration 10000: Current Learning Rate: 2.0009009009007906e-05\n",
            "Iteration 12000: Current Learning Rate: 0.01502851801801802\n",
            "Iteration 14000: Current Learning Rate: 0.005019509009009013\n",
            "Iteration 16000: Current Learning Rate: 0.010021511261261265\n",
            "Iteration 18000: Current Learning Rate: 0.005017006756756755\n",
            "Iteration 20000: Current Learning Rate: 1.2502252252246421e-05\n",
            "o_success_attack  tensor(49, device='cuda:0')\n",
            "o_success_attack_round  tensor(49, device='cuda:0')\n",
            "----------------------------------------\n",
            "tensor([10., 10., 10.,  ..., 10., 10., 10.], device='cuda:0')\n",
            "Iteration 2000: Current Learning Rate: 0.06008407207207209\n",
            "Iteration 4000: Current Learning Rate: 0.020048036036036054\n",
            "Iteration 6000: Current Learning Rate: 0.04005604504504506\n",
            "Iteration 8000: Current Learning Rate: 0.020038027027027023\n",
            "Iteration 10000: Current Learning Rate: 2.0009009009007906e-05\n",
            "Iteration 12000: Current Learning Rate: 0.01502851801801802\n",
            "Iteration 14000: Current Learning Rate: 0.005019509009009013\n",
            "Iteration 16000: Current Learning Rate: 0.010021511261261265\n",
            "Iteration 18000: Current Learning Rate: 0.005017006756756755\n",
            "Iteration 20000: Current Learning Rate: 1.2502252252246421e-05\n",
            "o_success_attack  tensor(145, device='cuda:0')\n",
            "o_success_attack_round  tensor(142, device='cuda:0')\n",
            "----------------------------------------\n",
            "tensor([100., 100., 100.,  ..., 100., 100., 100.], device='cuda:0')\n",
            "Iteration 2000: Current Learning Rate: 0.06008407207207209\n",
            "Iteration 4000: Current Learning Rate: 0.020048036036036054\n",
            "Iteration 6000: Current Learning Rate: 0.04005604504504506\n",
            "Iteration 8000: Current Learning Rate: 0.020038027027027023\n",
            "Iteration 10000: Current Learning Rate: 2.0009009009007906e-05\n",
            "Iteration 12000: Current Learning Rate: 0.01502851801801802\n",
            "Iteration 14000: Current Learning Rate: 0.005019509009009013\n",
            "Iteration 16000: Current Learning Rate: 0.010021511261261265\n",
            "Iteration 18000: Current Learning Rate: 0.005017006756756755\n",
            "Iteration 20000: Current Learning Rate: 1.2502252252246421e-05\n",
            "o_success_attack  tensor(233, device='cuda:0')\n",
            "o_success_attack_round  tensor(220, device='cuda:0')\n",
            "----------------------------------------\n",
            "tensor([  55., 1000., 1000.,  ..., 1000., 1000., 1000.], device='cuda:0')\n",
            "Iteration 2000: Current Learning Rate: 0.06008407207207209\n",
            "Iteration 4000: Current Learning Rate: 0.020048036036036054\n",
            "Iteration 6000: Current Learning Rate: 0.04005604504504506\n",
            "Iteration 8000: Current Learning Rate: 0.020038027027027023\n",
            "Iteration 10000: Current Learning Rate: 2.0009009009007906e-05\n",
            "Iteration 12000: Current Learning Rate: 0.01502851801801802\n",
            "Iteration 14000: Current Learning Rate: 0.005019509009009013\n",
            "Iteration 16000: Current Learning Rate: 0.010021511261261265\n",
            "Iteration 18000: Current Learning Rate: 0.005017006756756755\n",
            "Iteration 20000: Current Learning Rate: 1.2502252252246421e-05\n",
            "o_success_attack  tensor(255, device='cuda:0')\n",
            "o_success_attack_round  tensor(232, device='cuda:0')\n",
            "----------------------------------------\n",
            "tensor([   32.5000, 10000.0000, 10000.0000,  ..., 10000.0000, 10000.0000,\n",
            "        10000.0000], device='cuda:0')\n",
            "Iteration 2000: Current Learning Rate: 0.06008407207207209\n",
            "Iteration 4000: Current Learning Rate: 0.020048036036036054\n",
            "Iteration 6000: Current Learning Rate: 0.04005604504504506\n",
            "Iteration 8000: Current Learning Rate: 0.020038027027027023\n",
            "Iteration 10000: Current Learning Rate: 2.0009009009007906e-05\n",
            "Iteration 12000: Current Learning Rate: 0.01502851801801802\n",
            "Iteration 14000: Current Learning Rate: 0.005019509009009013\n",
            "Iteration 16000: Current Learning Rate: 0.010021511261261265\n",
            "Iteration 18000: Current Learning Rate: 0.005017006756756755\n",
            "Iteration 20000: Current Learning Rate: 1.2502252252246421e-05\n",
            "o_success_attack  tensor(261, device='cuda:0')\n",
            "o_success_attack_round  tensor(234, device='cuda:0')\n",
            "----------------------------------------\n",
            "tensor([2.1250e+01, 1.0000e+05, 1.0000e+05,  ..., 1.0000e+05, 1.0000e+05,\n",
            "        1.0000e+05], device='cuda:0')\n",
            "Iteration 2000: Current Learning Rate: 0.06008407207207209\n",
            "Iteration 4000: Current Learning Rate: 0.020048036036036054\n",
            "Iteration 6000: Current Learning Rate: 0.04005604504504506\n",
            "Iteration 8000: Current Learning Rate: 0.020038027027027023\n",
            "Iteration 10000: Current Learning Rate: 2.0009009009007906e-05\n",
            "Iteration 12000: Current Learning Rate: 0.01502851801801802\n",
            "Iteration 14000: Current Learning Rate: 0.005019509009009013\n",
            "Iteration 16000: Current Learning Rate: 0.010021511261261265\n",
            "Iteration 18000: Current Learning Rate: 0.005017006756756755\n",
            "Iteration 20000: Current Learning Rate: 1.2502252252246421e-05\n",
            "o_success_attack  tensor(264, device='cuda:0')\n",
            "o_success_attack_round  tensor(230, device='cuda:0')\n",
            "----------------------------------------\n",
            "tensor([1.5625e+01, 1.0000e+06, 1.0000e+06,  ..., 1.0000e+06, 1.0000e+06,\n",
            "        1.0000e+06], device='cuda:0')\n",
            "Iteration 2000: Current Learning Rate: 0.06008407207207209\n",
            "Iteration 4000: Current Learning Rate: 0.020048036036036054\n",
            "Iteration 6000: Current Learning Rate: 0.04005604504504506\n",
            "Iteration 8000: Current Learning Rate: 0.020038027027027023\n",
            "Iteration 10000: Current Learning Rate: 2.0009009009007906e-05\n",
            "Iteration 12000: Current Learning Rate: 0.01502851801801802\n",
            "Iteration 14000: Current Learning Rate: 0.005019509009009013\n",
            "Iteration 16000: Current Learning Rate: 0.010021511261261265\n",
            "Iteration 18000: Current Learning Rate: 0.005017006756756755\n",
            "Iteration 20000: Current Learning Rate: 1.2502252252246421e-05\n",
            "o_success_attack  tensor(269, device='cuda:0')\n",
            "o_success_attack_round  tensor(238, device='cuda:0')\n",
            "----------------------------------------\n",
            "tensor([1.2812e+01, 1.0000e+07, 1.0000e+07,  ..., 1.0000e+07, 1.0000e+07,\n",
            "        1.0000e+07], device='cuda:0')\n",
            "Iteration 2000: Current Learning Rate: 0.06008407207207209\n",
            "Iteration 4000: Current Learning Rate: 0.020048036036036054\n",
            "Iteration 6000: Current Learning Rate: 0.04005604504504506\n",
            "Iteration 8000: Current Learning Rate: 0.020038027027027023\n",
            "Iteration 10000: Current Learning Rate: 2.0009009009007906e-05\n",
            "Iteration 12000: Current Learning Rate: 0.01502851801801802\n",
            "Iteration 14000: Current Learning Rate: 0.005019509009009013\n",
            "Iteration 16000: Current Learning Rate: 0.010021511261261265\n",
            "Iteration 18000: Current Learning Rate: 0.005017006756756755\n",
            "Iteration 20000: Current Learning Rate: 1.2502252252246421e-05\n",
            "1130\n",
            "275\n",
            "Accuracy of just malwares (without attack): 98.85% | Under attack: 75.66%.\n"
          ]
        }
      ],
      "source": [
        "#CW(mal_x_batch, mal_y_batch, model_AT_rFGSM, removal_array, max_iterations=1000, learning_rate=0.01, binary_search_steps=3, confidence=0.01, initial_const=0.1, device=device)\n",
        "attack_params =  {'device':device, 'confidence':0.5, 'removal_array':removal_array,'max_iterations': 20000, 'learning_rate': 0.3, 'initial_const': 0.1 , 'binary_search_steps' : 9, 'cons_increase_factor' : 10}\n",
        "adv = adv_predict(test_loader, combined_model, CW_l1, **attack_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pW2CXpc-6XLY"
      },
      "outputs": [],
      "source": [
        "for x_test, y_test in test_loader:\n",
        "    x_test, y_test = x_test.to(torch.float32).to(device), y_test.to(device)\n",
        "    mal_x_batch, mal_y_batch = x_test[y_test.squeeze() == 1], y_test[y_test.squeeze() == 1]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation on non-rounded adversarial examples\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "combined_model.eval()\n",
        "\n",
        "# Get the model's predictions on adversarial examples\n",
        "outputs = combined_model(adv)\n",
        "y_pred = outputs.argmax(dim=1)\n",
        "\n",
        "# Calculate metrics\n",
        "num_malwares = len(mal_y_batch)\n",
        "num_successful_attacks = (y_pred == 0).sum().item()\n",
        "total_difference = (torch.abs(adv[y_pred == 0] - mal_x_batch[y_pred == 0])).sum().item()\n",
        "accuracy = (y_pred == 1).sum().item() / num_malwares * 100\n",
        "\n",
        "# Print results with clear formatting\n",
        "print(f\"Evaluation Results on NOT_Rounded Adversarial Examples\")\n",
        "print(f\"-----------------------------------------------\")\n",
        "print(f\"Number of malware samples: {num_malwares}\")\n",
        "print(f\"Number of successful attacks: {num_successful_attacks}\")\n",
        "print(f\"Total difference (L1 norm) between successful adversarial examples and original: {total_difference}\")\n",
        "print(f\"Accuracy of the model on malwares under attack: {accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf0222d2-f053-4b37-8b83-7a098fa50965",
        "id": "Ii4vsydC6XLY"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Results on NOT_Rounded Adversarial Examples\n",
            "-----------------------------------------------\n",
            "Number of malware samples: 1130\n",
            "Number of successful attacks: 275\n",
            "Total difference (L1 norm) between successful adversarial examples and original: 5782.31494140625\n",
            "Accuracy of the model on malwares under attack: 75.66%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation on rounded adversarial examples\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "combined_model.eval()\n",
        "\n",
        "# Apply the final rounding solution to adversarial examples\n",
        "adv_rounded = final_rounding(mal_x_batch, adv, combined_model)\n",
        "\n",
        "# Get the model's predictions on the rounded adversarial examples\n",
        "outputs = combined_model(adv_rounded)\n",
        "y_pred = outputs.argmax(dim=1)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "num_malwares = len(mal_y_batch)\n",
        "num_successful_attacks = (y_pred == 0).sum().item()\n",
        "total_difference = (torch.abs(adv_rounded[y_pred == 0] - mal_x_batch[y_pred == 0])).sum().item()\n",
        "accuracy = (y_pred == 1).sum().item() / num_malwares * 100\n",
        "\n",
        "# Display the results with clear formatting\n",
        "print(f\"Evaluation Results on Rounded Adversarial Examples\")\n",
        "print(f\"-----------------------------------------------\")\n",
        "print(f\"Number of malware samples: {num_malwares}\")\n",
        "print(f\"Number of successful attacks: {num_successful_attacks}\")\n",
        "print(f\"Total L1 norm difference (successful adversarial examples vs. original): {total_difference}\")\n",
        "print(f\"Accuracy of the model on malware under attack: {accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b3212c6-61a4-4143-ae09-2f1264d329d1",
        "id": "q6lmVI436XLY"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Results on Rounded Adversarial Examples\n",
            "-----------------------------------------------\n",
            "Number of malware samples: 1130\n",
            "Number of successful attacks: 239\n",
            "Total L1 norm difference (successful adversarial examples vs. original): 4648.0\n",
            "Accuracy of the model on malware under attack: 78.85%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure that fixed elements are not changed\n",
        "\n",
        "# Ensure `removal_array` is expanded to match the batch size\n",
        "expanded_removal_array = removal_array.expand(mal_x_batch.shape[0], -1)\n",
        "\n",
        "# Create a mask where non-fixed features are set to 1\n",
        "non_fixed_features_mask = torch.bitwise_or(expanded_removal_array, 1 - mal_x_batch.to(torch.uint8))\n",
        "\n",
        "# Calculate indices where the mask is 0 (fixed) or 1 (non-fixed)\n",
        "mask_fixed = non_fixed_features_mask == 0\n",
        "mask_non_fixed = non_fixed_features_mask == 1\n",
        "\n",
        "# Calculate and print the differences\n",
        "diff_adv_fixed = (torch.abs(adv - mal_x_batch) * mask_fixed).sum().item()\n",
        "diff_adv_non_fixed = (torch.abs(adv - mal_x_batch) * mask_non_fixed).sum().item()\n",
        "diff_adv_rounded_fixed = (torch.abs(adv_rounded - mal_x_batch) * mask_fixed).sum().item()\n",
        "diff_adv_rounded_non_fixed = (torch.abs(adv_rounded - mal_x_batch) * mask_non_fixed).sum().item()\n",
        "\n",
        "# Display results with clear formatting\n",
        "print(f\"Differences when mask is 0 (fixed features):\")\n",
        "print(f\"  Adv vs. Original: {diff_adv_fixed}\")\n",
        "print(f\"  Rounded Adv vs. Original: {diff_adv_rounded_fixed}\")\n",
        "print(f\"Differences when mask is 1 (non-fixed features):\")\n",
        "print(f\"  Adv vs. Original: {diff_adv_non_fixed}\")\n",
        "print(f\"  Rounded Adv vs. Original: {diff_adv_rounded_non_fixed}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c06584ee-34c4-4d16-f848-5d33bf840e37",
        "id": "sJNBFf-t6XLZ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Differences when mask is 0 (fixed features):\n",
            "  Adv vs. Original: 0.0\n",
            "  Rounded Adv vs. Original: 0.0\n",
            "Differences when mask is 1 (non-fixed features):\n",
            "  Adv vs. Original: 59298.15234375\n",
            "  Rounded Adv vs. Original: 61045.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UosK2HUXXrt-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adv[y_pred==0][0][:200]- mal_x_batch[y_pred==0][0][:200]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af84312a-eacc-4a43-970a-cac41e2a7475",
        "id": "QnVFEk6iXr10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00, -1.2825e-01,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  2.9802e-08,  5.9605e-08,\n",
              "         0.0000e+00,  1.1921e-07,  1.0000e+00,  3.7481e-01,  0.0000e+00,\n",
              "         2.9802e-08,  2.1508e-02,  0.0000e+00,  0.0000e+00,  9.9996e-01,\n",
              "         0.0000e+00,  1.5508e-01,  1.1921e-07,  0.0000e+00,  8.9407e-08,\n",
              "         9.9557e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  9.5120e-01,\n",
              "         0.0000e+00,  0.0000e+00,  1.4007e-06,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  7.0733e-04,\n",
              "         1.0000e+00,  0.0000e+00,  1.3709e-06,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  1.0000e+00,  0.0000e+00,  0.0000e+00,  1.4901e-07,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  2.6822e-07,  0.0000e+00,\n",
              "         7.7240e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  5.5462e-05,  0.0000e+00,  0.0000e+00,\n",
              "         4.2602e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  4.1227e-02,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  2.0862e-07,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "adv[y_pred==0][1][:200] - mal_x_batch[y_pred==0][1][:200]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af4c61f4-7393-461e-d0da-099e481c8f2b",
        "id": "GShI2P4FXr11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.0000e+00,  2.9802e-08,  0.0000e+00, -4.0293e-05,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  2.0862e-07,  0.0000e+00, -1.0192e-04,\n",
              "         0.0000e+00,  0.0000e+00,  4.9379e-01,  1.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  2.9802e-08,  1.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  2.9802e-08,  7.5877e-05,  5.8711e-06,\n",
              "         0.0000e+00,  0.0000e+00, -1.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00, -1.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  8.7380e-05,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00, -6.1989e-06,  0.0000e+00,\n",
              "         0.0000e+00, -1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0649e-03,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00,\n",
              "         0.0000e+00,  1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  6.7949e-06,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  4.7684e-07,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  1.7881e-07,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pmbkD_GEcLs4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A8_bJWdgcWcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New approach\n",
        "- rounding whole input tensor at once\n",
        "\n",
        "- new function for loss\n",
        "\n",
        "- our criterion is rounded adversarial examples during training"
      ],
      "metadata": {
        "id": "usFDK9SyiBQl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "nYAT3l88diYY"
      },
      "outputs": [],
      "source": [
        "def adv_predict(test_loader, model, attack, device, **kwargs):\n",
        "\n",
        "    model.eval()\n",
        "    n_samples = 0\n",
        "    cor_test = 0\n",
        "    cor_ad_test = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x_test, y_test in test_loader:\n",
        "            x_test, y_test = x_test.to(torch.float32).to(device), y_test.to(device)\n",
        "            #outputs = model(x_test)\n",
        "            #predicted = outputs.argmax(1).unsqueeze(1)\n",
        "            #acc_test = (predicted == y_test).sum().item() / len(y_test)\n",
        "            #avg_acc_test.append(acc_test)\n",
        "\n",
        "            mal_x_batch, mal_y_batch = x_test[y_test.squeeze() == 1], y_test[y_test.squeeze() == 1]\n",
        "            n_samples += len(mal_y_batch)\n",
        "\n",
        "            outputs = model(mal_x_batch)\n",
        "            predicted = outputs.argmax(1)\n",
        "            cor_test += (predicted == 1).sum().item()\n",
        "\n",
        "            # Generate adversarial examples for test set\n",
        "            with torch.enable_grad():\n",
        "                pertb_mal_x= attack(mal_x_batch, mal_y_batch, model, **kwargs)\n",
        "\n",
        "            adv_rounded = final_rounding(mal_x_batch, pertb_mal_x, model,removal_array)\n",
        "\n",
        "            outputs = model(adv_rounded)\n",
        "            y_pred = outputs.argmax(1)\n",
        "            cor_ad_test += (y_pred == 1).sum().item()\n",
        "\n",
        "            #print(outputs)\n",
        "            print(len(mal_y_batch))\n",
        "            print((y_pred == 0).sum().item())\n",
        "            #print((y_pred == 0))\n",
        "            #print('mean difference ',(torch.abs(pertb_mal_x - mal_x_batch)).sum()/len(mal_y_batch))\n",
        "            #print((torch.abs(pertb_mal_x[y_pred == 0] - mal_x_batch[y_pred == 0])).sum())\n",
        "            #print((torch.abs(pertb_mal_x - mal_x_batch)).sum(1))\n",
        "\n",
        "            #print('***************************')\n",
        "\n",
        "\n",
        "    # Calculate test accuracy\n",
        "    print(f\"Accuracy of just malwares (without attack): {(cor_test / n_samples) * 100:.4}% | Under attack: {(cor_ad_test / n_samples) * 100:.4}%.\")\n",
        "\n",
        "    return pertb_mal_x\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def final_rounding(oimgs, newimg, model, removal_array):\n",
        "    # Expand removal array to match the batch size\n",
        "    non_fixed_features_mask = torch.bitwise_or(removal_array.expand(oimgs.shape[0], -1), 1 - oimgs.to(torch.uint8))\n",
        "\n",
        "    # Ensure all tensors are on the correct device\n",
        "    oimgs = oimgs.to(device)\n",
        "    perturbed = newimg.to(device)\n",
        "\n",
        "    # Set the model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Initialize active_mask based on model's initial prediction\n",
        "    active_mask = (model(oimgs).argmax(dim=1) == 1)\n",
        "\n",
        "    # Calculate the absolute difference and direction of changes\n",
        "    total_change = (perturbed - oimgs).abs()\n",
        "    changes = (perturbed > oimgs).float()\n",
        "\n",
        "    # Determine significant changes\n",
        "    significant_mask = total_change > 0.1\n",
        "\n",
        "    # Sort indices of changes in descending order of significance\n",
        "    sorted_idx = total_change.argsort(dim=1, descending=True)\n",
        "\n",
        "    # Copy original images to apply final rounding\n",
        "    final = oimgs.clone()\n",
        "\n",
        "    # Iterate up to the maximum number of significant changes\n",
        "    max_significant_changes = significant_mask.sum(dim=1).max().item()\n",
        "\n",
        "    for i in range(max_significant_changes):\n",
        "        # Safeguard against empty active mask\n",
        "        if not active_mask.any():\n",
        "            break\n",
        "\n",
        "        # Get the indices of active samples\n",
        "        active_indices = torch.where(active_mask)[0]\n",
        "\n",
        "        # Get the relevant indices for current change\n",
        "        indices = sorted_idx[active_indices, i]\n",
        "\n",
        "        # since max_significant_changes is relatively large for some samples, possible that zero changes which belong to fixed_features get updated\n",
        "        final[active_indices, indices] = torch.where(non_fixed_features_mask[active_indices, indices],\n",
        "                                                     changes[active_indices, indices], final[active_indices, indices])\n",
        "\n",
        "        # Update the active_mask based on the model's prediction\n",
        "        active_mask[active_indices] = (model(final[active_indices]).argmax(dim=1) == 1)\n",
        "\n",
        "    return final\n"
      ],
      "metadata": {
        "id": "PsrCfM3INaT3"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#(1 - (perturbation - 1)^2 + eps)^0.5 - eps^0.5"
      ],
      "metadata": {
        "id": "c4tVc4eI7HSI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "from torch.optim.lr_scheduler import CyclicLR\n",
        "\n",
        "def compare(output, target, confidence):\n",
        "    if not isinstance(output, (float, int, torch.LongTensor, torch.cuda.LongTensor)):\n",
        "        output = torch.clone(output).detach()\n",
        "        output[target] -= confidence  # Slightly reduce the confidence for targeted attacks\n",
        "        output = torch.argmax(output, dim=1)\n",
        "    return output == target\n",
        "\n",
        "def compare_round(mal_x_batch,newimg, model,removal_array):\n",
        "\n",
        "    adv_rounded = final_rounding(mal_x_batch, newimg, model,removal_array)\n",
        "    #l0dist = torch.sum((mal_x_batch - newimg)!=0, dim=1)\n",
        "    l0dist = torch.sum(torch.abs(adv_rounded - mal_x_batch), dim=1)\n",
        "    logits = model(adv_rounded)\n",
        "    output = torch.argmax(logits, dim=1)\n",
        "\n",
        "    return output == 0, l0dist\n",
        "\n",
        "def sin_func_torch(input_tensor):\n",
        "    \"\"\"\n",
        "    A sine-based transformation applied to the input tensor.\n",
        "\n",
        "    The function applies a sine transformation and rescales the output to [0, 1].\n",
        "    \"\"\"\n",
        "    return (torch.sin((input_tensor - 0.5) * torch.pi) + 1) / 2.0\n",
        "\n",
        "def CW_l1(imgs, y, model, removal_array, max_iterations=1000, max_learning_rate=0.1,\n",
        "       binary_search_steps=3, confidence=0.01, initial_const=0.1,\n",
        "       cons_increase_factor=2, device='cuda'):\n",
        "    \"\"\"\n",
        "    Implements the Carlini & Wagner (CW) L1 attack.\n",
        "\n",
        "    Parameters:\n",
        "    - imgs: The original images to be attacked.\n",
        "    - y: The true labels of the images.\n",
        "    - model: The target model to attack.\n",
        "    - removal_array: A mask array specifying which features are fixed.\n",
        "    - max_iterations: The maximum number of iterations for the attack.\n",
        "    - learning_rate: The learning rate for the optimizer.\n",
        "    - binary_search_steps: The number of steps for the binary search over the constant.\n",
        "    - confidence: Confidence value for the attack.\n",
        "    - initial_const: The initial constant for the attack loss.\n",
        "    - cons_increase_factor: The factor by which the constant is increased during binary search.\n",
        "    - device: The device on which to perform computations.\n",
        "\n",
        "    Returns:\n",
        "    - final_output: The perturbed images after the attack.\n",
        "    \"\"\"\n",
        "\n",
        "    labs = torch.zeros_like(y.squeeze(), dtype=torch.int64)\n",
        "    batch_size = imgs.shape[0]\n",
        "\n",
        "    # Expand removal array to match the batch size\n",
        "    expanded_removal_array = removal_array.expand(imgs.shape[0], -1)\n",
        "    non_fixed_features_mask = torch.bitwise_or(expanded_removal_array, 1 - imgs.to(torch.uint8))\n",
        "\n",
        "    mask_0 = non_fixed_features_mask == 0\n",
        "    mask_1 = non_fixed_features_mask == 1\n",
        "\n",
        "    imgs_tensor = imgs.clone().detach().to(dtype=torch.float32, device=device)\n",
        "    labs_tensor = labs.clone().detach().to(dtype=torch.int64, device=device)\n",
        "\n",
        "    lower_bound = torch.zeros(batch_size, dtype=torch.float32, device=device)\n",
        "    CONST = torch.full((batch_size,), initial_const, dtype=torch.float32, device=device)\n",
        "    upper_bound = torch.full((batch_size,), 1e10, dtype=torch.float32, device=device)\n",
        "\n",
        "    o_bestl0 = torch.full((batch_size,), 1e10, device=device)\n",
        "    o_bestattack = imgs.clone().detach()\n",
        "\n",
        "    o_best_loss1 = torch.full((batch_size,), float('inf'), device=device)\n",
        "    o_best_unsuccessfull_attack = imgs.clone().detach()\n",
        "    o_success_attack = torch.zeros(batch_size, dtype=torch.bool, device=device)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    for outer_step in range(binary_search_steps):\n",
        "        print(CONST)\n",
        "\n",
        "        modifier = torch.zeros_like(imgs_tensor, requires_grad=True).to(device)\n",
        "        optimizer = torch.optim.Adam([modifier], lr=max_learning_rate)\n",
        "\n",
        "        def grad_hook(grad):\n",
        "            return grad * non_fixed_features_mask\n",
        "        modifier.register_hook(grad_hook)\n",
        "\n",
        "        scheduler = CyclicLR(optimizer, base_lr=0.0001, max_lr=max_learning_rate, step_size_up=5,\n",
        "                             step_size_down=4995, mode='triangular2')\n",
        "\n",
        "        best_loss = torch.full((batch_size,), float('inf'), device=device)\n",
        "        no_improvement_count = torch.zeros(batch_size, dtype=torch.int, device=device)\n",
        "        active_mask = torch.ones(batch_size, dtype=torch.bool, device=device)\n",
        "\n",
        "        bestl0 = torch.full((batch_size,), 1e10, device=device)\n",
        "        bestscore = torch.full((batch_size,), -1, device=device)\n",
        "        success_mask = torch.zeros(batch_size, dtype=torch.bool, device=device)\n",
        "\n",
        "        for iteration in range(max_iterations):\n",
        "\n",
        "            active_indices = torch.where(active_mask)[0]\n",
        "            active_modifier = modifier[active_mask]\n",
        "            active_imgs_tensor = imgs_tensor[active_mask]\n",
        "            active_labs_tensor = labs_tensor[active_mask]\n",
        "            active_CONST = CONST[active_mask]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            modified_input = active_imgs_tensor + active_modifier\n",
        "            newimg = sin_func_torch(modified_input)\n",
        "            output = model(newimg)\n",
        "\n",
        "            eps = 1e-2\n",
        "            perturbation = torch.where((active_imgs_tensor==1.), (active_imgs_tensor - newimg), (newimg - active_imgs_tensor))\n",
        "\n",
        "            l2dist = torch.sum((1 - (perturbation - 1)**2 + eps)**0.5 - eps**0.5 , dim=1)\n",
        "\n",
        "            real = output[torch.arange(active_indices.size(0)), active_labs_tensor]\n",
        "            other = output[torch.arange(active_indices.size(0)), active_labs_tensor + 1]\n",
        "\n",
        "            loss1 = torch.clamp((other - real + confidence), min=0.0)\n",
        "            #loss2 = l2dist\n",
        "\n",
        "            if outer_step != (binary_search_steps - 1):\n",
        "                loss = (active_CONST * loss1) + l2dist\n",
        "            else:\n",
        "                loss = loss1\n",
        "\n",
        "            total_loss = torch.sum(loss)\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            if (iteration + 1) % 10 == 0:\n",
        "                with torch.no_grad():\n",
        "                    success_mask_inloop, l0dist = compare_round(active_imgs_tensor,newimg, model,removal_array)\n",
        "                    success_mask  = torch.logical_or(success_mask_inloop, success_mask)\n",
        "                    current_lr = scheduler.get_last_lr()[0]\n",
        "\n",
        "                    improved_mask2 = (loss1 < o_best_loss1[active_mask])\n",
        "                    o_best_loss1[active_mask] = torch.where(improved_mask2, loss1, o_best_loss1[active_mask])\n",
        "                    o_best_unsuccessfull_attack[active_mask] = torch.where(improved_mask2.unsqueeze(-1), newimg, o_best_unsuccessfull_attack[active_mask])\n",
        "\n",
        "                    update_mask = (l0dist < bestl0[active_mask]) & success_mask_inloop\n",
        "                    bestl0[active_mask] = torch.where(update_mask, l0dist, bestl0[active_mask])\n",
        "                    bestscore[active_mask] = torch.where(update_mask, torch.argmax(output, dim=1), bestscore[active_mask])\n",
        "\n",
        "                    update_mask2 = (l0dist < o_bestl0[active_mask]) & success_mask_inloop\n",
        "                    o_bestl0[active_mask] = torch.where(update_mask2, l0dist, o_bestl0[active_mask])\n",
        "                    o_bestattack[active_mask] = torch.where(update_mask2.unsqueeze(-1), newimg, o_bestattack[active_mask])\n",
        "\n",
        "                    # Skip the rest of the loop for samples that haven't improved in 50 iterations\n",
        "                    #active_mask[active_mask.clone()] = no_improvement_count[active_mask.clone()] < 50\n",
        "\n",
        "                    if (iteration + 1) % 100 == 0:\n",
        "                        print(f\"Iteration {iteration + 1}: Current success : {success_mask_inloop.sum()} \\t All success : {success_mask.sum()} \\t mean(l0) : {o_bestl0[success_mask].mean():.6f} \\t Current Learning Rate: {current_lr:.3f}\")\n",
        "\n",
        "        if outer_step != (binary_search_steps - 1):\n",
        "            #o_success_attack  = torch.logical_or(o_success_attack, success_mask)\n",
        "            o_success_attack, o_l0dist = compare_round(imgs,o_bestattack, model,removal_array)\n",
        "            print(f\"outer_step {outer_step + 1}: all success : {o_success_attack.sum()} \\t mean(l0)(success) : {o_l0dist[o_success_attack].mean():.4f}\")\n",
        "            #print('o_success_attack ', o_success_attack.sum())\n",
        "            print('-------------------------------------------------------------')\n",
        "            failure_mask = ~success_mask\n",
        "            upper_bound[success_mask] = torch.minimum(upper_bound[success_mask], CONST[success_mask])\n",
        "            lower_bound[failure_mask] = torch.maximum(lower_bound[failure_mask], CONST[failure_mask])\n",
        "\n",
        "            upper_limit_mask = upper_bound < 1e10\n",
        "            CONST[upper_limit_mask] = (lower_bound[upper_limit_mask] + upper_bound[upper_limit_mask]) / 2\n",
        "            CONST[failure_mask & ~upper_limit_mask] *= cons_increase_factor\n",
        "\n",
        "    success_attack_mask = (o_bestl0 != 1e10)\n",
        "    final_output = torch.where(success_attack_mask.unsqueeze(-1), o_bestattack, o_best_unsuccessfull_attack)\n",
        "\n",
        "    return final_output\n"
      ],
      "metadata": {
        "id": "bCgDCL23Ix06"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# eps=1e4\n",
        "attack_params =  {'device':device, 'confidence':0.5, 'removal_array':removal_array,'max_iterations': 20000, 'max_learning_rate': 0.1, 'initial_const': 1. , 'binary_search_steps' : 5, 'cons_increase_factor' : 10}\n",
        "adv = adv_predict(test_loader, model_AT_rFGSM, CW_l1, **attack_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frq7j2egCLf8",
        "outputId": "acb1e8aa-ce95-4aac-9ee9-641147ed7682"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0')\n",
            "Iteration 100: Current success : 417 \t All success : 439 \t mean(l0) : 3.216401 \t Current Learning Rate: 0.098\n",
            "Iteration 200: Current success : 416 \t All success : 454 \t mean(l0) : 3.312775 \t Current Learning Rate: 0.096\n",
            "Iteration 300: Current success : 409 \t All success : 455 \t mean(l0) : 3.309890 \t Current Learning Rate: 0.094\n",
            "Iteration 400: Current success : 418 \t All success : 455 \t mean(l0) : 3.309890 \t Current Learning Rate: 0.092\n",
            "Iteration 500: Current success : 417 \t All success : 455 \t mean(l0) : 3.309890 \t Current Learning Rate: 0.090\n",
            "Iteration 600: Current success : 412 \t All success : 457 \t mean(l0) : 3.315099 \t Current Learning Rate: 0.088\n",
            "Iteration 700: Current success : 421 \t All success : 462 \t mean(l0) : 3.361472 \t Current Learning Rate: 0.086\n",
            "Iteration 800: Current success : 417 \t All success : 465 \t mean(l0) : 3.393548 \t Current Learning Rate: 0.084\n",
            "Iteration 900: Current success : 428 \t All success : 477 \t mean(l0) : 3.538784 \t Current Learning Rate: 0.082\n",
            "Iteration 1000: Current success : 433 \t All success : 480 \t mean(l0) : 3.543750 \t Current Learning Rate: 0.080\n",
            "Iteration 1100: Current success : 422 \t All success : 480 \t mean(l0) : 3.539583 \t Current Learning Rate: 0.078\n",
            "Iteration 1200: Current success : 423 \t All success : 480 \t mean(l0) : 3.533334 \t Current Learning Rate: 0.076\n",
            "Iteration 1300: Current success : 451 \t All success : 489 \t mean(l0) : 3.554192 \t Current Learning Rate: 0.074\n",
            "Iteration 1400: Current success : 422 \t All success : 489 \t mean(l0) : 3.554192 \t Current Learning Rate: 0.072\n",
            "Iteration 1500: Current success : 428 \t All success : 490 \t mean(l0) : 3.569388 \t Current Learning Rate: 0.070\n",
            "Iteration 1600: Current success : 456 \t All success : 490 \t mean(l0) : 3.567347 \t Current Learning Rate: 0.068\n",
            "Iteration 1700: Current success : 422 \t All success : 491 \t mean(l0) : 3.584522 \t Current Learning Rate: 0.066\n",
            "Iteration 1800: Current success : 441 \t All success : 492 \t mean(l0) : 3.591463 \t Current Learning Rate: 0.064\n",
            "Iteration 1900: Current success : 455 \t All success : 495 \t mean(l0) : 3.616162 \t Current Learning Rate: 0.062\n",
            "Iteration 2000: Current success : 449 \t All success : 502 \t mean(l0) : 3.661355 \t Current Learning Rate: 0.060\n",
            "Iteration 2100: Current success : 482 \t All success : 504 \t mean(l0) : 3.615080 \t Current Learning Rate: 0.058\n",
            "Iteration 2200: Current success : 491 \t All success : 505 \t mean(l0) : 3.615842 \t Current Learning Rate: 0.056\n",
            "Iteration 2300: Current success : 491 \t All success : 506 \t mean(l0) : 3.630435 \t Current Learning Rate: 0.054\n",
            "Iteration 2400: Current success : 479 \t All success : 508 \t mean(l0) : 3.651575 \t Current Learning Rate: 0.052\n",
            "Iteration 2500: Current success : 512 \t All success : 519 \t mean(l0) : 3.788054 \t Current Learning Rate: 0.050\n",
            "Iteration 2600: Current success : 483 \t All success : 599 \t mean(l0) : 4.338898 \t Current Learning Rate: 0.048\n",
            "Iteration 2700: Current success : 502 \t All success : 602 \t mean(l0) : 4.353820 \t Current Learning Rate: 0.046\n",
            "Iteration 2800: Current success : 437 \t All success : 606 \t mean(l0) : 4.384489 \t Current Learning Rate: 0.044\n",
            "Iteration 2900: Current success : 437 \t All success : 606 \t mean(l0) : 4.384489 \t Current Learning Rate: 0.042\n",
            "Iteration 3000: Current success : 437 \t All success : 606 \t mean(l0) : 4.384489 \t Current Learning Rate: 0.040\n",
            "Iteration 3100: Current success : 437 \t All success : 606 \t mean(l0) : 4.384489 \t Current Learning Rate: 0.038\n",
            "Iteration 3200: Current success : 437 \t All success : 606 \t mean(l0) : 4.384489 \t Current Learning Rate: 0.036\n",
            "Iteration 3300: Current success : 437 \t All success : 606 \t mean(l0) : 4.384489 \t Current Learning Rate: 0.034\n",
            "Iteration 3400: Current success : 437 \t All success : 606 \t mean(l0) : 4.384489 \t Current Learning Rate: 0.032\n",
            "Iteration 3500: Current success : 437 \t All success : 606 \t mean(l0) : 4.384489 \t Current Learning Rate: 0.030\n",
            "Iteration 3600: Current success : 437 \t All success : 606 \t mean(l0) : 4.384489 \t Current Learning Rate: 0.028\n",
            "Iteration 3700: Current success : 437 \t All success : 606 \t mean(l0) : 4.384489 \t Current Learning Rate: 0.026\n",
            "Iteration 3800: Current success : 437 \t All success : 606 \t mean(l0) : 4.384489 \t Current Learning Rate: 0.024\n",
            "Iteration 3900: Current success : 437 \t All success : 606 \t mean(l0) : 4.384489 \t Current Learning Rate: 0.022\n",
            "Iteration 4000: Current success : 437 \t All success : 606 \t mean(l0) : 4.384489 \t Current Learning Rate: 0.020\n",
            "Iteration 4100: Current success : 437 \t All success : 606 \t mean(l0) : 4.384489 \t Current Learning Rate: 0.018\n",
            "Iteration 4200: Current success : 437 \t All success : 606 \t mean(l0) : 4.384489 \t Current Learning Rate: 0.016\n",
            "Iteration 4300: Current success : 437 \t All success : 606 \t mean(l0) : 4.384489 \t Current Learning Rate: 0.014\n",
            "Iteration 4400: Current success : 437 \t All success : 606 \t mean(l0) : 4.384489 \t Current Learning Rate: 0.012\n",
            "Iteration 4500: Current success : 437 \t All success : 606 \t mean(l0) : 4.384489 \t Current Learning Rate: 0.010\n",
            "Iteration 4600: Current success : 437 \t All success : 606 \t mean(l0) : 4.384489 \t Current Learning Rate: 0.008\n",
            "Iteration 4700: Current success : 437 \t All success : 606 \t mean(l0) : 4.384489 \t Current Learning Rate: 0.006\n",
            "Iteration 4800: Current success : 437 \t All success : 606 \t mean(l0) : 4.384489 \t Current Learning Rate: 0.004\n",
            "Iteration 4900: Current success : 437 \t All success : 606 \t mean(l0) : 4.384489 \t Current Learning Rate: 0.002\n",
            "Iteration 5000: Current success : 437 \t All success : 606 \t mean(l0) : 4.384489 \t Current Learning Rate: 0.000\n",
            "Iteration 5100: Current success : 502 \t All success : 614 \t mean(l0) : 4.385993 \t Current Learning Rate: 0.049\n",
            "Iteration 5200: Current success : 500 \t All success : 614 \t mean(l0) : 4.376221 \t Current Learning Rate: 0.048\n",
            "Iteration 5300: Current success : 501 \t All success : 614 \t mean(l0) : 4.374593 \t Current Learning Rate: 0.047\n",
            "Iteration 5400: Current success : 501 \t All success : 614 \t mean(l0) : 4.374593 \t Current Learning Rate: 0.046\n",
            "Iteration 5500: Current success : 502 \t All success : 614 \t mean(l0) : 4.374593 \t Current Learning Rate: 0.045\n",
            "Iteration 5600: Current success : 504 \t All success : 615 \t mean(l0) : 4.382114 \t Current Learning Rate: 0.044\n",
            "Iteration 5700: Current success : 533 \t All success : 622 \t mean(l0) : 4.422830 \t Current Learning Rate: 0.043\n",
            "Iteration 5800: Current success : 538 \t All success : 622 \t mean(l0) : 4.419614 \t Current Learning Rate: 0.042\n",
            "Iteration 5900: Current success : 544 \t All success : 623 \t mean(l0) : 4.434992 \t Current Learning Rate: 0.041\n",
            "Iteration 6000: Current success : 550 \t All success : 641 \t mean(l0) : 4.708268 \t Current Learning Rate: 0.040\n",
            "Iteration 6100: Current success : 560 \t All success : 652 \t mean(l0) : 4.907975 \t Current Learning Rate: 0.039\n",
            "Iteration 6200: Current success : 539 \t All success : 652 \t mean(l0) : 4.900307 \t Current Learning Rate: 0.038\n",
            "Iteration 6300: Current success : 526 \t All success : 652 \t mean(l0) : 4.889570 \t Current Learning Rate: 0.037\n",
            "Iteration 6400: Current success : 538 \t All success : 652 \t mean(l0) : 4.889570 \t Current Learning Rate: 0.036\n",
            "Iteration 6500: Current success : 505 \t All success : 652 \t mean(l0) : 4.889570 \t Current Learning Rate: 0.035\n",
            "Iteration 6600: Current success : 575 \t All success : 652 \t mean(l0) : 4.874233 \t Current Learning Rate: 0.034\n",
            "Iteration 6700: Current success : 512 \t All success : 652 \t mean(l0) : 4.861963 \t Current Learning Rate: 0.033\n",
            "Iteration 6800: Current success : 517 \t All success : 652 \t mean(l0) : 4.861963 \t Current Learning Rate: 0.032\n",
            "Iteration 6900: Current success : 510 \t All success : 652 \t mean(l0) : 4.860429 \t Current Learning Rate: 0.031\n",
            "Iteration 7000: Current success : 518 \t All success : 652 \t mean(l0) : 4.860429 \t Current Learning Rate: 0.030\n",
            "Iteration 7100: Current success : 506 \t All success : 652 \t mean(l0) : 4.860429 \t Current Learning Rate: 0.029\n",
            "Iteration 7200: Current success : 522 \t All success : 652 \t mean(l0) : 4.860429 \t Current Learning Rate: 0.028\n",
            "Iteration 7300: Current success : 524 \t All success : 652 \t mean(l0) : 4.860429 \t Current Learning Rate: 0.027\n",
            "Iteration 7400: Current success : 510 \t All success : 652 \t mean(l0) : 4.860429 \t Current Learning Rate: 0.026\n",
            "Iteration 7500: Current success : 509 \t All success : 652 \t mean(l0) : 4.848159 \t Current Learning Rate: 0.025\n",
            "Iteration 7600: Current success : 509 \t All success : 652 \t mean(l0) : 4.848159 \t Current Learning Rate: 0.024\n",
            "Iteration 7700: Current success : 532 \t All success : 652 \t mean(l0) : 4.848159 \t Current Learning Rate: 0.023\n",
            "Iteration 7800: Current success : 533 \t All success : 652 \t mean(l0) : 4.848159 \t Current Learning Rate: 0.022\n",
            "Iteration 7900: Current success : 533 \t All success : 652 \t mean(l0) : 4.848159 \t Current Learning Rate: 0.021\n",
            "Iteration 8000: Current success : 533 \t All success : 652 \t mean(l0) : 4.848159 \t Current Learning Rate: 0.020\n",
            "Iteration 8100: Current success : 533 \t All success : 652 \t mean(l0) : 4.848159 \t Current Learning Rate: 0.019\n",
            "Iteration 8200: Current success : 533 \t All success : 652 \t mean(l0) : 4.848159 \t Current Learning Rate: 0.018\n",
            "Iteration 8300: Current success : 533 \t All success : 652 \t mean(l0) : 4.848159 \t Current Learning Rate: 0.017\n",
            "Iteration 8400: Current success : 533 \t All success : 652 \t mean(l0) : 4.848159 \t Current Learning Rate: 0.016\n",
            "Iteration 8500: Current success : 533 \t All success : 652 \t mean(l0) : 4.848159 \t Current Learning Rate: 0.015\n",
            "Iteration 8600: Current success : 533 \t All success : 652 \t mean(l0) : 4.848159 \t Current Learning Rate: 0.014\n",
            "Iteration 8700: Current success : 533 \t All success : 652 \t mean(l0) : 4.848159 \t Current Learning Rate: 0.013\n",
            "Iteration 8800: Current success : 533 \t All success : 652 \t mean(l0) : 4.848159 \t Current Learning Rate: 0.012\n",
            "Iteration 8900: Current success : 533 \t All success : 652 \t mean(l0) : 4.848159 \t Current Learning Rate: 0.011\n",
            "Iteration 9000: Current success : 533 \t All success : 652 \t mean(l0) : 4.848159 \t Current Learning Rate: 0.010\n",
            "Iteration 9100: Current success : 533 \t All success : 652 \t mean(l0) : 4.848159 \t Current Learning Rate: 0.009\n",
            "Iteration 9200: Current success : 533 \t All success : 652 \t mean(l0) : 4.848159 \t Current Learning Rate: 0.008\n",
            "Iteration 9300: Current success : 533 \t All success : 652 \t mean(l0) : 4.848159 \t Current Learning Rate: 0.007\n",
            "Iteration 9400: Current success : 533 \t All success : 652 \t mean(l0) : 4.848159 \t Current Learning Rate: 0.006\n",
            "Iteration 9500: Current success : 533 \t All success : 652 \t mean(l0) : 4.848159 \t Current Learning Rate: 0.005\n",
            "Iteration 9600: Current success : 533 \t All success : 652 \t mean(l0) : 4.848159 \t Current Learning Rate: 0.004\n",
            "Iteration 9700: Current success : 533 \t All success : 652 \t mean(l0) : 4.848159 \t Current Learning Rate: 0.003\n",
            "Iteration 9800: Current success : 533 \t All success : 652 \t mean(l0) : 4.848159 \t Current Learning Rate: 0.002\n",
            "Iteration 9900: Current success : 533 \t All success : 652 \t mean(l0) : 4.848159 \t Current Learning Rate: 0.001\n",
            "Iteration 10000: Current success : 533 \t All success : 652 \t mean(l0) : 4.848159 \t Current Learning Rate: 0.000\n",
            "Iteration 10100: Current success : 501 \t All success : 652 \t mean(l0) : 4.828221 \t Current Learning Rate: 0.025\n",
            "Iteration 10200: Current success : 553 \t All success : 652 \t mean(l0) : 4.826687 \t Current Learning Rate: 0.024\n",
            "Iteration 10300: Current success : 500 \t All success : 652 \t mean(l0) : 4.825153 \t Current Learning Rate: 0.024\n",
            "Iteration 10400: Current success : 500 \t All success : 652 \t mean(l0) : 4.817484 \t Current Learning Rate: 0.023\n",
            "Iteration 10500: Current success : 500 \t All success : 652 \t mean(l0) : 4.817484 \t Current Learning Rate: 0.023\n",
            "Iteration 10600: Current success : 500 \t All success : 652 \t mean(l0) : 4.817484 \t Current Learning Rate: 0.022\n",
            "Iteration 10700: Current success : 500 \t All success : 652 \t mean(l0) : 4.817484 \t Current Learning Rate: 0.022\n",
            "Iteration 10800: Current success : 500 \t All success : 652 \t mean(l0) : 4.817484 \t Current Learning Rate: 0.021\n",
            "Iteration 10900: Current success : 500 \t All success : 652 \t mean(l0) : 4.817484 \t Current Learning Rate: 0.021\n",
            "Iteration 11000: Current success : 506 \t All success : 652 \t mean(l0) : 4.817484 \t Current Learning Rate: 0.020\n",
            "Iteration 11100: Current success : 500 \t All success : 652 \t mean(l0) : 4.817484 \t Current Learning Rate: 0.020\n",
            "Iteration 11200: Current success : 500 \t All success : 652 \t mean(l0) : 4.817484 \t Current Learning Rate: 0.019\n",
            "Iteration 11300: Current success : 521 \t All success : 652 \t mean(l0) : 4.817484 \t Current Learning Rate: 0.019\n",
            "Iteration 11400: Current success : 503 \t All success : 652 \t mean(l0) : 4.817484 \t Current Learning Rate: 0.018\n",
            "Iteration 11500: Current success : 501 \t All success : 652 \t mean(l0) : 4.817484 \t Current Learning Rate: 0.018\n",
            "Iteration 11600: Current success : 501 \t All success : 652 \t mean(l0) : 4.817484 \t Current Learning Rate: 0.017\n",
            "Iteration 11700: Current success : 510 \t All success : 652 \t mean(l0) : 4.817484 \t Current Learning Rate: 0.017\n",
            "Iteration 11800: Current success : 504 \t All success : 652 \t mean(l0) : 4.817484 \t Current Learning Rate: 0.016\n",
            "Iteration 11900: Current success : 503 \t All success : 652 \t mean(l0) : 4.817484 \t Current Learning Rate: 0.016\n",
            "Iteration 12000: Current success : 503 \t All success : 652 \t mean(l0) : 4.817484 \t Current Learning Rate: 0.015\n",
            "Iteration 12100: Current success : 503 \t All success : 652 \t mean(l0) : 4.817484 \t Current Learning Rate: 0.015\n",
            "Iteration 12200: Current success : 502 \t All success : 652 \t mean(l0) : 4.817484 \t Current Learning Rate: 0.014\n",
            "Iteration 12300: Current success : 500 \t All success : 652 \t mean(l0) : 4.817484 \t Current Learning Rate: 0.014\n",
            "Iteration 12400: Current success : 500 \t All success : 652 \t mean(l0) : 4.817484 \t Current Learning Rate: 0.013\n",
            "Iteration 12500: Current success : 500 \t All success : 652 \t mean(l0) : 4.817484 \t Current Learning Rate: 0.013\n",
            "Iteration 12600: Current success : 500 \t All success : 652 \t mean(l0) : 4.817484 \t Current Learning Rate: 0.012\n",
            "Iteration 12700: Current success : 501 \t All success : 652 \t mean(l0) : 4.817484 \t Current Learning Rate: 0.012\n",
            "Iteration 12800: Current success : 500 \t All success : 652 \t mean(l0) : 4.817484 \t Current Learning Rate: 0.011\n",
            "Iteration 12900: Current success : 520 \t All success : 652 \t mean(l0) : 4.817484 \t Current Learning Rate: 0.011\n",
            "Iteration 13000: Current success : 536 \t All success : 652 \t mean(l0) : 4.817484 \t Current Learning Rate: 0.010\n",
            "Iteration 13100: Current success : 536 \t All success : 652 \t mean(l0) : 4.817484 \t Current Learning Rate: 0.010\n",
            "Iteration 13200: Current success : 536 \t All success : 652 \t mean(l0) : 4.817484 \t Current Learning Rate: 0.009\n",
            "Iteration 13300: Current success : 535 \t All success : 652 \t mean(l0) : 4.817484 \t Current Learning Rate: 0.009\n",
            "Iteration 13400: Current success : 536 \t All success : 652 \t mean(l0) : 4.817484 \t Current Learning Rate: 0.008\n",
            "Iteration 13500: Current success : 536 \t All success : 652 \t mean(l0) : 4.817484 \t Current Learning Rate: 0.008\n",
            "Iteration 13600: Current success : 536 \t All success : 652 \t mean(l0) : 4.817484 \t Current Learning Rate: 0.007\n",
            "Iteration 13700: Current success : 536 \t All success : 652 \t mean(l0) : 4.817484 \t Current Learning Rate: 0.007\n",
            "Iteration 13800: Current success : 536 \t All success : 652 \t mean(l0) : 4.817484 \t Current Learning Rate: 0.006\n",
            "Iteration 13900: Current success : 536 \t All success : 652 \t mean(l0) : 4.817484 \t Current Learning Rate: 0.006\n",
            "Iteration 14000: Current success : 536 \t All success : 652 \t mean(l0) : 4.817484 \t Current Learning Rate: 0.005\n",
            "Iteration 14100: Current success : 536 \t All success : 652 \t mean(l0) : 4.817484 \t Current Learning Rate: 0.005\n",
            "Iteration 14200: Current success : 536 \t All success : 652 \t mean(l0) : 4.817484 \t Current Learning Rate: 0.004\n",
            "Iteration 14300: Current success : 536 \t All success : 652 \t mean(l0) : 4.817484 \t Current Learning Rate: 0.004\n",
            "Iteration 14400: Current success : 536 \t All success : 652 \t mean(l0) : 4.817484 \t Current Learning Rate: 0.003\n",
            "Iteration 14500: Current success : 536 \t All success : 652 \t mean(l0) : 4.817484 \t Current Learning Rate: 0.003\n",
            "Iteration 14600: Current success : 536 \t All success : 652 \t mean(l0) : 4.817484 \t Current Learning Rate: 0.002\n",
            "Iteration 14700: Current success : 536 \t All success : 652 \t mean(l0) : 4.817484 \t Current Learning Rate: 0.002\n",
            "Iteration 14800: Current success : 536 \t All success : 652 \t mean(l0) : 4.817484 \t Current Learning Rate: 0.001\n",
            "Iteration 14900: Current success : 536 \t All success : 652 \t mean(l0) : 4.817484 \t Current Learning Rate: 0.001\n",
            "Iteration 15000: Current success : 536 \t All success : 652 \t mean(l0) : 4.817484 \t Current Learning Rate: 0.000\n",
            "Iteration 15100: Current success : 520 \t All success : 652 \t mean(l0) : 4.812883 \t Current Learning Rate: 0.012\n",
            "Iteration 15200: Current success : 536 \t All success : 652 \t mean(l0) : 4.812883 \t Current Learning Rate: 0.012\n",
            "Iteration 15300: Current success : 536 \t All success : 652 \t mean(l0) : 4.812883 \t Current Learning Rate: 0.012\n",
            "Iteration 15400: Current success : 536 \t All success : 652 \t mean(l0) : 4.812883 \t Current Learning Rate: 0.012\n",
            "Iteration 15500: Current success : 530 \t All success : 652 \t mean(l0) : 4.812883 \t Current Learning Rate: 0.011\n",
            "Iteration 15600: Current success : 502 \t All success : 652 \t mean(l0) : 4.812883 \t Current Learning Rate: 0.011\n",
            "Iteration 15700: Current success : 500 \t All success : 652 \t mean(l0) : 4.812883 \t Current Learning Rate: 0.011\n",
            "Iteration 15800: Current success : 500 \t All success : 652 \t mean(l0) : 4.812883 \t Current Learning Rate: 0.011\n",
            "Iteration 15900: Current success : 500 \t All success : 652 \t mean(l0) : 4.812883 \t Current Learning Rate: 0.010\n",
            "Iteration 16000: Current success : 500 \t All success : 652 \t mean(l0) : 4.812883 \t Current Learning Rate: 0.010\n",
            "Iteration 16100: Current success : 500 \t All success : 652 \t mean(l0) : 4.812883 \t Current Learning Rate: 0.010\n",
            "Iteration 16200: Current success : 500 \t All success : 652 \t mean(l0) : 4.812883 \t Current Learning Rate: 0.010\n",
            "Iteration 16300: Current success : 500 \t All success : 652 \t mean(l0) : 4.812883 \t Current Learning Rate: 0.009\n",
            "Iteration 16400: Current success : 500 \t All success : 652 \t mean(l0) : 4.812883 \t Current Learning Rate: 0.009\n",
            "Iteration 16500: Current success : 500 \t All success : 652 \t mean(l0) : 4.812883 \t Current Learning Rate: 0.009\n",
            "Iteration 16600: Current success : 500 \t All success : 652 \t mean(l0) : 4.812883 \t Current Learning Rate: 0.009\n",
            "Iteration 16700: Current success : 500 \t All success : 652 \t mean(l0) : 4.812883 \t Current Learning Rate: 0.008\n",
            "Iteration 16800: Current success : 500 \t All success : 652 \t mean(l0) : 4.812883 \t Current Learning Rate: 0.008\n",
            "Iteration 16900: Current success : 500 \t All success : 652 \t mean(l0) : 4.812883 \t Current Learning Rate: 0.008\n",
            "Iteration 17000: Current success : 500 \t All success : 652 \t mean(l0) : 4.812883 \t Current Learning Rate: 0.008\n",
            "Iteration 17100: Current success : 500 \t All success : 652 \t mean(l0) : 4.812883 \t Current Learning Rate: 0.007\n",
            "Iteration 17200: Current success : 500 \t All success : 652 \t mean(l0) : 4.812883 \t Current Learning Rate: 0.007\n",
            "Iteration 17300: Current success : 500 \t All success : 652 \t mean(l0) : 4.812883 \t Current Learning Rate: 0.007\n",
            "Iteration 17400: Current success : 500 \t All success : 652 \t mean(l0) : 4.812883 \t Current Learning Rate: 0.007\n",
            "Iteration 17500: Current success : 500 \t All success : 652 \t mean(l0) : 4.812883 \t Current Learning Rate: 0.006\n",
            "Iteration 17600: Current success : 500 \t All success : 652 \t mean(l0) : 4.812883 \t Current Learning Rate: 0.006\n",
            "Iteration 17700: Current success : 500 \t All success : 652 \t mean(l0) : 4.812883 \t Current Learning Rate: 0.006\n",
            "Iteration 17800: Current success : 500 \t All success : 652 \t mean(l0) : 4.812883 \t Current Learning Rate: 0.006\n",
            "Iteration 17900: Current success : 500 \t All success : 652 \t mean(l0) : 4.812883 \t Current Learning Rate: 0.005\n",
            "Iteration 18000: Current success : 531 \t All success : 652 \t mean(l0) : 4.812883 \t Current Learning Rate: 0.005\n",
            "Iteration 18100: Current success : 536 \t All success : 652 \t mean(l0) : 4.812883 \t Current Learning Rate: 0.005\n",
            "Iteration 18200: Current success : 536 \t All success : 652 \t mean(l0) : 4.812883 \t Current Learning Rate: 0.005\n",
            "Iteration 18300: Current success : 536 \t All success : 652 \t mean(l0) : 4.812883 \t Current Learning Rate: 0.004\n",
            "Iteration 18400: Current success : 536 \t All success : 652 \t mean(l0) : 4.812883 \t Current Learning Rate: 0.004\n",
            "Iteration 18500: Current success : 536 \t All success : 652 \t mean(l0) : 4.812883 \t Current Learning Rate: 0.004\n",
            "Iteration 18600: Current success : 536 \t All success : 652 \t mean(l0) : 4.812883 \t Current Learning Rate: 0.004\n",
            "Iteration 18700: Current success : 536 \t All success : 652 \t mean(l0) : 4.812883 \t Current Learning Rate: 0.003\n",
            "Iteration 18800: Current success : 536 \t All success : 652 \t mean(l0) : 4.812883 \t Current Learning Rate: 0.003\n",
            "Iteration 18900: Current success : 536 \t All success : 652 \t mean(l0) : 4.812883 \t Current Learning Rate: 0.003\n",
            "Iteration 19000: Current success : 536 \t All success : 652 \t mean(l0) : 4.812883 \t Current Learning Rate: 0.003\n",
            "Iteration 19100: Current success : 536 \t All success : 652 \t mean(l0) : 4.812883 \t Current Learning Rate: 0.002\n",
            "Iteration 19200: Current success : 536 \t All success : 652 \t mean(l0) : 4.812883 \t Current Learning Rate: 0.002\n",
            "Iteration 19300: Current success : 536 \t All success : 652 \t mean(l0) : 4.812883 \t Current Learning Rate: 0.002\n",
            "Iteration 19400: Current success : 536 \t All success : 652 \t mean(l0) : 4.812883 \t Current Learning Rate: 0.002\n",
            "Iteration 19500: Current success : 536 \t All success : 652 \t mean(l0) : 4.812883 \t Current Learning Rate: 0.001\n",
            "Iteration 19600: Current success : 536 \t All success : 652 \t mean(l0) : 4.812883 \t Current Learning Rate: 0.001\n",
            "Iteration 19700: Current success : 536 \t All success : 652 \t mean(l0) : 4.812883 \t Current Learning Rate: 0.001\n",
            "Iteration 19800: Current success : 536 \t All success : 652 \t mean(l0) : 4.812883 \t Current Learning Rate: 0.001\n",
            "Iteration 19900: Current success : 536 \t All success : 652 \t mean(l0) : 4.812883 \t Current Learning Rate: 0.000\n",
            "Iteration 20000: Current success : 536 \t All success : 652 \t mean(l0) : 4.812883 \t Current Learning Rate: 0.000\n",
            "outer_step 1: all success : 647 \t mean(l0)(success) : 4.7063\n",
            "-------------------------------------------------------------\n",
            "tensor([ 0.5000,  0.5000,  0.5000,  ...,  0.5000, 10.0000, 10.0000],\n",
            "       device='cuda:0')\n",
            "Iteration 100: Current success : 402 \t All success : 457 \t mean(l0) : 10.822757 \t Current Learning Rate: 0.098\n",
            "Iteration 200: Current success : 401 \t All success : 464 \t mean(l0) : 10.564655 \t Current Learning Rate: 0.096\n",
            "Iteration 300: Current success : 400 \t All success : 465 \t mean(l0) : 10.546237 \t Current Learning Rate: 0.094\n",
            "Iteration 400: Current success : 400 \t All success : 468 \t mean(l0) : 10.555556 \t Current Learning Rate: 0.092\n",
            "Iteration 500: Current success : 398 \t All success : 468 \t mean(l0) : 10.551283 \t Current Learning Rate: 0.090\n",
            "Iteration 600: Current success : 399 \t All success : 469 \t mean(l0) : 10.524521 \t Current Learning Rate: 0.088\n",
            "Iteration 700: Current success : 411 \t All success : 474 \t mean(l0) : 10.440928 \t Current Learning Rate: 0.086\n",
            "Iteration 800: Current success : 413 \t All success : 486 \t mean(l0) : 10.304526 \t Current Learning Rate: 0.084\n",
            "Iteration 900: Current success : 413 \t All success : 496 \t mean(l0) : 10.157258 \t Current Learning Rate: 0.082\n",
            "Iteration 1000: Current success : 446 \t All success : 507 \t mean(l0) : 10.009862 \t Current Learning Rate: 0.080\n",
            "Iteration 1100: Current success : 417 \t All success : 511 \t mean(l0) : 9.970646 \t Current Learning Rate: 0.078\n",
            "Iteration 1200: Current success : 420 \t All success : 517 \t mean(l0) : 9.891683 \t Current Learning Rate: 0.076\n",
            "Iteration 1300: Current success : 465 \t All success : 524 \t mean(l0) : 9.837787 \t Current Learning Rate: 0.074\n",
            "Iteration 1400: Current success : 417 \t All success : 525 \t mean(l0) : 9.822857 \t Current Learning Rate: 0.072\n",
            "Iteration 1500: Current success : 420 \t All success : 525 \t mean(l0) : 9.822857 \t Current Learning Rate: 0.070\n",
            "Iteration 1600: Current success : 477 \t All success : 526 \t mean(l0) : 9.806084 \t Current Learning Rate: 0.068\n",
            "Iteration 1700: Current success : 422 \t All success : 526 \t mean(l0) : 9.806084 \t Current Learning Rate: 0.066\n",
            "Iteration 1800: Current success : 426 \t All success : 528 \t mean(l0) : 9.774622 \t Current Learning Rate: 0.064\n",
            "Iteration 1900: Current success : 493 \t All success : 529 \t mean(l0) : 9.761815 \t Current Learning Rate: 0.062\n",
            "Iteration 2000: Current success : 477 \t All success : 530 \t mean(l0) : 9.794340 \t Current Learning Rate: 0.060\n",
            "Iteration 2100: Current success : 485 \t All success : 534 \t mean(l0) : 9.808989 \t Current Learning Rate: 0.058\n",
            "Iteration 2200: Current success : 483 \t All success : 534 \t mean(l0) : 9.808989 \t Current Learning Rate: 0.056\n",
            "Iteration 2300: Current success : 485 \t All success : 534 \t mean(l0) : 9.801498 \t Current Learning Rate: 0.054\n",
            "Iteration 2400: Current success : 503 \t All success : 554 \t mean(l0) : 9.662455 \t Current Learning Rate: 0.052\n",
            "Iteration 2500: Current success : 549 \t All success : 579 \t mean(l0) : 9.526771 \t Current Learning Rate: 0.050\n",
            "Iteration 2600: Current success : 547 \t All success : 808 \t mean(l0) : 8.863861 \t Current Learning Rate: 0.048\n",
            "Iteration 2700: Current success : 611 \t All success : 813 \t mean(l0) : 8.846249 \t Current Learning Rate: 0.046\n",
            "Iteration 2800: Current success : 439 \t All success : 816 \t mean(l0) : 8.843138 \t Current Learning Rate: 0.044\n",
            "Iteration 2900: Current success : 437 \t All success : 816 \t mean(l0) : 8.843138 \t Current Learning Rate: 0.042\n",
            "Iteration 3000: Current success : 437 \t All success : 816 \t mean(l0) : 8.843138 \t Current Learning Rate: 0.040\n",
            "Iteration 3100: Current success : 437 \t All success : 816 \t mean(l0) : 8.838236 \t Current Learning Rate: 0.038\n",
            "Iteration 3200: Current success : 437 \t All success : 816 \t mean(l0) : 8.838236 \t Current Learning Rate: 0.036\n",
            "Iteration 3300: Current success : 437 \t All success : 816 \t mean(l0) : 8.838236 \t Current Learning Rate: 0.034\n",
            "Iteration 3400: Current success : 437 \t All success : 816 \t mean(l0) : 8.838236 \t Current Learning Rate: 0.032\n",
            "Iteration 3500: Current success : 437 \t All success : 816 \t mean(l0) : 8.838236 \t Current Learning Rate: 0.030\n",
            "Iteration 3600: Current success : 437 \t All success : 816 \t mean(l0) : 8.837010 \t Current Learning Rate: 0.028\n",
            "Iteration 3700: Current success : 437 \t All success : 816 \t mean(l0) : 8.837010 \t Current Learning Rate: 0.026\n",
            "Iteration 3800: Current success : 437 \t All success : 816 \t mean(l0) : 8.837010 \t Current Learning Rate: 0.024\n",
            "Iteration 3900: Current success : 437 \t All success : 816 \t mean(l0) : 8.837010 \t Current Learning Rate: 0.022\n",
            "Iteration 4000: Current success : 437 \t All success : 816 \t mean(l0) : 8.837010 \t Current Learning Rate: 0.020\n",
            "Iteration 4100: Current success : 437 \t All success : 816 \t mean(l0) : 8.837010 \t Current Learning Rate: 0.018\n",
            "Iteration 4200: Current success : 437 \t All success : 816 \t mean(l0) : 8.837010 \t Current Learning Rate: 0.016\n",
            "Iteration 4300: Current success : 437 \t All success : 816 \t mean(l0) : 8.837010 \t Current Learning Rate: 0.014\n",
            "Iteration 4400: Current success : 437 \t All success : 816 \t mean(l0) : 8.837010 \t Current Learning Rate: 0.012\n",
            "Iteration 4500: Current success : 437 \t All success : 816 \t mean(l0) : 8.837010 \t Current Learning Rate: 0.010\n",
            "Iteration 4600: Current success : 437 \t All success : 816 \t mean(l0) : 8.837010 \t Current Learning Rate: 0.008\n",
            "Iteration 4700: Current success : 437 \t All success : 816 \t mean(l0) : 8.837010 \t Current Learning Rate: 0.006\n",
            "Iteration 4800: Current success : 437 \t All success : 816 \t mean(l0) : 8.837010 \t Current Learning Rate: 0.004\n",
            "Iteration 4900: Current success : 437 \t All success : 816 \t mean(l0) : 8.837010 \t Current Learning Rate: 0.002\n",
            "Iteration 5000: Current success : 437 \t All success : 816 \t mean(l0) : 8.837010 \t Current Learning Rate: 0.000\n",
            "Iteration 5100: Current success : 525 \t All success : 825 \t mean(l0) : 9.043636 \t Current Learning Rate: 0.049\n",
            "Iteration 5200: Current success : 521 \t All success : 828 \t mean(l0) : 9.135265 \t Current Learning Rate: 0.048\n",
            "Iteration 5300: Current success : 521 \t All success : 828 \t mean(l0) : 9.132850 \t Current Learning Rate: 0.047\n",
            "Iteration 5400: Current success : 531 \t All success : 828 \t mean(l0) : 9.128019 \t Current Learning Rate: 0.046\n",
            "Iteration 5500: Current success : 532 \t All success : 828 \t mean(l0) : 9.126811 \t Current Learning Rate: 0.045\n",
            "Iteration 5600: Current success : 532 \t All success : 828 \t mean(l0) : 9.125604 \t Current Learning Rate: 0.044\n",
            "Iteration 5700: Current success : 627 \t All success : 828 \t mean(l0) : 9.124396 \t Current Learning Rate: 0.043\n",
            "Iteration 5800: Current success : 619 \t All success : 828 \t mean(l0) : 9.124396 \t Current Learning Rate: 0.042\n",
            "Iteration 5900: Current success : 631 \t All success : 828 \t mean(l0) : 9.123188 \t Current Learning Rate: 0.041\n",
            "Iteration 6000: Current success : 585 \t All success : 836 \t mean(l0) : 9.129187 \t Current Learning Rate: 0.040\n",
            "Iteration 6100: Current success : 637 \t All success : 839 \t mean(l0) : 9.122766 \t Current Learning Rate: 0.039\n",
            "Iteration 6200: Current success : 599 \t All success : 839 \t mean(l0) : 9.121573 \t Current Learning Rate: 0.038\n",
            "Iteration 6300: Current success : 598 \t All success : 839 \t mean(l0) : 9.121573 \t Current Learning Rate: 0.037\n",
            "Iteration 6400: Current success : 631 \t All success : 839 \t mean(l0) : 9.120381 \t Current Learning Rate: 0.036\n",
            "Iteration 6500: Current success : 600 \t All success : 839 \t mean(l0) : 9.119189 \t Current Learning Rate: 0.035\n",
            "Iteration 6600: Current success : 672 \t All success : 839 \t mean(l0) : 9.119189 \t Current Learning Rate: 0.034\n",
            "Iteration 6700: Current success : 546 \t All success : 839 \t mean(l0) : 9.119189 \t Current Learning Rate: 0.033\n",
            "Iteration 6800: Current success : 611 \t All success : 839 \t mean(l0) : 9.119189 \t Current Learning Rate: 0.032\n",
            "Iteration 6900: Current success : 564 \t All success : 839 \t mean(l0) : 9.119189 \t Current Learning Rate: 0.031\n",
            "Iteration 7000: Current success : 589 \t All success : 840 \t mean(l0) : 9.113095 \t Current Learning Rate: 0.030\n",
            "Iteration 7100: Current success : 553 \t All success : 840 \t mean(l0) : 9.113095 \t Current Learning Rate: 0.029\n",
            "Iteration 7200: Current success : 547 \t All success : 840 \t mean(l0) : 9.113095 \t Current Learning Rate: 0.028\n",
            "Iteration 7300: Current success : 607 \t All success : 840 \t mean(l0) : 9.111905 \t Current Learning Rate: 0.027\n",
            "Iteration 7400: Current success : 566 \t All success : 840 \t mean(l0) : 9.111905 \t Current Learning Rate: 0.026\n",
            "Iteration 7500: Current success : 555 \t All success : 840 \t mean(l0) : 9.111905 \t Current Learning Rate: 0.025\n",
            "Iteration 7600: Current success : 594 \t All success : 840 \t mean(l0) : 9.111905 \t Current Learning Rate: 0.024\n",
            "Iteration 7700: Current success : 631 \t All success : 840 \t mean(l0) : 9.111905 \t Current Learning Rate: 0.023\n",
            "Iteration 7800: Current success : 633 \t All success : 840 \t mean(l0) : 9.111905 \t Current Learning Rate: 0.022\n",
            "Iteration 7900: Current success : 633 \t All success : 840 \t mean(l0) : 9.111905 \t Current Learning Rate: 0.021\n",
            "Iteration 8000: Current success : 633 \t All success : 840 \t mean(l0) : 9.111905 \t Current Learning Rate: 0.020\n",
            "Iteration 8100: Current success : 633 \t All success : 840 \t mean(l0) : 9.111905 \t Current Learning Rate: 0.019\n",
            "Iteration 8200: Current success : 633 \t All success : 840 \t mean(l0) : 9.111905 \t Current Learning Rate: 0.018\n",
            "Iteration 8300: Current success : 633 \t All success : 840 \t mean(l0) : 9.111905 \t Current Learning Rate: 0.017\n",
            "Iteration 8400: Current success : 633 \t All success : 840 \t mean(l0) : 9.111905 \t Current Learning Rate: 0.016\n",
            "Iteration 8500: Current success : 633 \t All success : 840 \t mean(l0) : 9.111905 \t Current Learning Rate: 0.015\n",
            "Iteration 8600: Current success : 633 \t All success : 840 \t mean(l0) : 9.111905 \t Current Learning Rate: 0.014\n",
            "Iteration 8700: Current success : 633 \t All success : 840 \t mean(l0) : 9.111905 \t Current Learning Rate: 0.013\n",
            "Iteration 8800: Current success : 633 \t All success : 840 \t mean(l0) : 9.111905 \t Current Learning Rate: 0.012\n",
            "Iteration 8900: Current success : 633 \t All success : 840 \t mean(l0) : 9.111905 \t Current Learning Rate: 0.011\n",
            "Iteration 9000: Current success : 633 \t All success : 840 \t mean(l0) : 9.111905 \t Current Learning Rate: 0.010\n",
            "Iteration 9100: Current success : 633 \t All success : 840 \t mean(l0) : 9.111905 \t Current Learning Rate: 0.009\n",
            "Iteration 9200: Current success : 633 \t All success : 840 \t mean(l0) : 9.111905 \t Current Learning Rate: 0.008\n",
            "Iteration 9300: Current success : 633 \t All success : 840 \t mean(l0) : 9.111905 \t Current Learning Rate: 0.007\n",
            "Iteration 9400: Current success : 633 \t All success : 840 \t mean(l0) : 9.111905 \t Current Learning Rate: 0.006\n",
            "Iteration 9500: Current success : 633 \t All success : 840 \t mean(l0) : 9.111905 \t Current Learning Rate: 0.005\n",
            "Iteration 9600: Current success : 633 \t All success : 840 \t mean(l0) : 9.111905 \t Current Learning Rate: 0.004\n",
            "Iteration 9700: Current success : 633 \t All success : 840 \t mean(l0) : 9.111905 \t Current Learning Rate: 0.003\n",
            "Iteration 9800: Current success : 633 \t All success : 840 \t mean(l0) : 9.111905 \t Current Learning Rate: 0.002\n",
            "Iteration 9900: Current success : 633 \t All success : 840 \t mean(l0) : 9.111905 \t Current Learning Rate: 0.001\n",
            "Iteration 10000: Current success : 633 \t All success : 840 \t mean(l0) : 9.111905 \t Current Learning Rate: 0.000\n",
            "Iteration 10100: Current success : 550 \t All success : 853 \t mean(l0) : 9.511137 \t Current Learning Rate: 0.025\n",
            "Iteration 10200: Current success : 639 \t All success : 853 \t mean(l0) : 9.507620 \t Current Learning Rate: 0.024\n",
            "Iteration 10300: Current success : 548 \t All success : 853 \t mean(l0) : 9.507620 \t Current Learning Rate: 0.024\n",
            "Iteration 10400: Current success : 547 \t All success : 853 \t mean(l0) : 9.506448 \t Current Learning Rate: 0.023\n",
            "Iteration 10500: Current success : 547 \t All success : 853 \t mean(l0) : 9.505275 \t Current Learning Rate: 0.023\n",
            "Iteration 10600: Current success : 547 \t All success : 853 \t mean(l0) : 9.504103 \t Current Learning Rate: 0.022\n",
            "Iteration 10700: Current success : 580 \t All success : 854 \t mean(l0) : 9.502341 \t Current Learning Rate: 0.022\n",
            "Iteration 10800: Current success : 576 \t All success : 854 \t mean(l0) : 9.497658 \t Current Learning Rate: 0.021\n",
            "Iteration 10900: Current success : 546 \t All success : 854 \t mean(l0) : 9.497658 \t Current Learning Rate: 0.021\n",
            "Iteration 11000: Current success : 582 \t All success : 854 \t mean(l0) : 9.497658 \t Current Learning Rate: 0.020\n",
            "Iteration 11100: Current success : 574 \t All success : 854 \t mean(l0) : 9.496487 \t Current Learning Rate: 0.020\n",
            "Iteration 11200: Current success : 548 \t All success : 854 \t mean(l0) : 9.494144 \t Current Learning Rate: 0.019\n",
            "Iteration 11300: Current success : 667 \t All success : 854 \t mean(l0) : 9.494144 \t Current Learning Rate: 0.019\n",
            "Iteration 11400: Current success : 594 \t All success : 854 \t mean(l0) : 9.494144 \t Current Learning Rate: 0.018\n",
            "Iteration 11500: Current success : 563 \t All success : 854 \t mean(l0) : 9.494144 \t Current Learning Rate: 0.018\n",
            "Iteration 11600: Current success : 549 \t All success : 854 \t mean(l0) : 9.492974 \t Current Learning Rate: 0.017\n",
            "Iteration 11700: Current success : 632 \t All success : 854 \t mean(l0) : 9.489461 \t Current Learning Rate: 0.017\n",
            "Iteration 11800: Current success : 596 \t All success : 854 \t mean(l0) : 9.489461 \t Current Learning Rate: 0.016\n",
            "Iteration 11900: Current success : 566 \t All success : 854 \t mean(l0) : 9.489461 \t Current Learning Rate: 0.016\n",
            "Iteration 12000: Current success : 576 \t All success : 854 \t mean(l0) : 9.489461 \t Current Learning Rate: 0.015\n",
            "Iteration 12100: Current success : 593 \t All success : 854 \t mean(l0) : 9.489461 \t Current Learning Rate: 0.015\n",
            "Iteration 12200: Current success : 601 \t All success : 854 \t mean(l0) : 9.489461 \t Current Learning Rate: 0.014\n",
            "Iteration 12300: Current success : 545 \t All success : 854 \t mean(l0) : 9.489461 \t Current Learning Rate: 0.014\n",
            "Iteration 12400: Current success : 545 \t All success : 854 \t mean(l0) : 9.489461 \t Current Learning Rate: 0.013\n",
            "Iteration 12500: Current success : 546 \t All success : 854 \t mean(l0) : 9.489461 \t Current Learning Rate: 0.013\n",
            "Iteration 12600: Current success : 545 \t All success : 854 \t mean(l0) : 9.489461 \t Current Learning Rate: 0.012\n",
            "Iteration 12700: Current success : 547 \t All success : 854 \t mean(l0) : 9.489461 \t Current Learning Rate: 0.012\n",
            "Iteration 12800: Current success : 552 \t All success : 854 \t mean(l0) : 9.489461 \t Current Learning Rate: 0.011\n",
            "Iteration 12900: Current success : 634 \t All success : 854 \t mean(l0) : 9.489461 \t Current Learning Rate: 0.011\n",
            "Iteration 13000: Current success : 656 \t All success : 854 \t mean(l0) : 9.489461 \t Current Learning Rate: 0.010\n",
            "Iteration 13100: Current success : 656 \t All success : 854 \t mean(l0) : 9.489461 \t Current Learning Rate: 0.010\n",
            "Iteration 13200: Current success : 656 \t All success : 854 \t mean(l0) : 9.489461 \t Current Learning Rate: 0.009\n",
            "Iteration 13300: Current success : 656 \t All success : 854 \t mean(l0) : 9.489461 \t Current Learning Rate: 0.009\n",
            "Iteration 13400: Current success : 656 \t All success : 854 \t mean(l0) : 9.489461 \t Current Learning Rate: 0.008\n",
            "Iteration 13500: Current success : 656 \t All success : 854 \t mean(l0) : 9.489461 \t Current Learning Rate: 0.008\n",
            "Iteration 13600: Current success : 656 \t All success : 854 \t mean(l0) : 9.489461 \t Current Learning Rate: 0.007\n",
            "Iteration 13700: Current success : 656 \t All success : 854 \t mean(l0) : 9.489461 \t Current Learning Rate: 0.007\n",
            "Iteration 13800: Current success : 656 \t All success : 854 \t mean(l0) : 9.489461 \t Current Learning Rate: 0.006\n",
            "Iteration 13900: Current success : 656 \t All success : 854 \t mean(l0) : 9.489461 \t Current Learning Rate: 0.006\n",
            "Iteration 14000: Current success : 656 \t All success : 854 \t mean(l0) : 9.489461 \t Current Learning Rate: 0.005\n",
            "Iteration 14100: Current success : 656 \t All success : 854 \t mean(l0) : 9.489461 \t Current Learning Rate: 0.005\n",
            "Iteration 14200: Current success : 656 \t All success : 854 \t mean(l0) : 9.489461 \t Current Learning Rate: 0.004\n",
            "Iteration 14300: Current success : 656 \t All success : 854 \t mean(l0) : 9.489461 \t Current Learning Rate: 0.004\n",
            "Iteration 14400: Current success : 656 \t All success : 854 \t mean(l0) : 9.489461 \t Current Learning Rate: 0.003\n",
            "Iteration 14500: Current success : 656 \t All success : 854 \t mean(l0) : 9.489461 \t Current Learning Rate: 0.003\n",
            "Iteration 14600: Current success : 656 \t All success : 854 \t mean(l0) : 9.489461 \t Current Learning Rate: 0.002\n",
            "Iteration 14700: Current success : 656 \t All success : 854 \t mean(l0) : 9.489461 \t Current Learning Rate: 0.002\n",
            "Iteration 14800: Current success : 656 \t All success : 854 \t mean(l0) : 9.489461 \t Current Learning Rate: 0.001\n",
            "Iteration 14900: Current success : 656 \t All success : 854 \t mean(l0) : 9.489461 \t Current Learning Rate: 0.001\n",
            "Iteration 15000: Current success : 656 \t All success : 854 \t mean(l0) : 9.489461 \t Current Learning Rate: 0.000\n",
            "Iteration 15100: Current success : 557 \t All success : 854 \t mean(l0) : 9.460187 \t Current Learning Rate: 0.012\n",
            "Iteration 15200: Current success : 662 \t All success : 854 \t mean(l0) : 9.447307 \t Current Learning Rate: 0.012\n",
            "Iteration 15300: Current success : 662 \t All success : 854 \t mean(l0) : 9.446136 \t Current Learning Rate: 0.012\n",
            "Iteration 15400: Current success : 662 \t All success : 854 \t mean(l0) : 9.441452 \t Current Learning Rate: 0.012\n",
            "Iteration 15500: Current success : 655 \t All success : 854 \t mean(l0) : 9.441452 \t Current Learning Rate: 0.011\n",
            "Iteration 15600: Current success : 552 \t All success : 856 \t mean(l0) : 9.494159 \t Current Learning Rate: 0.011\n",
            "Iteration 15700: Current success : 552 \t All success : 856 \t mean(l0) : 9.491822 \t Current Learning Rate: 0.011\n",
            "Iteration 15800: Current success : 549 \t All success : 856 \t mean(l0) : 9.491822 \t Current Learning Rate: 0.011\n",
            "Iteration 15900: Current success : 551 \t All success : 856 \t mean(l0) : 9.491822 \t Current Learning Rate: 0.010\n",
            "Iteration 16000: Current success : 549 \t All success : 856 \t mean(l0) : 9.491822 \t Current Learning Rate: 0.010\n",
            "Iteration 16100: Current success : 549 \t All success : 856 \t mean(l0) : 9.490654 \t Current Learning Rate: 0.010\n",
            "Iteration 16200: Current success : 549 \t All success : 856 \t mean(l0) : 9.489486 \t Current Learning Rate: 0.010\n",
            "Iteration 16300: Current success : 550 \t All success : 856 \t mean(l0) : 9.489486 \t Current Learning Rate: 0.009\n",
            "Iteration 16400: Current success : 550 \t All success : 856 \t mean(l0) : 9.489486 \t Current Learning Rate: 0.009\n",
            "Iteration 16500: Current success : 549 \t All success : 856 \t mean(l0) : 9.488317 \t Current Learning Rate: 0.009\n",
            "Iteration 16600: Current success : 550 \t All success : 856 \t mean(l0) : 9.488317 \t Current Learning Rate: 0.009\n",
            "Iteration 16700: Current success : 549 \t All success : 856 \t mean(l0) : 9.488317 \t Current Learning Rate: 0.008\n",
            "Iteration 16800: Current success : 549 \t All success : 856 \t mean(l0) : 9.488317 \t Current Learning Rate: 0.008\n",
            "Iteration 16900: Current success : 549 \t All success : 856 \t mean(l0) : 9.488317 \t Current Learning Rate: 0.008\n",
            "Iteration 17000: Current success : 549 \t All success : 856 \t mean(l0) : 9.488317 \t Current Learning Rate: 0.008\n",
            "Iteration 17100: Current success : 550 \t All success : 856 \t mean(l0) : 9.488317 \t Current Learning Rate: 0.007\n",
            "Iteration 17200: Current success : 548 \t All success : 856 \t mean(l0) : 9.475467 \t Current Learning Rate: 0.007\n",
            "Iteration 17300: Current success : 555 \t All success : 856 \t mean(l0) : 9.475467 \t Current Learning Rate: 0.007\n",
            "Iteration 17400: Current success : 555 \t All success : 856 \t mean(l0) : 9.475467 \t Current Learning Rate: 0.007\n",
            "Iteration 17500: Current success : 548 \t All success : 856 \t mean(l0) : 9.475467 \t Current Learning Rate: 0.006\n",
            "Iteration 17600: Current success : 549 \t All success : 856 \t mean(l0) : 9.474298 \t Current Learning Rate: 0.006\n",
            "Iteration 17700: Current success : 548 \t All success : 856 \t mean(l0) : 9.474298 \t Current Learning Rate: 0.006\n",
            "Iteration 17800: Current success : 549 \t All success : 856 \t mean(l0) : 9.474298 \t Current Learning Rate: 0.006\n",
            "Iteration 17900: Current success : 548 \t All success : 856 \t mean(l0) : 9.474298 \t Current Learning Rate: 0.005\n",
            "Iteration 18000: Current success : 625 \t All success : 856 \t mean(l0) : 9.474298 \t Current Learning Rate: 0.005\n",
            "Iteration 18100: Current success : 661 \t All success : 856 \t mean(l0) : 9.474298 \t Current Learning Rate: 0.005\n",
            "Iteration 18200: Current success : 661 \t All success : 856 \t mean(l0) : 9.474298 \t Current Learning Rate: 0.005\n",
            "Iteration 18300: Current success : 661 \t All success : 856 \t mean(l0) : 9.474298 \t Current Learning Rate: 0.004\n",
            "Iteration 18400: Current success : 661 \t All success : 856 \t mean(l0) : 9.474298 \t Current Learning Rate: 0.004\n",
            "Iteration 18500: Current success : 661 \t All success : 856 \t mean(l0) : 9.474298 \t Current Learning Rate: 0.004\n",
            "Iteration 18600: Current success : 661 \t All success : 856 \t mean(l0) : 9.474298 \t Current Learning Rate: 0.004\n",
            "Iteration 18700: Current success : 661 \t All success : 856 \t mean(l0) : 9.474298 \t Current Learning Rate: 0.003\n",
            "Iteration 18800: Current success : 661 \t All success : 856 \t mean(l0) : 9.474298 \t Current Learning Rate: 0.003\n",
            "Iteration 18900: Current success : 660 \t All success : 856 \t mean(l0) : 9.474298 \t Current Learning Rate: 0.003\n",
            "Iteration 19000: Current success : 660 \t All success : 856 \t mean(l0) : 9.474298 \t Current Learning Rate: 0.003\n",
            "Iteration 19100: Current success : 660 \t All success : 856 \t mean(l0) : 9.474298 \t Current Learning Rate: 0.002\n",
            "Iteration 19200: Current success : 660 \t All success : 856 \t mean(l0) : 9.474298 \t Current Learning Rate: 0.002\n",
            "Iteration 19300: Current success : 660 \t All success : 856 \t mean(l0) : 9.474298 \t Current Learning Rate: 0.002\n",
            "Iteration 19400: Current success : 660 \t All success : 856 \t mean(l0) : 9.474298 \t Current Learning Rate: 0.002\n",
            "Iteration 19500: Current success : 660 \t All success : 856 \t mean(l0) : 9.474298 \t Current Learning Rate: 0.001\n",
            "Iteration 19600: Current success : 660 \t All success : 856 \t mean(l0) : 9.474298 \t Current Learning Rate: 0.001\n",
            "Iteration 19700: Current success : 660 \t All success : 856 \t mean(l0) : 9.474298 \t Current Learning Rate: 0.001\n",
            "Iteration 19800: Current success : 660 \t All success : 856 \t mean(l0) : 9.474298 \t Current Learning Rate: 0.001\n",
            "Iteration 19900: Current success : 660 \t All success : 856 \t mean(l0) : 9.474298 \t Current Learning Rate: 0.000\n",
            "Iteration 20000: Current success : 660 \t All success : 856 \t mean(l0) : 9.474298 \t Current Learning Rate: 0.000\n",
            "outer_step 2: all success : 892 \t mean(l0)(success) : 9.5527\n",
            "-------------------------------------------------------------\n",
            "tensor([  0.2500,   0.2500,   0.7500,  ...,   0.2500, 100.0000,   5.5000],\n",
            "       device='cuda:0')\n",
            "Iteration 100: Current success : 229 \t All success : 275 \t mean(l0) : 9.592728 \t Current Learning Rate: 0.098\n",
            "Iteration 200: Current success : 227 \t All success : 297 \t mean(l0) : 10.464646 \t Current Learning Rate: 0.096\n",
            "Iteration 300: Current success : 218 \t All success : 304 \t mean(l0) : 10.953947 \t Current Learning Rate: 0.094\n",
            "Iteration 400: Current success : 229 \t All success : 307 \t mean(l0) : 11.140065 \t Current Learning Rate: 0.092\n",
            "Iteration 500: Current success : 230 \t All success : 308 \t mean(l0) : 11.243506 \t Current Learning Rate: 0.090\n",
            "Iteration 600: Current success : 231 \t All success : 311 \t mean(l0) : 11.282958 \t Current Learning Rate: 0.088\n",
            "Iteration 700: Current success : 242 \t All success : 313 \t mean(l0) : 11.175718 \t Current Learning Rate: 0.086\n",
            "Iteration 800: Current success : 246 \t All success : 321 \t mean(l0) : 10.959501 \t Current Learning Rate: 0.084\n",
            "Iteration 900: Current success : 239 \t All success : 324 \t mean(l0) : 10.864198 \t Current Learning Rate: 0.082\n",
            "Iteration 1000: Current success : 259 \t All success : 326 \t mean(l0) : 11.018405 \t Current Learning Rate: 0.080\n",
            "Iteration 1100: Current success : 254 \t All success : 328 \t mean(l0) : 11.036585 \t Current Learning Rate: 0.078\n",
            "Iteration 1200: Current success : 244 \t All success : 329 \t mean(l0) : 11.139818 \t Current Learning Rate: 0.076\n",
            "Iteration 1300: Current success : 266 \t All success : 335 \t mean(l0) : 10.961194 \t Current Learning Rate: 0.074\n",
            "Iteration 1400: Current success : 255 \t All success : 335 \t mean(l0) : 10.958209 \t Current Learning Rate: 0.072\n",
            "Iteration 1500: Current success : 252 \t All success : 336 \t mean(l0) : 10.931548 \t Current Learning Rate: 0.070\n",
            "Iteration 1600: Current success : 268 \t All success : 336 \t mean(l0) : 10.931548 \t Current Learning Rate: 0.068\n",
            "Iteration 1700: Current success : 260 \t All success : 337 \t mean(l0) : 10.899110 \t Current Learning Rate: 0.066\n",
            "Iteration 1800: Current success : 252 \t All success : 338 \t mean(l0) : 10.866864 \t Current Learning Rate: 0.064\n",
            "Iteration 1900: Current success : 275 \t All success : 338 \t mean(l0) : 10.866864 \t Current Learning Rate: 0.062\n",
            "Iteration 2000: Current success : 279 \t All success : 340 \t mean(l0) : 10.861765 \t Current Learning Rate: 0.060\n",
            "Iteration 2100: Current success : 271 \t All success : 341 \t mean(l0) : 10.832845 \t Current Learning Rate: 0.058\n",
            "Iteration 2200: Current success : 272 \t All success : 341 \t mean(l0) : 10.832845 \t Current Learning Rate: 0.056\n",
            "Iteration 2300: Current success : 267 \t All success : 342 \t mean(l0) : 10.824561 \t Current Learning Rate: 0.054\n",
            "Iteration 2400: Current success : 271 \t All success : 345 \t mean(l0) : 11.046377 \t Current Learning Rate: 0.052\n",
            "Iteration 2500: Current success : 378 \t All success : 429 \t mean(l0) : 9.995338 \t Current Learning Rate: 0.050\n",
            "Iteration 2600: Current success : 290 \t All success : 653 \t mean(l0) : 8.494640 \t Current Learning Rate: 0.048\n",
            "Iteration 2700: Current success : 324 \t All success : 655 \t mean(l0) : 8.474810 \t Current Learning Rate: 0.046\n",
            "Iteration 2800: Current success : 261 \t All success : 659 \t mean(l0) : 8.450683 \t Current Learning Rate: 0.044\n",
            "Iteration 2900: Current success : 263 \t All success : 659 \t mean(l0) : 8.450683 \t Current Learning Rate: 0.042\n",
            "Iteration 3000: Current success : 263 \t All success : 659 \t mean(l0) : 8.450683 \t Current Learning Rate: 0.040\n",
            "Iteration 3100: Current success : 261 \t All success : 659 \t mean(l0) : 8.450683 \t Current Learning Rate: 0.038\n",
            "Iteration 3200: Current success : 261 \t All success : 659 \t mean(l0) : 8.450683 \t Current Learning Rate: 0.036\n",
            "Iteration 3300: Current success : 261 \t All success : 659 \t mean(l0) : 8.450683 \t Current Learning Rate: 0.034\n",
            "Iteration 3400: Current success : 261 \t All success : 659 \t mean(l0) : 8.450683 \t Current Learning Rate: 0.032\n",
            "Iteration 3500: Current success : 261 \t All success : 659 \t mean(l0) : 8.450683 \t Current Learning Rate: 0.030\n",
            "Iteration 3600: Current success : 261 \t All success : 659 \t mean(l0) : 8.450683 \t Current Learning Rate: 0.028\n",
            "Iteration 3700: Current success : 261 \t All success : 659 \t mean(l0) : 8.450683 \t Current Learning Rate: 0.026\n",
            "Iteration 3800: Current success : 263 \t All success : 659 \t mean(l0) : 8.450683 \t Current Learning Rate: 0.024\n",
            "Iteration 3900: Current success : 261 \t All success : 659 \t mean(l0) : 8.450683 \t Current Learning Rate: 0.022\n",
            "Iteration 4000: Current success : 260 \t All success : 659 \t mean(l0) : 8.450683 \t Current Learning Rate: 0.020\n",
            "Iteration 4100: Current success : 260 \t All success : 659 \t mean(l0) : 8.450683 \t Current Learning Rate: 0.018\n",
            "Iteration 4200: Current success : 260 \t All success : 659 \t mean(l0) : 8.450683 \t Current Learning Rate: 0.016\n",
            "Iteration 4300: Current success : 260 \t All success : 659 \t mean(l0) : 8.450683 \t Current Learning Rate: 0.014\n",
            "Iteration 4400: Current success : 260 \t All success : 659 \t mean(l0) : 8.449165 \t Current Learning Rate: 0.012\n",
            "Iteration 4500: Current success : 260 \t All success : 659 \t mean(l0) : 8.449165 \t Current Learning Rate: 0.010\n",
            "Iteration 4600: Current success : 260 \t All success : 659 \t mean(l0) : 8.449165 \t Current Learning Rate: 0.008\n",
            "Iteration 4700: Current success : 260 \t All success : 659 \t mean(l0) : 8.449165 \t Current Learning Rate: 0.006\n",
            "Iteration 4800: Current success : 260 \t All success : 659 \t mean(l0) : 8.449165 \t Current Learning Rate: 0.004\n",
            "Iteration 4900: Current success : 260 \t All success : 659 \t mean(l0) : 8.447648 \t Current Learning Rate: 0.002\n",
            "Iteration 5000: Current success : 260 \t All success : 659 \t mean(l0) : 8.447648 \t Current Learning Rate: 0.000\n",
            "Iteration 5100: Current success : 389 \t All success : 750 \t mean(l0) : 10.586666 \t Current Learning Rate: 0.049\n",
            "Iteration 5200: Current success : 394 \t All success : 752 \t mean(l0) : 10.667553 \t Current Learning Rate: 0.048\n",
            "Iteration 5300: Current success : 394 \t All success : 752 \t mean(l0) : 10.659575 \t Current Learning Rate: 0.047\n",
            "Iteration 5400: Current success : 398 \t All success : 752 \t mean(l0) : 10.647606 \t Current Learning Rate: 0.046\n",
            "Iteration 5500: Current success : 399 \t All success : 752 \t mean(l0) : 10.644946 \t Current Learning Rate: 0.045\n",
            "Iteration 5600: Current success : 393 \t All success : 752 \t mean(l0) : 10.636968 \t Current Learning Rate: 0.044\n",
            "Iteration 5700: Current success : 416 \t All success : 763 \t mean(l0) : 10.757536 \t Current Learning Rate: 0.043\n",
            "Iteration 5800: Current success : 415 \t All success : 764 \t mean(l0) : 10.768325 \t Current Learning Rate: 0.042\n",
            "Iteration 5900: Current success : 439 \t All success : 765 \t mean(l0) : 10.792157 \t Current Learning Rate: 0.041\n",
            "Iteration 6000: Current success : 425 \t All success : 779 \t mean(l0) : 10.725289 \t Current Learning Rate: 0.040\n",
            "Iteration 6100: Current success : 472 \t All success : 782 \t mean(l0) : 10.710998 \t Current Learning Rate: 0.039\n",
            "Iteration 6200: Current success : 398 \t All success : 782 \t mean(l0) : 10.710998 \t Current Learning Rate: 0.038\n",
            "Iteration 6300: Current success : 408 \t All success : 782 \t mean(l0) : 10.710998 \t Current Learning Rate: 0.037\n",
            "Iteration 6400: Current success : 430 \t All success : 783 \t mean(l0) : 10.702426 \t Current Learning Rate: 0.036\n",
            "Iteration 6500: Current success : 401 \t All success : 783 \t mean(l0) : 10.702426 \t Current Learning Rate: 0.035\n",
            "Iteration 6600: Current success : 480 \t All success : 785 \t mean(l0) : 10.690446 \t Current Learning Rate: 0.034\n",
            "Iteration 6700: Current success : 400 \t All success : 785 \t mean(l0) : 10.690446 \t Current Learning Rate: 0.033\n",
            "Iteration 6800: Current success : 412 \t All success : 785 \t mean(l0) : 10.690446 \t Current Learning Rate: 0.032\n",
            "Iteration 6900: Current success : 403 \t All success : 785 \t mean(l0) : 10.690446 \t Current Learning Rate: 0.031\n",
            "Iteration 7000: Current success : 406 \t All success : 785 \t mean(l0) : 10.690446 \t Current Learning Rate: 0.030\n",
            "Iteration 7100: Current success : 397 \t All success : 785 \t mean(l0) : 10.690446 \t Current Learning Rate: 0.029\n",
            "Iteration 7200: Current success : 400 \t All success : 785 \t mean(l0) : 10.689172 \t Current Learning Rate: 0.028\n",
            "Iteration 7300: Current success : 412 \t All success : 785 \t mean(l0) : 10.689172 \t Current Learning Rate: 0.027\n",
            "Iteration 7400: Current success : 404 \t All success : 785 \t mean(l0) : 10.689172 \t Current Learning Rate: 0.026\n",
            "Iteration 7500: Current success : 404 \t All success : 785 \t mean(l0) : 10.689172 \t Current Learning Rate: 0.025\n",
            "Iteration 7600: Current success : 401 \t All success : 785 \t mean(l0) : 10.689172 \t Current Learning Rate: 0.024\n",
            "Iteration 7700: Current success : 450 \t All success : 785 \t mean(l0) : 10.689172 \t Current Learning Rate: 0.023\n",
            "Iteration 7800: Current success : 451 \t All success : 785 \t mean(l0) : 10.689172 \t Current Learning Rate: 0.022\n",
            "Iteration 7900: Current success : 451 \t All success : 785 \t mean(l0) : 10.689172 \t Current Learning Rate: 0.021\n",
            "Iteration 8000: Current success : 451 \t All success : 785 \t mean(l0) : 10.689172 \t Current Learning Rate: 0.020\n",
            "Iteration 8100: Current success : 451 \t All success : 785 \t mean(l0) : 10.689172 \t Current Learning Rate: 0.019\n",
            "Iteration 8200: Current success : 451 \t All success : 785 \t mean(l0) : 10.689172 \t Current Learning Rate: 0.018\n",
            "Iteration 8300: Current success : 450 \t All success : 785 \t mean(l0) : 10.689172 \t Current Learning Rate: 0.017\n",
            "Iteration 8400: Current success : 450 \t All success : 785 \t mean(l0) : 10.689172 \t Current Learning Rate: 0.016\n",
            "Iteration 8500: Current success : 450 \t All success : 785 \t mean(l0) : 10.689172 \t Current Learning Rate: 0.015\n",
            "Iteration 8600: Current success : 450 \t All success : 785 \t mean(l0) : 10.689172 \t Current Learning Rate: 0.014\n",
            "Iteration 8700: Current success : 450 \t All success : 785 \t mean(l0) : 10.689172 \t Current Learning Rate: 0.013\n",
            "Iteration 8800: Current success : 450 \t All success : 785 \t mean(l0) : 10.689172 \t Current Learning Rate: 0.012\n",
            "Iteration 8900: Current success : 450 \t All success : 785 \t mean(l0) : 10.689172 \t Current Learning Rate: 0.011\n",
            "Iteration 9000: Current success : 450 \t All success : 785 \t mean(l0) : 10.689172 \t Current Learning Rate: 0.010\n",
            "Iteration 9100: Current success : 450 \t All success : 785 \t mean(l0) : 10.689172 \t Current Learning Rate: 0.009\n",
            "Iteration 9200: Current success : 450 \t All success : 785 \t mean(l0) : 10.689172 \t Current Learning Rate: 0.008\n",
            "Iteration 9300: Current success : 450 \t All success : 785 \t mean(l0) : 10.689172 \t Current Learning Rate: 0.007\n",
            "Iteration 9400: Current success : 450 \t All success : 785 \t mean(l0) : 10.689172 \t Current Learning Rate: 0.006\n",
            "Iteration 9500: Current success : 450 \t All success : 785 \t mean(l0) : 10.689172 \t Current Learning Rate: 0.005\n",
            "Iteration 9600: Current success : 450 \t All success : 785 \t mean(l0) : 10.689172 \t Current Learning Rate: 0.004\n",
            "Iteration 9700: Current success : 450 \t All success : 785 \t mean(l0) : 10.687899 \t Current Learning Rate: 0.003\n",
            "Iteration 9800: Current success : 450 \t All success : 785 \t mean(l0) : 10.687899 \t Current Learning Rate: 0.002\n",
            "Iteration 9900: Current success : 450 \t All success : 785 \t mean(l0) : 10.687899 \t Current Learning Rate: 0.001\n",
            "Iteration 10000: Current success : 450 \t All success : 785 \t mean(l0) : 10.687899 \t Current Learning Rate: 0.000\n",
            "Iteration 10100: Current success : 422 \t All success : 789 \t mean(l0) : 10.880862 \t Current Learning Rate: 0.025\n",
            "Iteration 10200: Current success : 450 \t All success : 790 \t mean(l0) : 10.936709 \t Current Learning Rate: 0.024\n",
            "Iteration 10300: Current success : 422 \t All success : 794 \t mean(l0) : 11.025188 \t Current Learning Rate: 0.024\n",
            "Iteration 10400: Current success : 423 \t All success : 795 \t mean(l0) : 11.103145 \t Current Learning Rate: 0.023\n",
            "Iteration 10500: Current success : 426 \t All success : 795 \t mean(l0) : 11.103145 \t Current Learning Rate: 0.023\n",
            "Iteration 10600: Current success : 422 \t All success : 795 \t mean(l0) : 11.103145 \t Current Learning Rate: 0.022\n",
            "Iteration 10700: Current success : 447 \t All success : 796 \t mean(l0) : 11.091708 \t Current Learning Rate: 0.022\n",
            "Iteration 10800: Current success : 428 \t All success : 796 \t mean(l0) : 11.090452 \t Current Learning Rate: 0.021\n",
            "Iteration 10900: Current success : 418 \t All success : 796 \t mean(l0) : 11.090452 \t Current Learning Rate: 0.021\n",
            "Iteration 11000: Current success : 456 \t All success : 796 \t mean(l0) : 11.089196 \t Current Learning Rate: 0.020\n",
            "Iteration 11100: Current success : 428 \t All success : 796 \t mean(l0) : 11.089196 \t Current Learning Rate: 0.020\n",
            "Iteration 11200: Current success : 420 \t All success : 796 \t mean(l0) : 11.087939 \t Current Learning Rate: 0.019\n",
            "Iteration 11300: Current success : 456 \t All success : 796 \t mean(l0) : 11.087939 \t Current Learning Rate: 0.019\n",
            "Iteration 11400: Current success : 439 \t All success : 796 \t mean(l0) : 11.087939 \t Current Learning Rate: 0.018\n",
            "Iteration 11500: Current success : 419 \t All success : 796 \t mean(l0) : 11.087939 \t Current Learning Rate: 0.018\n",
            "Iteration 11600: Current success : 418 \t All success : 796 \t mean(l0) : 11.087939 \t Current Learning Rate: 0.017\n",
            "Iteration 11700: Current success : 462 \t All success : 796 \t mean(l0) : 11.087939 \t Current Learning Rate: 0.017\n",
            "Iteration 11800: Current success : 434 \t All success : 796 \t mean(l0) : 11.087939 \t Current Learning Rate: 0.016\n",
            "Iteration 11900: Current success : 422 \t All success : 796 \t mean(l0) : 11.086683 \t Current Learning Rate: 0.016\n",
            "Iteration 12000: Current success : 422 \t All success : 796 \t mean(l0) : 11.086683 \t Current Learning Rate: 0.015\n",
            "Iteration 12100: Current success : 431 \t All success : 796 \t mean(l0) : 11.086683 \t Current Learning Rate: 0.015\n",
            "Iteration 12200: Current success : 427 \t All success : 796 \t mean(l0) : 11.086683 \t Current Learning Rate: 0.014\n",
            "Iteration 12300: Current success : 419 \t All success : 796 \t mean(l0) : 11.086683 \t Current Learning Rate: 0.014\n",
            "Iteration 12400: Current success : 419 \t All success : 796 \t mean(l0) : 11.086683 \t Current Learning Rate: 0.013\n",
            "Iteration 12500: Current success : 421 \t All success : 796 \t mean(l0) : 11.085427 \t Current Learning Rate: 0.013\n",
            "Iteration 12600: Current success : 419 \t All success : 796 \t mean(l0) : 11.085427 \t Current Learning Rate: 0.012\n",
            "Iteration 12700: Current success : 426 \t All success : 796 \t mean(l0) : 11.084170 \t Current Learning Rate: 0.012\n",
            "Iteration 12800: Current success : 469 \t All success : 796 \t mean(l0) : 11.084170 \t Current Learning Rate: 0.011\n",
            "Iteration 12900: Current success : 491 \t All success : 796 \t mean(l0) : 11.084170 \t Current Learning Rate: 0.011\n",
            "Iteration 13000: Current success : 495 \t All success : 796 \t mean(l0) : 11.084170 \t Current Learning Rate: 0.010\n",
            "Iteration 13100: Current success : 495 \t All success : 796 \t mean(l0) : 11.084170 \t Current Learning Rate: 0.010\n",
            "Iteration 13200: Current success : 495 \t All success : 796 \t mean(l0) : 11.084170 \t Current Learning Rate: 0.009\n",
            "Iteration 13300: Current success : 501 \t All success : 796 \t mean(l0) : 11.084170 \t Current Learning Rate: 0.009\n",
            "Iteration 13400: Current success : 501 \t All success : 796 \t mean(l0) : 11.082914 \t Current Learning Rate: 0.008\n",
            "Iteration 13500: Current success : 501 \t All success : 796 \t mean(l0) : 11.082914 \t Current Learning Rate: 0.008\n",
            "Iteration 13600: Current success : 501 \t All success : 796 \t mean(l0) : 11.081658 \t Current Learning Rate: 0.007\n",
            "Iteration 13700: Current success : 501 \t All success : 796 \t mean(l0) : 11.080401 \t Current Learning Rate: 0.007\n",
            "Iteration 13800: Current success : 501 \t All success : 796 \t mean(l0) : 11.080401 \t Current Learning Rate: 0.006\n",
            "Iteration 13900: Current success : 501 \t All success : 796 \t mean(l0) : 11.080401 \t Current Learning Rate: 0.006\n",
            "Iteration 14000: Current success : 501 \t All success : 796 \t mean(l0) : 11.079145 \t Current Learning Rate: 0.005\n",
            "Iteration 14100: Current success : 501 \t All success : 796 \t mean(l0) : 11.079145 \t Current Learning Rate: 0.005\n",
            "Iteration 14200: Current success : 501 \t All success : 796 \t mean(l0) : 11.079145 \t Current Learning Rate: 0.004\n",
            "Iteration 14300: Current success : 501 \t All success : 796 \t mean(l0) : 11.079145 \t Current Learning Rate: 0.004\n",
            "Iteration 14400: Current success : 501 \t All success : 796 \t mean(l0) : 11.079145 \t Current Learning Rate: 0.003\n",
            "Iteration 14500: Current success : 501 \t All success : 796 \t mean(l0) : 11.079145 \t Current Learning Rate: 0.003\n",
            "Iteration 14600: Current success : 501 \t All success : 796 \t mean(l0) : 11.079145 \t Current Learning Rate: 0.002\n",
            "Iteration 14700: Current success : 501 \t All success : 796 \t mean(l0) : 11.079145 \t Current Learning Rate: 0.002\n",
            "Iteration 14800: Current success : 501 \t All success : 796 \t mean(l0) : 11.079145 \t Current Learning Rate: 0.001\n",
            "Iteration 14900: Current success : 501 \t All success : 796 \t mean(l0) : 11.079145 \t Current Learning Rate: 0.001\n",
            "Iteration 15000: Current success : 501 \t All success : 796 \t mean(l0) : 11.079145 \t Current Learning Rate: 0.000\n",
            "Iteration 15100: Current success : 437 \t All success : 798 \t mean(l0) : 11.181705 \t Current Learning Rate: 0.012\n",
            "Iteration 15200: Current success : 507 \t All success : 801 \t mean(l0) : 11.323345 \t Current Learning Rate: 0.012\n",
            "Iteration 15300: Current success : 507 \t All success : 801 \t mean(l0) : 11.322097 \t Current Learning Rate: 0.012\n",
            "Iteration 15400: Current success : 505 \t All success : 801 \t mean(l0) : 11.313358 \t Current Learning Rate: 0.012\n",
            "Iteration 15500: Current success : 486 \t All success : 803 \t mean(l0) : 11.298879 \t Current Learning Rate: 0.011\n",
            "Iteration 15600: Current success : 428 \t All success : 803 \t mean(l0) : 11.296389 \t Current Learning Rate: 0.011\n",
            "Iteration 15700: Current success : 430 \t All success : 803 \t mean(l0) : 11.295143 \t Current Learning Rate: 0.011\n",
            "Iteration 15800: Current success : 430 \t All success : 803 \t mean(l0) : 11.293898 \t Current Learning Rate: 0.011\n",
            "Iteration 15900: Current success : 430 \t All success : 803 \t mean(l0) : 11.293898 \t Current Learning Rate: 0.010\n",
            "Iteration 16000: Current success : 429 \t All success : 803 \t mean(l0) : 11.292652 \t Current Learning Rate: 0.010\n",
            "Iteration 16100: Current success : 429 \t All success : 803 \t mean(l0) : 11.291407 \t Current Learning Rate: 0.010\n",
            "Iteration 16200: Current success : 427 \t All success : 803 \t mean(l0) : 11.291407 \t Current Learning Rate: 0.010\n",
            "Iteration 16300: Current success : 426 \t All success : 803 \t mean(l0) : 11.291407 \t Current Learning Rate: 0.009\n",
            "Iteration 16400: Current success : 428 \t All success : 803 \t mean(l0) : 11.291407 \t Current Learning Rate: 0.009\n",
            "Iteration 16500: Current success : 426 \t All success : 803 \t mean(l0) : 11.291407 \t Current Learning Rate: 0.009\n",
            "Iteration 16600: Current success : 427 \t All success : 803 \t mean(l0) : 11.291407 \t Current Learning Rate: 0.009\n",
            "Iteration 16700: Current success : 428 \t All success : 803 \t mean(l0) : 11.291407 \t Current Learning Rate: 0.008\n",
            "Iteration 16800: Current success : 426 \t All success : 803 \t mean(l0) : 11.290161 \t Current Learning Rate: 0.008\n",
            "Iteration 16900: Current success : 428 \t All success : 803 \t mean(l0) : 11.290161 \t Current Learning Rate: 0.008\n",
            "Iteration 17000: Current success : 427 \t All success : 803 \t mean(l0) : 11.290161 \t Current Learning Rate: 0.008\n",
            "Iteration 17100: Current success : 430 \t All success : 803 \t mean(l0) : 11.290161 \t Current Learning Rate: 0.007\n",
            "Iteration 17200: Current success : 429 \t All success : 803 \t mean(l0) : 11.290161 \t Current Learning Rate: 0.007\n",
            "Iteration 17300: Current success : 428 \t All success : 803 \t mean(l0) : 11.290161 \t Current Learning Rate: 0.007\n",
            "Iteration 17400: Current success : 429 \t All success : 803 \t mean(l0) : 11.290161 \t Current Learning Rate: 0.007\n",
            "Iteration 17500: Current success : 428 \t All success : 803 \t mean(l0) : 11.290161 \t Current Learning Rate: 0.006\n",
            "Iteration 17600: Current success : 428 \t All success : 803 \t mean(l0) : 11.290161 \t Current Learning Rate: 0.006\n",
            "Iteration 17700: Current success : 430 \t All success : 803 \t mean(l0) : 11.290161 \t Current Learning Rate: 0.006\n",
            "Iteration 17800: Current success : 430 \t All success : 803 \t mean(l0) : 11.290161 \t Current Learning Rate: 0.006\n",
            "Iteration 17900: Current success : 429 \t All success : 803 \t mean(l0) : 11.287671 \t Current Learning Rate: 0.005\n",
            "Iteration 18000: Current success : 487 \t All success : 803 \t mean(l0) : 11.287671 \t Current Learning Rate: 0.005\n",
            "Iteration 18100: Current success : 508 \t All success : 803 \t mean(l0) : 11.287671 \t Current Learning Rate: 0.005\n",
            "Iteration 18200: Current success : 508 \t All success : 803 \t mean(l0) : 11.287671 \t Current Learning Rate: 0.005\n",
            "Iteration 18300: Current success : 506 \t All success : 803 \t mean(l0) : 11.287671 \t Current Learning Rate: 0.004\n",
            "Iteration 18400: Current success : 506 \t All success : 803 \t mean(l0) : 11.287671 \t Current Learning Rate: 0.004\n",
            "Iteration 18500: Current success : 507 \t All success : 803 \t mean(l0) : 11.282689 \t Current Learning Rate: 0.004\n",
            "Iteration 18600: Current success : 507 \t All success : 803 \t mean(l0) : 11.281445 \t Current Learning Rate: 0.004\n",
            "Iteration 18700: Current success : 507 \t All success : 803 \t mean(l0) : 11.281445 \t Current Learning Rate: 0.003\n",
            "Iteration 18800: Current success : 506 \t All success : 803 \t mean(l0) : 11.281445 \t Current Learning Rate: 0.003\n",
            "Iteration 18900: Current success : 507 \t All success : 803 \t mean(l0) : 11.276463 \t Current Learning Rate: 0.003\n",
            "Iteration 19000: Current success : 507 \t All success : 803 \t mean(l0) : 11.276463 \t Current Learning Rate: 0.003\n",
            "Iteration 19100: Current success : 507 \t All success : 803 \t mean(l0) : 11.276463 \t Current Learning Rate: 0.002\n",
            "Iteration 19200: Current success : 507 \t All success : 803 \t mean(l0) : 11.276463 \t Current Learning Rate: 0.002\n",
            "Iteration 19300: Current success : 507 \t All success : 803 \t mean(l0) : 11.276463 \t Current Learning Rate: 0.002\n",
            "Iteration 19400: Current success : 507 \t All success : 803 \t mean(l0) : 11.276463 \t Current Learning Rate: 0.002\n",
            "Iteration 19500: Current success : 507 \t All success : 803 \t mean(l0) : 11.276463 \t Current Learning Rate: 0.001\n",
            "Iteration 19600: Current success : 507 \t All success : 803 \t mean(l0) : 11.276463 \t Current Learning Rate: 0.001\n",
            "Iteration 19700: Current success : 507 \t All success : 803 \t mean(l0) : 11.276463 \t Current Learning Rate: 0.001\n",
            "Iteration 19800: Current success : 507 \t All success : 803 \t mean(l0) : 11.276463 \t Current Learning Rate: 0.001\n",
            "Iteration 19900: Current success : 507 \t All success : 803 \t mean(l0) : 11.276463 \t Current Learning Rate: 0.000\n",
            "Iteration 20000: Current success : 507 \t All success : 803 \t mean(l0) : 11.276463 \t Current Learning Rate: 0.000\n",
            "outer_step 3: all success : 941 \t mean(l0)(success) : 11.5282\n",
            "-------------------------------------------------------------\n",
            "tensor([ 0.1250,  0.1250,  0.6250,  ...,  0.3750, 55.0000,  3.2500],\n",
            "       device='cuda:0')\n",
            "Iteration 100: Current success : 138 \t All success : 172 \t mean(l0) : 6.691861 \t Current Learning Rate: 0.098\n",
            "Iteration 200: Current success : 161 \t All success : 195 \t mean(l0) : 9.779488 \t Current Learning Rate: 0.096\n",
            "Iteration 300: Current success : 156 \t All success : 198 \t mean(l0) : 10.520202 \t Current Learning Rate: 0.094\n",
            "Iteration 400: Current success : 166 \t All success : 204 \t mean(l0) : 11.970589 \t Current Learning Rate: 0.092\n",
            "Iteration 500: Current success : 170 \t All success : 205 \t mean(l0) : 12.078049 \t Current Learning Rate: 0.090\n",
            "Iteration 600: Current success : 173 \t All success : 208 \t mean(l0) : 12.466347 \t Current Learning Rate: 0.088\n",
            "Iteration 700: Current success : 183 \t All success : 214 \t mean(l0) : 12.761682 \t Current Learning Rate: 0.086\n",
            "Iteration 800: Current success : 191 \t All success : 220 \t mean(l0) : 12.472727 \t Current Learning Rate: 0.084\n",
            "Iteration 900: Current success : 191 \t All success : 230 \t mean(l0) : 12.795651 \t Current Learning Rate: 0.082\n",
            "Iteration 1000: Current success : 197 \t All success : 233 \t mean(l0) : 13.240343 \t Current Learning Rate: 0.080\n",
            "Iteration 1100: Current success : 202 \t All success : 239 \t mean(l0) : 13.468619 \t Current Learning Rate: 0.078\n",
            "Iteration 1200: Current success : 197 \t All success : 243 \t mean(l0) : 13.411522 \t Current Learning Rate: 0.076\n",
            "Iteration 1300: Current success : 212 \t All success : 243 \t mean(l0) : 13.407407 \t Current Learning Rate: 0.074\n",
            "Iteration 1400: Current success : 207 \t All success : 243 \t mean(l0) : 13.407407 \t Current Learning Rate: 0.072\n",
            "Iteration 1500: Current success : 201 \t All success : 246 \t mean(l0) : 13.784553 \t Current Learning Rate: 0.070\n",
            "Iteration 1600: Current success : 219 \t All success : 248 \t mean(l0) : 13.911290 \t Current Learning Rate: 0.068\n",
            "Iteration 1700: Current success : 215 \t All success : 248 \t mean(l0) : 13.911290 \t Current Learning Rate: 0.066\n",
            "Iteration 1800: Current success : 208 \t All success : 249 \t mean(l0) : 13.863453 \t Current Learning Rate: 0.064\n",
            "Iteration 1900: Current success : 221 \t All success : 249 \t mean(l0) : 13.863453 \t Current Learning Rate: 0.062\n",
            "Iteration 2000: Current success : 222 \t All success : 249 \t mean(l0) : 13.839357 \t Current Learning Rate: 0.060\n",
            "Iteration 2100: Current success : 214 \t All success : 252 \t mean(l0) : 13.773810 \t Current Learning Rate: 0.058\n",
            "Iteration 2200: Current success : 216 \t All success : 253 \t mean(l0) : 13.723321 \t Current Learning Rate: 0.056\n",
            "Iteration 2300: Current success : 214 \t All success : 256 \t mean(l0) : 13.585938 \t Current Learning Rate: 0.054\n",
            "Iteration 2400: Current success : 219 \t All success : 263 \t mean(l0) : 13.661597 \t Current Learning Rate: 0.052\n",
            "Iteration 2500: Current success : 312 \t All success : 367 \t mean(l0) : 10.828338 \t Current Learning Rate: 0.050\n",
            "Iteration 2600: Current success : 191 \t All success : 496 \t mean(l0) : 9.491935 \t Current Learning Rate: 0.048\n",
            "Iteration 2700: Current success : 220 \t All success : 500 \t mean(l0) : 9.484000 \t Current Learning Rate: 0.046\n",
            "Iteration 2800: Current success : 206 \t All success : 510 \t mean(l0) : 9.435295 \t Current Learning Rate: 0.044\n",
            "Iteration 2900: Current success : 206 \t All success : 510 \t mean(l0) : 9.435295 \t Current Learning Rate: 0.042\n",
            "Iteration 3000: Current success : 207 \t All success : 510 \t mean(l0) : 9.435295 \t Current Learning Rate: 0.040\n",
            "Iteration 3100: Current success : 206 \t All success : 510 \t mean(l0) : 9.433334 \t Current Learning Rate: 0.038\n",
            "Iteration 3200: Current success : 205 \t All success : 510 \t mean(l0) : 9.433334 \t Current Learning Rate: 0.036\n",
            "Iteration 3300: Current success : 207 \t All success : 510 \t mean(l0) : 9.433334 \t Current Learning Rate: 0.034\n",
            "Iteration 3400: Current success : 207 \t All success : 510 \t mean(l0) : 9.431373 \t Current Learning Rate: 0.032\n",
            "Iteration 3500: Current success : 207 \t All success : 510 \t mean(l0) : 9.431373 \t Current Learning Rate: 0.030\n",
            "Iteration 3600: Current success : 207 \t All success : 510 \t mean(l0) : 9.431373 \t Current Learning Rate: 0.028\n",
            "Iteration 3700: Current success : 206 \t All success : 510 \t mean(l0) : 9.431373 \t Current Learning Rate: 0.026\n",
            "Iteration 3800: Current success : 206 \t All success : 510 \t mean(l0) : 9.431373 \t Current Learning Rate: 0.024\n",
            "Iteration 3900: Current success : 206 \t All success : 510 \t mean(l0) : 9.431373 \t Current Learning Rate: 0.022\n",
            "Iteration 4000: Current success : 206 \t All success : 510 \t mean(l0) : 9.431373 \t Current Learning Rate: 0.020\n",
            "Iteration 4100: Current success : 205 \t All success : 510 \t mean(l0) : 9.431373 \t Current Learning Rate: 0.018\n",
            "Iteration 4200: Current success : 205 \t All success : 510 \t mean(l0) : 9.431373 \t Current Learning Rate: 0.016\n",
            "Iteration 4300: Current success : 206 \t All success : 510 \t mean(l0) : 9.431373 \t Current Learning Rate: 0.014\n",
            "Iteration 4400: Current success : 206 \t All success : 510 \t mean(l0) : 9.431373 \t Current Learning Rate: 0.012\n",
            "Iteration 4500: Current success : 206 \t All success : 510 \t mean(l0) : 9.431373 \t Current Learning Rate: 0.010\n",
            "Iteration 4600: Current success : 206 \t All success : 510 \t mean(l0) : 9.431373 \t Current Learning Rate: 0.008\n",
            "Iteration 4700: Current success : 207 \t All success : 510 \t mean(l0) : 9.431373 \t Current Learning Rate: 0.006\n",
            "Iteration 4800: Current success : 207 \t All success : 510 \t mean(l0) : 9.431373 \t Current Learning Rate: 0.004\n",
            "Iteration 4900: Current success : 207 \t All success : 510 \t mean(l0) : 9.431373 \t Current Learning Rate: 0.002\n",
            "Iteration 5000: Current success : 207 \t All success : 510 \t mean(l0) : 9.431373 \t Current Learning Rate: 0.000\n",
            "Iteration 5100: Current success : 193 \t All success : 546 \t mean(l0) : 10.133699 \t Current Learning Rate: 0.049\n",
            "Iteration 5200: Current success : 195 \t All success : 554 \t mean(l0) : 10.297833 \t Current Learning Rate: 0.048\n",
            "Iteration 5300: Current success : 200 \t All success : 559 \t mean(l0) : 10.769231 \t Current Learning Rate: 0.047\n",
            "Iteration 5400: Current success : 198 \t All success : 559 \t mean(l0) : 10.760286 \t Current Learning Rate: 0.046\n",
            "Iteration 5500: Current success : 201 \t All success : 560 \t mean(l0) : 10.848214 \t Current Learning Rate: 0.045\n",
            "Iteration 5600: Current success : 198 \t All success : 560 \t mean(l0) : 10.848214 \t Current Learning Rate: 0.044\n",
            "Iteration 5700: Current success : 232 \t All success : 566 \t mean(l0) : 10.893993 \t Current Learning Rate: 0.043\n",
            "Iteration 5800: Current success : 230 \t All success : 569 \t mean(l0) : 10.973639 \t Current Learning Rate: 0.042\n",
            "Iteration 5900: Current success : 261 \t All success : 589 \t mean(l0) : 11.292021 \t Current Learning Rate: 0.041\n",
            "Iteration 6000: Current success : 231 \t All success : 611 \t mean(l0) : 11.407529 \t Current Learning Rate: 0.040\n",
            "Iteration 6100: Current success : 291 \t All success : 628 \t mean(l0) : 11.385350 \t Current Learning Rate: 0.039\n",
            "Iteration 6200: Current success : 217 \t All success : 628 \t mean(l0) : 11.380573 \t Current Learning Rate: 0.038\n",
            "Iteration 6300: Current success : 202 \t All success : 628 \t mean(l0) : 11.380573 \t Current Learning Rate: 0.037\n",
            "Iteration 6400: Current success : 244 \t All success : 628 \t mean(l0) : 11.380573 \t Current Learning Rate: 0.036\n",
            "Iteration 6500: Current success : 212 \t All success : 628 \t mean(l0) : 11.378981 \t Current Learning Rate: 0.035\n",
            "Iteration 6600: Current success : 273 \t All success : 630 \t mean(l0) : 11.363492 \t Current Learning Rate: 0.034\n",
            "Iteration 6700: Current success : 203 \t All success : 631 \t mean(l0) : 11.429478 \t Current Learning Rate: 0.033\n",
            "Iteration 6800: Current success : 211 \t All success : 631 \t mean(l0) : 11.429478 \t Current Learning Rate: 0.032\n",
            "Iteration 6900: Current success : 206 \t All success : 632 \t mean(l0) : 11.424051 \t Current Learning Rate: 0.031\n",
            "Iteration 7000: Current success : 211 \t All success : 632 \t mean(l0) : 11.422468 \t Current Learning Rate: 0.030\n",
            "Iteration 7100: Current success : 202 \t All success : 632 \t mean(l0) : 11.422468 \t Current Learning Rate: 0.029\n",
            "Iteration 7200: Current success : 213 \t All success : 632 \t mean(l0) : 11.422468 \t Current Learning Rate: 0.028\n",
            "Iteration 7300: Current success : 210 \t All success : 632 \t mean(l0) : 11.422468 \t Current Learning Rate: 0.027\n",
            "Iteration 7400: Current success : 218 \t All success : 632 \t mean(l0) : 11.422468 \t Current Learning Rate: 0.026\n",
            "Iteration 7500: Current success : 222 \t All success : 636 \t mean(l0) : 11.437106 \t Current Learning Rate: 0.025\n",
            "Iteration 7600: Current success : 202 \t All success : 636 \t mean(l0) : 11.437106 \t Current Learning Rate: 0.024\n",
            "Iteration 7700: Current success : 247 \t All success : 636 \t mean(l0) : 11.437106 \t Current Learning Rate: 0.023\n",
            "Iteration 7800: Current success : 248 \t All success : 636 \t mean(l0) : 11.437106 \t Current Learning Rate: 0.022\n",
            "Iteration 7900: Current success : 250 \t All success : 636 \t mean(l0) : 11.437106 \t Current Learning Rate: 0.021\n",
            "Iteration 8000: Current success : 250 \t All success : 636 \t mean(l0) : 11.437106 \t Current Learning Rate: 0.020\n",
            "Iteration 8100: Current success : 250 \t All success : 636 \t mean(l0) : 11.437106 \t Current Learning Rate: 0.019\n",
            "Iteration 8200: Current success : 250 \t All success : 636 \t mean(l0) : 11.437106 \t Current Learning Rate: 0.018\n",
            "Iteration 8300: Current success : 250 \t All success : 636 \t mean(l0) : 11.437106 \t Current Learning Rate: 0.017\n",
            "Iteration 8400: Current success : 250 \t All success : 636 \t mean(l0) : 11.437106 \t Current Learning Rate: 0.016\n",
            "Iteration 8500: Current success : 250 \t All success : 636 \t mean(l0) : 11.437106 \t Current Learning Rate: 0.015\n",
            "Iteration 8600: Current success : 250 \t All success : 636 \t mean(l0) : 11.437106 \t Current Learning Rate: 0.014\n",
            "Iteration 8700: Current success : 250 \t All success : 636 \t mean(l0) : 11.437106 \t Current Learning Rate: 0.013\n",
            "Iteration 8800: Current success : 250 \t All success : 636 \t mean(l0) : 11.437106 \t Current Learning Rate: 0.012\n",
            "Iteration 8900: Current success : 250 \t All success : 636 \t mean(l0) : 11.437106 \t Current Learning Rate: 0.011\n",
            "Iteration 9000: Current success : 250 \t All success : 636 \t mean(l0) : 11.437106 \t Current Learning Rate: 0.010\n",
            "Iteration 9100: Current success : 250 \t All success : 636 \t mean(l0) : 11.437106 \t Current Learning Rate: 0.009\n",
            "Iteration 9200: Current success : 250 \t All success : 636 \t mean(l0) : 11.437106 \t Current Learning Rate: 0.008\n",
            "Iteration 9300: Current success : 250 \t All success : 636 \t mean(l0) : 11.437106 \t Current Learning Rate: 0.007\n",
            "Iteration 9400: Current success : 250 \t All success : 636 \t mean(l0) : 11.437106 \t Current Learning Rate: 0.006\n",
            "Iteration 9500: Current success : 250 \t All success : 636 \t mean(l0) : 11.437106 \t Current Learning Rate: 0.005\n",
            "Iteration 9600: Current success : 250 \t All success : 636 \t mean(l0) : 11.437106 \t Current Learning Rate: 0.004\n",
            "Iteration 9700: Current success : 250 \t All success : 636 \t mean(l0) : 11.437106 \t Current Learning Rate: 0.003\n",
            "Iteration 9800: Current success : 250 \t All success : 636 \t mean(l0) : 11.437106 \t Current Learning Rate: 0.002\n",
            "Iteration 9900: Current success : 250 \t All success : 636 \t mean(l0) : 11.437106 \t Current Learning Rate: 0.001\n",
            "Iteration 10000: Current success : 250 \t All success : 636 \t mean(l0) : 11.437106 \t Current Learning Rate: 0.000\n",
            "Iteration 10100: Current success : 210 \t All success : 639 \t mean(l0) : 11.604069 \t Current Learning Rate: 0.025\n",
            "Iteration 10200: Current success : 242 \t All success : 641 \t mean(l0) : 11.606864 \t Current Learning Rate: 0.024\n",
            "Iteration 10300: Current success : 212 \t All success : 641 \t mean(l0) : 11.605304 \t Current Learning Rate: 0.024\n",
            "Iteration 10400: Current success : 211 \t All success : 641 \t mean(l0) : 11.600624 \t Current Learning Rate: 0.023\n",
            "Iteration 10500: Current success : 211 \t All success : 641 \t mean(l0) : 11.600624 \t Current Learning Rate: 0.023\n",
            "Iteration 10600: Current success : 211 \t All success : 641 \t mean(l0) : 11.600624 \t Current Learning Rate: 0.022\n",
            "Iteration 10700: Current success : 250 \t All success : 643 \t mean(l0) : 11.575428 \t Current Learning Rate: 0.022\n",
            "Iteration 10800: Current success : 214 \t All success : 643 \t mean(l0) : 11.575428 \t Current Learning Rate: 0.021\n",
            "Iteration 10900: Current success : 211 \t All success : 643 \t mean(l0) : 11.575428 \t Current Learning Rate: 0.021\n",
            "Iteration 11000: Current success : 257 \t All success : 643 \t mean(l0) : 11.573873 \t Current Learning Rate: 0.020\n",
            "Iteration 11100: Current success : 215 \t All success : 643 \t mean(l0) : 11.572317 \t Current Learning Rate: 0.020\n",
            "Iteration 11200: Current success : 210 \t All success : 643 \t mean(l0) : 11.572317 \t Current Learning Rate: 0.019\n",
            "Iteration 11300: Current success : 265 \t All success : 643 \t mean(l0) : 11.572317 \t Current Learning Rate: 0.019\n",
            "Iteration 11400: Current success : 219 \t All success : 643 \t mean(l0) : 11.572317 \t Current Learning Rate: 0.018\n",
            "Iteration 11500: Current success : 212 \t All success : 643 \t mean(l0) : 11.572317 \t Current Learning Rate: 0.018\n",
            "Iteration 11600: Current success : 211 \t All success : 643 \t mean(l0) : 11.572317 \t Current Learning Rate: 0.017\n",
            "Iteration 11700: Current success : 248 \t All success : 643 \t mean(l0) : 11.572317 \t Current Learning Rate: 0.017\n",
            "Iteration 11800: Current success : 219 \t All success : 643 \t mean(l0) : 11.572317 \t Current Learning Rate: 0.016\n",
            "Iteration 11900: Current success : 215 \t All success : 643 \t mean(l0) : 11.572317 \t Current Learning Rate: 0.016\n",
            "Iteration 12000: Current success : 213 \t All success : 643 \t mean(l0) : 11.572317 \t Current Learning Rate: 0.015\n",
            "Iteration 12100: Current success : 232 \t All success : 643 \t mean(l0) : 11.572317 \t Current Learning Rate: 0.015\n",
            "Iteration 12200: Current success : 227 \t All success : 643 \t mean(l0) : 11.572317 \t Current Learning Rate: 0.014\n",
            "Iteration 12300: Current success : 211 \t All success : 643 \t mean(l0) : 11.572317 \t Current Learning Rate: 0.014\n",
            "Iteration 12400: Current success : 214 \t All success : 643 \t mean(l0) : 11.572317 \t Current Learning Rate: 0.013\n",
            "Iteration 12500: Current success : 215 \t All success : 643 \t mean(l0) : 11.572317 \t Current Learning Rate: 0.013\n",
            "Iteration 12600: Current success : 212 \t All success : 643 \t mean(l0) : 11.572317 \t Current Learning Rate: 0.012\n",
            "Iteration 12700: Current success : 212 \t All success : 643 \t mean(l0) : 11.569207 \t Current Learning Rate: 0.012\n",
            "Iteration 12800: Current success : 247 \t All success : 643 \t mean(l0) : 11.569207 \t Current Learning Rate: 0.011\n",
            "Iteration 12900: Current success : 259 \t All success : 643 \t mean(l0) : 11.569207 \t Current Learning Rate: 0.011\n",
            "Iteration 13000: Current success : 259 \t All success : 643 \t mean(l0) : 11.569207 \t Current Learning Rate: 0.010\n",
            "Iteration 13100: Current success : 259 \t All success : 643 \t mean(l0) : 11.569207 \t Current Learning Rate: 0.010\n",
            "Iteration 13200: Current success : 259 \t All success : 643 \t mean(l0) : 11.569207 \t Current Learning Rate: 0.009\n",
            "Iteration 13300: Current success : 259 \t All success : 643 \t mean(l0) : 11.569207 \t Current Learning Rate: 0.009\n",
            "Iteration 13400: Current success : 257 \t All success : 643 \t mean(l0) : 11.569207 \t Current Learning Rate: 0.008\n",
            "Iteration 13500: Current success : 257 \t All success : 643 \t mean(l0) : 11.569207 \t Current Learning Rate: 0.008\n",
            "Iteration 13600: Current success : 259 \t All success : 643 \t mean(l0) : 11.569207 \t Current Learning Rate: 0.007\n",
            "Iteration 13700: Current success : 259 \t All success : 643 \t mean(l0) : 11.569207 \t Current Learning Rate: 0.007\n",
            "Iteration 13800: Current success : 260 \t All success : 643 \t mean(l0) : 11.569207 \t Current Learning Rate: 0.006\n",
            "Iteration 13900: Current success : 260 \t All success : 643 \t mean(l0) : 11.569207 \t Current Learning Rate: 0.006\n",
            "Iteration 14000: Current success : 260 \t All success : 643 \t mean(l0) : 11.569207 \t Current Learning Rate: 0.005\n",
            "Iteration 14100: Current success : 260 \t All success : 643 \t mean(l0) : 11.569207 \t Current Learning Rate: 0.005\n",
            "Iteration 14200: Current success : 260 \t All success : 643 \t mean(l0) : 11.569207 \t Current Learning Rate: 0.004\n",
            "Iteration 14300: Current success : 260 \t All success : 643 \t mean(l0) : 11.569207 \t Current Learning Rate: 0.004\n",
            "Iteration 14400: Current success : 260 \t All success : 643 \t mean(l0) : 11.569207 \t Current Learning Rate: 0.003\n",
            "Iteration 14500: Current success : 260 \t All success : 643 \t mean(l0) : 11.569207 \t Current Learning Rate: 0.003\n",
            "Iteration 14600: Current success : 260 \t All success : 643 \t mean(l0) : 11.567652 \t Current Learning Rate: 0.002\n",
            "Iteration 14700: Current success : 260 \t All success : 643 \t mean(l0) : 11.567652 \t Current Learning Rate: 0.002\n",
            "Iteration 14800: Current success : 260 \t All success : 643 \t mean(l0) : 11.567652 \t Current Learning Rate: 0.001\n",
            "Iteration 14900: Current success : 260 \t All success : 643 \t mean(l0) : 11.567652 \t Current Learning Rate: 0.001\n",
            "Iteration 15000: Current success : 260 \t All success : 643 \t mean(l0) : 11.567652 \t Current Learning Rate: 0.000\n",
            "Iteration 15100: Current success : 219 \t All success : 643 \t mean(l0) : 11.547434 \t Current Learning Rate: 0.012\n",
            "Iteration 15200: Current success : 269 \t All success : 645 \t mean(l0) : 11.655814 \t Current Learning Rate: 0.012\n",
            "Iteration 15300: Current success : 268 \t All success : 645 \t mean(l0) : 11.646512 \t Current Learning Rate: 0.012\n",
            "Iteration 15400: Current success : 270 \t All success : 645 \t mean(l0) : 11.643411 \t Current Learning Rate: 0.012\n",
            "Iteration 15500: Current success : 250 \t All success : 645 \t mean(l0) : 11.624806 \t Current Learning Rate: 0.011\n",
            "Iteration 15600: Current success : 215 \t All success : 645 \t mean(l0) : 11.612403 \t Current Learning Rate: 0.011\n",
            "Iteration 15700: Current success : 215 \t All success : 645 \t mean(l0) : 11.612403 \t Current Learning Rate: 0.011\n",
            "Iteration 15800: Current success : 213 \t All success : 645 \t mean(l0) : 11.612403 \t Current Learning Rate: 0.011\n",
            "Iteration 15900: Current success : 214 \t All success : 645 \t mean(l0) : 11.612403 \t Current Learning Rate: 0.010\n",
            "Iteration 16000: Current success : 215 \t All success : 645 \t mean(l0) : 11.582946 \t Current Learning Rate: 0.010\n",
            "Iteration 16100: Current success : 216 \t All success : 645 \t mean(l0) : 11.581395 \t Current Learning Rate: 0.010\n",
            "Iteration 16200: Current success : 214 \t All success : 645 \t mean(l0) : 11.575193 \t Current Learning Rate: 0.010\n",
            "Iteration 16300: Current success : 214 \t All success : 645 \t mean(l0) : 11.575193 \t Current Learning Rate: 0.009\n",
            "Iteration 16400: Current success : 214 \t All success : 645 \t mean(l0) : 11.575193 \t Current Learning Rate: 0.009\n",
            "Iteration 16500: Current success : 215 \t All success : 645 \t mean(l0) : 11.573644 \t Current Learning Rate: 0.009\n",
            "Iteration 16600: Current success : 216 \t All success : 645 \t mean(l0) : 11.573644 \t Current Learning Rate: 0.009\n",
            "Iteration 16700: Current success : 215 \t All success : 645 \t mean(l0) : 11.572093 \t Current Learning Rate: 0.008\n",
            "Iteration 16800: Current success : 215 \t All success : 645 \t mean(l0) : 11.572093 \t Current Learning Rate: 0.008\n",
            "Iteration 16900: Current success : 216 \t All success : 645 \t mean(l0) : 11.570542 \t Current Learning Rate: 0.008\n",
            "Iteration 17000: Current success : 215 \t All success : 645 \t mean(l0) : 11.570542 \t Current Learning Rate: 0.008\n",
            "Iteration 17100: Current success : 215 \t All success : 645 \t mean(l0) : 11.567442 \t Current Learning Rate: 0.007\n",
            "Iteration 17200: Current success : 215 \t All success : 645 \t mean(l0) : 11.567442 \t Current Learning Rate: 0.007\n",
            "Iteration 17300: Current success : 216 \t All success : 645 \t mean(l0) : 11.567442 \t Current Learning Rate: 0.007\n",
            "Iteration 17400: Current success : 215 \t All success : 645 \t mean(l0) : 11.567442 \t Current Learning Rate: 0.007\n",
            "Iteration 17500: Current success : 215 \t All success : 645 \t mean(l0) : 11.567442 \t Current Learning Rate: 0.006\n",
            "Iteration 17600: Current success : 215 \t All success : 645 \t mean(l0) : 11.567442 \t Current Learning Rate: 0.006\n",
            "Iteration 17700: Current success : 215 \t All success : 645 \t mean(l0) : 11.567442 \t Current Learning Rate: 0.006\n",
            "Iteration 17800: Current success : 215 \t All success : 645 \t mean(l0) : 11.567442 \t Current Learning Rate: 0.006\n",
            "Iteration 17900: Current success : 215 \t All success : 645 \t mean(l0) : 11.567442 \t Current Learning Rate: 0.005\n",
            "Iteration 18000: Current success : 256 \t All success : 645 \t mean(l0) : 11.567442 \t Current Learning Rate: 0.005\n",
            "Iteration 18100: Current success : 268 \t All success : 645 \t mean(l0) : 11.567442 \t Current Learning Rate: 0.005\n",
            "Iteration 18200: Current success : 268 \t All success : 645 \t mean(l0) : 11.567442 \t Current Learning Rate: 0.005\n",
            "Iteration 18300: Current success : 268 \t All success : 645 \t mean(l0) : 11.564341 \t Current Learning Rate: 0.004\n",
            "Iteration 18400: Current success : 268 \t All success : 645 \t mean(l0) : 11.564341 \t Current Learning Rate: 0.004\n",
            "Iteration 18500: Current success : 268 \t All success : 645 \t mean(l0) : 11.564341 \t Current Learning Rate: 0.004\n",
            "Iteration 18600: Current success : 268 \t All success : 645 \t mean(l0) : 11.564341 \t Current Learning Rate: 0.004\n",
            "Iteration 18700: Current success : 267 \t All success : 645 \t mean(l0) : 11.564341 \t Current Learning Rate: 0.003\n",
            "Iteration 18800: Current success : 267 \t All success : 645 \t mean(l0) : 11.564341 \t Current Learning Rate: 0.003\n",
            "Iteration 18900: Current success : 267 \t All success : 645 \t mean(l0) : 11.564341 \t Current Learning Rate: 0.003\n",
            "Iteration 19000: Current success : 267 \t All success : 645 \t mean(l0) : 11.564341 \t Current Learning Rate: 0.003\n",
            "Iteration 19100: Current success : 267 \t All success : 645 \t mean(l0) : 11.564341 \t Current Learning Rate: 0.002\n",
            "Iteration 19200: Current success : 267 \t All success : 645 \t mean(l0) : 11.564341 \t Current Learning Rate: 0.002\n",
            "Iteration 19300: Current success : 267 \t All success : 645 \t mean(l0) : 11.564341 \t Current Learning Rate: 0.002\n",
            "Iteration 19400: Current success : 267 \t All success : 645 \t mean(l0) : 11.564341 \t Current Learning Rate: 0.002\n",
            "Iteration 19500: Current success : 267 \t All success : 645 \t mean(l0) : 11.564341 \t Current Learning Rate: 0.001\n",
            "Iteration 19600: Current success : 267 \t All success : 645 \t mean(l0) : 11.561240 \t Current Learning Rate: 0.001\n",
            "Iteration 19700: Current success : 267 \t All success : 645 \t mean(l0) : 11.561240 \t Current Learning Rate: 0.001\n",
            "Iteration 19800: Current success : 267 \t All success : 645 \t mean(l0) : 11.561240 \t Current Learning Rate: 0.001\n",
            "Iteration 19900: Current success : 267 \t All success : 645 \t mean(l0) : 11.561240 \t Current Learning Rate: 0.000\n",
            "Iteration 20000: Current success : 267 \t All success : 645 \t mean(l0) : 11.561240 \t Current Learning Rate: 0.000\n",
            "outer_step 4: all success : 968 \t mean(l0)(success) : 12.8110\n",
            "-------------------------------------------------------------\n",
            "tensor([6.2500e-02, 1.8750e-01, 6.8750e-01,  ..., 4.3750e-01, 7.7500e+01,\n",
            "        4.3750e+00], device='cuda:0')\n",
            "Iteration 100: Current success : 888 \t All success : 894 \t mean(l0) : 9.507830 \t Current Learning Rate: 0.098\n",
            "Iteration 200: Current success : 889 \t All success : 905 \t mean(l0) : 9.871822 \t Current Learning Rate: 0.096\n",
            "Iteration 300: Current success : 895 \t All success : 911 \t mean(l0) : 10.216246 \t Current Learning Rate: 0.094\n",
            "Iteration 400: Current success : 898 \t All success : 918 \t mean(l0) : 10.494554 \t Current Learning Rate: 0.092\n",
            "Iteration 500: Current success : 902 \t All success : 922 \t mean(l0) : 10.673536 \t Current Learning Rate: 0.090\n",
            "Iteration 600: Current success : 921 \t All success : 926 \t mean(l0) : 10.886609 \t Current Learning Rate: 0.088\n",
            "Iteration 700: Current success : 916 \t All success : 927 \t mean(l0) : 10.932038 \t Current Learning Rate: 0.086\n",
            "Iteration 800: Current success : 922 \t All success : 929 \t mean(l0) : 11.022605 \t Current Learning Rate: 0.084\n",
            "Iteration 900: Current success : 925 \t All success : 929 \t mean(l0) : 11.020452 \t Current Learning Rate: 0.082\n",
            "Iteration 1000: Current success : 925 \t All success : 930 \t mean(l0) : 11.072043 \t Current Learning Rate: 0.080\n",
            "Iteration 1100: Current success : 926 \t All success : 930 \t mean(l0) : 11.072043 \t Current Learning Rate: 0.078\n",
            "Iteration 1200: Current success : 926 \t All success : 931 \t mean(l0) : 11.125671 \t Current Learning Rate: 0.076\n",
            "Iteration 1300: Current success : 928 \t All success : 931 \t mean(l0) : 11.123524 \t Current Learning Rate: 0.074\n",
            "Iteration 1400: Current success : 928 \t All success : 931 \t mean(l0) : 11.123524 \t Current Learning Rate: 0.072\n",
            "Iteration 1500: Current success : 927 \t All success : 931 \t mean(l0) : 11.123524 \t Current Learning Rate: 0.070\n",
            "Iteration 1600: Current success : 928 \t All success : 931 \t mean(l0) : 11.123524 \t Current Learning Rate: 0.068\n",
            "Iteration 1700: Current success : 929 \t All success : 931 \t mean(l0) : 11.123524 \t Current Learning Rate: 0.066\n",
            "Iteration 1800: Current success : 928 \t All success : 931 \t mean(l0) : 11.123524 \t Current Learning Rate: 0.064\n",
            "Iteration 1900: Current success : 927 \t All success : 931 \t mean(l0) : 11.123524 \t Current Learning Rate: 0.062\n",
            "Iteration 2000: Current success : 927 \t All success : 931 \t mean(l0) : 11.123524 \t Current Learning Rate: 0.060\n",
            "Iteration 2100: Current success : 929 \t All success : 931 \t mean(l0) : 11.123524 \t Current Learning Rate: 0.058\n",
            "Iteration 2200: Current success : 927 \t All success : 931 \t mean(l0) : 11.123524 \t Current Learning Rate: 0.056\n",
            "Iteration 2300: Current success : 927 \t All success : 931 \t mean(l0) : 11.123524 \t Current Learning Rate: 0.054\n",
            "Iteration 2400: Current success : 927 \t All success : 931 \t mean(l0) : 11.123524 \t Current Learning Rate: 0.052\n",
            "Iteration 2500: Current success : 928 \t All success : 931 \t mean(l0) : 11.123524 \t Current Learning Rate: 0.050\n",
            "Iteration 2600: Current success : 927 \t All success : 931 \t mean(l0) : 11.123524 \t Current Learning Rate: 0.048\n",
            "Iteration 2700: Current success : 927 \t All success : 931 \t mean(l0) : 11.123524 \t Current Learning Rate: 0.046\n",
            "Iteration 2800: Current success : 928 \t All success : 931 \t mean(l0) : 11.123524 \t Current Learning Rate: 0.044\n",
            "Iteration 2900: Current success : 927 \t All success : 931 \t mean(l0) : 11.123524 \t Current Learning Rate: 0.042\n",
            "Iteration 3000: Current success : 927 \t All success : 931 \t mean(l0) : 11.123524 \t Current Learning Rate: 0.040\n",
            "Iteration 3100: Current success : 927 \t All success : 931 \t mean(l0) : 11.123524 \t Current Learning Rate: 0.038\n",
            "Iteration 3200: Current success : 927 \t All success : 931 \t mean(l0) : 11.123524 \t Current Learning Rate: 0.036\n",
            "Iteration 3300: Current success : 927 \t All success : 931 \t mean(l0) : 11.123524 \t Current Learning Rate: 0.034\n",
            "Iteration 3400: Current success : 927 \t All success : 931 \t mean(l0) : 11.123524 \t Current Learning Rate: 0.032\n",
            "Iteration 3500: Current success : 928 \t All success : 931 \t mean(l0) : 11.123524 \t Current Learning Rate: 0.030\n",
            "Iteration 3600: Current success : 928 \t All success : 931 \t mean(l0) : 11.123524 \t Current Learning Rate: 0.028\n",
            "Iteration 3700: Current success : 927 \t All success : 931 \t mean(l0) : 11.123524 \t Current Learning Rate: 0.026\n",
            "Iteration 3800: Current success : 927 \t All success : 931 \t mean(l0) : 11.123524 \t Current Learning Rate: 0.024\n",
            "Iteration 3900: Current success : 927 \t All success : 931 \t mean(l0) : 11.123524 \t Current Learning Rate: 0.022\n",
            "Iteration 4000: Current success : 927 \t All success : 931 \t mean(l0) : 11.123524 \t Current Learning Rate: 0.020\n",
            "Iteration 4100: Current success : 927 \t All success : 931 \t mean(l0) : 11.123524 \t Current Learning Rate: 0.018\n",
            "Iteration 4200: Current success : 927 \t All success : 931 \t mean(l0) : 11.123524 \t Current Learning Rate: 0.016\n",
            "Iteration 4300: Current success : 927 \t All success : 931 \t mean(l0) : 11.123524 \t Current Learning Rate: 0.014\n",
            "Iteration 4400: Current success : 927 \t All success : 931 \t mean(l0) : 11.123524 \t Current Learning Rate: 0.012\n",
            "Iteration 4500: Current success : 927 \t All success : 931 \t mean(l0) : 11.123524 \t Current Learning Rate: 0.010\n",
            "Iteration 4600: Current success : 927 \t All success : 931 \t mean(l0) : 11.123524 \t Current Learning Rate: 0.008\n",
            "Iteration 4700: Current success : 927 \t All success : 931 \t mean(l0) : 11.123524 \t Current Learning Rate: 0.006\n",
            "Iteration 4800: Current success : 927 \t All success : 931 \t mean(l0) : 11.123524 \t Current Learning Rate: 0.004\n",
            "Iteration 4900: Current success : 927 \t All success : 931 \t mean(l0) : 11.123524 \t Current Learning Rate: 0.002\n",
            "Iteration 5000: Current success : 927 \t All success : 931 \t mean(l0) : 11.123524 \t Current Learning Rate: 0.000\n",
            "Iteration 5100: Current success : 929 \t All success : 934 \t mean(l0) : 11.267666 \t Current Learning Rate: 0.049\n",
            "Iteration 5200: Current success : 929 \t All success : 935 \t mean(l0) : 11.341177 \t Current Learning Rate: 0.048\n",
            "Iteration 5300: Current success : 930 \t All success : 935 \t mean(l0) : 11.341177 \t Current Learning Rate: 0.047\n",
            "Iteration 5400: Current success : 934 \t All success : 939 \t mean(l0) : 11.589989 \t Current Learning Rate: 0.046\n",
            "Iteration 5500: Current success : 934 \t All success : 939 \t mean(l0) : 11.583599 \t Current Learning Rate: 0.045\n",
            "Iteration 5600: Current success : 934 \t All success : 939 \t mean(l0) : 11.583599 \t Current Learning Rate: 0.044\n",
            "Iteration 5700: Current success : 934 \t All success : 939 \t mean(l0) : 11.583599 \t Current Learning Rate: 0.043\n",
            "Iteration 5800: Current success : 934 \t All success : 939 \t mean(l0) : 11.583599 \t Current Learning Rate: 0.042\n",
            "Iteration 5900: Current success : 934 \t All success : 939 \t mean(l0) : 11.583599 \t Current Learning Rate: 0.041\n",
            "Iteration 6000: Current success : 934 \t All success : 939 \t mean(l0) : 11.583599 \t Current Learning Rate: 0.040\n",
            "Iteration 6100: Current success : 934 \t All success : 939 \t mean(l0) : 11.583599 \t Current Learning Rate: 0.039\n",
            "Iteration 6200: Current success : 934 \t All success : 939 \t mean(l0) : 11.583599 \t Current Learning Rate: 0.038\n",
            "Iteration 6300: Current success : 934 \t All success : 939 \t mean(l0) : 11.583599 \t Current Learning Rate: 0.037\n",
            "Iteration 6400: Current success : 934 \t All success : 939 \t mean(l0) : 11.583599 \t Current Learning Rate: 0.036\n",
            "Iteration 6500: Current success : 934 \t All success : 939 \t mean(l0) : 11.583599 \t Current Learning Rate: 0.035\n",
            "Iteration 6600: Current success : 934 \t All success : 939 \t mean(l0) : 11.583599 \t Current Learning Rate: 0.034\n",
            "Iteration 6700: Current success : 934 \t All success : 939 \t mean(l0) : 11.583599 \t Current Learning Rate: 0.033\n",
            "Iteration 6800: Current success : 934 \t All success : 939 \t mean(l0) : 11.583599 \t Current Learning Rate: 0.032\n",
            "Iteration 6900: Current success : 934 \t All success : 939 \t mean(l0) : 11.583599 \t Current Learning Rate: 0.031\n",
            "Iteration 7000: Current success : 934 \t All success : 939 \t mean(l0) : 11.583599 \t Current Learning Rate: 0.030\n",
            "Iteration 7100: Current success : 934 \t All success : 939 \t mean(l0) : 11.583599 \t Current Learning Rate: 0.029\n",
            "Iteration 7200: Current success : 934 \t All success : 939 \t mean(l0) : 11.583599 \t Current Learning Rate: 0.028\n",
            "Iteration 7300: Current success : 934 \t All success : 939 \t mean(l0) : 11.583599 \t Current Learning Rate: 0.027\n",
            "Iteration 7400: Current success : 934 \t All success : 939 \t mean(l0) : 11.583599 \t Current Learning Rate: 0.026\n",
            "Iteration 7500: Current success : 934 \t All success : 939 \t mean(l0) : 11.583599 \t Current Learning Rate: 0.025\n",
            "Iteration 7600: Current success : 934 \t All success : 939 \t mean(l0) : 11.583599 \t Current Learning Rate: 0.024\n",
            "Iteration 7700: Current success : 934 \t All success : 939 \t mean(l0) : 11.583599 \t Current Learning Rate: 0.023\n",
            "Iteration 7800: Current success : 934 \t All success : 939 \t mean(l0) : 11.583599 \t Current Learning Rate: 0.022\n",
            "Iteration 7900: Current success : 934 \t All success : 939 \t mean(l0) : 11.583599 \t Current Learning Rate: 0.021\n",
            "Iteration 8000: Current success : 934 \t All success : 939 \t mean(l0) : 11.583599 \t Current Learning Rate: 0.020\n",
            "Iteration 8100: Current success : 934 \t All success : 939 \t mean(l0) : 11.583599 \t Current Learning Rate: 0.019\n",
            "Iteration 8200: Current success : 934 \t All success : 939 \t mean(l0) : 11.583599 \t Current Learning Rate: 0.018\n",
            "Iteration 8300: Current success : 934 \t All success : 939 \t mean(l0) : 11.583599 \t Current Learning Rate: 0.017\n",
            "Iteration 8400: Current success : 934 \t All success : 939 \t mean(l0) : 11.583599 \t Current Learning Rate: 0.016\n",
            "Iteration 8500: Current success : 934 \t All success : 939 \t mean(l0) : 11.583599 \t Current Learning Rate: 0.015\n",
            "Iteration 8600: Current success : 934 \t All success : 939 \t mean(l0) : 11.583599 \t Current Learning Rate: 0.014\n",
            "Iteration 8700: Current success : 934 \t All success : 939 \t mean(l0) : 11.583599 \t Current Learning Rate: 0.013\n",
            "Iteration 8800: Current success : 934 \t All success : 939 \t mean(l0) : 11.583599 \t Current Learning Rate: 0.012\n",
            "Iteration 8900: Current success : 934 \t All success : 939 \t mean(l0) : 11.583599 \t Current Learning Rate: 0.011\n",
            "Iteration 9000: Current success : 934 \t All success : 939 \t mean(l0) : 11.583599 \t Current Learning Rate: 0.010\n",
            "Iteration 9100: Current success : 934 \t All success : 939 \t mean(l0) : 11.583599 \t Current Learning Rate: 0.009\n",
            "Iteration 9200: Current success : 934 \t All success : 939 \t mean(l0) : 11.583599 \t Current Learning Rate: 0.008\n",
            "Iteration 9300: Current success : 934 \t All success : 939 \t mean(l0) : 11.583599 \t Current Learning Rate: 0.007\n",
            "Iteration 9400: Current success : 934 \t All success : 939 \t mean(l0) : 11.583599 \t Current Learning Rate: 0.006\n",
            "Iteration 9500: Current success : 934 \t All success : 939 \t mean(l0) : 11.583599 \t Current Learning Rate: 0.005\n",
            "Iteration 9600: Current success : 934 \t All success : 939 \t mean(l0) : 11.583599 \t Current Learning Rate: 0.004\n",
            "Iteration 9700: Current success : 934 \t All success : 939 \t mean(l0) : 11.583599 \t Current Learning Rate: 0.003\n",
            "Iteration 9800: Current success : 934 \t All success : 939 \t mean(l0) : 11.583599 \t Current Learning Rate: 0.002\n",
            "Iteration 9900: Current success : 934 \t All success : 939 \t mean(l0) : 11.583599 \t Current Learning Rate: 0.001\n",
            "Iteration 10000: Current success : 934 \t All success : 939 \t mean(l0) : 11.583599 \t Current Learning Rate: 0.000\n",
            "Iteration 10100: Current success : 936 \t All success : 941 \t mean(l0) : 11.763018 \t Current Learning Rate: 0.025\n",
            "Iteration 10200: Current success : 937 \t All success : 942 \t mean(l0) : 11.820595 \t Current Learning Rate: 0.024\n",
            "Iteration 10300: Current success : 937 \t All success : 942 \t mean(l0) : 11.820595 \t Current Learning Rate: 0.024\n",
            "Iteration 10400: Current success : 935 \t All success : 942 \t mean(l0) : 11.820595 \t Current Learning Rate: 0.023\n",
            "Iteration 10500: Current success : 935 \t All success : 942 \t mean(l0) : 11.820595 \t Current Learning Rate: 0.023\n",
            "Iteration 10600: Current success : 935 \t All success : 942 \t mean(l0) : 11.820595 \t Current Learning Rate: 0.022\n",
            "Iteration 10700: Current success : 935 \t All success : 942 \t mean(l0) : 11.820595 \t Current Learning Rate: 0.022\n",
            "Iteration 10800: Current success : 935 \t All success : 942 \t mean(l0) : 11.820595 \t Current Learning Rate: 0.021\n",
            "Iteration 10900: Current success : 935 \t All success : 942 \t mean(l0) : 11.820595 \t Current Learning Rate: 0.021\n",
            "Iteration 11000: Current success : 935 \t All success : 942 \t mean(l0) : 11.820595 \t Current Learning Rate: 0.020\n",
            "Iteration 11100: Current success : 935 \t All success : 942 \t mean(l0) : 11.820595 \t Current Learning Rate: 0.020\n",
            "Iteration 11200: Current success : 935 \t All success : 942 \t mean(l0) : 11.820595 \t Current Learning Rate: 0.019\n",
            "Iteration 11300: Current success : 935 \t All success : 942 \t mean(l0) : 11.820595 \t Current Learning Rate: 0.019\n",
            "Iteration 11400: Current success : 937 \t All success : 942 \t mean(l0) : 11.820595 \t Current Learning Rate: 0.018\n",
            "Iteration 11500: Current success : 937 \t All success : 942 \t mean(l0) : 11.820595 \t Current Learning Rate: 0.018\n",
            "Iteration 11600: Current success : 937 \t All success : 942 \t mean(l0) : 11.820595 \t Current Learning Rate: 0.017\n",
            "Iteration 11700: Current success : 937 \t All success : 942 \t mean(l0) : 11.820595 \t Current Learning Rate: 0.017\n",
            "Iteration 11800: Current success : 937 \t All success : 942 \t mean(l0) : 11.820595 \t Current Learning Rate: 0.016\n",
            "Iteration 11900: Current success : 937 \t All success : 942 \t mean(l0) : 11.820595 \t Current Learning Rate: 0.016\n",
            "Iteration 12000: Current success : 937 \t All success : 942 \t mean(l0) : 11.820595 \t Current Learning Rate: 0.015\n",
            "Iteration 12100: Current success : 937 \t All success : 942 \t mean(l0) : 11.820595 \t Current Learning Rate: 0.015\n",
            "Iteration 12200: Current success : 937 \t All success : 942 \t mean(l0) : 11.820595 \t Current Learning Rate: 0.014\n",
            "Iteration 12300: Current success : 937 \t All success : 942 \t mean(l0) : 11.820595 \t Current Learning Rate: 0.014\n",
            "Iteration 12400: Current success : 938 \t All success : 943 \t mean(l0) : 11.873808 \t Current Learning Rate: 0.013\n",
            "Iteration 12500: Current success : 937 \t All success : 943 \t mean(l0) : 11.873808 \t Current Learning Rate: 0.013\n",
            "Iteration 12600: Current success : 937 \t All success : 943 \t mean(l0) : 11.873808 \t Current Learning Rate: 0.012\n",
            "Iteration 12700: Current success : 937 \t All success : 943 \t mean(l0) : 11.873808 \t Current Learning Rate: 0.012\n",
            "Iteration 12800: Current success : 937 \t All success : 943 \t mean(l0) : 11.873808 \t Current Learning Rate: 0.011\n",
            "Iteration 12900: Current success : 937 \t All success : 943 \t mean(l0) : 11.873808 \t Current Learning Rate: 0.011\n",
            "Iteration 13000: Current success : 937 \t All success : 943 \t mean(l0) : 11.873808 \t Current Learning Rate: 0.010\n",
            "Iteration 13100: Current success : 937 \t All success : 943 \t mean(l0) : 11.873808 \t Current Learning Rate: 0.010\n",
            "Iteration 13200: Current success : 937 \t All success : 943 \t mean(l0) : 11.873808 \t Current Learning Rate: 0.009\n",
            "Iteration 13300: Current success : 937 \t All success : 943 \t mean(l0) : 11.873808 \t Current Learning Rate: 0.009\n",
            "Iteration 13400: Current success : 937 \t All success : 943 \t mean(l0) : 11.873808 \t Current Learning Rate: 0.008\n",
            "Iteration 13500: Current success : 937 \t All success : 943 \t mean(l0) : 11.873808 \t Current Learning Rate: 0.008\n",
            "Iteration 13600: Current success : 937 \t All success : 943 \t mean(l0) : 11.873808 \t Current Learning Rate: 0.007\n",
            "Iteration 13700: Current success : 937 \t All success : 943 \t mean(l0) : 11.873808 \t Current Learning Rate: 0.007\n",
            "Iteration 13800: Current success : 937 \t All success : 943 \t mean(l0) : 11.873808 \t Current Learning Rate: 0.006\n",
            "Iteration 13900: Current success : 937 \t All success : 943 \t mean(l0) : 11.873808 \t Current Learning Rate: 0.006\n",
            "Iteration 14000: Current success : 937 \t All success : 943 \t mean(l0) : 11.873808 \t Current Learning Rate: 0.005\n",
            "Iteration 14100: Current success : 937 \t All success : 943 \t mean(l0) : 11.873808 \t Current Learning Rate: 0.005\n",
            "Iteration 14200: Current success : 937 \t All success : 943 \t mean(l0) : 11.873808 \t Current Learning Rate: 0.004\n",
            "Iteration 14300: Current success : 937 \t All success : 943 \t mean(l0) : 11.873808 \t Current Learning Rate: 0.004\n",
            "Iteration 14400: Current success : 937 \t All success : 943 \t mean(l0) : 11.873808 \t Current Learning Rate: 0.003\n",
            "Iteration 14500: Current success : 937 \t All success : 943 \t mean(l0) : 11.873808 \t Current Learning Rate: 0.003\n",
            "Iteration 14600: Current success : 937 \t All success : 943 \t mean(l0) : 11.873808 \t Current Learning Rate: 0.002\n",
            "Iteration 14700: Current success : 937 \t All success : 943 \t mean(l0) : 11.873808 \t Current Learning Rate: 0.002\n",
            "Iteration 14800: Current success : 937 \t All success : 943 \t mean(l0) : 11.873808 \t Current Learning Rate: 0.001\n",
            "Iteration 14900: Current success : 937 \t All success : 943 \t mean(l0) : 11.873808 \t Current Learning Rate: 0.001\n",
            "Iteration 15000: Current success : 937 \t All success : 943 \t mean(l0) : 11.873808 \t Current Learning Rate: 0.000\n",
            "Iteration 15100: Current success : 935 \t All success : 943 \t mean(l0) : 11.873808 \t Current Learning Rate: 0.012\n",
            "Iteration 15200: Current success : 935 \t All success : 943 \t mean(l0) : 11.873808 \t Current Learning Rate: 0.012\n",
            "Iteration 15300: Current success : 937 \t All success : 943 \t mean(l0) : 11.873808 \t Current Learning Rate: 0.012\n",
            "Iteration 15400: Current success : 937 \t All success : 943 \t mean(l0) : 11.873808 \t Current Learning Rate: 0.012\n",
            "Iteration 15500: Current success : 937 \t All success : 943 \t mean(l0) : 11.873808 \t Current Learning Rate: 0.011\n",
            "Iteration 15600: Current success : 938 \t All success : 944 \t mean(l0) : 11.925847 \t Current Learning Rate: 0.011\n",
            "Iteration 15700: Current success : 938 \t All success : 944 \t mean(l0) : 11.925847 \t Current Learning Rate: 0.011\n",
            "Iteration 15800: Current success : 938 \t All success : 944 \t mean(l0) : 11.925847 \t Current Learning Rate: 0.011\n",
            "Iteration 15900: Current success : 938 \t All success : 944 \t mean(l0) : 11.925847 \t Current Learning Rate: 0.010\n",
            "Iteration 16000: Current success : 938 \t All success : 944 \t mean(l0) : 11.925847 \t Current Learning Rate: 0.010\n",
            "Iteration 16100: Current success : 938 \t All success : 944 \t mean(l0) : 11.925847 \t Current Learning Rate: 0.010\n",
            "Iteration 16200: Current success : 938 \t All success : 944 \t mean(l0) : 11.925847 \t Current Learning Rate: 0.010\n",
            "Iteration 16300: Current success : 938 \t All success : 944 \t mean(l0) : 11.925847 \t Current Learning Rate: 0.009\n",
            "Iteration 16400: Current success : 938 \t All success : 944 \t mean(l0) : 11.925847 \t Current Learning Rate: 0.009\n",
            "Iteration 16500: Current success : 938 \t All success : 944 \t mean(l0) : 11.925847 \t Current Learning Rate: 0.009\n",
            "Iteration 16600: Current success : 938 \t All success : 944 \t mean(l0) : 11.925847 \t Current Learning Rate: 0.009\n",
            "Iteration 16700: Current success : 938 \t All success : 944 \t mean(l0) : 11.925847 \t Current Learning Rate: 0.008\n",
            "Iteration 16800: Current success : 938 \t All success : 944 \t mean(l0) : 11.925847 \t Current Learning Rate: 0.008\n",
            "Iteration 16900: Current success : 938 \t All success : 944 \t mean(l0) : 11.925847 \t Current Learning Rate: 0.008\n",
            "Iteration 17000: Current success : 938 \t All success : 944 \t mean(l0) : 11.925847 \t Current Learning Rate: 0.008\n",
            "Iteration 17100: Current success : 938 \t All success : 944 \t mean(l0) : 11.925847 \t Current Learning Rate: 0.007\n",
            "Iteration 17200: Current success : 938 \t All success : 944 \t mean(l0) : 11.925847 \t Current Learning Rate: 0.007\n",
            "Iteration 17300: Current success : 938 \t All success : 944 \t mean(l0) : 11.925847 \t Current Learning Rate: 0.007\n",
            "Iteration 17400: Current success : 938 \t All success : 944 \t mean(l0) : 11.925847 \t Current Learning Rate: 0.007\n",
            "Iteration 17500: Current success : 938 \t All success : 944 \t mean(l0) : 11.925847 \t Current Learning Rate: 0.006\n",
            "Iteration 17600: Current success : 938 \t All success : 944 \t mean(l0) : 11.925847 \t Current Learning Rate: 0.006\n",
            "Iteration 17700: Current success : 938 \t All success : 944 \t mean(l0) : 11.925847 \t Current Learning Rate: 0.006\n",
            "Iteration 17800: Current success : 938 \t All success : 944 \t mean(l0) : 11.925847 \t Current Learning Rate: 0.006\n",
            "Iteration 17900: Current success : 938 \t All success : 944 \t mean(l0) : 11.925847 \t Current Learning Rate: 0.005\n",
            "Iteration 18000: Current success : 938 \t All success : 944 \t mean(l0) : 11.925847 \t Current Learning Rate: 0.005\n",
            "Iteration 18100: Current success : 938 \t All success : 944 \t mean(l0) : 11.925847 \t Current Learning Rate: 0.005\n",
            "Iteration 18200: Current success : 938 \t All success : 944 \t mean(l0) : 11.925847 \t Current Learning Rate: 0.005\n",
            "Iteration 18300: Current success : 938 \t All success : 944 \t mean(l0) : 11.925847 \t Current Learning Rate: 0.004\n",
            "Iteration 18400: Current success : 938 \t All success : 944 \t mean(l0) : 11.925847 \t Current Learning Rate: 0.004\n",
            "Iteration 18500: Current success : 938 \t All success : 944 \t mean(l0) : 11.925847 \t Current Learning Rate: 0.004\n",
            "Iteration 18600: Current success : 938 \t All success : 944 \t mean(l0) : 11.925847 \t Current Learning Rate: 0.004\n",
            "Iteration 18700: Current success : 938 \t All success : 944 \t mean(l0) : 11.925847 \t Current Learning Rate: 0.003\n",
            "Iteration 18800: Current success : 938 \t All success : 944 \t mean(l0) : 11.925847 \t Current Learning Rate: 0.003\n",
            "Iteration 18900: Current success : 938 \t All success : 944 \t mean(l0) : 11.925847 \t Current Learning Rate: 0.003\n",
            "Iteration 19000: Current success : 938 \t All success : 944 \t mean(l0) : 11.925847 \t Current Learning Rate: 0.003\n",
            "Iteration 19100: Current success : 938 \t All success : 944 \t mean(l0) : 11.925847 \t Current Learning Rate: 0.002\n",
            "Iteration 19200: Current success : 938 \t All success : 944 \t mean(l0) : 11.925847 \t Current Learning Rate: 0.002\n",
            "Iteration 19300: Current success : 938 \t All success : 944 \t mean(l0) : 11.925847 \t Current Learning Rate: 0.002\n",
            "Iteration 19400: Current success : 938 \t All success : 944 \t mean(l0) : 11.925847 \t Current Learning Rate: 0.002\n",
            "Iteration 19500: Current success : 938 \t All success : 944 \t mean(l0) : 11.925847 \t Current Learning Rate: 0.001\n",
            "Iteration 19600: Current success : 938 \t All success : 944 \t mean(l0) : 11.925847 \t Current Learning Rate: 0.001\n",
            "Iteration 19700: Current success : 938 \t All success : 944 \t mean(l0) : 11.925847 \t Current Learning Rate: 0.001\n",
            "Iteration 19800: Current success : 938 \t All success : 944 \t mean(l0) : 11.925847 \t Current Learning Rate: 0.001\n",
            "Iteration 19900: Current success : 938 \t All success : 944 \t mean(l0) : 11.925847 \t Current Learning Rate: 0.000\n",
            "Iteration 20000: Current success : 938 \t All success : 944 \t mean(l0) : 11.925847 \t Current Learning Rate: 0.000\n",
            "1130\n",
            "977\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 13.54%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation on rounded adversarial examples\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model_AT_rFGSM.eval()\n",
        "\n",
        "# Apply the final rounding solution to adversarial examples\n",
        "adv_rounded = final_rounding(mal_x_batch, adv, model_AT_rFGSM,removal_array)\n",
        "\n",
        "# Get the model's predictions on the rounded adversarial examples\n",
        "outputs = model_AT_rFGSM(adv_rounded)\n",
        "y_pred = outputs.argmax(dim=1)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "num_malwares = len(mal_y_batch)\n",
        "num_successful_attacks = (y_pred == 0).sum().item()\n",
        "total_difference = (torch.abs(adv_rounded[y_pred == 0] - mal_x_batch[y_pred == 0])).sum().item()\n",
        "accuracy = (y_pred == 1).sum().item() / num_malwares * 100\n",
        "\n",
        "# Display the results with clear formatting\n",
        "print(f\"Evaluation Results on Rounded Adversarial Examples\")\n",
        "print(f\"-----------------------------------------------\")\n",
        "print(f\"Number of malware samples: {num_malwares}\")\n",
        "print(f\"Number of successful attacks: {num_successful_attacks}\")\n",
        "print(f\"Total L1 norm difference (successful adversarial examples vs. original): {total_difference}\")\n",
        "print(f\"Accuracy of the model on malware under attack: {accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7465bd2d-7cb2-4a05-b108-7b27964829db",
        "id": "1aYwHgcRRKJq"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Results on Rounded Adversarial Examples\n",
            "-----------------------------------------------\n",
            "Number of malware samples: 1130\n",
            "Number of successful attacks: 977\n",
            "Total L1 norm difference (successful adversarial examples vs. original): 13066.0\n",
            "Accuracy of the model on malware under attack: 13.54%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure that fixed elements are not changed\n",
        "\n",
        "# Ensure `removal_array` is expanded to match the batch size\n",
        "expanded_removal_array = removal_array.expand(mal_x_batch.shape[0], -1)\n",
        "\n",
        "# Create a mask where non-fixed features are set to 1\n",
        "non_fixed_features_mask = torch.bitwise_or(expanded_removal_array, 1 - mal_x_batch.to(torch.uint8))\n",
        "\n",
        "# Calculate indices where the mask is 0 (fixed) or 1 (non-fixed)\n",
        "mask_fixed = non_fixed_features_mask == 0\n",
        "mask_non_fixed = non_fixed_features_mask == 1\n",
        "\n",
        "# Calculate and print the differences\n",
        "diff_adv_fixed = (torch.abs(adv - mal_x_batch) * mask_fixed).sum().item()\n",
        "diff_adv_non_fixed = (torch.abs(adv - mal_x_batch) * mask_non_fixed).sum().item()\n",
        "diff_adv_rounded_fixed = (torch.abs(adv_rounded - mal_x_batch) * mask_fixed).sum().item()\n",
        "diff_adv_rounded_non_fixed = (torch.abs(adv_rounded - mal_x_batch) * mask_non_fixed).sum().item()\n",
        "\n",
        "# Display results with clear formatting\n",
        "print(f\"Differences when mask is 0 (fixed features):\")\n",
        "print(f\"  Adv vs. Original: {diff_adv_fixed}\")\n",
        "print(f\"  Rounded Adv vs. Original: {diff_adv_rounded_fixed}\")\n",
        "print(f\"Differences when mask is 1 (non-fixed features):\")\n",
        "print(f\"  Adv vs. Original: {diff_adv_non_fixed}\")\n",
        "print(f\"  Rounded Adv vs. Original: {diff_adv_rounded_non_fixed}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53e8519a-9e08-4525-bdb7-925c4cd9ca97",
        "id": "AcZik6LnRKJr"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Differences when mask is 0 (fixed features):\n",
            "  Adv vs. Original: 0.0\n",
            "  Rounded Adv vs. Original: 0.0\n",
            "Differences when mask is 1 (non-fixed features):\n",
            "  Adv vs. Original: 24532.40234375\n",
            "  Rounded Adv vs. Original: 25656.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming differences is already computed as in your code\n",
        "differences = (torch.abs(adv_rounded[y_pred == 0] - mal_x_batch[y_pred == 0])).sum(1)\n",
        "\n",
        "# Convert differences to a numpy array for plotting\n",
        "differences_np = differences.cpu().numpy()\n",
        "\n",
        "# Set the style of the plot\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# Create a histogram with Seaborn\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(differences_np, bins=16, kde=False, color='skyblue')\n",
        "#sns.histplot(differences_np, bins=30, kde=True, color='skyblue')\n",
        "\n",
        "# Set the title and labels\n",
        "plt.title('Abundance of Differences', fontsize=16)\n",
        "plt.xlabel('Sum of Absolute Differences', fontsize=14)\n",
        "plt.ylabel('Frequency', fontsize=14)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 576
        },
        "outputId": "e5ceddf8-55ec-4a54-c3b1-0ced7264a0b9",
        "id": "bXvajRdNRKJr"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# eps=1e2\n",
        "attack_params =  {'device':device, 'confidence':0.5, 'removal_array':removal_array,'max_iterations': 20000, 'max_learning_rate': 0.1, 'initial_const': 1. , 'binary_search_steps' : 5, 'cons_increase_factor' : 10}\n",
        "adv = adv_predict(test_loader, model_AT_rFGSM, CW_l1, **attack_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kOptRj2ANeYR",
        "outputId": "a9da4595-c5a7-4b91-bbb4-5c5052108cc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0')\n",
            "Iteration 100: Current success : 386 \t All success : 450 \t mean(l0) : 3.235556 \t Current Learning Rate: 0.098\n",
            "Iteration 200: Current success : 385 \t All success : 450 \t mean(l0) : 3.226667 \t Current Learning Rate: 0.096\n",
            "Iteration 300: Current success : 390 \t All success : 451 \t mean(l0) : 3.210643 \t Current Learning Rate: 0.094\n",
            "Iteration 400: Current success : 412 \t All success : 459 \t mean(l0) : 3.235294 \t Current Learning Rate: 0.092\n",
            "Iteration 500: Current success : 411 \t All success : 459 \t mean(l0) : 3.235294 \t Current Learning Rate: 0.090\n",
            "Iteration 600: Current success : 412 \t All success : 459 \t mean(l0) : 3.235294 \t Current Learning Rate: 0.088\n",
            "Iteration 700: Current success : 412 \t All success : 459 \t mean(l0) : 3.235294 \t Current Learning Rate: 0.086\n",
            "Iteration 800: Current success : 420 \t All success : 461 \t mean(l0) : 3.234273 \t Current Learning Rate: 0.084\n",
            "Iteration 900: Current success : 412 \t All success : 461 \t mean(l0) : 3.234273 \t Current Learning Rate: 0.082\n",
            "Iteration 1000: Current success : 412 \t All success : 461 \t mean(l0) : 3.234273 \t Current Learning Rate: 0.080\n",
            "Iteration 1100: Current success : 418 \t All success : 463 \t mean(l0) : 3.246220 \t Current Learning Rate: 0.078\n",
            "Iteration 1200: Current success : 416 \t All success : 464 \t mean(l0) : 3.234914 \t Current Learning Rate: 0.076\n",
            "Iteration 1300: Current success : 415 \t All success : 464 \t mean(l0) : 3.234914 \t Current Learning Rate: 0.074\n",
            "Iteration 1400: Current success : 415 \t All success : 464 \t mean(l0) : 3.234914 \t Current Learning Rate: 0.072\n",
            "Iteration 1500: Current success : 416 \t All success : 466 \t mean(l0) : 3.257511 \t Current Learning Rate: 0.070\n",
            "Iteration 1600: Current success : 415 \t All success : 466 \t mean(l0) : 3.257511 \t Current Learning Rate: 0.068\n",
            "Iteration 1700: Current success : 415 \t All success : 466 \t mean(l0) : 3.257511 \t Current Learning Rate: 0.066\n",
            "Iteration 1800: Current success : 415 \t All success : 466 \t mean(l0) : 3.257511 \t Current Learning Rate: 0.064\n",
            "Iteration 1900: Current success : 415 \t All success : 466 \t mean(l0) : 3.257511 \t Current Learning Rate: 0.062\n",
            "Iteration 2000: Current success : 415 \t All success : 466 \t mean(l0) : 3.240343 \t Current Learning Rate: 0.060\n",
            "Iteration 2100: Current success : 415 \t All success : 467 \t mean(l0) : 3.241970 \t Current Learning Rate: 0.058\n",
            "Iteration 2200: Current success : 416 \t All success : 467 \t mean(l0) : 3.241970 \t Current Learning Rate: 0.056\n",
            "Iteration 2300: Current success : 415 \t All success : 467 \t mean(l0) : 3.241970 \t Current Learning Rate: 0.054\n",
            "Iteration 2400: Current success : 415 \t All success : 467 \t mean(l0) : 3.241970 \t Current Learning Rate: 0.052\n",
            "Iteration 2500: Current success : 415 \t All success : 469 \t mean(l0) : 3.262260 \t Current Learning Rate: 0.050\n",
            "Iteration 2600: Current success : 415 \t All success : 469 \t mean(l0) : 3.262260 \t Current Learning Rate: 0.048\n",
            "Iteration 2700: Current success : 420 \t All success : 469 \t mean(l0) : 3.260128 \t Current Learning Rate: 0.046\n",
            "Iteration 2800: Current success : 424 \t All success : 469 \t mean(l0) : 3.260128 \t Current Learning Rate: 0.044\n",
            "Iteration 2900: Current success : 424 \t All success : 469 \t mean(l0) : 3.260128 \t Current Learning Rate: 0.042\n",
            "Iteration 3000: Current success : 424 \t All success : 469 \t mean(l0) : 3.260128 \t Current Learning Rate: 0.040\n",
            "Iteration 3100: Current success : 424 \t All success : 469 \t mean(l0) : 3.260128 \t Current Learning Rate: 0.038\n",
            "Iteration 3200: Current success : 424 \t All success : 469 \t mean(l0) : 3.260128 \t Current Learning Rate: 0.036\n",
            "Iteration 3300: Current success : 424 \t All success : 469 \t mean(l0) : 3.260128 \t Current Learning Rate: 0.034\n",
            "Iteration 3400: Current success : 424 \t All success : 469 \t mean(l0) : 3.260128 \t Current Learning Rate: 0.032\n",
            "Iteration 3500: Current success : 424 \t All success : 469 \t mean(l0) : 3.260128 \t Current Learning Rate: 0.030\n",
            "Iteration 3600: Current success : 424 \t All success : 469 \t mean(l0) : 3.260128 \t Current Learning Rate: 0.028\n",
            "Iteration 3700: Current success : 424 \t All success : 469 \t mean(l0) : 3.260128 \t Current Learning Rate: 0.026\n",
            "Iteration 3800: Current success : 424 \t All success : 469 \t mean(l0) : 3.260128 \t Current Learning Rate: 0.024\n",
            "Iteration 3900: Current success : 424 \t All success : 469 \t mean(l0) : 3.260128 \t Current Learning Rate: 0.022\n",
            "Iteration 4000: Current success : 424 \t All success : 469 \t mean(l0) : 3.260128 \t Current Learning Rate: 0.020\n",
            "Iteration 4100: Current success : 424 \t All success : 469 \t mean(l0) : 3.260128 \t Current Learning Rate: 0.018\n",
            "Iteration 4200: Current success : 424 \t All success : 469 \t mean(l0) : 3.260128 \t Current Learning Rate: 0.016\n",
            "Iteration 4300: Current success : 424 \t All success : 469 \t mean(l0) : 3.260128 \t Current Learning Rate: 0.014\n",
            "Iteration 4400: Current success : 424 \t All success : 469 \t mean(l0) : 3.260128 \t Current Learning Rate: 0.012\n",
            "Iteration 4500: Current success : 424 \t All success : 469 \t mean(l0) : 3.260128 \t Current Learning Rate: 0.010\n",
            "Iteration 4600: Current success : 424 \t All success : 469 \t mean(l0) : 3.260128 \t Current Learning Rate: 0.008\n",
            "Iteration 4700: Current success : 424 \t All success : 469 \t mean(l0) : 3.260128 \t Current Learning Rate: 0.006\n",
            "Iteration 4800: Current success : 424 \t All success : 469 \t mean(l0) : 3.260128 \t Current Learning Rate: 0.004\n",
            "Iteration 4900: Current success : 424 \t All success : 469 \t mean(l0) : 3.260128 \t Current Learning Rate: 0.002\n",
            "Iteration 5000: Current success : 424 \t All success : 469 \t mean(l0) : 3.260128 \t Current Learning Rate: 0.000\n",
            "Iteration 5100: Current success : 679 \t All success : 680 \t mean(l0) : 5.617647 \t Current Learning Rate: 0.049\n",
            "Iteration 5200: Current success : 681 \t All success : 682 \t mean(l0) : 5.602639 \t Current Learning Rate: 0.048\n",
            "Iteration 5300: Current success : 681 \t All success : 683 \t mean(l0) : 5.623719 \t Current Learning Rate: 0.047\n",
            "Iteration 5400: Current success : 681 \t All success : 684 \t mean(l0) : 5.643275 \t Current Learning Rate: 0.046\n",
            "Iteration 5500: Current success : 681 \t All success : 684 \t mean(l0) : 5.643275 \t Current Learning Rate: 0.045\n",
            "Iteration 5600: Current success : 682 \t All success : 684 \t mean(l0) : 5.643275 \t Current Learning Rate: 0.044\n",
            "Iteration 5700: Current success : 680 \t All success : 684 \t mean(l0) : 5.643275 \t Current Learning Rate: 0.043\n",
            "Iteration 5800: Current success : 682 \t All success : 684 \t mean(l0) : 5.643275 \t Current Learning Rate: 0.042\n",
            "Iteration 5900: Current success : 680 \t All success : 685 \t mean(l0) : 5.671533 \t Current Learning Rate: 0.041\n",
            "Iteration 6000: Current success : 681 \t All success : 686 \t mean(l0) : 5.686589 \t Current Learning Rate: 0.040\n",
            "Iteration 6100: Current success : 680 \t All success : 687 \t mean(l0) : 5.707424 \t Current Learning Rate: 0.039\n",
            "Iteration 6200: Current success : 680 \t All success : 688 \t mean(l0) : 5.713663 \t Current Learning Rate: 0.038\n",
            "Iteration 6300: Current success : 679 \t All success : 688 \t mean(l0) : 5.709302 \t Current Learning Rate: 0.037\n",
            "Iteration 6400: Current success : 679 \t All success : 688 \t mean(l0) : 5.709302 \t Current Learning Rate: 0.036\n",
            "Iteration 6500: Current success : 679 \t All success : 688 \t mean(l0) : 5.704942 \t Current Learning Rate: 0.035\n",
            "Iteration 6600: Current success : 679 \t All success : 690 \t mean(l0) : 5.726087 \t Current Learning Rate: 0.034\n",
            "Iteration 6700: Current success : 680 \t All success : 690 \t mean(l0) : 5.726087 \t Current Learning Rate: 0.033\n",
            "Iteration 6800: Current success : 679 \t All success : 690 \t mean(l0) : 5.724638 \t Current Learning Rate: 0.032\n",
            "Iteration 6900: Current success : 680 \t All success : 690 \t mean(l0) : 5.724638 \t Current Learning Rate: 0.031\n",
            "Iteration 7000: Current success : 680 \t All success : 690 \t mean(l0) : 5.724638 \t Current Learning Rate: 0.030\n",
            "Iteration 7100: Current success : 679 \t All success : 690 \t mean(l0) : 5.724638 \t Current Learning Rate: 0.029\n",
            "Iteration 7200: Current success : 679 \t All success : 690 \t mean(l0) : 5.724638 \t Current Learning Rate: 0.028\n",
            "Iteration 7300: Current success : 679 \t All success : 690 \t mean(l0) : 5.724638 \t Current Learning Rate: 0.027\n",
            "Iteration 7400: Current success : 679 \t All success : 690 \t mean(l0) : 5.724638 \t Current Learning Rate: 0.026\n",
            "Iteration 7500: Current success : 679 \t All success : 690 \t mean(l0) : 5.724638 \t Current Learning Rate: 0.025\n",
            "Iteration 7600: Current success : 682 \t All success : 690 \t mean(l0) : 5.724638 \t Current Learning Rate: 0.024\n",
            "Iteration 7700: Current success : 680 \t All success : 690 \t mean(l0) : 5.724638 \t Current Learning Rate: 0.023\n",
            "Iteration 7800: Current success : 681 \t All success : 690 \t mean(l0) : 5.724638 \t Current Learning Rate: 0.022\n",
            "Iteration 7900: Current success : 681 \t All success : 690 \t mean(l0) : 5.724638 \t Current Learning Rate: 0.021\n",
            "Iteration 8000: Current success : 681 \t All success : 690 \t mean(l0) : 5.724638 \t Current Learning Rate: 0.020\n",
            "Iteration 8100: Current success : 681 \t All success : 690 \t mean(l0) : 5.724638 \t Current Learning Rate: 0.019\n",
            "Iteration 8200: Current success : 681 \t All success : 690 \t mean(l0) : 5.724638 \t Current Learning Rate: 0.018\n",
            "Iteration 8300: Current success : 681 \t All success : 690 \t mean(l0) : 5.724638 \t Current Learning Rate: 0.017\n",
            "Iteration 8400: Current success : 681 \t All success : 690 \t mean(l0) : 5.724638 \t Current Learning Rate: 0.016\n",
            "Iteration 8500: Current success : 681 \t All success : 690 \t mean(l0) : 5.724638 \t Current Learning Rate: 0.015\n",
            "Iteration 8600: Current success : 681 \t All success : 690 \t mean(l0) : 5.724638 \t Current Learning Rate: 0.014\n",
            "Iteration 8700: Current success : 681 \t All success : 690 \t mean(l0) : 5.724638 \t Current Learning Rate: 0.013\n",
            "Iteration 8800: Current success : 681 \t All success : 690 \t mean(l0) : 5.724638 \t Current Learning Rate: 0.012\n",
            "Iteration 8900: Current success : 681 \t All success : 690 \t mean(l0) : 5.724638 \t Current Learning Rate: 0.011\n",
            "Iteration 9000: Current success : 681 \t All success : 690 \t mean(l0) : 5.724638 \t Current Learning Rate: 0.010\n",
            "Iteration 9100: Current success : 681 \t All success : 690 \t mean(l0) : 5.724638 \t Current Learning Rate: 0.009\n",
            "Iteration 9200: Current success : 681 \t All success : 690 \t mean(l0) : 5.724638 \t Current Learning Rate: 0.008\n",
            "Iteration 9300: Current success : 681 \t All success : 690 \t mean(l0) : 5.724638 \t Current Learning Rate: 0.007\n",
            "Iteration 9400: Current success : 681 \t All success : 690 \t mean(l0) : 5.724638 \t Current Learning Rate: 0.006\n",
            "Iteration 9500: Current success : 681 \t All success : 690 \t mean(l0) : 5.724638 \t Current Learning Rate: 0.005\n",
            "Iteration 9600: Current success : 681 \t All success : 690 \t mean(l0) : 5.724638 \t Current Learning Rate: 0.004\n",
            "Iteration 9700: Current success : 681 \t All success : 690 \t mean(l0) : 5.724638 \t Current Learning Rate: 0.003\n",
            "Iteration 9800: Current success : 681 \t All success : 690 \t mean(l0) : 5.724638 \t Current Learning Rate: 0.002\n",
            "Iteration 9900: Current success : 681 \t All success : 690 \t mean(l0) : 5.724638 \t Current Learning Rate: 0.001\n",
            "Iteration 10000: Current success : 681 \t All success : 690 \t mean(l0) : 5.724638 \t Current Learning Rate: 0.000\n",
            "Iteration 10100: Current success : 678 \t All success : 691 \t mean(l0) : 5.738060 \t Current Learning Rate: 0.025\n",
            "Iteration 10200: Current success : 680 \t All success : 691 \t mean(l0) : 5.738060 \t Current Learning Rate: 0.024\n",
            "Iteration 10300: Current success : 679 \t All success : 691 \t mean(l0) : 5.738060 \t Current Learning Rate: 0.024\n",
            "Iteration 10400: Current success : 679 \t All success : 691 \t mean(l0) : 5.738060 \t Current Learning Rate: 0.023\n",
            "Iteration 10500: Current success : 679 \t All success : 691 \t mean(l0) : 5.738060 \t Current Learning Rate: 0.023\n",
            "Iteration 10600: Current success : 679 \t All success : 691 \t mean(l0) : 5.727931 \t Current Learning Rate: 0.022\n",
            "Iteration 10700: Current success : 680 \t All success : 691 \t mean(l0) : 5.727931 \t Current Learning Rate: 0.022\n",
            "Iteration 10800: Current success : 680 \t All success : 691 \t mean(l0) : 5.726483 \t Current Learning Rate: 0.021\n",
            "Iteration 10900: Current success : 679 \t All success : 691 \t mean(l0) : 5.726483 \t Current Learning Rate: 0.021\n",
            "Iteration 11000: Current success : 679 \t All success : 691 \t mean(l0) : 5.725036 \t Current Learning Rate: 0.020\n",
            "Iteration 11100: Current success : 679 \t All success : 691 \t mean(l0) : 5.725036 \t Current Learning Rate: 0.020\n",
            "Iteration 11200: Current success : 679 \t All success : 691 \t mean(l0) : 5.725036 \t Current Learning Rate: 0.019\n",
            "Iteration 11300: Current success : 678 \t All success : 691 \t mean(l0) : 5.725036 \t Current Learning Rate: 0.019\n",
            "Iteration 11400: Current success : 678 \t All success : 692 \t mean(l0) : 5.739884 \t Current Learning Rate: 0.018\n",
            "Iteration 11500: Current success : 679 \t All success : 692 \t mean(l0) : 5.739884 \t Current Learning Rate: 0.018\n",
            "Iteration 11600: Current success : 679 \t All success : 692 \t mean(l0) : 5.739884 \t Current Learning Rate: 0.017\n",
            "Iteration 11700: Current success : 678 \t All success : 692 \t mean(l0) : 5.739884 \t Current Learning Rate: 0.017\n",
            "Iteration 11800: Current success : 678 \t All success : 692 \t mean(l0) : 5.739884 \t Current Learning Rate: 0.016\n",
            "Iteration 11900: Current success : 678 \t All success : 692 \t mean(l0) : 5.739884 \t Current Learning Rate: 0.016\n",
            "Iteration 12000: Current success : 678 \t All success : 692 \t mean(l0) : 5.739884 \t Current Learning Rate: 0.015\n",
            "Iteration 12100: Current success : 679 \t All success : 692 \t mean(l0) : 5.739884 \t Current Learning Rate: 0.015\n",
            "Iteration 12200: Current success : 679 \t All success : 692 \t mean(l0) : 5.739884 \t Current Learning Rate: 0.014\n",
            "Iteration 12300: Current success : 678 \t All success : 692 \t mean(l0) : 5.739884 \t Current Learning Rate: 0.014\n",
            "Iteration 12400: Current success : 678 \t All success : 692 \t mean(l0) : 5.739884 \t Current Learning Rate: 0.013\n",
            "Iteration 12500: Current success : 678 \t All success : 692 \t mean(l0) : 5.739884 \t Current Learning Rate: 0.013\n",
            "Iteration 12600: Current success : 679 \t All success : 692 \t mean(l0) : 5.739884 \t Current Learning Rate: 0.012\n",
            "Iteration 12700: Current success : 679 \t All success : 692 \t mean(l0) : 5.739884 \t Current Learning Rate: 0.012\n",
            "Iteration 12800: Current success : 680 \t All success : 692 \t mean(l0) : 5.739884 \t Current Learning Rate: 0.011\n",
            "Iteration 12900: Current success : 680 \t All success : 692 \t mean(l0) : 5.739884 \t Current Learning Rate: 0.011\n",
            "Iteration 13000: Current success : 681 \t All success : 692 \t mean(l0) : 5.739884 \t Current Learning Rate: 0.010\n",
            "Iteration 13100: Current success : 681 \t All success : 692 \t mean(l0) : 5.739884 \t Current Learning Rate: 0.010\n",
            "Iteration 13200: Current success : 681 \t All success : 692 \t mean(l0) : 5.739884 \t Current Learning Rate: 0.009\n",
            "Iteration 13300: Current success : 681 \t All success : 692 \t mean(l0) : 5.739884 \t Current Learning Rate: 0.009\n",
            "Iteration 13400: Current success : 681 \t All success : 692 \t mean(l0) : 5.739884 \t Current Learning Rate: 0.008\n",
            "Iteration 13500: Current success : 681 \t All success : 692 \t mean(l0) : 5.739884 \t Current Learning Rate: 0.008\n",
            "Iteration 13600: Current success : 681 \t All success : 692 \t mean(l0) : 5.739884 \t Current Learning Rate: 0.007\n",
            "Iteration 13700: Current success : 681 \t All success : 692 \t mean(l0) : 5.739884 \t Current Learning Rate: 0.007\n",
            "Iteration 13800: Current success : 681 \t All success : 692 \t mean(l0) : 5.739884 \t Current Learning Rate: 0.006\n",
            "Iteration 13900: Current success : 681 \t All success : 692 \t mean(l0) : 5.739884 \t Current Learning Rate: 0.006\n",
            "Iteration 14000: Current success : 681 \t All success : 692 \t mean(l0) : 5.739884 \t Current Learning Rate: 0.005\n",
            "Iteration 14100: Current success : 681 \t All success : 692 \t mean(l0) : 5.739884 \t Current Learning Rate: 0.005\n",
            "Iteration 14200: Current success : 681 \t All success : 692 \t mean(l0) : 5.739884 \t Current Learning Rate: 0.004\n",
            "Iteration 14300: Current success : 681 \t All success : 692 \t mean(l0) : 5.739884 \t Current Learning Rate: 0.004\n",
            "Iteration 14400: Current success : 681 \t All success : 692 \t mean(l0) : 5.739884 \t Current Learning Rate: 0.003\n",
            "Iteration 14500: Current success : 681 \t All success : 692 \t mean(l0) : 5.739884 \t Current Learning Rate: 0.003\n",
            "Iteration 14600: Current success : 681 \t All success : 692 \t mean(l0) : 5.739884 \t Current Learning Rate: 0.002\n",
            "Iteration 14700: Current success : 681 \t All success : 692 \t mean(l0) : 5.739884 \t Current Learning Rate: 0.002\n",
            "Iteration 14800: Current success : 681 \t All success : 692 \t mean(l0) : 5.739884 \t Current Learning Rate: 0.001\n",
            "Iteration 14900: Current success : 681 \t All success : 692 \t mean(l0) : 5.739884 \t Current Learning Rate: 0.001\n",
            "Iteration 15000: Current success : 681 \t All success : 692 \t mean(l0) : 5.739884 \t Current Learning Rate: 0.000\n",
            "Iteration 15100: Current success : 680 \t All success : 692 \t mean(l0) : 5.709538 \t Current Learning Rate: 0.012\n",
            "Iteration 15200: Current success : 681 \t All success : 692 \t mean(l0) : 5.708092 \t Current Learning Rate: 0.012\n",
            "Iteration 15300: Current success : 681 \t All success : 692 \t mean(l0) : 5.692196 \t Current Learning Rate: 0.012\n",
            "Iteration 15400: Current success : 681 \t All success : 692 \t mean(l0) : 5.689306 \t Current Learning Rate: 0.012\n",
            "Iteration 15500: Current success : 681 \t All success : 692 \t mean(l0) : 5.689306 \t Current Learning Rate: 0.011\n",
            "Iteration 15600: Current success : 681 \t All success : 692 \t mean(l0) : 5.689306 \t Current Learning Rate: 0.011\n",
            "Iteration 15700: Current success : 681 \t All success : 692 \t mean(l0) : 5.689306 \t Current Learning Rate: 0.011\n",
            "Iteration 15800: Current success : 681 \t All success : 692 \t mean(l0) : 5.689306 \t Current Learning Rate: 0.011\n",
            "Iteration 15900: Current success : 681 \t All success : 692 \t mean(l0) : 5.689306 \t Current Learning Rate: 0.010\n",
            "Iteration 16000: Current success : 681 \t All success : 692 \t mean(l0) : 5.689306 \t Current Learning Rate: 0.010\n",
            "Iteration 16100: Current success : 681 \t All success : 692 \t mean(l0) : 5.684971 \t Current Learning Rate: 0.010\n",
            "Iteration 16200: Current success : 681 \t All success : 692 \t mean(l0) : 5.684971 \t Current Learning Rate: 0.010\n",
            "Iteration 16300: Current success : 681 \t All success : 692 \t mean(l0) : 5.684971 \t Current Learning Rate: 0.009\n",
            "Iteration 16400: Current success : 681 \t All success : 692 \t mean(l0) : 5.684971 \t Current Learning Rate: 0.009\n",
            "Iteration 16500: Current success : 681 \t All success : 692 \t mean(l0) : 5.684971 \t Current Learning Rate: 0.009\n",
            "Iteration 16600: Current success : 681 \t All success : 692 \t mean(l0) : 5.684971 \t Current Learning Rate: 0.009\n",
            "Iteration 16700: Current success : 681 \t All success : 692 \t mean(l0) : 5.684971 \t Current Learning Rate: 0.008\n",
            "Iteration 16800: Current success : 681 \t All success : 692 \t mean(l0) : 5.684971 \t Current Learning Rate: 0.008\n",
            "Iteration 16900: Current success : 681 \t All success : 692 \t mean(l0) : 5.684971 \t Current Learning Rate: 0.008\n",
            "Iteration 17000: Current success : 681 \t All success : 692 \t mean(l0) : 5.684971 \t Current Learning Rate: 0.008\n",
            "Iteration 17100: Current success : 681 \t All success : 692 \t mean(l0) : 5.684971 \t Current Learning Rate: 0.007\n",
            "Iteration 17200: Current success : 681 \t All success : 692 \t mean(l0) : 5.684971 \t Current Learning Rate: 0.007\n",
            "Iteration 17300: Current success : 681 \t All success : 692 \t mean(l0) : 5.684971 \t Current Learning Rate: 0.007\n",
            "Iteration 17400: Current success : 681 \t All success : 692 \t mean(l0) : 5.684971 \t Current Learning Rate: 0.007\n",
            "Iteration 17500: Current success : 681 \t All success : 692 \t mean(l0) : 5.684971 \t Current Learning Rate: 0.006\n",
            "Iteration 17600: Current success : 681 \t All success : 692 \t mean(l0) : 5.684971 \t Current Learning Rate: 0.006\n",
            "Iteration 17700: Current success : 681 \t All success : 692 \t mean(l0) : 5.684971 \t Current Learning Rate: 0.006\n",
            "Iteration 17800: Current success : 681 \t All success : 692 \t mean(l0) : 5.684971 \t Current Learning Rate: 0.006\n",
            "Iteration 17900: Current success : 681 \t All success : 692 \t mean(l0) : 5.684971 \t Current Learning Rate: 0.005\n",
            "Iteration 18000: Current success : 681 \t All success : 692 \t mean(l0) : 5.684971 \t Current Learning Rate: 0.005\n",
            "Iteration 18100: Current success : 681 \t All success : 692 \t mean(l0) : 5.684971 \t Current Learning Rate: 0.005\n",
            "Iteration 18200: Current success : 681 \t All success : 692 \t mean(l0) : 5.684971 \t Current Learning Rate: 0.005\n",
            "Iteration 18300: Current success : 681 \t All success : 692 \t mean(l0) : 5.684971 \t Current Learning Rate: 0.004\n",
            "Iteration 18400: Current success : 681 \t All success : 692 \t mean(l0) : 5.684971 \t Current Learning Rate: 0.004\n",
            "Iteration 18500: Current success : 681 \t All success : 692 \t mean(l0) : 5.684971 \t Current Learning Rate: 0.004\n",
            "Iteration 18600: Current success : 681 \t All success : 692 \t mean(l0) : 5.684971 \t Current Learning Rate: 0.004\n",
            "Iteration 18700: Current success : 681 \t All success : 692 \t mean(l0) : 5.684971 \t Current Learning Rate: 0.003\n",
            "Iteration 18800: Current success : 681 \t All success : 692 \t mean(l0) : 5.684971 \t Current Learning Rate: 0.003\n",
            "Iteration 18900: Current success : 681 \t All success : 692 \t mean(l0) : 5.684971 \t Current Learning Rate: 0.003\n",
            "Iteration 19000: Current success : 681 \t All success : 692 \t mean(l0) : 5.684971 \t Current Learning Rate: 0.003\n",
            "Iteration 19100: Current success : 681 \t All success : 692 \t mean(l0) : 5.684971 \t Current Learning Rate: 0.002\n",
            "Iteration 19200: Current success : 681 \t All success : 692 \t mean(l0) : 5.684971 \t Current Learning Rate: 0.002\n",
            "Iteration 19300: Current success : 681 \t All success : 692 \t mean(l0) : 5.684971 \t Current Learning Rate: 0.002\n",
            "Iteration 19400: Current success : 681 \t All success : 692 \t mean(l0) : 5.684971 \t Current Learning Rate: 0.002\n",
            "Iteration 19500: Current success : 681 \t All success : 692 \t mean(l0) : 5.684971 \t Current Learning Rate: 0.001\n",
            "Iteration 19600: Current success : 681 \t All success : 692 \t mean(l0) : 5.684971 \t Current Learning Rate: 0.001\n",
            "Iteration 19700: Current success : 681 \t All success : 692 \t mean(l0) : 5.684971 \t Current Learning Rate: 0.001\n",
            "Iteration 19800: Current success : 681 \t All success : 692 \t mean(l0) : 5.684971 \t Current Learning Rate: 0.001\n",
            "Iteration 19900: Current success : 681 \t All success : 692 \t mean(l0) : 5.684971 \t Current Learning Rate: 0.000\n",
            "Iteration 20000: Current success : 681 \t All success : 692 \t mean(l0) : 5.684971 \t Current Learning Rate: 0.000\n",
            "outer_step 1: all success : 692 \t mean(l0)(success) : 5.6850\n",
            "-------------------------------------------------------------\n",
            "tensor([ 0.5000,  0.5000,  0.5000,  ...,  0.5000, 10.0000, 10.0000],\n",
            "       device='cuda:0')\n",
            "Iteration 100: Current success : 351 \t All success : 457 \t mean(l0) : 11.472648 \t Current Learning Rate: 0.098\n",
            "Iteration 200: Current success : 351 \t All success : 461 \t mean(l0) : 11.232104 \t Current Learning Rate: 0.096\n",
            "Iteration 300: Current success : 359 \t All success : 461 \t mean(l0) : 11.186551 \t Current Learning Rate: 0.094\n",
            "Iteration 400: Current success : 359 \t All success : 462 \t mean(l0) : 11.201299 \t Current Learning Rate: 0.092\n",
            "Iteration 500: Current success : 360 \t All success : 462 \t mean(l0) : 11.173161 \t Current Learning Rate: 0.090\n",
            "Iteration 600: Current success : 359 \t All success : 462 \t mean(l0) : 11.162337 \t Current Learning Rate: 0.088\n",
            "Iteration 700: Current success : 357 \t All success : 462 \t mean(l0) : 11.136364 \t Current Learning Rate: 0.086\n",
            "Iteration 800: Current success : 357 \t All success : 462 \t mean(l0) : 11.127706 \t Current Learning Rate: 0.084\n",
            "Iteration 900: Current success : 359 \t All success : 463 \t mean(l0) : 11.088552 \t Current Learning Rate: 0.082\n",
            "Iteration 1000: Current success : 359 \t All success : 463 \t mean(l0) : 11.082073 \t Current Learning Rate: 0.080\n",
            "Iteration 1100: Current success : 359 \t All success : 464 \t mean(l0) : 11.064655 \t Current Learning Rate: 0.078\n",
            "Iteration 1200: Current success : 359 \t All success : 464 \t mean(l0) : 11.062500 \t Current Learning Rate: 0.076\n",
            "Iteration 1300: Current success : 359 \t All success : 464 \t mean(l0) : 11.060345 \t Current Learning Rate: 0.074\n",
            "Iteration 1400: Current success : 359 \t All success : 465 \t mean(l0) : 11.006452 \t Current Learning Rate: 0.072\n",
            "Iteration 1500: Current success : 359 \t All success : 466 \t mean(l0) : 10.984979 \t Current Learning Rate: 0.070\n",
            "Iteration 1600: Current success : 360 \t All success : 466 \t mean(l0) : 10.982833 \t Current Learning Rate: 0.068\n",
            "Iteration 1700: Current success : 360 \t All success : 466 \t mean(l0) : 10.976395 \t Current Learning Rate: 0.066\n",
            "Iteration 1800: Current success : 358 \t All success : 466 \t mean(l0) : 10.976395 \t Current Learning Rate: 0.064\n",
            "Iteration 1900: Current success : 361 \t All success : 467 \t mean(l0) : 10.944325 \t Current Learning Rate: 0.062\n",
            "Iteration 2000: Current success : 373 \t All success : 480 \t mean(l0) : 10.741667 \t Current Learning Rate: 0.060\n",
            "Iteration 2100: Current success : 361 \t All success : 480 \t mean(l0) : 10.693750 \t Current Learning Rate: 0.058\n",
            "Iteration 2200: Current success : 358 \t All success : 480 \t mean(l0) : 10.685417 \t Current Learning Rate: 0.056\n",
            "Iteration 2300: Current success : 363 \t All success : 481 \t mean(l0) : 10.669438 \t Current Learning Rate: 0.054\n",
            "Iteration 2400: Current success : 362 \t All success : 482 \t mean(l0) : 10.620333 \t Current Learning Rate: 0.052\n",
            "Iteration 2500: Current success : 362 \t All success : 482 \t mean(l0) : 10.620333 \t Current Learning Rate: 0.050\n",
            "Iteration 2600: Current success : 363 \t All success : 482 \t mean(l0) : 10.612034 \t Current Learning Rate: 0.048\n",
            "Iteration 2700: Current success : 402 \t All success : 498 \t mean(l0) : 10.355421 \t Current Learning Rate: 0.046\n",
            "Iteration 2800: Current success : 412 \t All success : 498 \t mean(l0) : 10.351405 \t Current Learning Rate: 0.044\n",
            "Iteration 2900: Current success : 412 \t All success : 498 \t mean(l0) : 10.351405 \t Current Learning Rate: 0.042\n",
            "Iteration 3000: Current success : 412 \t All success : 498 \t mean(l0) : 10.351405 \t Current Learning Rate: 0.040\n",
            "Iteration 3100: Current success : 412 \t All success : 498 \t mean(l0) : 10.351405 \t Current Learning Rate: 0.038\n",
            "Iteration 3200: Current success : 412 \t All success : 498 \t mean(l0) : 10.351405 \t Current Learning Rate: 0.036\n",
            "Iteration 3300: Current success : 412 \t All success : 498 \t mean(l0) : 10.351405 \t Current Learning Rate: 0.034\n",
            "Iteration 3400: Current success : 412 \t All success : 498 \t mean(l0) : 10.351405 \t Current Learning Rate: 0.032\n",
            "Iteration 3500: Current success : 412 \t All success : 498 \t mean(l0) : 10.351405 \t Current Learning Rate: 0.030\n",
            "Iteration 3600: Current success : 412 \t All success : 498 \t mean(l0) : 10.347389 \t Current Learning Rate: 0.028\n",
            "Iteration 3700: Current success : 412 \t All success : 498 \t mean(l0) : 10.347389 \t Current Learning Rate: 0.026\n",
            "Iteration 3800: Current success : 412 \t All success : 498 \t mean(l0) : 10.347389 \t Current Learning Rate: 0.024\n",
            "Iteration 3900: Current success : 412 \t All success : 498 \t mean(l0) : 10.343373 \t Current Learning Rate: 0.022\n",
            "Iteration 4000: Current success : 412 \t All success : 498 \t mean(l0) : 10.343373 \t Current Learning Rate: 0.020\n",
            "Iteration 4100: Current success : 412 \t All success : 498 \t mean(l0) : 10.343373 \t Current Learning Rate: 0.018\n",
            "Iteration 4200: Current success : 412 \t All success : 498 \t mean(l0) : 10.343373 \t Current Learning Rate: 0.016\n",
            "Iteration 4300: Current success : 412 \t All success : 498 \t mean(l0) : 10.343373 \t Current Learning Rate: 0.014\n",
            "Iteration 4400: Current success : 412 \t All success : 498 \t mean(l0) : 10.343373 \t Current Learning Rate: 0.012\n",
            "Iteration 4500: Current success : 412 \t All success : 498 \t mean(l0) : 10.343373 \t Current Learning Rate: 0.010\n",
            "Iteration 4600: Current success : 412 \t All success : 498 \t mean(l0) : 10.343373 \t Current Learning Rate: 0.008\n",
            "Iteration 4700: Current success : 412 \t All success : 498 \t mean(l0) : 10.343373 \t Current Learning Rate: 0.006\n",
            "Iteration 4800: Current success : 412 \t All success : 498 \t mean(l0) : 10.343373 \t Current Learning Rate: 0.004\n",
            "Iteration 4900: Current success : 412 \t All success : 498 \t mean(l0) : 10.343373 \t Current Learning Rate: 0.002\n",
            "Iteration 5000: Current success : 412 \t All success : 498 \t mean(l0) : 10.343373 \t Current Learning Rate: 0.000\n",
            "Iteration 5100: Current success : 690 \t All success : 715 \t mean(l0) : 9.344056 \t Current Learning Rate: 0.049\n",
            "Iteration 5200: Current success : 720 \t All success : 734 \t mean(l0) : 9.211171 \t Current Learning Rate: 0.048\n",
            "Iteration 5300: Current success : 716 \t All success : 734 \t mean(l0) : 9.208447 \t Current Learning Rate: 0.047\n",
            "Iteration 5400: Current success : 713 \t All success : 734 \t mean(l0) : 9.205722 \t Current Learning Rate: 0.046\n",
            "Iteration 5500: Current success : 716 \t All success : 735 \t mean(l0) : 9.202722 \t Current Learning Rate: 0.045\n",
            "Iteration 5600: Current success : 709 \t All success : 736 \t mean(l0) : 9.207881 \t Current Learning Rate: 0.044\n",
            "Iteration 5700: Current success : 713 \t All success : 736 \t mean(l0) : 9.202446 \t Current Learning Rate: 0.043\n",
            "Iteration 5800: Current success : 711 \t All success : 736 \t mean(l0) : 9.201087 \t Current Learning Rate: 0.042\n",
            "Iteration 5900: Current success : 714 \t All success : 737 \t mean(l0) : 9.202171 \t Current Learning Rate: 0.041\n",
            "Iteration 6000: Current success : 706 \t All success : 737 \t mean(l0) : 9.202171 \t Current Learning Rate: 0.040\n",
            "Iteration 6100: Current success : 705 \t All success : 739 \t mean(l0) : 9.196211 \t Current Learning Rate: 0.039\n",
            "Iteration 6200: Current success : 701 \t All success : 743 \t mean(l0) : 9.183042 \t Current Learning Rate: 0.038\n",
            "Iteration 6300: Current success : 697 \t All success : 745 \t mean(l0) : 9.189262 \t Current Learning Rate: 0.037\n",
            "Iteration 6400: Current success : 702 \t All success : 746 \t mean(l0) : 9.191689 \t Current Learning Rate: 0.036\n",
            "Iteration 6500: Current success : 696 \t All success : 747 \t mean(l0) : 9.192771 \t Current Learning Rate: 0.035\n",
            "Iteration 6600: Current success : 701 \t All success : 748 \t mean(l0) : 9.185829 \t Current Learning Rate: 0.034\n",
            "Iteration 6700: Current success : 700 \t All success : 748 \t mean(l0) : 9.185829 \t Current Learning Rate: 0.033\n",
            "Iteration 6800: Current success : 693 \t All success : 749 \t mean(l0) : 9.177570 \t Current Learning Rate: 0.032\n",
            "Iteration 6900: Current success : 697 \t All success : 749 \t mean(l0) : 9.177570 \t Current Learning Rate: 0.031\n",
            "Iteration 7000: Current success : 690 \t All success : 749 \t mean(l0) : 9.177570 \t Current Learning Rate: 0.030\n",
            "Iteration 7100: Current success : 695 \t All success : 750 \t mean(l0) : 9.174666 \t Current Learning Rate: 0.029\n",
            "Iteration 7200: Current success : 692 \t All success : 750 \t mean(l0) : 9.174666 \t Current Learning Rate: 0.028\n",
            "Iteration 7300: Current success : 695 \t All success : 750 \t mean(l0) : 9.174666 \t Current Learning Rate: 0.027\n",
            "Iteration 7400: Current success : 694 \t All success : 750 \t mean(l0) : 9.174666 \t Current Learning Rate: 0.026\n",
            "Iteration 7500: Current success : 696 \t All success : 750 \t mean(l0) : 9.174666 \t Current Learning Rate: 0.025\n",
            "Iteration 7600: Current success : 702 \t All success : 750 \t mean(l0) : 9.174666 \t Current Learning Rate: 0.024\n",
            "Iteration 7700: Current success : 705 \t All success : 750 \t mean(l0) : 9.174666 \t Current Learning Rate: 0.023\n",
            "Iteration 7800: Current success : 714 \t All success : 750 \t mean(l0) : 9.174666 \t Current Learning Rate: 0.022\n",
            "Iteration 7900: Current success : 714 \t All success : 750 \t mean(l0) : 9.174666 \t Current Learning Rate: 0.021\n",
            "Iteration 8000: Current success : 714 \t All success : 750 \t mean(l0) : 9.174666 \t Current Learning Rate: 0.020\n",
            "Iteration 8100: Current success : 714 \t All success : 750 \t mean(l0) : 9.174666 \t Current Learning Rate: 0.019\n",
            "Iteration 8200: Current success : 714 \t All success : 750 \t mean(l0) : 9.174666 \t Current Learning Rate: 0.018\n",
            "Iteration 8300: Current success : 714 \t All success : 750 \t mean(l0) : 9.174666 \t Current Learning Rate: 0.017\n",
            "Iteration 8400: Current success : 714 \t All success : 750 \t mean(l0) : 9.174666 \t Current Learning Rate: 0.016\n",
            "Iteration 8500: Current success : 714 \t All success : 750 \t mean(l0) : 9.174666 \t Current Learning Rate: 0.015\n",
            "Iteration 8600: Current success : 714 \t All success : 750 \t mean(l0) : 9.174666 \t Current Learning Rate: 0.014\n",
            "Iteration 8700: Current success : 714 \t All success : 750 \t mean(l0) : 9.174666 \t Current Learning Rate: 0.013\n",
            "Iteration 8800: Current success : 714 \t All success : 750 \t mean(l0) : 9.174666 \t Current Learning Rate: 0.012\n",
            "Iteration 8900: Current success : 714 \t All success : 750 \t mean(l0) : 9.174666 \t Current Learning Rate: 0.011\n",
            "Iteration 9000: Current success : 714 \t All success : 750 \t mean(l0) : 9.174666 \t Current Learning Rate: 0.010\n",
            "Iteration 9100: Current success : 714 \t All success : 750 \t mean(l0) : 9.174666 \t Current Learning Rate: 0.009\n",
            "Iteration 9200: Current success : 714 \t All success : 750 \t mean(l0) : 9.174666 \t Current Learning Rate: 0.008\n",
            "Iteration 9300: Current success : 714 \t All success : 750 \t mean(l0) : 9.174666 \t Current Learning Rate: 0.007\n",
            "Iteration 9400: Current success : 714 \t All success : 750 \t mean(l0) : 9.174666 \t Current Learning Rate: 0.006\n",
            "Iteration 9500: Current success : 714 \t All success : 750 \t mean(l0) : 9.174666 \t Current Learning Rate: 0.005\n",
            "Iteration 9600: Current success : 714 \t All success : 750 \t mean(l0) : 9.174666 \t Current Learning Rate: 0.004\n",
            "Iteration 9700: Current success : 714 \t All success : 750 \t mean(l0) : 9.174666 \t Current Learning Rate: 0.003\n",
            "Iteration 9800: Current success : 714 \t All success : 750 \t mean(l0) : 9.174666 \t Current Learning Rate: 0.002\n",
            "Iteration 9900: Current success : 714 \t All success : 750 \t mean(l0) : 9.174666 \t Current Learning Rate: 0.001\n",
            "Iteration 10000: Current success : 714 \t All success : 750 \t mean(l0) : 9.174666 \t Current Learning Rate: 0.000\n",
            "Iteration 10100: Current success : 689 \t All success : 755 \t mean(l0) : 9.324504 \t Current Learning Rate: 0.025\n",
            "Iteration 10200: Current success : 715 \t All success : 755 \t mean(l0) : 9.323179 \t Current Learning Rate: 0.024\n",
            "Iteration 10300: Current success : 717 \t All success : 759 \t mean(l0) : 9.428195 \t Current Learning Rate: 0.024\n",
            "Iteration 10400: Current success : 713 \t All success : 759 \t mean(l0) : 9.428195 \t Current Learning Rate: 0.023\n",
            "Iteration 10500: Current success : 716 \t All success : 759 \t mean(l0) : 9.426878 \t Current Learning Rate: 0.023\n",
            "Iteration 10600: Current success : 710 \t All success : 759 \t mean(l0) : 9.424243 \t Current Learning Rate: 0.022\n",
            "Iteration 10700: Current success : 712 \t All success : 759 \t mean(l0) : 9.424243 \t Current Learning Rate: 0.022\n",
            "Iteration 10800: Current success : 709 \t All success : 759 \t mean(l0) : 9.424243 \t Current Learning Rate: 0.021\n",
            "Iteration 10900: Current success : 709 \t All success : 759 \t mean(l0) : 9.424243 \t Current Learning Rate: 0.021\n",
            "Iteration 11000: Current success : 707 \t All success : 759 \t mean(l0) : 9.424243 \t Current Learning Rate: 0.020\n",
            "Iteration 11100: Current success : 710 \t All success : 759 \t mean(l0) : 9.424243 \t Current Learning Rate: 0.020\n",
            "Iteration 11200: Current success : 714 \t All success : 759 \t mean(l0) : 9.424243 \t Current Learning Rate: 0.019\n",
            "Iteration 11300: Current success : 705 \t All success : 759 \t mean(l0) : 9.424243 \t Current Learning Rate: 0.019\n",
            "Iteration 11400: Current success : 706 \t All success : 759 \t mean(l0) : 9.424243 \t Current Learning Rate: 0.018\n",
            "Iteration 11500: Current success : 712 \t All success : 759 \t mean(l0) : 9.422925 \t Current Learning Rate: 0.018\n",
            "Iteration 11600: Current success : 704 \t All success : 759 \t mean(l0) : 9.422925 \t Current Learning Rate: 0.017\n",
            "Iteration 11700: Current success : 713 \t All success : 759 \t mean(l0) : 9.422925 \t Current Learning Rate: 0.017\n",
            "Iteration 11800: Current success : 707 \t All success : 759 \t mean(l0) : 9.422925 \t Current Learning Rate: 0.016\n",
            "Iteration 11900: Current success : 708 \t All success : 759 \t mean(l0) : 9.422925 \t Current Learning Rate: 0.016\n",
            "Iteration 12000: Current success : 707 \t All success : 759 \t mean(l0) : 9.422925 \t Current Learning Rate: 0.015\n",
            "Iteration 12100: Current success : 706 \t All success : 759 \t mean(l0) : 9.422925 \t Current Learning Rate: 0.015\n",
            "Iteration 12200: Current success : 706 \t All success : 759 \t mean(l0) : 9.422925 \t Current Learning Rate: 0.014\n",
            "Iteration 12300: Current success : 708 \t All success : 759 \t mean(l0) : 9.422925 \t Current Learning Rate: 0.014\n",
            "Iteration 12400: Current success : 713 \t All success : 759 \t mean(l0) : 9.422925 \t Current Learning Rate: 0.013\n",
            "Iteration 12500: Current success : 704 \t All success : 759 \t mean(l0) : 9.422925 \t Current Learning Rate: 0.013\n",
            "Iteration 12600: Current success : 708 \t All success : 759 \t mean(l0) : 9.416337 \t Current Learning Rate: 0.012\n",
            "Iteration 12700: Current success : 710 \t All success : 759 \t mean(l0) : 9.416337 \t Current Learning Rate: 0.012\n",
            "Iteration 12800: Current success : 717 \t All success : 759 \t mean(l0) : 9.416337 \t Current Learning Rate: 0.011\n",
            "Iteration 12900: Current success : 721 \t All success : 759 \t mean(l0) : 9.416337 \t Current Learning Rate: 0.011\n",
            "Iteration 13000: Current success : 722 \t All success : 759 \t mean(l0) : 9.416337 \t Current Learning Rate: 0.010\n",
            "Iteration 13100: Current success : 721 \t All success : 759 \t mean(l0) : 9.416337 \t Current Learning Rate: 0.010\n",
            "Iteration 13200: Current success : 721 \t All success : 759 \t mean(l0) : 9.416337 \t Current Learning Rate: 0.009\n",
            "Iteration 13300: Current success : 721 \t All success : 759 \t mean(l0) : 9.413702 \t Current Learning Rate: 0.009\n",
            "Iteration 13400: Current success : 721 \t All success : 759 \t mean(l0) : 9.413702 \t Current Learning Rate: 0.008\n",
            "Iteration 13500: Current success : 721 \t All success : 759 \t mean(l0) : 9.413702 \t Current Learning Rate: 0.008\n",
            "Iteration 13600: Current success : 721 \t All success : 759 \t mean(l0) : 9.413702 \t Current Learning Rate: 0.007\n",
            "Iteration 13700: Current success : 721 \t All success : 759 \t mean(l0) : 9.413702 \t Current Learning Rate: 0.007\n",
            "Iteration 13800: Current success : 721 \t All success : 759 \t mean(l0) : 9.413702 \t Current Learning Rate: 0.006\n",
            "Iteration 13900: Current success : 721 \t All success : 759 \t mean(l0) : 9.413702 \t Current Learning Rate: 0.006\n",
            "Iteration 14000: Current success : 724 \t All success : 762 \t mean(l0) : 9.530184 \t Current Learning Rate: 0.005\n",
            "Iteration 14100: Current success : 724 \t All success : 762 \t mean(l0) : 9.530184 \t Current Learning Rate: 0.005\n",
            "Iteration 14200: Current success : 724 \t All success : 762 \t mean(l0) : 9.530184 \t Current Learning Rate: 0.004\n",
            "Iteration 14300: Current success : 724 \t All success : 762 \t mean(l0) : 9.530184 \t Current Learning Rate: 0.004\n",
            "Iteration 14400: Current success : 721 \t All success : 762 \t mean(l0) : 9.530184 \t Current Learning Rate: 0.003\n",
            "Iteration 14500: Current success : 721 \t All success : 762 \t mean(l0) : 9.530184 \t Current Learning Rate: 0.003\n",
            "Iteration 14600: Current success : 721 \t All success : 762 \t mean(l0) : 9.530184 \t Current Learning Rate: 0.002\n",
            "Iteration 14700: Current success : 721 \t All success : 762 \t mean(l0) : 9.530184 \t Current Learning Rate: 0.002\n",
            "Iteration 14800: Current success : 721 \t All success : 762 \t mean(l0) : 9.530184 \t Current Learning Rate: 0.001\n",
            "Iteration 14900: Current success : 721 \t All success : 762 \t mean(l0) : 9.530184 \t Current Learning Rate: 0.001\n",
            "Iteration 15000: Current success : 721 \t All success : 762 \t mean(l0) : 9.530184 \t Current Learning Rate: 0.000\n",
            "Iteration 15100: Current success : 698 \t All success : 764 \t mean(l0) : 9.613874 \t Current Learning Rate: 0.012\n",
            "Iteration 15200: Current success : 733 \t All success : 764 \t mean(l0) : 9.558901 \t Current Learning Rate: 0.012\n",
            "Iteration 15300: Current success : 727 \t All success : 764 \t mean(l0) : 9.517015 \t Current Learning Rate: 0.012\n",
            "Iteration 15400: Current success : 727 \t All success : 764 \t mean(l0) : 9.514398 \t Current Learning Rate: 0.012\n",
            "Iteration 15500: Current success : 727 \t All success : 764 \t mean(l0) : 9.511780 \t Current Learning Rate: 0.011\n",
            "Iteration 15600: Current success : 726 \t All success : 764 \t mean(l0) : 9.510471 \t Current Learning Rate: 0.011\n",
            "Iteration 15700: Current success : 725 \t All success : 764 \t mean(l0) : 9.510471 \t Current Learning Rate: 0.011\n",
            "Iteration 15800: Current success : 726 \t All success : 764 \t mean(l0) : 9.506545 \t Current Learning Rate: 0.011\n",
            "Iteration 15900: Current success : 726 \t All success : 764 \t mean(l0) : 9.503927 \t Current Learning Rate: 0.010\n",
            "Iteration 16000: Current success : 726 \t All success : 764 \t mean(l0) : 9.503927 \t Current Learning Rate: 0.010\n",
            "Iteration 16100: Current success : 726 \t All success : 764 \t mean(l0) : 9.497382 \t Current Learning Rate: 0.010\n",
            "Iteration 16200: Current success : 726 \t All success : 764 \t mean(l0) : 9.494764 \t Current Learning Rate: 0.010\n",
            "Iteration 16300: Current success : 726 \t All success : 764 \t mean(l0) : 9.492146 \t Current Learning Rate: 0.009\n",
            "Iteration 16400: Current success : 726 \t All success : 764 \t mean(l0) : 9.492146 \t Current Learning Rate: 0.009\n",
            "Iteration 16500: Current success : 726 \t All success : 764 \t mean(l0) : 9.492146 \t Current Learning Rate: 0.009\n",
            "Iteration 16600: Current success : 725 \t All success : 764 \t mean(l0) : 9.492146 \t Current Learning Rate: 0.009\n",
            "Iteration 16700: Current success : 726 \t All success : 764 \t mean(l0) : 9.492146 \t Current Learning Rate: 0.008\n",
            "Iteration 16800: Current success : 725 \t All success : 764 \t mean(l0) : 9.492146 \t Current Learning Rate: 0.008\n",
            "Iteration 16900: Current success : 725 \t All success : 764 \t mean(l0) : 9.492146 \t Current Learning Rate: 0.008\n",
            "Iteration 17000: Current success : 725 \t All success : 764 \t mean(l0) : 9.492146 \t Current Learning Rate: 0.008\n",
            "Iteration 17100: Current success : 725 \t All success : 764 \t mean(l0) : 9.492146 \t Current Learning Rate: 0.007\n",
            "Iteration 17200: Current success : 725 \t All success : 764 \t mean(l0) : 9.492146 \t Current Learning Rate: 0.007\n",
            "Iteration 17300: Current success : 725 \t All success : 764 \t mean(l0) : 9.492146 \t Current Learning Rate: 0.007\n",
            "Iteration 17400: Current success : 725 \t All success : 764 \t mean(l0) : 9.492146 \t Current Learning Rate: 0.007\n",
            "Iteration 17500: Current success : 725 \t All success : 764 \t mean(l0) : 9.492146 \t Current Learning Rate: 0.006\n",
            "Iteration 17600: Current success : 725 \t All success : 764 \t mean(l0) : 9.492146 \t Current Learning Rate: 0.006\n",
            "Iteration 17700: Current success : 725 \t All success : 764 \t mean(l0) : 9.492146 \t Current Learning Rate: 0.006\n",
            "Iteration 17800: Current success : 725 \t All success : 764 \t mean(l0) : 9.492146 \t Current Learning Rate: 0.006\n",
            "Iteration 17900: Current success : 726 \t All success : 764 \t mean(l0) : 9.492146 \t Current Learning Rate: 0.005\n",
            "Iteration 18000: Current success : 725 \t All success : 764 \t mean(l0) : 9.490838 \t Current Learning Rate: 0.005\n",
            "Iteration 18100: Current success : 725 \t All success : 764 \t mean(l0) : 9.490838 \t Current Learning Rate: 0.005\n",
            "Iteration 18200: Current success : 725 \t All success : 764 \t mean(l0) : 9.490838 \t Current Learning Rate: 0.005\n",
            "Iteration 18300: Current success : 726 \t All success : 764 \t mean(l0) : 9.490838 \t Current Learning Rate: 0.004\n",
            "Iteration 18400: Current success : 726 \t All success : 764 \t mean(l0) : 9.490838 \t Current Learning Rate: 0.004\n",
            "Iteration 18500: Current success : 726 \t All success : 764 \t mean(l0) : 9.490838 \t Current Learning Rate: 0.004\n",
            "Iteration 18600: Current success : 726 \t All success : 764 \t mean(l0) : 9.490838 \t Current Learning Rate: 0.004\n",
            "Iteration 18700: Current success : 726 \t All success : 764 \t mean(l0) : 9.490838 \t Current Learning Rate: 0.003\n",
            "Iteration 18800: Current success : 726 \t All success : 764 \t mean(l0) : 9.490838 \t Current Learning Rate: 0.003\n",
            "Iteration 18900: Current success : 726 \t All success : 764 \t mean(l0) : 9.490838 \t Current Learning Rate: 0.003\n",
            "Iteration 19000: Current success : 726 \t All success : 764 \t mean(l0) : 9.490838 \t Current Learning Rate: 0.003\n",
            "Iteration 19100: Current success : 726 \t All success : 764 \t mean(l0) : 9.490838 \t Current Learning Rate: 0.002\n",
            "Iteration 19200: Current success : 726 \t All success : 764 \t mean(l0) : 9.490838 \t Current Learning Rate: 0.002\n",
            "Iteration 19300: Current success : 726 \t All success : 764 \t mean(l0) : 9.490838 \t Current Learning Rate: 0.002\n",
            "Iteration 19400: Current success : 726 \t All success : 764 \t mean(l0) : 9.490838 \t Current Learning Rate: 0.002\n",
            "Iteration 19500: Current success : 725 \t All success : 764 \t mean(l0) : 9.490838 \t Current Learning Rate: 0.001\n",
            "Iteration 19600: Current success : 725 \t All success : 764 \t mean(l0) : 9.490838 \t Current Learning Rate: 0.001\n",
            "Iteration 19700: Current success : 725 \t All success : 764 \t mean(l0) : 9.490838 \t Current Learning Rate: 0.001\n",
            "Iteration 19800: Current success : 726 \t All success : 764 \t mean(l0) : 9.490838 \t Current Learning Rate: 0.001\n",
            "Iteration 19900: Current success : 726 \t All success : 764 \t mean(l0) : 9.490838 \t Current Learning Rate: 0.000\n",
            "Iteration 20000: Current success : 726 \t All success : 764 \t mean(l0) : 9.490838 \t Current Learning Rate: 0.000\n",
            "outer_step 2: all success : 901 \t mean(l0)(success) : 10.0411\n",
            "-------------------------------------------------------------\n",
            "tensor([  0.2500,   0.2500,   0.7500,  ...,   0.7500, 100.0000,   5.5000],\n",
            "       device='cuda:0')\n",
            "Iteration 100: Current success : 218 \t All success : 336 \t mean(l0) : 9.455358 \t Current Learning Rate: 0.098\n",
            "Iteration 200: Current success : 236 \t All success : 355 \t mean(l0) : 10.597182 \t Current Learning Rate: 0.096\n",
            "Iteration 300: Current success : 238 \t All success : 362 \t mean(l0) : 11.229282 \t Current Learning Rate: 0.094\n",
            "Iteration 400: Current success : 252 \t All success : 373 \t mean(l0) : 12.037534 \t Current Learning Rate: 0.092\n",
            "Iteration 500: Current success : 254 \t All success : 378 \t mean(l0) : 12.515873 \t Current Learning Rate: 0.090\n",
            "Iteration 600: Current success : 258 \t All success : 381 \t mean(l0) : 12.881889 \t Current Learning Rate: 0.088\n",
            "Iteration 700: Current success : 273 \t All success : 401 \t mean(l0) : 13.645885 \t Current Learning Rate: 0.086\n",
            "Iteration 800: Current success : 286 \t All success : 408 \t mean(l0) : 13.772059 \t Current Learning Rate: 0.084\n",
            "Iteration 900: Current success : 298 \t All success : 420 \t mean(l0) : 14.035715 \t Current Learning Rate: 0.082\n",
            "Iteration 1000: Current success : 296 \t All success : 420 \t mean(l0) : 14.004763 \t Current Learning Rate: 0.080\n",
            "Iteration 1100: Current success : 296 \t All success : 421 \t mean(l0) : 14.125892 \t Current Learning Rate: 0.078\n",
            "Iteration 1200: Current success : 298 \t All success : 422 \t mean(l0) : 14.137442 \t Current Learning Rate: 0.076\n",
            "Iteration 1300: Current success : 298 \t All success : 422 \t mean(l0) : 14.135072 \t Current Learning Rate: 0.074\n",
            "Iteration 1400: Current success : 298 \t All success : 422 \t mean(l0) : 14.132702 \t Current Learning Rate: 0.072\n",
            "Iteration 1500: Current success : 298 \t All success : 423 \t mean(l0) : 14.144208 \t Current Learning Rate: 0.070\n",
            "Iteration 1600: Current success : 297 \t All success : 423 \t mean(l0) : 14.144208 \t Current Learning Rate: 0.068\n",
            "Iteration 1700: Current success : 302 \t All success : 426 \t mean(l0) : 14.565728 \t Current Learning Rate: 0.066\n",
            "Iteration 1800: Current success : 305 \t All success : 427 \t mean(l0) : 14.683840 \t Current Learning Rate: 0.064\n",
            "Iteration 1900: Current success : 304 \t All success : 428 \t mean(l0) : 14.647196 \t Current Learning Rate: 0.062\n",
            "Iteration 2000: Current success : 304 \t All success : 430 \t mean(l0) : 14.576744 \t Current Learning Rate: 0.060\n",
            "Iteration 2100: Current success : 303 \t All success : 430 \t mean(l0) : 14.576744 \t Current Learning Rate: 0.058\n",
            "Iteration 2200: Current success : 304 \t All success : 430 \t mean(l0) : 14.576744 \t Current Learning Rate: 0.056\n",
            "Iteration 2300: Current success : 304 \t All success : 430 \t mean(l0) : 14.574419 \t Current Learning Rate: 0.054\n",
            "Iteration 2400: Current success : 304 \t All success : 430 \t mean(l0) : 14.574419 \t Current Learning Rate: 0.052\n",
            "Iteration 2500: Current success : 304 \t All success : 430 \t mean(l0) : 14.574419 \t Current Learning Rate: 0.050\n",
            "Iteration 2600: Current success : 305 \t All success : 434 \t mean(l0) : 14.900922 \t Current Learning Rate: 0.048\n",
            "Iteration 2700: Current success : 336 \t All success : 448 \t mean(l0) : 14.508929 \t Current Learning Rate: 0.046\n",
            "Iteration 2800: Current success : 345 \t All success : 452 \t mean(l0) : 14.422566 \t Current Learning Rate: 0.044\n",
            "Iteration 2900: Current success : 342 \t All success : 452 \t mean(l0) : 14.422566 \t Current Learning Rate: 0.042\n",
            "Iteration 3000: Current success : 342 \t All success : 452 \t mean(l0) : 14.422566 \t Current Learning Rate: 0.040\n",
            "Iteration 3100: Current success : 342 \t All success : 452 \t mean(l0) : 14.422566 \t Current Learning Rate: 0.038\n",
            "Iteration 3200: Current success : 343 \t All success : 452 \t mean(l0) : 14.422566 \t Current Learning Rate: 0.036\n",
            "Iteration 3300: Current success : 345 \t All success : 452 \t mean(l0) : 14.422566 \t Current Learning Rate: 0.034\n",
            "Iteration 3400: Current success : 342 \t All success : 452 \t mean(l0) : 14.422566 \t Current Learning Rate: 0.032\n",
            "Iteration 3500: Current success : 342 \t All success : 452 \t mean(l0) : 14.420354 \t Current Learning Rate: 0.030\n",
            "Iteration 3600: Current success : 342 \t All success : 452 \t mean(l0) : 14.411505 \t Current Learning Rate: 0.028\n",
            "Iteration 3700: Current success : 342 \t All success : 452 \t mean(l0) : 14.411505 \t Current Learning Rate: 0.026\n",
            "Iteration 3800: Current success : 342 \t All success : 452 \t mean(l0) : 14.411505 \t Current Learning Rate: 0.024\n",
            "Iteration 3900: Current success : 342 \t All success : 452 \t mean(l0) : 14.411505 \t Current Learning Rate: 0.022\n",
            "Iteration 4000: Current success : 342 \t All success : 452 \t mean(l0) : 14.411505 \t Current Learning Rate: 0.020\n",
            "Iteration 4100: Current success : 343 \t All success : 452 \t mean(l0) : 14.411505 \t Current Learning Rate: 0.018\n",
            "Iteration 4200: Current success : 342 \t All success : 452 \t mean(l0) : 14.411505 \t Current Learning Rate: 0.016\n",
            "Iteration 4300: Current success : 342 \t All success : 452 \t mean(l0) : 14.411505 \t Current Learning Rate: 0.014\n",
            "Iteration 4400: Current success : 342 \t All success : 452 \t mean(l0) : 14.411505 \t Current Learning Rate: 0.012\n",
            "Iteration 4500: Current success : 342 \t All success : 452 \t mean(l0) : 14.411505 \t Current Learning Rate: 0.010\n",
            "Iteration 4600: Current success : 342 \t All success : 452 \t mean(l0) : 14.411505 \t Current Learning Rate: 0.008\n",
            "Iteration 4700: Current success : 342 \t All success : 452 \t mean(l0) : 14.411505 \t Current Learning Rate: 0.006\n",
            "Iteration 4800: Current success : 342 \t All success : 452 \t mean(l0) : 14.411505 \t Current Learning Rate: 0.004\n",
            "Iteration 4900: Current success : 342 \t All success : 452 \t mean(l0) : 14.411505 \t Current Learning Rate: 0.002\n",
            "Iteration 5000: Current success : 342 \t All success : 452 \t mean(l0) : 14.411505 \t Current Learning Rate: 0.000\n",
            "Iteration 5100: Current success : 561 \t All success : 614 \t mean(l0) : 13.333876 \t Current Learning Rate: 0.049\n",
            "Iteration 5200: Current success : 656 \t All success : 718 \t mean(l0) : 12.681058 \t Current Learning Rate: 0.048\n",
            "Iteration 5300: Current success : 650 \t All success : 718 \t mean(l0) : 12.649025 \t Current Learning Rate: 0.047\n",
            "Iteration 5400: Current success : 649 \t All success : 718 \t mean(l0) : 12.644847 \t Current Learning Rate: 0.046\n",
            "Iteration 5500: Current success : 665 \t All success : 731 \t mean(l0) : 12.876882 \t Current Learning Rate: 0.045\n",
            "Iteration 5600: Current success : 654 \t All success : 734 \t mean(l0) : 12.833787 \t Current Learning Rate: 0.044\n",
            "Iteration 5700: Current success : 665 \t All success : 744 \t mean(l0) : 12.877688 \t Current Learning Rate: 0.043\n",
            "Iteration 5800: Current success : 670 \t All success : 747 \t mean(l0) : 12.838018 \t Current Learning Rate: 0.042\n",
            "Iteration 5900: Current success : 657 \t All success : 747 \t mean(l0) : 12.838018 \t Current Learning Rate: 0.041\n",
            "Iteration 6000: Current success : 670 \t All success : 750 \t mean(l0) : 12.800000 \t Current Learning Rate: 0.040\n",
            "Iteration 6100: Current success : 653 \t All success : 771 \t mean(l0) : 12.643320 \t Current Learning Rate: 0.039\n",
            "Iteration 6200: Current success : 646 \t All success : 771 \t mean(l0) : 12.640726 \t Current Learning Rate: 0.038\n",
            "Iteration 6300: Current success : 648 \t All success : 772 \t mean(l0) : 12.636010 \t Current Learning Rate: 0.037\n",
            "Iteration 6400: Current success : 638 \t All success : 772 \t mean(l0) : 12.636010 \t Current Learning Rate: 0.036\n",
            "Iteration 6500: Current success : 659 \t All success : 778 \t mean(l0) : 12.619537 \t Current Learning Rate: 0.035\n",
            "Iteration 6600: Current success : 633 \t All success : 778 \t mean(l0) : 12.618252 \t Current Learning Rate: 0.034\n",
            "Iteration 6700: Current success : 640 \t All success : 779 \t mean(l0) : 12.604622 \t Current Learning Rate: 0.033\n",
            "Iteration 6800: Current success : 631 \t All success : 779 \t mean(l0) : 12.604622 \t Current Learning Rate: 0.032\n",
            "Iteration 6900: Current success : 649 \t All success : 781 \t mean(l0) : 12.580026 \t Current Learning Rate: 0.031\n",
            "Iteration 7000: Current success : 633 \t All success : 784 \t mean(l0) : 12.542091 \t Current Learning Rate: 0.030\n",
            "Iteration 7100: Current success : 661 \t All success : 810 \t mean(l0) : 12.332099 \t Current Learning Rate: 0.029\n",
            "Iteration 7200: Current success : 670 \t All success : 810 \t mean(l0) : 12.329630 \t Current Learning Rate: 0.028\n",
            "Iteration 7300: Current success : 636 \t All success : 810 \t mean(l0) : 12.329630 \t Current Learning Rate: 0.027\n",
            "Iteration 7400: Current success : 633 \t All success : 810 \t mean(l0) : 12.328396 \t Current Learning Rate: 0.026\n",
            "Iteration 7500: Current success : 658 \t All success : 811 \t mean(l0) : 12.320592 \t Current Learning Rate: 0.025\n",
            "Iteration 7600: Current success : 645 \t All success : 811 \t mean(l0) : 12.320592 \t Current Learning Rate: 0.024\n",
            "Iteration 7700: Current success : 642 \t All success : 811 \t mean(l0) : 12.320592 \t Current Learning Rate: 0.023\n",
            "Iteration 7800: Current success : 648 \t All success : 811 \t mean(l0) : 12.320592 \t Current Learning Rate: 0.022\n",
            "Iteration 7900: Current success : 648 \t All success : 811 \t mean(l0) : 12.320592 \t Current Learning Rate: 0.021\n",
            "Iteration 8000: Current success : 648 \t All success : 811 \t mean(l0) : 12.320592 \t Current Learning Rate: 0.020\n",
            "Iteration 8100: Current success : 648 \t All success : 811 \t mean(l0) : 12.319359 \t Current Learning Rate: 0.019\n",
            "Iteration 8200: Current success : 648 \t All success : 811 \t mean(l0) : 12.316893 \t Current Learning Rate: 0.018\n",
            "Iteration 8300: Current success : 648 \t All success : 811 \t mean(l0) : 12.316893 \t Current Learning Rate: 0.017\n",
            "Iteration 8400: Current success : 647 \t All success : 811 \t mean(l0) : 12.316893 \t Current Learning Rate: 0.016\n",
            "Iteration 8500: Current success : 648 \t All success : 811 \t mean(l0) : 12.316893 \t Current Learning Rate: 0.015\n",
            "Iteration 8600: Current success : 647 \t All success : 811 \t mean(l0) : 12.315660 \t Current Learning Rate: 0.014\n",
            "Iteration 8700: Current success : 647 \t All success : 811 \t mean(l0) : 12.315660 \t Current Learning Rate: 0.013\n",
            "Iteration 8800: Current success : 647 \t All success : 811 \t mean(l0) : 12.315660 \t Current Learning Rate: 0.012\n",
            "Iteration 8900: Current success : 648 \t All success : 811 \t mean(l0) : 12.314426 \t Current Learning Rate: 0.011\n",
            "Iteration 9000: Current success : 648 \t All success : 811 \t mean(l0) : 12.313193 \t Current Learning Rate: 0.010\n",
            "Iteration 9100: Current success : 648 \t All success : 811 \t mean(l0) : 12.313193 \t Current Learning Rate: 0.009\n",
            "Iteration 9200: Current success : 648 \t All success : 811 \t mean(l0) : 12.313193 \t Current Learning Rate: 0.008\n",
            "Iteration 9300: Current success : 648 \t All success : 811 \t mean(l0) : 12.313193 \t Current Learning Rate: 0.007\n",
            "Iteration 9400: Current success : 648 \t All success : 811 \t mean(l0) : 12.313193 \t Current Learning Rate: 0.006\n",
            "Iteration 9500: Current success : 648 \t All success : 811 \t mean(l0) : 12.313193 \t Current Learning Rate: 0.005\n",
            "Iteration 9600: Current success : 648 \t All success : 811 \t mean(l0) : 12.313193 \t Current Learning Rate: 0.004\n",
            "Iteration 9700: Current success : 648 \t All success : 811 \t mean(l0) : 12.313193 \t Current Learning Rate: 0.003\n",
            "Iteration 9800: Current success : 648 \t All success : 811 \t mean(l0) : 12.313193 \t Current Learning Rate: 0.002\n",
            "Iteration 9900: Current success : 648 \t All success : 811 \t mean(l0) : 12.313193 \t Current Learning Rate: 0.001\n",
            "Iteration 10000: Current success : 648 \t All success : 811 \t mean(l0) : 12.313193 \t Current Learning Rate: 0.000\n",
            "Iteration 10100: Current success : 576 \t All success : 816 \t mean(l0) : 12.502452 \t Current Learning Rate: 0.025\n",
            "Iteration 10200: Current success : 652 \t All success : 818 \t mean(l0) : 12.610025 \t Current Learning Rate: 0.024\n",
            "Iteration 10300: Current success : 653 \t All success : 818 \t mean(l0) : 12.561125 \t Current Learning Rate: 0.024\n",
            "Iteration 10400: Current success : 654 \t All success : 818 \t mean(l0) : 12.550123 \t Current Learning Rate: 0.023\n",
            "Iteration 10500: Current success : 655 \t All success : 818 \t mean(l0) : 12.536675 \t Current Learning Rate: 0.023\n",
            "Iteration 10600: Current success : 658 \t All success : 818 \t mean(l0) : 12.520783 \t Current Learning Rate: 0.022\n",
            "Iteration 10700: Current success : 657 \t All success : 818 \t mean(l0) : 12.515893 \t Current Learning Rate: 0.022\n",
            "Iteration 10800: Current success : 655 \t All success : 818 \t mean(l0) : 12.515893 \t Current Learning Rate: 0.021\n",
            "Iteration 10900: Current success : 655 \t All success : 818 \t mean(l0) : 12.515893 \t Current Learning Rate: 0.021\n",
            "Iteration 11000: Current success : 655 \t All success : 818 \t mean(l0) : 12.513448 \t Current Learning Rate: 0.020\n",
            "Iteration 11100: Current success : 657 \t All success : 818 \t mean(l0) : 12.511003 \t Current Learning Rate: 0.020\n",
            "Iteration 11200: Current success : 649 \t All success : 818 \t mean(l0) : 12.511003 \t Current Learning Rate: 0.019\n",
            "Iteration 11300: Current success : 644 \t All success : 818 \t mean(l0) : 12.509781 \t Current Learning Rate: 0.019\n",
            "Iteration 11400: Current success : 654 \t All success : 819 \t mean(l0) : 12.578755 \t Current Learning Rate: 0.018\n",
            "Iteration 11500: Current success : 653 \t All success : 819 \t mean(l0) : 12.575092 \t Current Learning Rate: 0.018\n",
            "Iteration 11600: Current success : 650 \t All success : 819 \t mean(l0) : 12.575092 \t Current Learning Rate: 0.017\n",
            "Iteration 11700: Current success : 652 \t All success : 819 \t mean(l0) : 12.573871 \t Current Learning Rate: 0.017\n",
            "Iteration 11800: Current success : 655 \t All success : 819 \t mean(l0) : 12.570208 \t Current Learning Rate: 0.016\n",
            "Iteration 11900: Current success : 653 \t All success : 819 \t mean(l0) : 12.566545 \t Current Learning Rate: 0.016\n",
            "Iteration 12000: Current success : 641 \t All success : 819 \t mean(l0) : 12.566545 \t Current Learning Rate: 0.015\n",
            "Iteration 12100: Current success : 653 \t All success : 819 \t mean(l0) : 12.562882 \t Current Learning Rate: 0.015\n",
            "Iteration 12200: Current success : 644 \t All success : 819 \t mean(l0) : 12.556777 \t Current Learning Rate: 0.014\n",
            "Iteration 12300: Current success : 654 \t All success : 819 \t mean(l0) : 12.556777 \t Current Learning Rate: 0.014\n",
            "Iteration 12400: Current success : 655 \t All success : 819 \t mean(l0) : 12.556777 \t Current Learning Rate: 0.013\n",
            "Iteration 12500: Current success : 655 \t All success : 819 \t mean(l0) : 12.556777 \t Current Learning Rate: 0.013\n",
            "Iteration 12600: Current success : 655 \t All success : 819 \t mean(l0) : 12.555556 \t Current Learning Rate: 0.012\n",
            "Iteration 12700: Current success : 646 \t All success : 819 \t mean(l0) : 12.551893 \t Current Learning Rate: 0.012\n",
            "Iteration 12800: Current success : 657 \t All success : 819 \t mean(l0) : 12.551893 \t Current Learning Rate: 0.011\n",
            "Iteration 12900: Current success : 657 \t All success : 819 \t mean(l0) : 12.551893 \t Current Learning Rate: 0.011\n",
            "Iteration 13000: Current success : 658 \t All success : 819 \t mean(l0) : 12.551893 \t Current Learning Rate: 0.010\n",
            "Iteration 13100: Current success : 658 \t All success : 819 \t mean(l0) : 12.549451 \t Current Learning Rate: 0.010\n",
            "Iteration 13200: Current success : 658 \t All success : 819 \t mean(l0) : 12.548230 \t Current Learning Rate: 0.009\n",
            "Iteration 13300: Current success : 658 \t All success : 819 \t mean(l0) : 12.548230 \t Current Learning Rate: 0.009\n",
            "Iteration 13400: Current success : 658 \t All success : 819 \t mean(l0) : 12.548230 \t Current Learning Rate: 0.008\n",
            "Iteration 13500: Current success : 658 \t All success : 819 \t mean(l0) : 12.548230 \t Current Learning Rate: 0.008\n",
            "Iteration 13600: Current success : 658 \t All success : 819 \t mean(l0) : 12.548230 \t Current Learning Rate: 0.007\n",
            "Iteration 13700: Current success : 658 \t All success : 819 \t mean(l0) : 12.545788 \t Current Learning Rate: 0.007\n",
            "Iteration 13800: Current success : 658 \t All success : 819 \t mean(l0) : 12.545788 \t Current Learning Rate: 0.006\n",
            "Iteration 13900: Current success : 658 \t All success : 819 \t mean(l0) : 12.544567 \t Current Learning Rate: 0.006\n",
            "Iteration 14000: Current success : 658 \t All success : 819 \t mean(l0) : 12.544567 \t Current Learning Rate: 0.005\n",
            "Iteration 14100: Current success : 658 \t All success : 819 \t mean(l0) : 12.544567 \t Current Learning Rate: 0.005\n",
            "Iteration 14200: Current success : 658 \t All success : 819 \t mean(l0) : 12.540904 \t Current Learning Rate: 0.004\n",
            "Iteration 14300: Current success : 658 \t All success : 819 \t mean(l0) : 12.540904 \t Current Learning Rate: 0.004\n",
            "Iteration 14400: Current success : 658 \t All success : 819 \t mean(l0) : 12.540904 \t Current Learning Rate: 0.003\n",
            "Iteration 14500: Current success : 658 \t All success : 819 \t mean(l0) : 12.540904 \t Current Learning Rate: 0.003\n",
            "Iteration 14600: Current success : 658 \t All success : 819 \t mean(l0) : 12.540904 \t Current Learning Rate: 0.002\n",
            "Iteration 14700: Current success : 658 \t All success : 819 \t mean(l0) : 12.540904 \t Current Learning Rate: 0.002\n",
            "Iteration 14800: Current success : 658 \t All success : 819 \t mean(l0) : 12.540904 \t Current Learning Rate: 0.001\n",
            "Iteration 14900: Current success : 658 \t All success : 819 \t mean(l0) : 12.540904 \t Current Learning Rate: 0.001\n",
            "Iteration 15000: Current success : 658 \t All success : 819 \t mean(l0) : 12.540904 \t Current Learning Rate: 0.000\n",
            "Iteration 15100: Current success : 591 \t All success : 820 \t mean(l0) : 12.607317 \t Current Learning Rate: 0.012\n",
            "Iteration 15200: Current success : 661 \t All success : 820 \t mean(l0) : 12.598781 \t Current Learning Rate: 0.012\n",
            "Iteration 15300: Current success : 661 \t All success : 820 \t mean(l0) : 12.590244 \t Current Learning Rate: 0.012\n",
            "Iteration 15400: Current success : 659 \t All success : 820 \t mean(l0) : 12.586585 \t Current Learning Rate: 0.012\n",
            "Iteration 15500: Current success : 658 \t All success : 820 \t mean(l0) : 12.570732 \t Current Learning Rate: 0.011\n",
            "Iteration 15600: Current success : 657 \t All success : 820 \t mean(l0) : 12.567073 \t Current Learning Rate: 0.011\n",
            "Iteration 15700: Current success : 656 \t All success : 820 \t mean(l0) : 12.565854 \t Current Learning Rate: 0.011\n",
            "Iteration 15800: Current success : 657 \t All success : 820 \t mean(l0) : 12.556098 \t Current Learning Rate: 0.011\n",
            "Iteration 15900: Current success : 657 \t All success : 820 \t mean(l0) : 12.543902 \t Current Learning Rate: 0.010\n",
            "Iteration 16000: Current success : 656 \t All success : 820 \t mean(l0) : 12.532927 \t Current Learning Rate: 0.010\n",
            "Iteration 16100: Current success : 658 \t All success : 820 \t mean(l0) : 12.525610 \t Current Learning Rate: 0.010\n",
            "Iteration 16200: Current success : 658 \t All success : 820 \t mean(l0) : 12.525610 \t Current Learning Rate: 0.010\n",
            "Iteration 16300: Current success : 661 \t All success : 820 \t mean(l0) : 12.524390 \t Current Learning Rate: 0.009\n",
            "Iteration 16400: Current success : 659 \t All success : 820 \t mean(l0) : 12.524390 \t Current Learning Rate: 0.009\n",
            "Iteration 16500: Current success : 659 \t All success : 820 \t mean(l0) : 12.523170 \t Current Learning Rate: 0.009\n",
            "Iteration 16600: Current success : 661 \t All success : 820 \t mean(l0) : 12.521952 \t Current Learning Rate: 0.009\n",
            "Iteration 16700: Current success : 658 \t All success : 820 \t mean(l0) : 12.520732 \t Current Learning Rate: 0.008\n",
            "Iteration 16800: Current success : 658 \t All success : 820 \t mean(l0) : 12.518292 \t Current Learning Rate: 0.008\n",
            "Iteration 16900: Current success : 655 \t All success : 820 \t mean(l0) : 12.518292 \t Current Learning Rate: 0.008\n",
            "Iteration 17000: Current success : 655 \t All success : 820 \t mean(l0) : 12.517074 \t Current Learning Rate: 0.008\n",
            "Iteration 17100: Current success : 655 \t All success : 820 \t mean(l0) : 12.515854 \t Current Learning Rate: 0.007\n",
            "Iteration 17200: Current success : 655 \t All success : 820 \t mean(l0) : 12.514634 \t Current Learning Rate: 0.007\n",
            "Iteration 17300: Current success : 656 \t All success : 820 \t mean(l0) : 12.514634 \t Current Learning Rate: 0.007\n",
            "Iteration 17400: Current success : 655 \t All success : 820 \t mean(l0) : 12.512196 \t Current Learning Rate: 0.007\n",
            "Iteration 17500: Current success : 655 \t All success : 820 \t mean(l0) : 12.508536 \t Current Learning Rate: 0.006\n",
            "Iteration 17600: Current success : 655 \t All success : 820 \t mean(l0) : 12.508536 \t Current Learning Rate: 0.006\n",
            "Iteration 17700: Current success : 655 \t All success : 820 \t mean(l0) : 12.507318 \t Current Learning Rate: 0.006\n",
            "Iteration 17800: Current success : 655 \t All success : 820 \t mean(l0) : 12.507318 \t Current Learning Rate: 0.006\n",
            "Iteration 17900: Current success : 655 \t All success : 820 \t mean(l0) : 12.506098 \t Current Learning Rate: 0.005\n",
            "Iteration 18000: Current success : 655 \t All success : 820 \t mean(l0) : 12.503658 \t Current Learning Rate: 0.005\n",
            "Iteration 18100: Current success : 655 \t All success : 820 \t mean(l0) : 12.498780 \t Current Learning Rate: 0.005\n",
            "Iteration 18200: Current success : 655 \t All success : 820 \t mean(l0) : 12.497561 \t Current Learning Rate: 0.005\n",
            "Iteration 18300: Current success : 655 \t All success : 820 \t mean(l0) : 12.497561 \t Current Learning Rate: 0.004\n",
            "Iteration 18400: Current success : 655 \t All success : 820 \t mean(l0) : 12.492683 \t Current Learning Rate: 0.004\n",
            "Iteration 18500: Current success : 655 \t All success : 820 \t mean(l0) : 12.492683 \t Current Learning Rate: 0.004\n",
            "Iteration 18600: Current success : 655 \t All success : 820 \t mean(l0) : 12.492683 \t Current Learning Rate: 0.004\n",
            "Iteration 18700: Current success : 655 \t All success : 820 \t mean(l0) : 12.492683 \t Current Learning Rate: 0.003\n",
            "Iteration 18800: Current success : 655 \t All success : 820 \t mean(l0) : 12.492683 \t Current Learning Rate: 0.003\n",
            "Iteration 18900: Current success : 655 \t All success : 820 \t mean(l0) : 12.492683 \t Current Learning Rate: 0.003\n",
            "Iteration 19000: Current success : 655 \t All success : 820 \t mean(l0) : 12.492683 \t Current Learning Rate: 0.003\n",
            "Iteration 19100: Current success : 655 \t All success : 820 \t mean(l0) : 12.492683 \t Current Learning Rate: 0.002\n",
            "Iteration 19200: Current success : 655 \t All success : 820 \t mean(l0) : 12.487804 \t Current Learning Rate: 0.002\n",
            "Iteration 19300: Current success : 655 \t All success : 820 \t mean(l0) : 12.486586 \t Current Learning Rate: 0.002\n",
            "Iteration 19400: Current success : 655 \t All success : 820 \t mean(l0) : 12.486586 \t Current Learning Rate: 0.002\n",
            "Iteration 19500: Current success : 655 \t All success : 820 \t mean(l0) : 12.486586 \t Current Learning Rate: 0.001\n",
            "Iteration 19600: Current success : 655 \t All success : 820 \t mean(l0) : 12.486586 \t Current Learning Rate: 0.001\n",
            "Iteration 19700: Current success : 655 \t All success : 820 \t mean(l0) : 12.486586 \t Current Learning Rate: 0.001\n",
            "Iteration 19800: Current success : 655 \t All success : 820 \t mean(l0) : 12.486586 \t Current Learning Rate: 0.001\n",
            "Iteration 19900: Current success : 655 \t All success : 820 \t mean(l0) : 12.486586 \t Current Learning Rate: 0.000\n",
            "Iteration 20000: Current success : 655 \t All success : 820 \t mean(l0) : 12.486586 \t Current Learning Rate: 0.000\n",
            "outer_step 3: all success : 965 \t mean(l0)(success) : 12.8259\n",
            "-------------------------------------------------------------\n",
            "tensor([ 0.1250,  0.1250,  0.6250,  ...,  0.6250, 55.0000,  3.2500],\n",
            "       device='cuda:0')\n",
            "Iteration 100: Current success : 78 \t All success : 226 \t mean(l0) : 4.168141 \t Current Learning Rate: 0.098\n",
            "Iteration 200: Current success : 83 \t All success : 232 \t mean(l0) : 5.129310 \t Current Learning Rate: 0.096\n",
            "Iteration 300: Current success : 90 \t All success : 241 \t mean(l0) : 6.825727 \t Current Learning Rate: 0.094\n",
            "Iteration 400: Current success : 94 \t All success : 244 \t mean(l0) : 7.315573 \t Current Learning Rate: 0.092\n",
            "Iteration 500: Current success : 95 \t All success : 247 \t mean(l0) : 7.732794 \t Current Learning Rate: 0.090\n",
            "Iteration 600: Current success : 96 \t All success : 248 \t mean(l0) : 7.951612 \t Current Learning Rate: 0.088\n",
            "Iteration 700: Current success : 98 \t All success : 252 \t mean(l0) : 8.638889 \t Current Learning Rate: 0.086\n",
            "Iteration 800: Current success : 100 \t All success : 253 \t mean(l0) : 8.830040 \t Current Learning Rate: 0.084\n",
            "Iteration 900: Current success : 98 \t All success : 254 \t mean(l0) : 8.925197 \t Current Learning Rate: 0.082\n",
            "Iteration 1000: Current success : 99 \t All success : 254 \t mean(l0) : 8.925197 \t Current Learning Rate: 0.080\n",
            "Iteration 1100: Current success : 102 \t All success : 257 \t mean(l0) : 9.404669 \t Current Learning Rate: 0.078\n",
            "Iteration 1200: Current success : 103 \t All success : 258 \t mean(l0) : 9.468992 \t Current Learning Rate: 0.076\n",
            "Iteration 1300: Current success : 104 \t All success : 258 \t mean(l0) : 9.468992 \t Current Learning Rate: 0.074\n",
            "Iteration 1400: Current success : 107 \t All success : 260 \t mean(l0) : 9.873077 \t Current Learning Rate: 0.072\n",
            "Iteration 1500: Current success : 108 \t All success : 264 \t mean(l0) : 10.450758 \t Current Learning Rate: 0.070\n",
            "Iteration 1600: Current success : 110 \t All success : 268 \t mean(l0) : 10.682836 \t Current Learning Rate: 0.068\n",
            "Iteration 1700: Current success : 111 \t All success : 270 \t mean(l0) : 10.829630 \t Current Learning Rate: 0.066\n",
            "Iteration 1800: Current success : 111 \t All success : 270 \t mean(l0) : 10.829630 \t Current Learning Rate: 0.064\n",
            "Iteration 1900: Current success : 110 \t All success : 270 \t mean(l0) : 10.829630 \t Current Learning Rate: 0.062\n",
            "Iteration 2000: Current success : 110 \t All success : 275 \t mean(l0) : 11.058182 \t Current Learning Rate: 0.060\n",
            "Iteration 2100: Current success : 111 \t All success : 275 \t mean(l0) : 11.054545 \t Current Learning Rate: 0.058\n",
            "Iteration 2200: Current success : 111 \t All success : 276 \t mean(l0) : 11.018116 \t Current Learning Rate: 0.056\n",
            "Iteration 2300: Current success : 111 \t All success : 276 \t mean(l0) : 11.018116 \t Current Learning Rate: 0.054\n",
            "Iteration 2400: Current success : 111 \t All success : 277 \t mean(l0) : 11.162455 \t Current Learning Rate: 0.052\n",
            "Iteration 2500: Current success : 111 \t All success : 277 \t mean(l0) : 11.162455 \t Current Learning Rate: 0.050\n",
            "Iteration 2600: Current success : 111 \t All success : 277 \t mean(l0) : 11.158845 \t Current Learning Rate: 0.048\n",
            "Iteration 2700: Current success : 138 \t All success : 291 \t mean(l0) : 10.776632 \t Current Learning Rate: 0.046\n",
            "Iteration 2800: Current success : 150 \t All success : 297 \t mean(l0) : 10.629629 \t Current Learning Rate: 0.044\n",
            "Iteration 2900: Current success : 151 \t All success : 299 \t mean(l0) : 10.953177 \t Current Learning Rate: 0.042\n",
            "Iteration 3000: Current success : 151 \t All success : 299 \t mean(l0) : 10.953177 \t Current Learning Rate: 0.040\n",
            "Iteration 3100: Current success : 151 \t All success : 299 \t mean(l0) : 10.953177 \t Current Learning Rate: 0.038\n",
            "Iteration 3200: Current success : 151 \t All success : 299 \t mean(l0) : 10.953177 \t Current Learning Rate: 0.036\n",
            "Iteration 3300: Current success : 151 \t All success : 299 \t mean(l0) : 10.953177 \t Current Learning Rate: 0.034\n",
            "Iteration 3400: Current success : 151 \t All success : 299 \t mean(l0) : 10.953177 \t Current Learning Rate: 0.032\n",
            "Iteration 3500: Current success : 151 \t All success : 299 \t mean(l0) : 10.953177 \t Current Learning Rate: 0.030\n",
            "Iteration 3600: Current success : 151 \t All success : 299 \t mean(l0) : 10.953177 \t Current Learning Rate: 0.028\n",
            "Iteration 3700: Current success : 151 \t All success : 299 \t mean(l0) : 10.953177 \t Current Learning Rate: 0.026\n",
            "Iteration 3800: Current success : 151 \t All success : 299 \t mean(l0) : 10.953177 \t Current Learning Rate: 0.024\n",
            "Iteration 3900: Current success : 151 \t All success : 299 \t mean(l0) : 10.953177 \t Current Learning Rate: 0.022\n",
            "Iteration 4000: Current success : 149 \t All success : 299 \t mean(l0) : 10.953177 \t Current Learning Rate: 0.020\n",
            "Iteration 4100: Current success : 149 \t All success : 299 \t mean(l0) : 10.953177 \t Current Learning Rate: 0.018\n",
            "Iteration 4200: Current success : 149 \t All success : 299 \t mean(l0) : 10.953177 \t Current Learning Rate: 0.016\n",
            "Iteration 4300: Current success : 149 \t All success : 299 \t mean(l0) : 10.953177 \t Current Learning Rate: 0.014\n",
            "Iteration 4400: Current success : 149 \t All success : 299 \t mean(l0) : 10.953177 \t Current Learning Rate: 0.012\n",
            "Iteration 4500: Current success : 149 \t All success : 299 \t mean(l0) : 10.953177 \t Current Learning Rate: 0.010\n",
            "Iteration 4600: Current success : 149 \t All success : 299 \t mean(l0) : 10.953177 \t Current Learning Rate: 0.008\n",
            "Iteration 4700: Current success : 149 \t All success : 299 \t mean(l0) : 10.953177 \t Current Learning Rate: 0.006\n",
            "Iteration 4800: Current success : 149 \t All success : 299 \t mean(l0) : 10.953177 \t Current Learning Rate: 0.004\n",
            "Iteration 4900: Current success : 149 \t All success : 299 \t mean(l0) : 10.953177 \t Current Learning Rate: 0.002\n",
            "Iteration 5000: Current success : 149 \t All success : 299 \t mean(l0) : 10.953177 \t Current Learning Rate: 0.000\n",
            "Iteration 5100: Current success : 399 \t All success : 535 \t mean(l0) : 13.854206 \t Current Learning Rate: 0.049\n",
            "Iteration 5200: Current success : 485 \t All success : 580 \t mean(l0) : 13.575862 \t Current Learning Rate: 0.048\n",
            "Iteration 5300: Current success : 482 \t All success : 592 \t mean(l0) : 13.729730 \t Current Learning Rate: 0.047\n",
            "Iteration 5400: Current success : 492 \t All success : 593 \t mean(l0) : 13.718381 \t Current Learning Rate: 0.046\n",
            "Iteration 5500: Current success : 477 \t All success : 593 \t mean(l0) : 13.715009 \t Current Learning Rate: 0.045\n",
            "Iteration 5600: Current success : 505 \t All success : 602 \t mean(l0) : 13.591362 \t Current Learning Rate: 0.044\n",
            "Iteration 5700: Current success : 478 \t All success : 606 \t mean(l0) : 13.566007 \t Current Learning Rate: 0.043\n",
            "Iteration 5800: Current success : 481 \t All success : 607 \t mean(l0) : 13.558484 \t Current Learning Rate: 0.042\n",
            "Iteration 5900: Current success : 467 \t All success : 608 \t mean(l0) : 13.547697 \t Current Learning Rate: 0.041\n",
            "Iteration 6000: Current success : 482 \t All success : 615 \t mean(l0) : 13.479674 \t Current Learning Rate: 0.040\n",
            "Iteration 6100: Current success : 473 \t All success : 615 \t mean(l0) : 13.479674 \t Current Learning Rate: 0.039\n",
            "Iteration 6200: Current success : 487 \t All success : 615 \t mean(l0) : 13.478048 \t Current Learning Rate: 0.038\n",
            "Iteration 6300: Current success : 485 \t All success : 622 \t mean(l0) : 13.538585 \t Current Learning Rate: 0.037\n",
            "Iteration 6400: Current success : 491 \t All success : 635 \t mean(l0) : 13.585827 \t Current Learning Rate: 0.036\n",
            "Iteration 6500: Current success : 469 \t All success : 635 \t mean(l0) : 13.585827 \t Current Learning Rate: 0.035\n",
            "Iteration 6600: Current success : 477 \t All success : 637 \t mean(l0) : 13.583988 \t Current Learning Rate: 0.034\n",
            "Iteration 6700: Current success : 468 \t All success : 639 \t mean(l0) : 13.561815 \t Current Learning Rate: 0.033\n",
            "Iteration 6800: Current success : 476 \t All success : 642 \t mean(l0) : 13.512461 \t Current Learning Rate: 0.032\n",
            "Iteration 6900: Current success : 482 \t All success : 681 \t mean(l0) : 13.289280 \t Current Learning Rate: 0.031\n",
            "Iteration 7000: Current success : 473 \t All success : 682 \t mean(l0) : 13.272727 \t Current Learning Rate: 0.030\n",
            "Iteration 7100: Current success : 463 \t All success : 682 \t mean(l0) : 13.272727 \t Current Learning Rate: 0.029\n",
            "Iteration 7200: Current success : 479 \t All success : 683 \t mean(l0) : 13.267936 \t Current Learning Rate: 0.028\n",
            "Iteration 7300: Current success : 463 \t All success : 683 \t mean(l0) : 13.265007 \t Current Learning Rate: 0.027\n",
            "Iteration 7400: Current success : 469 \t All success : 684 \t mean(l0) : 13.250000 \t Current Learning Rate: 0.026\n",
            "Iteration 7500: Current success : 472 \t All success : 684 \t mean(l0) : 13.250000 \t Current Learning Rate: 0.025\n",
            "Iteration 7600: Current success : 463 \t All success : 684 \t mean(l0) : 13.250000 \t Current Learning Rate: 0.024\n",
            "Iteration 7700: Current success : 484 \t All success : 684 \t mean(l0) : 13.250000 \t Current Learning Rate: 0.023\n",
            "Iteration 7800: Current success : 494 \t All success : 684 \t mean(l0) : 13.250000 \t Current Learning Rate: 0.022\n",
            "Iteration 7900: Current success : 494 \t All success : 684 \t mean(l0) : 13.250000 \t Current Learning Rate: 0.021\n",
            "Iteration 8000: Current success : 495 \t All success : 684 \t mean(l0) : 13.250000 \t Current Learning Rate: 0.020\n",
            "Iteration 8100: Current success : 495 \t All success : 684 \t mean(l0) : 13.250000 \t Current Learning Rate: 0.019\n",
            "Iteration 8200: Current success : 495 \t All success : 684 \t mean(l0) : 13.250000 \t Current Learning Rate: 0.018\n",
            "Iteration 8300: Current success : 494 \t All success : 684 \t mean(l0) : 13.248538 \t Current Learning Rate: 0.017\n",
            "Iteration 8400: Current success : 494 \t All success : 684 \t mean(l0) : 13.248538 \t Current Learning Rate: 0.016\n",
            "Iteration 8500: Current success : 495 \t All success : 684 \t mean(l0) : 13.248538 \t Current Learning Rate: 0.015\n",
            "Iteration 8600: Current success : 495 \t All success : 684 \t mean(l0) : 13.248538 \t Current Learning Rate: 0.014\n",
            "Iteration 8700: Current success : 495 \t All success : 684 \t mean(l0) : 13.248538 \t Current Learning Rate: 0.013\n",
            "Iteration 8800: Current success : 495 \t All success : 684 \t mean(l0) : 13.248538 \t Current Learning Rate: 0.012\n",
            "Iteration 8900: Current success : 495 \t All success : 684 \t mean(l0) : 13.248538 \t Current Learning Rate: 0.011\n",
            "Iteration 9000: Current success : 495 \t All success : 684 \t mean(l0) : 13.248538 \t Current Learning Rate: 0.010\n",
            "Iteration 9100: Current success : 495 \t All success : 684 \t mean(l0) : 13.248538 \t Current Learning Rate: 0.009\n",
            "Iteration 9200: Current success : 496 \t All success : 684 \t mean(l0) : 13.248538 \t Current Learning Rate: 0.008\n",
            "Iteration 9300: Current success : 496 \t All success : 684 \t mean(l0) : 13.248538 \t Current Learning Rate: 0.007\n",
            "Iteration 9400: Current success : 496 \t All success : 684 \t mean(l0) : 13.248538 \t Current Learning Rate: 0.006\n",
            "Iteration 9500: Current success : 496 \t All success : 684 \t mean(l0) : 13.248538 \t Current Learning Rate: 0.005\n",
            "Iteration 9600: Current success : 496 \t All success : 684 \t mean(l0) : 13.248538 \t Current Learning Rate: 0.004\n",
            "Iteration 9700: Current success : 496 \t All success : 684 \t mean(l0) : 13.248538 \t Current Learning Rate: 0.003\n",
            "Iteration 9800: Current success : 496 \t All success : 684 \t mean(l0) : 13.248538 \t Current Learning Rate: 0.002\n",
            "Iteration 9900: Current success : 496 \t All success : 684 \t mean(l0) : 13.248538 \t Current Learning Rate: 0.001\n",
            "Iteration 10000: Current success : 496 \t All success : 684 \t mean(l0) : 13.248538 \t Current Learning Rate: 0.000\n",
            "Iteration 10100: Current success : 407 \t All success : 694 \t mean(l0) : 13.368876 \t Current Learning Rate: 0.025\n",
            "Iteration 10200: Current success : 490 \t All success : 696 \t mean(l0) : 13.344828 \t Current Learning Rate: 0.024\n",
            "Iteration 10300: Current success : 485 \t All success : 697 \t mean(l0) : 13.410330 \t Current Learning Rate: 0.024\n",
            "Iteration 10400: Current success : 493 \t All success : 697 \t mean(l0) : 13.407460 \t Current Learning Rate: 0.023\n",
            "Iteration 10500: Current success : 487 \t All success : 697 \t mean(l0) : 13.400287 \t Current Learning Rate: 0.023\n",
            "Iteration 10600: Current success : 486 \t All success : 697 \t mean(l0) : 13.400287 \t Current Learning Rate: 0.022\n",
            "Iteration 10700: Current success : 486 \t All success : 697 \t mean(l0) : 13.384505 \t Current Learning Rate: 0.022\n",
            "Iteration 10800: Current success : 487 \t All success : 697 \t mean(l0) : 13.373028 \t Current Learning Rate: 0.021\n",
            "Iteration 10900: Current success : 483 \t All success : 697 \t mean(l0) : 13.364419 \t Current Learning Rate: 0.021\n",
            "Iteration 11000: Current success : 478 \t All success : 697 \t mean(l0) : 13.364419 \t Current Learning Rate: 0.020\n",
            "Iteration 11100: Current success : 482 \t All success : 697 \t mean(l0) : 13.364419 \t Current Learning Rate: 0.020\n",
            "Iteration 11200: Current success : 479 \t All success : 697 \t mean(l0) : 13.358680 \t Current Learning Rate: 0.019\n",
            "Iteration 11300: Current success : 484 \t All success : 697 \t mean(l0) : 13.358680 \t Current Learning Rate: 0.019\n",
            "Iteration 11400: Current success : 479 \t All success : 697 \t mean(l0) : 13.351506 \t Current Learning Rate: 0.018\n",
            "Iteration 11500: Current success : 477 \t All success : 697 \t mean(l0) : 13.350072 \t Current Learning Rate: 0.018\n",
            "Iteration 11600: Current success : 480 \t All success : 698 \t mean(l0) : 13.356734 \t Current Learning Rate: 0.017\n",
            "Iteration 11700: Current success : 478 \t All success : 698 \t mean(l0) : 13.356734 \t Current Learning Rate: 0.017\n",
            "Iteration 11800: Current success : 480 \t All success : 698 \t mean(l0) : 13.355301 \t Current Learning Rate: 0.016\n",
            "Iteration 11900: Current success : 480 \t All success : 698 \t mean(l0) : 13.349570 \t Current Learning Rate: 0.016\n",
            "Iteration 12000: Current success : 474 \t All success : 698 \t mean(l0) : 13.349570 \t Current Learning Rate: 0.015\n",
            "Iteration 12100: Current success : 479 \t All success : 698 \t mean(l0) : 13.349570 \t Current Learning Rate: 0.015\n",
            "Iteration 12200: Current success : 481 \t All success : 698 \t mean(l0) : 13.346705 \t Current Learning Rate: 0.014\n",
            "Iteration 12300: Current success : 482 \t All success : 698 \t mean(l0) : 13.346705 \t Current Learning Rate: 0.014\n",
            "Iteration 12400: Current success : 478 \t All success : 698 \t mean(l0) : 13.346705 \t Current Learning Rate: 0.013\n",
            "Iteration 12500: Current success : 483 \t All success : 698 \t mean(l0) : 13.336677 \t Current Learning Rate: 0.013\n",
            "Iteration 12600: Current success : 482 \t All success : 698 \t mean(l0) : 13.336677 \t Current Learning Rate: 0.012\n",
            "Iteration 12700: Current success : 491 \t All success : 698 \t mean(l0) : 13.336677 \t Current Learning Rate: 0.012\n",
            "Iteration 12800: Current success : 481 \t All success : 698 \t mean(l0) : 13.336677 \t Current Learning Rate: 0.011\n",
            "Iteration 12900: Current success : 484 \t All success : 698 \t mean(l0) : 13.335244 \t Current Learning Rate: 0.011\n",
            "Iteration 13000: Current success : 504 \t All success : 698 \t mean(l0) : 13.333812 \t Current Learning Rate: 0.010\n",
            "Iteration 13100: Current success : 504 \t All success : 698 \t mean(l0) : 13.330946 \t Current Learning Rate: 0.010\n",
            "Iteration 13200: Current success : 504 \t All success : 698 \t mean(l0) : 13.318052 \t Current Learning Rate: 0.009\n",
            "Iteration 13300: Current success : 505 \t All success : 698 \t mean(l0) : 13.318052 \t Current Learning Rate: 0.009\n",
            "Iteration 13400: Current success : 505 \t All success : 698 \t mean(l0) : 13.313754 \t Current Learning Rate: 0.008\n",
            "Iteration 13500: Current success : 505 \t All success : 698 \t mean(l0) : 13.312322 \t Current Learning Rate: 0.008\n",
            "Iteration 13600: Current success : 507 \t All success : 698 \t mean(l0) : 13.312322 \t Current Learning Rate: 0.007\n",
            "Iteration 13700: Current success : 507 \t All success : 698 \t mean(l0) : 13.312322 \t Current Learning Rate: 0.007\n",
            "Iteration 13800: Current success : 507 \t All success : 698 \t mean(l0) : 13.312322 \t Current Learning Rate: 0.006\n",
            "Iteration 13900: Current success : 507 \t All success : 698 \t mean(l0) : 13.312322 \t Current Learning Rate: 0.006\n",
            "Iteration 14000: Current success : 507 \t All success : 698 \t mean(l0) : 13.312322 \t Current Learning Rate: 0.005\n",
            "Iteration 14100: Current success : 507 \t All success : 698 \t mean(l0) : 13.312322 \t Current Learning Rate: 0.005\n",
            "Iteration 14200: Current success : 507 \t All success : 698 \t mean(l0) : 13.312322 \t Current Learning Rate: 0.004\n",
            "Iteration 14300: Current success : 507 \t All success : 698 \t mean(l0) : 13.312322 \t Current Learning Rate: 0.004\n",
            "Iteration 14400: Current success : 507 \t All success : 698 \t mean(l0) : 13.312322 \t Current Learning Rate: 0.003\n",
            "Iteration 14500: Current success : 506 \t All success : 698 \t mean(l0) : 13.312322 \t Current Learning Rate: 0.003\n",
            "Iteration 14600: Current success : 506 \t All success : 698 \t mean(l0) : 13.312322 \t Current Learning Rate: 0.002\n",
            "Iteration 14700: Current success : 506 \t All success : 698 \t mean(l0) : 13.312322 \t Current Learning Rate: 0.002\n",
            "Iteration 14800: Current success : 506 \t All success : 698 \t mean(l0) : 13.312322 \t Current Learning Rate: 0.001\n",
            "Iteration 14900: Current success : 506 \t All success : 698 \t mean(l0) : 13.312322 \t Current Learning Rate: 0.001\n",
            "Iteration 15000: Current success : 506 \t All success : 698 \t mean(l0) : 13.312322 \t Current Learning Rate: 0.000\n",
            "Iteration 15100: Current success : 435 \t All success : 706 \t mean(l0) : 13.604815 \t Current Learning Rate: 0.012\n",
            "Iteration 15200: Current success : 500 \t All success : 707 \t mean(l0) : 13.637907 \t Current Learning Rate: 0.012\n",
            "Iteration 15300: Current success : 509 \t All success : 707 \t mean(l0) : 13.633663 \t Current Learning Rate: 0.012\n",
            "Iteration 15400: Current success : 513 \t All success : 707 \t mean(l0) : 13.630835 \t Current Learning Rate: 0.012\n",
            "Iteration 15500: Current success : 511 \t All success : 707 \t mean(l0) : 13.629420 \t Current Learning Rate: 0.011\n",
            "Iteration 15600: Current success : 510 \t All success : 707 \t mean(l0) : 13.629420 \t Current Learning Rate: 0.011\n",
            "Iteration 15700: Current success : 511 \t All success : 707 \t mean(l0) : 13.629420 \t Current Learning Rate: 0.011\n",
            "Iteration 15800: Current success : 511 \t All success : 707 \t mean(l0) : 13.629420 \t Current Learning Rate: 0.011\n",
            "Iteration 15900: Current success : 511 \t All success : 707 \t mean(l0) : 13.629420 \t Current Learning Rate: 0.010\n",
            "Iteration 16000: Current success : 509 \t All success : 707 \t mean(l0) : 13.622348 \t Current Learning Rate: 0.010\n",
            "Iteration 16100: Current success : 510 \t All success : 707 \t mean(l0) : 13.622348 \t Current Learning Rate: 0.010\n",
            "Iteration 16200: Current success : 511 \t All success : 707 \t mean(l0) : 13.613862 \t Current Learning Rate: 0.010\n",
            "Iteration 16300: Current success : 511 \t All success : 707 \t mean(l0) : 13.613862 \t Current Learning Rate: 0.009\n",
            "Iteration 16400: Current success : 511 \t All success : 707 \t mean(l0) : 13.613862 \t Current Learning Rate: 0.009\n",
            "Iteration 16500: Current success : 511 \t All success : 707 \t mean(l0) : 13.613862 \t Current Learning Rate: 0.009\n",
            "Iteration 16600: Current success : 510 \t All success : 707 \t mean(l0) : 13.613862 \t Current Learning Rate: 0.009\n",
            "Iteration 16700: Current success : 510 \t All success : 707 \t mean(l0) : 13.612447 \t Current Learning Rate: 0.008\n",
            "Iteration 16800: Current success : 500 \t All success : 708 \t mean(l0) : 13.672317 \t Current Learning Rate: 0.008\n",
            "Iteration 16900: Current success : 499 \t All success : 708 \t mean(l0) : 13.672317 \t Current Learning Rate: 0.008\n",
            "Iteration 17000: Current success : 499 \t All success : 708 \t mean(l0) : 13.672317 \t Current Learning Rate: 0.008\n",
            "Iteration 17100: Current success : 510 \t All success : 708 \t mean(l0) : 13.672317 \t Current Learning Rate: 0.007\n",
            "Iteration 17200: Current success : 499 \t All success : 708 \t mean(l0) : 13.672317 \t Current Learning Rate: 0.007\n",
            "Iteration 17300: Current success : 510 \t All success : 708 \t mean(l0) : 13.672317 \t Current Learning Rate: 0.007\n",
            "Iteration 17400: Current success : 509 \t All success : 708 \t mean(l0) : 13.672317 \t Current Learning Rate: 0.007\n",
            "Iteration 17500: Current success : 509 \t All success : 708 \t mean(l0) : 13.672317 \t Current Learning Rate: 0.006\n",
            "Iteration 17600: Current success : 509 \t All success : 708 \t mean(l0) : 13.672317 \t Current Learning Rate: 0.006\n",
            "Iteration 17700: Current success : 509 \t All success : 708 \t mean(l0) : 13.672317 \t Current Learning Rate: 0.006\n",
            "Iteration 17800: Current success : 498 \t All success : 708 \t mean(l0) : 13.672317 \t Current Learning Rate: 0.006\n",
            "Iteration 17900: Current success : 509 \t All success : 708 \t mean(l0) : 13.672317 \t Current Learning Rate: 0.005\n",
            "Iteration 18000: Current success : 509 \t All success : 708 \t mean(l0) : 13.672317 \t Current Learning Rate: 0.005\n",
            "Iteration 18100: Current success : 509 \t All success : 708 \t mean(l0) : 13.672317 \t Current Learning Rate: 0.005\n",
            "Iteration 18200: Current success : 509 \t All success : 708 \t mean(l0) : 13.672317 \t Current Learning Rate: 0.005\n",
            "Iteration 18300: Current success : 509 \t All success : 708 \t mean(l0) : 13.672317 \t Current Learning Rate: 0.004\n",
            "Iteration 18400: Current success : 509 \t All success : 708 \t mean(l0) : 13.672317 \t Current Learning Rate: 0.004\n",
            "Iteration 18500: Current success : 509 \t All success : 708 \t mean(l0) : 13.672317 \t Current Learning Rate: 0.004\n",
            "Iteration 18600: Current success : 509 \t All success : 708 \t mean(l0) : 13.672317 \t Current Learning Rate: 0.004\n",
            "Iteration 18700: Current success : 509 \t All success : 708 \t mean(l0) : 13.672317 \t Current Learning Rate: 0.003\n",
            "Iteration 18800: Current success : 509 \t All success : 708 \t mean(l0) : 13.672317 \t Current Learning Rate: 0.003\n",
            "Iteration 18900: Current success : 509 \t All success : 708 \t mean(l0) : 13.672317 \t Current Learning Rate: 0.003\n",
            "Iteration 19000: Current success : 509 \t All success : 708 \t mean(l0) : 13.672317 \t Current Learning Rate: 0.003\n",
            "Iteration 19100: Current success : 509 \t All success : 708 \t mean(l0) : 13.672317 \t Current Learning Rate: 0.002\n",
            "Iteration 19200: Current success : 509 \t All success : 708 \t mean(l0) : 13.672317 \t Current Learning Rate: 0.002\n",
            "Iteration 19300: Current success : 509 \t All success : 708 \t mean(l0) : 13.672317 \t Current Learning Rate: 0.002\n",
            "Iteration 19400: Current success : 509 \t All success : 708 \t mean(l0) : 13.672317 \t Current Learning Rate: 0.002\n",
            "Iteration 19500: Current success : 509 \t All success : 708 \t mean(l0) : 13.672317 \t Current Learning Rate: 0.001\n",
            "Iteration 19600: Current success : 509 \t All success : 708 \t mean(l0) : 13.672317 \t Current Learning Rate: 0.001\n",
            "Iteration 19700: Current success : 509 \t All success : 708 \t mean(l0) : 13.672317 \t Current Learning Rate: 0.001\n",
            "Iteration 19800: Current success : 510 \t All success : 708 \t mean(l0) : 13.672317 \t Current Learning Rate: 0.001\n",
            "Iteration 19900: Current success : 510 \t All success : 708 \t mean(l0) : 13.672317 \t Current Learning Rate: 0.000\n",
            "Iteration 20000: Current success : 510 \t All success : 708 \t mean(l0) : 13.672317 \t Current Learning Rate: 0.000\n",
            "outer_step 4: all success : 980 \t mean(l0)(success) : 13.3939\n",
            "-------------------------------------------------------------\n",
            "tensor([6.2500e-02, 1.8750e-01, 5.6250e-01,  ..., 5.6250e-01, 7.7500e+01,\n",
            "        2.1250e+00], device='cuda:0')\n",
            "Iteration 100: Current success : 888 \t All success : 894 \t mean(l0) : 9.548099 \t Current Learning Rate: 0.098\n",
            "Iteration 200: Current success : 889 \t All success : 905 \t mean(l0) : 9.891712 \t Current Learning Rate: 0.096\n",
            "Iteration 300: Current success : 895 \t All success : 911 \t mean(l0) : 10.205269 \t Current Learning Rate: 0.094\n",
            "Iteration 400: Current success : 898 \t All success : 918 \t mean(l0) : 10.473856 \t Current Learning Rate: 0.092\n",
            "Iteration 500: Current success : 902 \t All success : 922 \t mean(l0) : 10.648590 \t Current Learning Rate: 0.090\n",
            "Iteration 600: Current success : 921 \t All success : 926 \t mean(l0) : 10.861771 \t Current Learning Rate: 0.088\n",
            "Iteration 700: Current success : 916 \t All success : 927 \t mean(l0) : 10.901834 \t Current Learning Rate: 0.086\n",
            "Iteration 800: Current success : 922 \t All success : 929 \t mean(l0) : 11.023681 \t Current Learning Rate: 0.084\n",
            "Iteration 900: Current success : 925 \t All success : 929 \t mean(l0) : 11.021528 \t Current Learning Rate: 0.082\n",
            "Iteration 1000: Current success : 925 \t All success : 930 \t mean(l0) : 11.072043 \t Current Learning Rate: 0.080\n",
            "Iteration 1100: Current success : 926 \t All success : 930 \t mean(l0) : 11.072043 \t Current Learning Rate: 0.078\n",
            "Iteration 1200: Current success : 926 \t All success : 931 \t mean(l0) : 11.122449 \t Current Learning Rate: 0.076\n",
            "Iteration 1300: Current success : 928 \t All success : 931 \t mean(l0) : 11.122449 \t Current Learning Rate: 0.074\n",
            "Iteration 1400: Current success : 928 \t All success : 931 \t mean(l0) : 11.122449 \t Current Learning Rate: 0.072\n",
            "Iteration 1500: Current success : 927 \t All success : 931 \t mean(l0) : 11.122449 \t Current Learning Rate: 0.070\n",
            "Iteration 1600: Current success : 928 \t All success : 931 \t mean(l0) : 11.122449 \t Current Learning Rate: 0.068\n",
            "Iteration 1700: Current success : 929 \t All success : 931 \t mean(l0) : 11.122449 \t Current Learning Rate: 0.066\n",
            "Iteration 1800: Current success : 928 \t All success : 931 \t mean(l0) : 11.122449 \t Current Learning Rate: 0.064\n",
            "Iteration 1900: Current success : 927 \t All success : 931 \t mean(l0) : 11.122449 \t Current Learning Rate: 0.062\n",
            "Iteration 2000: Current success : 927 \t All success : 931 \t mean(l0) : 11.122449 \t Current Learning Rate: 0.060\n",
            "Iteration 2100: Current success : 929 \t All success : 931 \t mean(l0) : 11.122449 \t Current Learning Rate: 0.058\n",
            "Iteration 2200: Current success : 927 \t All success : 931 \t mean(l0) : 11.122449 \t Current Learning Rate: 0.056\n",
            "Iteration 2300: Current success : 927 \t All success : 931 \t mean(l0) : 11.122449 \t Current Learning Rate: 0.054\n",
            "Iteration 2400: Current success : 927 \t All success : 931 \t mean(l0) : 11.122449 \t Current Learning Rate: 0.052\n",
            "Iteration 2500: Current success : 928 \t All success : 931 \t mean(l0) : 11.122449 \t Current Learning Rate: 0.050\n",
            "Iteration 2600: Current success : 927 \t All success : 931 \t mean(l0) : 11.122449 \t Current Learning Rate: 0.048\n",
            "Iteration 2700: Current success : 927 \t All success : 931 \t mean(l0) : 11.122449 \t Current Learning Rate: 0.046\n",
            "Iteration 2800: Current success : 928 \t All success : 931 \t mean(l0) : 11.122449 \t Current Learning Rate: 0.044\n",
            "Iteration 2900: Current success : 927 \t All success : 931 \t mean(l0) : 11.122449 \t Current Learning Rate: 0.042\n",
            "Iteration 3000: Current success : 927 \t All success : 931 \t mean(l0) : 11.122449 \t Current Learning Rate: 0.040\n",
            "Iteration 3100: Current success : 927 \t All success : 931 \t mean(l0) : 11.122449 \t Current Learning Rate: 0.038\n",
            "Iteration 3200: Current success : 927 \t All success : 931 \t mean(l0) : 11.122449 \t Current Learning Rate: 0.036\n",
            "Iteration 3300: Current success : 927 \t All success : 931 \t mean(l0) : 11.122449 \t Current Learning Rate: 0.034\n",
            "Iteration 3400: Current success : 927 \t All success : 931 \t mean(l0) : 11.122449 \t Current Learning Rate: 0.032\n",
            "Iteration 3500: Current success : 928 \t All success : 931 \t mean(l0) : 11.122449 \t Current Learning Rate: 0.030\n",
            "Iteration 3600: Current success : 928 \t All success : 931 \t mean(l0) : 11.122449 \t Current Learning Rate: 0.028\n",
            "Iteration 3700: Current success : 927 \t All success : 931 \t mean(l0) : 11.122449 \t Current Learning Rate: 0.026\n",
            "Iteration 3800: Current success : 927 \t All success : 931 \t mean(l0) : 11.122449 \t Current Learning Rate: 0.024\n",
            "Iteration 3900: Current success : 927 \t All success : 931 \t mean(l0) : 11.122449 \t Current Learning Rate: 0.022\n",
            "Iteration 4000: Current success : 927 \t All success : 931 \t mean(l0) : 11.122449 \t Current Learning Rate: 0.020\n",
            "Iteration 4100: Current success : 927 \t All success : 931 \t mean(l0) : 11.122449 \t Current Learning Rate: 0.018\n",
            "Iteration 4200: Current success : 927 \t All success : 931 \t mean(l0) : 11.122449 \t Current Learning Rate: 0.016\n",
            "Iteration 4300: Current success : 927 \t All success : 931 \t mean(l0) : 11.122449 \t Current Learning Rate: 0.014\n",
            "Iteration 4400: Current success : 927 \t All success : 931 \t mean(l0) : 11.122449 \t Current Learning Rate: 0.012\n",
            "Iteration 4500: Current success : 927 \t All success : 931 \t mean(l0) : 11.122449 \t Current Learning Rate: 0.010\n",
            "Iteration 4600: Current success : 927 \t All success : 931 \t mean(l0) : 11.122449 \t Current Learning Rate: 0.008\n",
            "Iteration 4700: Current success : 927 \t All success : 931 \t mean(l0) : 11.122449 \t Current Learning Rate: 0.006\n",
            "Iteration 4800: Current success : 927 \t All success : 931 \t mean(l0) : 11.122449 \t Current Learning Rate: 0.004\n",
            "Iteration 4900: Current success : 927 \t All success : 931 \t mean(l0) : 11.122449 \t Current Learning Rate: 0.002\n",
            "Iteration 5000: Current success : 927 \t All success : 931 \t mean(l0) : 11.122449 \t Current Learning Rate: 0.000\n",
            "Iteration 5100: Current success : 929 \t All success : 934 \t mean(l0) : 11.270878 \t Current Learning Rate: 0.049\n",
            "Iteration 5200: Current success : 929 \t All success : 935 \t mean(l0) : 11.349732 \t Current Learning Rate: 0.048\n",
            "Iteration 5300: Current success : 930 \t All success : 935 \t mean(l0) : 11.346524 \t Current Learning Rate: 0.047\n",
            "Iteration 5400: Current success : 934 \t All success : 939 \t mean(l0) : 11.564430 \t Current Learning Rate: 0.046\n",
            "Iteration 5500: Current success : 934 \t All success : 939 \t mean(l0) : 11.564430 \t Current Learning Rate: 0.045\n",
            "Iteration 5600: Current success : 934 \t All success : 939 \t mean(l0) : 11.564430 \t Current Learning Rate: 0.044\n",
            "Iteration 5700: Current success : 934 \t All success : 939 \t mean(l0) : 11.564430 \t Current Learning Rate: 0.043\n",
            "Iteration 5800: Current success : 934 \t All success : 939 \t mean(l0) : 11.564430 \t Current Learning Rate: 0.042\n",
            "Iteration 5900: Current success : 934 \t All success : 939 \t mean(l0) : 11.564430 \t Current Learning Rate: 0.041\n",
            "Iteration 6000: Current success : 934 \t All success : 939 \t mean(l0) : 11.564430 \t Current Learning Rate: 0.040\n",
            "Iteration 6100: Current success : 934 \t All success : 939 \t mean(l0) : 11.564430 \t Current Learning Rate: 0.039\n",
            "Iteration 6200: Current success : 934 \t All success : 939 \t mean(l0) : 11.564430 \t Current Learning Rate: 0.038\n",
            "Iteration 6300: Current success : 934 \t All success : 939 \t mean(l0) : 11.564430 \t Current Learning Rate: 0.037\n",
            "Iteration 6400: Current success : 934 \t All success : 939 \t mean(l0) : 11.564430 \t Current Learning Rate: 0.036\n",
            "Iteration 6500: Current success : 934 \t All success : 939 \t mean(l0) : 11.564430 \t Current Learning Rate: 0.035\n",
            "Iteration 6600: Current success : 934 \t All success : 939 \t mean(l0) : 11.564430 \t Current Learning Rate: 0.034\n",
            "Iteration 6700: Current success : 934 \t All success : 939 \t mean(l0) : 11.564430 \t Current Learning Rate: 0.033\n",
            "Iteration 6800: Current success : 934 \t All success : 939 \t mean(l0) : 11.564430 \t Current Learning Rate: 0.032\n",
            "Iteration 6900: Current success : 934 \t All success : 939 \t mean(l0) : 11.564430 \t Current Learning Rate: 0.031\n",
            "Iteration 7000: Current success : 934 \t All success : 939 \t mean(l0) : 11.564430 \t Current Learning Rate: 0.030\n",
            "Iteration 7100: Current success : 934 \t All success : 939 \t mean(l0) : 11.564430 \t Current Learning Rate: 0.029\n",
            "Iteration 7200: Current success : 934 \t All success : 939 \t mean(l0) : 11.564430 \t Current Learning Rate: 0.028\n",
            "Iteration 7300: Current success : 934 \t All success : 939 \t mean(l0) : 11.564430 \t Current Learning Rate: 0.027\n",
            "Iteration 7400: Current success : 934 \t All success : 939 \t mean(l0) : 11.564430 \t Current Learning Rate: 0.026\n",
            "Iteration 7500: Current success : 934 \t All success : 939 \t mean(l0) : 11.564430 \t Current Learning Rate: 0.025\n",
            "Iteration 7600: Current success : 934 \t All success : 939 \t mean(l0) : 11.564430 \t Current Learning Rate: 0.024\n",
            "Iteration 7700: Current success : 934 \t All success : 939 \t mean(l0) : 11.564430 \t Current Learning Rate: 0.023\n",
            "Iteration 7800: Current success : 934 \t All success : 939 \t mean(l0) : 11.564430 \t Current Learning Rate: 0.022\n",
            "Iteration 7900: Current success : 934 \t All success : 939 \t mean(l0) : 11.564430 \t Current Learning Rate: 0.021\n",
            "Iteration 8000: Current success : 934 \t All success : 939 \t mean(l0) : 11.564430 \t Current Learning Rate: 0.020\n",
            "Iteration 8100: Current success : 934 \t All success : 939 \t mean(l0) : 11.564430 \t Current Learning Rate: 0.019\n",
            "Iteration 8200: Current success : 934 \t All success : 939 \t mean(l0) : 11.564430 \t Current Learning Rate: 0.018\n",
            "Iteration 8300: Current success : 934 \t All success : 939 \t mean(l0) : 11.564430 \t Current Learning Rate: 0.017\n",
            "Iteration 8400: Current success : 934 \t All success : 939 \t mean(l0) : 11.564430 \t Current Learning Rate: 0.016\n",
            "Iteration 8500: Current success : 934 \t All success : 939 \t mean(l0) : 11.564430 \t Current Learning Rate: 0.015\n",
            "Iteration 8600: Current success : 934 \t All success : 939 \t mean(l0) : 11.564430 \t Current Learning Rate: 0.014\n",
            "Iteration 8700: Current success : 934 \t All success : 939 \t mean(l0) : 11.564430 \t Current Learning Rate: 0.013\n",
            "Iteration 8800: Current success : 934 \t All success : 939 \t mean(l0) : 11.564430 \t Current Learning Rate: 0.012\n",
            "Iteration 8900: Current success : 934 \t All success : 939 \t mean(l0) : 11.564430 \t Current Learning Rate: 0.011\n",
            "Iteration 9000: Current success : 934 \t All success : 939 \t mean(l0) : 11.564430 \t Current Learning Rate: 0.010\n",
            "Iteration 9100: Current success : 934 \t All success : 939 \t mean(l0) : 11.564430 \t Current Learning Rate: 0.009\n",
            "Iteration 9200: Current success : 934 \t All success : 939 \t mean(l0) : 11.564430 \t Current Learning Rate: 0.008\n",
            "Iteration 9300: Current success : 934 \t All success : 939 \t mean(l0) : 11.564430 \t Current Learning Rate: 0.007\n",
            "Iteration 9400: Current success : 934 \t All success : 939 \t mean(l0) : 11.564430 \t Current Learning Rate: 0.006\n",
            "Iteration 9500: Current success : 934 \t All success : 939 \t mean(l0) : 11.564430 \t Current Learning Rate: 0.005\n",
            "Iteration 9600: Current success : 934 \t All success : 939 \t mean(l0) : 11.564430 \t Current Learning Rate: 0.004\n",
            "Iteration 9700: Current success : 934 \t All success : 939 \t mean(l0) : 11.564430 \t Current Learning Rate: 0.003\n",
            "Iteration 9800: Current success : 934 \t All success : 939 \t mean(l0) : 11.564430 \t Current Learning Rate: 0.002\n",
            "Iteration 9900: Current success : 934 \t All success : 939 \t mean(l0) : 11.564430 \t Current Learning Rate: 0.001\n",
            "Iteration 10000: Current success : 934 \t All success : 939 \t mean(l0) : 11.564430 \t Current Learning Rate: 0.000\n",
            "Iteration 10100: Current success : 936 \t All success : 941 \t mean(l0) : 11.743890 \t Current Learning Rate: 0.025\n",
            "Iteration 10200: Current success : 937 \t All success : 942 \t mean(l0) : 11.801487 \t Current Learning Rate: 0.024\n",
            "Iteration 10300: Current success : 937 \t All success : 942 \t mean(l0) : 11.801487 \t Current Learning Rate: 0.024\n",
            "Iteration 10400: Current success : 935 \t All success : 942 \t mean(l0) : 11.801487 \t Current Learning Rate: 0.023\n",
            "Iteration 10500: Current success : 935 \t All success : 942 \t mean(l0) : 11.801487 \t Current Learning Rate: 0.023\n",
            "Iteration 10600: Current success : 935 \t All success : 942 \t mean(l0) : 11.801487 \t Current Learning Rate: 0.022\n",
            "Iteration 10700: Current success : 935 \t All success : 942 \t mean(l0) : 11.801487 \t Current Learning Rate: 0.022\n",
            "Iteration 10800: Current success : 935 \t All success : 942 \t mean(l0) : 11.801487 \t Current Learning Rate: 0.021\n",
            "Iteration 10900: Current success : 935 \t All success : 942 \t mean(l0) : 11.801487 \t Current Learning Rate: 0.021\n",
            "Iteration 11000: Current success : 935 \t All success : 942 \t mean(l0) : 11.801487 \t Current Learning Rate: 0.020\n",
            "Iteration 11100: Current success : 935 \t All success : 942 \t mean(l0) : 11.801487 \t Current Learning Rate: 0.020\n",
            "Iteration 11200: Current success : 935 \t All success : 942 \t mean(l0) : 11.801487 \t Current Learning Rate: 0.019\n",
            "Iteration 11300: Current success : 935 \t All success : 942 \t mean(l0) : 11.801487 \t Current Learning Rate: 0.019\n",
            "Iteration 11400: Current success : 937 \t All success : 942 \t mean(l0) : 11.801487 \t Current Learning Rate: 0.018\n",
            "Iteration 11500: Current success : 937 \t All success : 942 \t mean(l0) : 11.801487 \t Current Learning Rate: 0.018\n",
            "Iteration 11600: Current success : 937 \t All success : 942 \t mean(l0) : 11.801487 \t Current Learning Rate: 0.017\n",
            "Iteration 11700: Current success : 937 \t All success : 942 \t mean(l0) : 11.801487 \t Current Learning Rate: 0.017\n",
            "Iteration 11800: Current success : 937 \t All success : 942 \t mean(l0) : 11.801487 \t Current Learning Rate: 0.016\n",
            "Iteration 11900: Current success : 937 \t All success : 942 \t mean(l0) : 11.801487 \t Current Learning Rate: 0.016\n",
            "Iteration 12000: Current success : 937 \t All success : 942 \t mean(l0) : 11.801487 \t Current Learning Rate: 0.015\n",
            "Iteration 12100: Current success : 937 \t All success : 942 \t mean(l0) : 11.801487 \t Current Learning Rate: 0.015\n",
            "Iteration 12200: Current success : 937 \t All success : 942 \t mean(l0) : 11.801487 \t Current Learning Rate: 0.014\n",
            "Iteration 12300: Current success : 937 \t All success : 942 \t mean(l0) : 11.801487 \t Current Learning Rate: 0.014\n",
            "Iteration 12400: Current success : 938 \t All success : 943 \t mean(l0) : 11.867445 \t Current Learning Rate: 0.013\n",
            "Iteration 12500: Current success : 937 \t All success : 943 \t mean(l0) : 11.867445 \t Current Learning Rate: 0.013\n",
            "Iteration 12600: Current success : 937 \t All success : 943 \t mean(l0) : 11.867445 \t Current Learning Rate: 0.012\n",
            "Iteration 12700: Current success : 937 \t All success : 943 \t mean(l0) : 11.867445 \t Current Learning Rate: 0.012\n",
            "Iteration 12800: Current success : 937 \t All success : 943 \t mean(l0) : 11.867445 \t Current Learning Rate: 0.011\n",
            "Iteration 12900: Current success : 937 \t All success : 943 \t mean(l0) : 11.867445 \t Current Learning Rate: 0.011\n",
            "Iteration 13000: Current success : 937 \t All success : 943 \t mean(l0) : 11.867445 \t Current Learning Rate: 0.010\n",
            "Iteration 13100: Current success : 937 \t All success : 943 \t mean(l0) : 11.867445 \t Current Learning Rate: 0.010\n",
            "Iteration 13200: Current success : 937 \t All success : 943 \t mean(l0) : 11.867445 \t Current Learning Rate: 0.009\n",
            "Iteration 13300: Current success : 937 \t All success : 943 \t mean(l0) : 11.867445 \t Current Learning Rate: 0.009\n",
            "Iteration 13400: Current success : 937 \t All success : 943 \t mean(l0) : 11.867445 \t Current Learning Rate: 0.008\n",
            "Iteration 13500: Current success : 937 \t All success : 943 \t mean(l0) : 11.867445 \t Current Learning Rate: 0.008\n",
            "Iteration 13600: Current success : 937 \t All success : 943 \t mean(l0) : 11.867445 \t Current Learning Rate: 0.007\n",
            "Iteration 13700: Current success : 937 \t All success : 943 \t mean(l0) : 11.867445 \t Current Learning Rate: 0.007\n",
            "Iteration 13800: Current success : 937 \t All success : 943 \t mean(l0) : 11.867445 \t Current Learning Rate: 0.006\n",
            "Iteration 13900: Current success : 937 \t All success : 943 \t mean(l0) : 11.867445 \t Current Learning Rate: 0.006\n",
            "Iteration 14000: Current success : 937 \t All success : 943 \t mean(l0) : 11.867445 \t Current Learning Rate: 0.005\n",
            "Iteration 14100: Current success : 937 \t All success : 943 \t mean(l0) : 11.867445 \t Current Learning Rate: 0.005\n",
            "Iteration 14200: Current success : 937 \t All success : 943 \t mean(l0) : 11.867445 \t Current Learning Rate: 0.004\n",
            "Iteration 14300: Current success : 937 \t All success : 943 \t mean(l0) : 11.867445 \t Current Learning Rate: 0.004\n",
            "Iteration 14400: Current success : 937 \t All success : 943 \t mean(l0) : 11.867445 \t Current Learning Rate: 0.003\n",
            "Iteration 14500: Current success : 937 \t All success : 943 \t mean(l0) : 11.867445 \t Current Learning Rate: 0.003\n",
            "Iteration 14600: Current success : 937 \t All success : 943 \t mean(l0) : 11.867445 \t Current Learning Rate: 0.002\n",
            "Iteration 14700: Current success : 937 \t All success : 943 \t mean(l0) : 11.867445 \t Current Learning Rate: 0.002\n",
            "Iteration 14800: Current success : 937 \t All success : 943 \t mean(l0) : 11.867445 \t Current Learning Rate: 0.001\n",
            "Iteration 14900: Current success : 937 \t All success : 943 \t mean(l0) : 11.867445 \t Current Learning Rate: 0.001\n",
            "Iteration 15000: Current success : 937 \t All success : 943 \t mean(l0) : 11.867445 \t Current Learning Rate: 0.000\n",
            "Iteration 15100: Current success : 935 \t All success : 943 \t mean(l0) : 11.867445 \t Current Learning Rate: 0.012\n",
            "Iteration 15200: Current success : 935 \t All success : 943 \t mean(l0) : 11.867445 \t Current Learning Rate: 0.012\n",
            "Iteration 15300: Current success : 937 \t All success : 943 \t mean(l0) : 11.867445 \t Current Learning Rate: 0.012\n",
            "Iteration 15400: Current success : 937 \t All success : 943 \t mean(l0) : 11.867445 \t Current Learning Rate: 0.012\n",
            "Iteration 15500: Current success : 937 \t All success : 943 \t mean(l0) : 11.867445 \t Current Learning Rate: 0.011\n",
            "Iteration 15600: Current success : 938 \t All success : 944 \t mean(l0) : 11.923729 \t Current Learning Rate: 0.011\n",
            "Iteration 15700: Current success : 938 \t All success : 944 \t mean(l0) : 11.923729 \t Current Learning Rate: 0.011\n",
            "Iteration 15800: Current success : 938 \t All success : 944 \t mean(l0) : 11.923729 \t Current Learning Rate: 0.011\n",
            "Iteration 15900: Current success : 938 \t All success : 944 \t mean(l0) : 11.923729 \t Current Learning Rate: 0.010\n",
            "Iteration 16000: Current success : 938 \t All success : 944 \t mean(l0) : 11.923729 \t Current Learning Rate: 0.010\n",
            "Iteration 16100: Current success : 938 \t All success : 944 \t mean(l0) : 11.923729 \t Current Learning Rate: 0.010\n",
            "Iteration 16200: Current success : 938 \t All success : 944 \t mean(l0) : 11.923729 \t Current Learning Rate: 0.010\n",
            "Iteration 16300: Current success : 938 \t All success : 944 \t mean(l0) : 11.923729 \t Current Learning Rate: 0.009\n",
            "Iteration 16400: Current success : 938 \t All success : 944 \t mean(l0) : 11.923729 \t Current Learning Rate: 0.009\n",
            "Iteration 16500: Current success : 938 \t All success : 944 \t mean(l0) : 11.923729 \t Current Learning Rate: 0.009\n",
            "Iteration 16600: Current success : 938 \t All success : 944 \t mean(l0) : 11.923729 \t Current Learning Rate: 0.009\n",
            "Iteration 16700: Current success : 938 \t All success : 944 \t mean(l0) : 11.923729 \t Current Learning Rate: 0.008\n",
            "Iteration 16800: Current success : 938 \t All success : 944 \t mean(l0) : 11.923729 \t Current Learning Rate: 0.008\n",
            "Iteration 16900: Current success : 938 \t All success : 944 \t mean(l0) : 11.923729 \t Current Learning Rate: 0.008\n",
            "Iteration 17000: Current success : 938 \t All success : 944 \t mean(l0) : 11.923729 \t Current Learning Rate: 0.008\n",
            "Iteration 17100: Current success : 938 \t All success : 944 \t mean(l0) : 11.923729 \t Current Learning Rate: 0.007\n",
            "Iteration 17200: Current success : 938 \t All success : 944 \t mean(l0) : 11.923729 \t Current Learning Rate: 0.007\n",
            "Iteration 17300: Current success : 938 \t All success : 944 \t mean(l0) : 11.923729 \t Current Learning Rate: 0.007\n",
            "Iteration 17400: Current success : 938 \t All success : 944 \t mean(l0) : 11.923729 \t Current Learning Rate: 0.007\n",
            "Iteration 17500: Current success : 938 \t All success : 944 \t mean(l0) : 11.923729 \t Current Learning Rate: 0.006\n",
            "Iteration 17600: Current success : 938 \t All success : 944 \t mean(l0) : 11.923729 \t Current Learning Rate: 0.006\n",
            "Iteration 17700: Current success : 938 \t All success : 944 \t mean(l0) : 11.923729 \t Current Learning Rate: 0.006\n",
            "Iteration 17800: Current success : 938 \t All success : 944 \t mean(l0) : 11.923729 \t Current Learning Rate: 0.006\n",
            "Iteration 17900: Current success : 938 \t All success : 944 \t mean(l0) : 11.923729 \t Current Learning Rate: 0.005\n",
            "Iteration 18000: Current success : 938 \t All success : 944 \t mean(l0) : 11.923729 \t Current Learning Rate: 0.005\n",
            "Iteration 18100: Current success : 938 \t All success : 944 \t mean(l0) : 11.923729 \t Current Learning Rate: 0.005\n",
            "Iteration 18200: Current success : 938 \t All success : 944 \t mean(l0) : 11.923729 \t Current Learning Rate: 0.005\n",
            "Iteration 18300: Current success : 938 \t All success : 944 \t mean(l0) : 11.923729 \t Current Learning Rate: 0.004\n",
            "Iteration 18400: Current success : 938 \t All success : 944 \t mean(l0) : 11.923729 \t Current Learning Rate: 0.004\n",
            "Iteration 18500: Current success : 938 \t All success : 944 \t mean(l0) : 11.923729 \t Current Learning Rate: 0.004\n",
            "Iteration 18600: Current success : 938 \t All success : 944 \t mean(l0) : 11.923729 \t Current Learning Rate: 0.004\n",
            "Iteration 18700: Current success : 938 \t All success : 944 \t mean(l0) : 11.923729 \t Current Learning Rate: 0.003\n",
            "Iteration 18800: Current success : 938 \t All success : 944 \t mean(l0) : 11.923729 \t Current Learning Rate: 0.003\n",
            "Iteration 18900: Current success : 938 \t All success : 944 \t mean(l0) : 11.923729 \t Current Learning Rate: 0.003\n",
            "Iteration 19000: Current success : 938 \t All success : 944 \t mean(l0) : 11.923729 \t Current Learning Rate: 0.003\n",
            "Iteration 19100: Current success : 938 \t All success : 944 \t mean(l0) : 11.923729 \t Current Learning Rate: 0.002\n",
            "Iteration 19200: Current success : 938 \t All success : 944 \t mean(l0) : 11.923729 \t Current Learning Rate: 0.002\n",
            "Iteration 19300: Current success : 938 \t All success : 944 \t mean(l0) : 11.923729 \t Current Learning Rate: 0.002\n",
            "Iteration 19400: Current success : 938 \t All success : 944 \t mean(l0) : 11.923729 \t Current Learning Rate: 0.002\n",
            "Iteration 19500: Current success : 938 \t All success : 944 \t mean(l0) : 11.923729 \t Current Learning Rate: 0.001\n",
            "Iteration 19600: Current success : 938 \t All success : 944 \t mean(l0) : 11.923729 \t Current Learning Rate: 0.001\n",
            "Iteration 19700: Current success : 938 \t All success : 944 \t mean(l0) : 11.923729 \t Current Learning Rate: 0.001\n",
            "Iteration 19800: Current success : 938 \t All success : 944 \t mean(l0) : 11.923729 \t Current Learning Rate: 0.001\n",
            "Iteration 19900: Current success : 938 \t All success : 944 \t mean(l0) : 11.923729 \t Current Learning Rate: 0.000\n",
            "Iteration 20000: Current success : 938 \t All success : 944 \t mean(l0) : 11.923729 \t Current Learning Rate: 0.000\n",
            "1130\n",
            "984\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 12.92%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HGGi7N1bj29l"
      },
      "outputs": [],
      "source": [
        "for x_test, y_test in test_loader:\n",
        "    x_test, y_test = x_test.to(torch.float32).to(device), y_test.to(device)\n",
        "    mal_x_batch, mal_y_batch = x_test[y_test.squeeze() == 1], y_test[y_test.squeeze() == 1]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation on rounded adversarial examples\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model_AT_rFGSM.eval()\n",
        "\n",
        "# Apply the final rounding solution to adversarial examples\n",
        "adv_rounded = final_rounding(mal_x_batch, adv, model_AT_rFGSM,removal_array)\n",
        "\n",
        "# Get the model's predictions on the rounded adversarial examples\n",
        "outputs = model_AT_rFGSM(adv_rounded)\n",
        "y_pred = outputs.argmax(dim=1)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "num_malwares = len(mal_y_batch)\n",
        "num_successful_attacks = (y_pred == 0).sum().item()\n",
        "total_difference = (torch.abs(adv_rounded[y_pred == 0] - mal_x_batch[y_pred == 0])).sum().item()\n",
        "accuracy = (y_pred == 1).sum().item() / num_malwares * 100\n",
        "\n",
        "# Display the results with clear formatting\n",
        "print(f\"Evaluation Results on Rounded Adversarial Examples\")\n",
        "print(f\"-----------------------------------------------\")\n",
        "print(f\"Number of malware samples: {num_malwares}\")\n",
        "print(f\"Number of successful attacks: {num_successful_attacks}\")\n",
        "print(f\"Total L1 norm difference (successful adversarial examples vs. original): {total_difference}\")\n",
        "print(f\"Accuracy of the model on malware under attack: {accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Yu-aI8_MEzw",
        "outputId": "9f9b6aab-234c-4269-8916-44e79010578c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Results on Rounded Adversarial Examples\n",
            "-----------------------------------------------\n",
            "Number of malware samples: 1130\n",
            "Number of successful attacks: 984\n",
            "Total L1 norm difference (successful adversarial examples vs. original): 13398.0\n",
            "Accuracy of the model on malware under attack: 12.92%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure that fixed elements are not changed\n",
        "\n",
        "# Ensure `removal_array` is expanded to match the batch size\n",
        "expanded_removal_array = removal_array.expand(mal_x_batch.shape[0], -1)\n",
        "\n",
        "# Create a mask where non-fixed features are set to 1\n",
        "non_fixed_features_mask = torch.bitwise_or(expanded_removal_array, 1 - mal_x_batch.to(torch.uint8))\n",
        "\n",
        "# Calculate indices where the mask is 0 (fixed) or 1 (non-fixed)\n",
        "mask_fixed = non_fixed_features_mask == 0\n",
        "mask_non_fixed = non_fixed_features_mask == 1\n",
        "\n",
        "# Calculate and print the differences\n",
        "diff_adv_fixed = (torch.abs(adv - mal_x_batch) * mask_fixed).sum().item()\n",
        "diff_adv_non_fixed = (torch.abs(adv - mal_x_batch) * mask_non_fixed).sum().item()\n",
        "diff_adv_rounded_fixed = (torch.abs(adv_rounded - mal_x_batch) * mask_fixed).sum().item()\n",
        "diff_adv_rounded_non_fixed = (torch.abs(adv_rounded - mal_x_batch) * mask_non_fixed).sum().item()\n",
        "\n",
        "# Display results with clear formatting\n",
        "print(f\"Differences when mask is 0 (fixed features):\")\n",
        "print(f\"  Adv vs. Original: {diff_adv_fixed}\")\n",
        "print(f\"  Rounded Adv vs. Original: {diff_adv_rounded_fixed}\")\n",
        "print(f\"Differences when mask is 1 (non-fixed features):\")\n",
        "print(f\"  Adv vs. Original: {diff_adv_non_fixed}\")\n",
        "print(f\"  Rounded Adv vs. Original: {diff_adv_rounded_non_fixed}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce4f7caf-6e6f-4d62-8127-dfe65d5eaf38",
        "id": "4sqymyEpj29m"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Differences when mask is 0 (fixed features):\n",
            "  Adv vs. Original: 0.0\n",
            "  Rounded Adv vs. Original: 0.0\n",
            "Differences when mask is 1 (non-fixed features):\n",
            "  Adv vs. Original: 26199.9453125\n",
            "  Rounded Adv vs. Original: 24817.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming differences is already computed as in your code\n",
        "differences = (torch.abs(adv_rounded[y_pred == 0] - mal_x_batch[y_pred == 0])).sum(1)\n",
        "\n",
        "# Convert differences to a numpy array for plotting\n",
        "differences_np = differences.cpu().numpy()\n",
        "\n",
        "# Set the style of the plot\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# Create a histogram with Seaborn\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(differences_np, bins=16, kde=False, color='skyblue')\n",
        "#sns.histplot(differences_np, bins=30, kde=True, color='skyblue')\n",
        "\n",
        "# Set the title and labels\n",
        "plt.title('Abundance of Differences', fontsize=16)\n",
        "plt.xlabel('Sum of Absolute Differences', fontsize=14)\n",
        "plt.ylabel('Frequency', fontsize=14)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 576
        },
        "id": "x_z7qQXOJlxS",
        "outputId": "a6ac1167-0e6f-4cd3-bacb-9f030f00059f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA10AAAIvCAYAAACP5rEhAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABj3klEQVR4nO3dd3gU5f7+8XvTIAFCL9IkCW5oQYJAQAIIoUhAioJypKggRUWEox4iChZQ0CPSlUNVipViIyBSpEsXpJcEDChFSgoJpM3vD77ZH+sGCGyGTZb367q4IDPPzHxmn03YO8/MMxbDMAwBAAAAAEzh4eoCAAAAAMCdEboAAAAAwESELgAAAAAwEaELAAAAAExE6AIAAAAAExG6AAAAAMBEhC4AAAAAMBGhCwAAAABMROgCAAAAABMRugDgHzp06KDg4GDVqlVLFy5cuGHbnj17Kjg4WJs3b75D1Zlj8+bNCg4OVs+ePV1dSr62Y8cO9e7dWw0aNFC1atUUHBysRYsW3XS7rPfRtX/q1Kmj8PBwdevWTSNHjtSmTZtkGMZ19xEVFXXd4yUkJOjtt99W8+bNVatWLYe+Pn36tF599VWFh4erRo0aCg4OVlRU1O29CAAAB16uLgAA8pLdu3fr4MGDkqS0tDR9//33euqpp1xcFfKD06dPq3///kpMTNQDDzygChUqyMPDQ5UrV87xPqpVq6bq1atLuvr+u3jxog4cOKCdO3dq3rx5Cg4O1pgxY1SjRo1bqm348OFatmyZKlSooFatWqlAgQIKDAyUJBmGoYEDB2r37t2qWrWqwsLC5O3trQceeOCWjgEAuD5CFwBcY8GCBZKksmXL6vTp01qwYAGhCzmyYcMGJSQkqH379ho7duxt7aNly5Z68cUXHZZv27ZN77//vnbv3q0nn3xSc+fOVUhIiF2bf//73+rbt6/KlCljtzwtLU0rVqxQgQIF9P3336tw4cJ260+ePKndu3erfPny+u677+TlxUcDAMhtXF4IAP8nJSVFS5YskSR98MEH8vPz06FDh7R7924XV4b84M8//5QkValSJdf3Xa9ePc2fP18PPPCAUlJS9MorrygjI8OuTZkyZRQUFKQiRYrYLT979qzS09NVqlQph8AlSX/99ZckqWLFigQuADAJP10B4P8sW7ZMSUlJslqtatiwoSIjI7VgwQItWLBAtWvXvun2W7Zs0dSpU7Vnzx5duXJF9913n3r06KFOnTo5tO3Zs6e2bNmiOXPmKCwszGH9pEmTNHnyZA0cONBu5OPa5d27d9ekSZO0atUqnTt3TiVLllTLli310ksvyd/fP9sav/32W82dO1dHjhxRgQIFFBISoueee+6G57V8+XKtWbNGu3bt0unTp3X58mWVLl1aYWFh6tu3r+0ytWtFRUVp8eLFGj16tOrXr6+JEydq48aNio+PV7ly5dSuXTu98MIL8vHxyfaYe/bs0dy5c7V161adPXtWvr6+KleunBo3bqwePXqoQoUKdu1Pnz6tWbNmae3atfrzzz/l4eGhwMBAde7cWd26dbutMLFkyRJ9/fXX2r9/v5KTk1W6dGk1bNhQ/fr1U0BAgK3dokWL9Nprr9m+njx5siZPnixJqlChglatWnXLx86Oj4+P3n77bbVv317Hjh3TihUr1KZNG9v6a1/zRx99VJIUHBxsW3/y5Em7r0ePHm1X95YtW+zWr1y5UhUrVrR9vWzZMn3zzTfau3evkpKSVLx4cYWFhWnAgAGqWrWqXa0nTpxQRESEKlSooJ9//llz5szRd999p+PHjys5Odl2Ca8kxcbGavbs2dq4caNOnz4tHx8fVatWTY8//rg6duzo8Dpc+73j7++vKVOmaOvWrbp06ZIqV66sLl266JlnnpHFYsn2ddy0aZO++OIL/fbbbzp//rwKFy6sChUqqFmzZurZs6eKFy9u1/5W60tMTNSMGTO0atUqxcXFKT09XcWKFVPFihXVqFEjPf/88/L29s62NgDui9AFAP8n69LCxx57zPb3ggULFB0drWHDhqlgwYLX3fbnn3/W/PnzFRgYqPDwcJ05c0bbt2/X0KFDdeDAgVyflOCvv/5S586dlZ6errp16+rKlSvasWOH5s2bp127dumLL75w+GA3atQozZ07Vx4eHnrggQdUpkwZHTx4UD179lSPHj2ue6zBgwfLx8dHQUFBatiwodLT03X48GEtWrRIy5Yt08yZM1W3bt1st92/f7/effddFS1aVPXr11d8fLx27NihqVOn6siRI5oyZYrDNjNmzNDYsWOVmZmpKlWqKCIiQpcvX9Yff/yhWbNm6b777rOFCknaunWrXnjhBcXHx6tChQp68MEHlZqaqt9//10jR47U6tWrNXXq1Bx/0DUMQ1FRUfr222/l5eWlevXqqWTJktq7d68WLVqkpUuXauLEiWratKkkqXLlyurcubP279+vAwcO2N2X9c8P8M667777VKNGDe3bt08bNmywC13Z6dy5s5KTk/XTTz/Jz8/Prn1W3WfPntX69etVqlQpNWnSxLbez89PkpSenq5XXnlFS5culY+Pj2rWrKmyZcvq2LFj+uGHH/Tzzz9r0qRJttfjWln3i61bt0716tVTUFCQDh8+bFu/dOlSDR06VFeuXFFgYKCaNWumxMRE7d69W//5z3/066+/avTo0dme2/r16zV79mxVrlxZjRs31tmzZ7V9+3a9//77+uuvv/T66687bJP1PSBJ1atXV7169ZSYmKjY2FhNmTJFYWFhdr8EudX6UlJS9OSTT+rQoUMqUaKEGjZsKD8/P509e1axsbH6+OOP9cwzzxC6gLuRAQAwYmJiDKvVatSsWdM4d+6cbfnDDz9sWK1WY/Hixdlu16NHD8NqtRpWq9WYOnWq3brNmzcbtWvXNqxWq7F27dpst/v111+z3e/EiRMNq9VqTJw4MdvlVqvViIqKMq5cuWJb9+effxpNmjQxrFar8cMPP9htt3r1asNqtRp16tQxtm7dardu6tSptn326NHDoZYlS5YYly5dsluWmZlpzJs3z7BarUa7du2MzMxMu/VDhw617fOjjz4y0tPTbesOHjxo1KlTx7BarcaOHTvstluxYoVhtVqNkJAQY8mSJQ61HD582Dhy5Ijt6zNnzhgNGjQwgoODjfnz5xsZGRm2defPnzd69eplWK1WY9KkSQ77up7PP//csFqtRlhYmLFv3z67c856/evVq2f3PjGM6/dZTmS9H3Ky7euvv25YrVbjX//6l93yrNd84cKFdsvj4uIMq9VqNG/ePNv9/frrr9fte8MwjI8++siwWq1G165djT/++MNu3dKlS43q1asb9evXN+Lj4x2OabVajaZNmxoxMTEO+z1w4IBRq1YtIyQkxPjpp5/s1p04ccJo3759tt97137PffHFF3brNm7caAQHBxvVq1c3/vrrL7t1c+bMMaxWq9GgQQNj06ZNDvXs2rXL+PPPP52qb/HixYbVajWeffZZIzU11W6bjIwMY/PmzXbfswDuHtzTBQCSFi5cKElq0aKFSpQoYVueNeqVtf56atSoof79+9sta9CggZ588klJ0uzZs3OzXJUrV04jRoywuzzvnnvusY1Ybdy40a79Z599Jknq3r276tWrZ7euf//+tpGZ7ERGRtpGPbJYLBZ1795doaGhOnz4sI4ePZrttjVr1tTgwYPl6elpW2a1WtWhQ4ds65w0aZIkaciQIYqMjHTYX9WqVRUUFGR3XhcvXlT37t315JNPysPj//+3Vrx4cX3wwQfy9vbW/Pnzbzjd+rVmzZolSXrhhRfsXheLxaKBAwcqODhYCQkJ+vrrr3O0v9yWNXp28eJF04918eJFffrppypQoIAmTZqkSpUq2a1/+OGH9cQTTyg+Pl7ff/99tvsYMmSI3eWYWaZOnarU1FQNHjxYrVu3tltXoUIFvfvuu5KkOXPmZLvf1q1bq1u3bnbLGjVqpPDwcGVkZOjXX3+1LU9PT9fHH38sSRo5cqQaNmzosL/atWvrnnvucaq+v//+W5LUuHFjh9EsDw8PNWjQ4LqX1AJwb4QuAHe99PR0ffvtt5L+f8jK0qlTJ3l5eWnr1q36448/rruP7O7tyNpekrZv3+4w8YEzGjVqJF9fX4flWYHk9OnTtmXp6enavn27JNnCzvXqvJ7jx49r3rx5evfddzVs2DBFRUUpKirK9iEzNjY22+2aN2+e7b012dV59uxZ7d+/Xx4eHurSpcsN68myZs0aSVLbtm2zXV+2bFnde++9On/+vI4dO3bT/Z06dcrWz507d3ZYb7FYbJc2uurZbJmZmbZazLZ582ZdvnxZdevWVdmyZbNt06BBA0nSzp07s12f3SWQmZmZWrt2rSRlG64lKSQkRH5+ftq/f7+uXLnisL558+bZbpf13jpz5oxt2d69e3X+/HkVL15crVq1yna73Kgva0bJGTNm6Ntvv70jwRhA/sA9XQDuer/88ovOnj2rsmXLKjw83G5dqVKl1LRpU61atUoLFy7UkCFDst3HtRMOZLf88uXLunjxokqWLJkrNV/7G/lrZc1Ol5qaalt28eJF24fCm9X5TxkZGXrnnXf01Vdf3XCkKCkpyek6s2bRK126tMMMfNcTFxcn6eoI3s2cP38+2xGXa2WFwGLFimU7058k23O3rg2Md1LWA7uLFi1q+rGyXt9NmzbZTbKRnfPnzzssK1myZLa/HLh48aLtPdOsWbOb1nHx4kWH0Hez99a1Qe3kyZOSpICAgByF1dutL2tymZkzZ2ro0KGyWCy69957VbduXUVERKhFixZ2o7EA7h6ELgB3vawJNK5cuZLthBJZH64XLVqkQYMG2V0qdytyenmb9P9HM67nTn1wmzNnjr788kuVLl1aUVFRCg0NValSpVSgQAFJ0ssvv6wff/zxuudmdp1Zr1ObNm0cLoH8p2LFiplay52yb98+SVcv0zRb1uubFRxuJLtZLK83+cy17+/sRhT/KbuJJ8x8bzlT3yuvvKJu3bpp9erV2r59u3bs2KFFixZp0aJFCgkJ0Zw5c276XgXgfghdAO5qZ86csV1GdPHiRe3YseOGbdetW6eHHnrIYd2JEyey3SbrN+wFChSw+9Cf9SHt0qVL2W6X9cyn3FCsWDH5+PgoNTVVJ0+e1H333efQ5nr1L126VJL09ttvKyIiwmF9Ti7Zy6mskYuzZ88qMTExR6Nd99xzj44dO6a+ffs6PCz4dmSNpmSNdGQ32pU1+nO9y+3MdPjwYe3fv1+SHEZlzZDVJwEBARozZkyu7bd48eIqWLCgLl++rP/85z9291GaoXz58pKuvl8Nw7jpaJez9VWsWFE9e/ZUz549JUm7d+/Wq6++qt9//10zZszQoEGDbu9EAORbjHEDuKstXrxYGRkZuv/++3Xw4MHr/nn22Wcl/f9RsX+63iQCWfeKPfDAA3bPisr6wJ7dBBQpKSm5er+Ql5eXbZTihx9+yLbN9eqPj4+XJIfnYklXA8CBAwdyqcqrlxVWq1ZNmZmZN524JEvWFOdZ4dBZ5cqVs10+uGjRIof1hmFo8eLFkpTt89XMlJqaqjfffFPS1VGlFi1amH7MRo0aydvbW1u2bNG5c+dybb+enp568MEHJeVe391IrVq1VLx4cZ0/f14rVqy4afvcrq927dq2SXWyQjOAuwuhC8BdLevD/c0mksha/8svv2R778revXs1ffp0u2Xbtm3T559/Lkl6+umn7dY1atRIkvT555/b3RuUnJys4cOH2+5vyi1PPfWUJGnu3LkOo3nTp0/X3r17s90u65Kx+fPn211ydebMGQ0dOlTp6em5WufAgQMlSePGjdNPP/3ksP7IkSN2QfXZZ5+Vv7+/Pv30U82aNcvuHrEscXFx+u6773JcQ+/evSVJH3/8sV2oNAxDH3/8sfbv3y9/f389/vjjOd6ns7Zv367u3btr+/bt8vPz04cffnhHLjEtVaqUevbsqeTkZA0YMMDuocZZUlNTtXLlyuvOYHk9AwcOlLe3t/773/9q8eLF2V5Se+jQIS1fvvy268/i5eWlAQMGSJKGDx+urVu3OrTZvXu3Tp065VR9P//8s7Zu3erQNi0tTevWrZOU/S8wALg/Li8EcNfasmWLjh8/Lh8fH7Vr1+6Gbe+77z7VrFlTe/fu1bfffmv7YJ6lZ8+e+uijj/Tdd98pODhYZ86c0bZt25SZmalevXo53Izftm1bffbZZ9qzZ4/atWunBx54QJmZmdqzZ4+8vb312GOP5Xi0JydatGih7t27a/78+bZp47Mejnz06FH16tUr26m5BwwYoHXr1unrr7/W5s2bVaNGDSUlJWnr1q2qVKmSWrVqpZ9//jnX6mzVqpWGDBmi8ePHa9CgQQoMDFS1atVsD0c+cuSIRo8ebZuhrly5cvr444/14osv6v3339eMGTN03333qXTp0kpKStLRo0f1xx9/6P7777/uDJP/1K1bN+3cuVPfffedHnvsMdWvX9/2cOTY2FgVLFhQH374oSmXxK1YscJ2SWpaWpri4+N14MABnT17VpJUrVo1jRkz5oZT/Oe2l19+WWfOnNGPP/6oTp06qVq1aqpUqZI8PT116tQpHThwQMnJyZo+fbrddP43U7NmTf33v//Va6+9pqioKI0fP15Vq1ZV8eLFFR8fr0OHDunUqVOKjIx0mLL9djz11FOKjY3Vl19+qR49eqhGjRoKCAhQUlKSYmJiFBcXpzlz5qhcuXK3Xd+WLVs0Z84cFS9eXDVq1FCJEiV06dIl7dq1S+fOnVPZsmVto+YA7i6ELgB3raxLBZs3b56jmeA6duyovXv3asGCBQ6hq1WrVoqIiND//vc/rVmzRmlpaapRo4Z69OiR7Y343t7emj17tiZMmKAVK1Zow4YNKlGihFq1aqWXXnrJNkKWm0aMGKGaNWtq/vz52rVrl3x8fBQSEqLhw4dLyv55SPfff78WLlyo8ePH6/fff9eqVatszwN77rnnNGrUqFyvc8CAAWrYsKHmzp2rrVu36ueff1ahQoVUrlw5Pfvssw7PWKpfv76WLFmiefPmac2aNfr999+VmpqqkiVL6p577lGHDh1u6UO7xWLRBx98oKZNm+qrr77S3r17lZKSolKlSunRRx9V3759s500IjccOHDANrpWsGBBFSlSRBUrVlSbNm3UsmVLNWzY8I5MFX8tLy8vjR07Vh06dNCCBQu0a9cuHT58WL6+vipdurSaN2+uFi1aqH79+re877Zt2yokJERz587Vxo0btWPHDmVkZKhUqVKqXLmyunfvrocffjhXzsNisdjuTfzyyy9t55H1Gnfq1MlhhsZbre/RRx9VwYIFtX37dh05ckTnz59XkSJFdM899+ipp57S448/bnvOGoC7i8W4lem0AAAAAAC3hHu6AAAAAMBEhC4AAAAAMBGhCwAAAABMROgCAAAAABMRugAAAADARIQuAAAAADARz+m6RTt37pRhGPL29nZ1KQAAAABcKC0tTRaLRaGhoTdsx0jXLTIMQ3nl0WaGYSg1NTXP1IPcQb+6L/rWPdGv7ou+dU/0q3tyVb/mNBsw0nWLska4QkJCXFyJlJycrP3796tq1ary8/NzdTnIJfSr+6Jv3RP96r7oW/dEv7onV/Xr77//nqN2jHQBAAAAgIkIXQAAAABgIkIXAAAAAJiI0AUAAAAAJiJ0AQAAAICJCF0AAAAAYCJCFwAAAACYiNAFAAAAACYidAEAAACAiQhdAAAAAGAiQhcAAAAAmIjQBQAAAAAmInQBAAAAgIkIXQAAAABgojwXuhYtWqTg4GCHPx9++KFdu2+++UZt2rRRSEiIOnTooNWrVzvsKzExUcOGDVODBg0UGhqqQYMG6cyZM3fqVAAAAABAXq4u4HpmzJihIkWK2L4uW7as7d9LlizR8OHDNWDAADVs2FDR0dEaOHCg5s+frzp16tjaDR48WEeOHNFbb72lAgUKaPz48erbt68WLlwoL688e+oAAAAA3EieTR41a9ZUiRIlsl03ceJEtWvXToMHD5YkNWzYUIcOHdKUKVM0ffp0SdLOnTu1fv16zZw5U+Hh4ZKkgIAARUZGavny5YqMjLwj5wEAAADg7pbnLi+8mbi4OB07dkxt27a1Wx4ZGalNmzYpNTVVkrR27Vr5+/urcePGtjaBgYGqXr261q5de0drBgAAAHD3yrOhq3379qpevboiIiL0v//9TxkZGZKkmJgYSVdHra4VFBSktLQ0xcXF2doFBATIYrHYtQsMDLTtAwAAAADMlucuLyxdurRefPFF3X///bJYLFq1apXGjx+v06dPa8SIEYqPj5ck+fv7222X9XXW+oSEBLt7wrIULVpUe/bscapGwzCUnJzs1D5yQ0pKiry9vZWamuoQLvMywzBcXUKelpKSYvc33Ad9657oV/dF37on+tU9uapfDcPI0efwPBe6mjRpoiZNmti+Dg8PV4ECBfTZZ59pwIABLqzs/0tLS9P+/ftdXYa8vb1Vs2YteXrm2QFLBxkZmdq7d4/S0tJcXUqed+zYMVeXAJPQt+6JfnVf9K17ol/dkyv61cfH56Zt8lzoyk7btm01a9Ys7d+/X0WLFpV0dTr40qVL29okJCRIkm29v7+/Tp065bCv+Ph4W5vb5e3trapVqzq1j9yQmpoqT08PrT2RqMS0TFeXc1NFC3gqvHxh3XfffYx23UBKSoqOHTumKlWqyNfX19XlIBfRt+6JfnVf9K17ol/dk6v69ciRIzlqly9C17UCAwMlXb1nK+vfWV97e3urUqVKtnabNm1yGPKLjY2V1Wp1qgaLxSI/Pz+n9pEbss4rMS1T8ekuLiYHPDyvBi1+wOWMr69vnnifIffRt+6JfnVf9K17ol/d053u15ze4pMvrkuLjo6Wp6enatSooUqVKqlKlSpatmyZQ5tGjRrZhveaNm2q+Ph4bdq0ydYmNjZW+/btU9OmTe9o/QAAAADuXnlupKtPnz4KCwtTcHCwJGnlypX6+uuv1atXL9vlhC+++KJeeeUVVa5cWWFhYYqOjtbu3bs1b948235CQ0MVHh6uYcOGaejQoSpQoIDGjRun4OBgtW7d2iXnBgAAAODuk+dCV0BAgBYuXKhTp04pMzNTVapU0bBhw9SzZ09bm/bt2yslJUXTp0/XtGnTFBAQoMmTJys0NNRuX+PHj9fo0aM1YsQIpaenKzw8XG+88Ya8vPLcaQMAAABwU3kufbzxxhs5ate1a1d17dr1hm2KFCmi9957T++9915ulAYAAAAAtyxf3NMFAAAAAPkVoQsAAAAATEToAgAAAAATEboAAAAAwESELgAAAAAwEaELAAAAAExE6AIAAAAAExG6AAAAAMBEhC4AAAAAMBGhCwAAAABMROgCAAAAABMRugAAAADARIQuAAAAADARoQsAAAAATEToAgAAAAATEboAAAAAwESELgAAAAAwEaELAAAAAExE6AIAAAAAExG6AAAAAMBEhC4AAAAAMBGhCwAAAABMROgCAAAAABMRugAAAADARIQuAAAAADARoQsAAAAATEToAgAAAAATEboAAAAAwESELgAAAAAwEaELAAAAAExE6AIAAAAAExG6AAAAAMBEhC4AAAAAMBGhCwAAAABMROgCAAAAABMRugAAAADARIQuAAAAADARoQsAAAAATEToAgAAAAATEboAAAAAwESELgAAAAAwEaELAAAAAExE6AIAAAAAExG6AAAAAMBEhC4AAAAAMBGhCwAAAABMROgCAAAAABMRugAAAADARIQuAAAAADARoQsAAAAATEToAgAAAAATEboAAAAAwESELgAAAAAwEaELAAAAAExE6AIAAAAAExG6AAAAAMBEhC4AAAAAMBGhCwAAAABMROgCAAAAABMRugAAAADARIQuAAAAADARoQsAAAAATEToAgAAAAATEboAAAAAwESELgAAAAAwEaELAAAAAExE6AIAAAAAExG6AAAAAMBEhC4AAAAAMBGhCwAAAABMROgCAAAAABMRugAAAADARIQuAAAAADARoQsAAAAATEToAgAAAAAT5enQdenSJTVt2lTBwcH6/fff7dZ98803atOmjUJCQtShQwetXr3aYfvExEQNGzZMDRo0UGhoqAYNGqQzZ87cqfIBAAAAIG+Hro8//lgZGRkOy5csWaLhw4erbdu2mj59uurUqaOBAwfqt99+s2s3ePBgbdiwQW+99ZY+/PBDxcbGqm/fvkpPT79DZwAAAADgbpdnQ9fRo0f1+eef68UXX3RYN3HiRLVr106DBw9Ww4YN9c477ygkJERTpkyxtdm5c6fWr1+vd999V5GRkYqIiNCECRN08OBBLV++/E6eCgAAAIC7WJ4NXaNGjVK3bt0UEBBgtzwuLk7Hjh1T27Zt7ZZHRkZq06ZNSk1NlSStXbtW/v7+aty4sa1NYGCgqlevrrVr15p/AgAAAACgPBq6li1bpkOHDumFF15wWBcTEyNJDmEsKChIaWlpiouLs7ULCAiQxWKxaxcYGGjbBwAAAACYzcvVBfxTSkqKxowZoyFDhqhw4cIO6+Pj4yVJ/v7+dsuzvs5an5CQoCJFijhsX7RoUe3Zs8epGg3DUHJyslP7yA2pqany9fVVppGpjAzD1eXcVOb/3Z6XkpIiw8j79bpKSkqK3d9wH/Ste6Jf3Rd9657oV/fkqn41DMNhkCc7eS50ffLJJypZsqQee+wxV5dyXWlpadq/f7+ry5Cvr6+KFSum1CupSk5Jc3U5N+VneEsqqtjYWH7Q5cCxY8dcXQJMQt+6J/rVfdG37ol+dU+u6FcfH5+btslToevkyZOaNWuWpkyZosTEREmyjSglJyfr0qVLKlq0qKSr08GXLl3atm1CQoIk2db7+/vr1KlTDseIj4+3tbld3t7eqlq1qlP7yA1Z96/5FPCRn8XbxdXcXMECnpKuXhrKSNf1paSk6NixY6pSpYp8fX1dXQ5yEX3rnuhX90Xfuif61T25ql+PHDmSo3Z5KnSdOHFCaWlp6tevn8O6Xr166f7779fYsWMlXb1nKzAw0LY+JiZG3t7eqlSpkqSr925t2rTJYcgvNjZWVqvVqTotFov8/Pyc2kduyDovD4uHPD1dXEwOeHhevYWQH3A54+vrmyfeZ8h99K17ol/dF33rnuhX93Sn+zUnlxZKeSx0Va9eXXPmzLFbtn//fo0ePVpvv/22QkJCVKlSJVWpUkXLli1Ty5Ytbe2io6PVqFEj2/Be06ZN9fHHH2vTpk168MEHJV0NXPv27dOzzz57504KAAAAwF0tT4Uuf39/hYWFZbuuZs2aqlmzpiTpxRdf1CuvvKLKlSsrLCxM0dHR2r17t+bNm2drHxoaqvDwcA0bNkxDhw5VgQIFNG7cOAUHB6t169Z35HwAAAAAIE+Frpxq3769UlJSNH36dE2bNk0BAQGaPHmyQkND7dqNHz9eo0eP1ogRI5Senq7w8HC98cYb8vLKl6cNAAAAIB/K8+kjLCxMBw8edFjetWtXde3a9YbbFilSRO+9957ee+89s8oDAAAAgBvKkw9HBgAAAAB3QegCAAAAABMRugAAAADARIQuAAAAADARoQsAAAAATEToAgAAAAATEboAAAAAwESELgAAAAAwEaELAAAAAExE6AIAAAAAExG6AAAAAMBEhC4AAAAAMBGhCwAAAABMROgCAAAAABMRugAAAADARIQuAAAAADARoQsAAAAATEToAgAAAAATEboAAAAAwESELgAAAAAwEaELAAAAAExE6AIAAAAAExG6AAAAAMBEhC4AAAAAMBGhCwAAAABMROgCAAAAABMRugAAAADARIQuAAAAADARoQsAAAAATEToAgAAAAATEboAAAAAwESELgAAAAAwEaELAAAAAExE6AIAAAAAExG6AAAAAMBEhC4AAAAAMBGhCwAAAABMROgCAAAAABMRugAAAADARIQuAAAAADARoQsAAAAATEToAgAAAAATEboAAAAAwESELgAAAAAwEaELAAAAAExE6AIAAAAAExG6AAAAAMBEhC4AAAAAMBGhCwAAAABMROgCAAAAABMRugAAAADARIQuAAAAADARoQsAAAAATEToAgAAAAATEboAAAAAwESELgAAAAAwEaELAAAAAExE6AIAAAAAExG6AAAAAMBEhC4AAAAAMBGhCwAAAABMROgCAAAAABMRugAAAADARIQuAAAAADARoQsAAAAATEToAgAAAAATEboAAAAAwEROha7U1NTcqgMAAAAA3JJToatJkyYaNWqUDh48mFv1AAAAAIBbcSp0FSpUSPPmzVOnTp30xBNPaMGCBUpJScmt2gAAAAAg33MqdK1cuVLTp09Xq1attG/fPg0fPlzh4eEaMWKEfv/999yqEQAAAADyLS9nNrZYLGrSpImaNGmi8+fP69tvv9WCBQv09ddf65tvvlFwcLAef/xxdejQQYULF86tmgEAAAAg38i12QtLlCih3r17Kzo6WvPnz1enTp10/PhxjRw5Uk2aNNFrr72m3bt359bhAAAAACBfMGXK+EKFCsnX11deXl4yDEMZGRlavHixnnjiCfXr10/nzp0z47AAAAAAkOc4dXnhtS5duqQff/xR33zzjfbu3SvDMBQSEqJu3bqpXbt2Onz4sGbOnKlly5ZpxIgRmjJlSm4dGgAAAADyLKdD12+//aavv/5ay5YtU3Jysvz8/PT444+rW7duql69uq1dSEiIxo8fr1dffVWrVq267v7WrFmj6dOn68iRI0pKSlLZsmXVsmVLDRw4UEWKFLG1W7VqlcaPH6/Y2FiVL19e/fr102OPPWa3r9TUVI0bN07ff/+9Ll26pNDQUA0fPlyBgYHOnjYAAAAA5IhToeuRRx7RkSNHZBiGatSooSeeeELt27dXoUKFrrvNfffdpx9++OG66y9evKjatWurZ8+eKlasmA4fPqxJkybp8OHDmjVrliRp27ZtGjhwoLp06aJhw4bp119/1euvv65ChQrp4Ycftu1r1KhRio6OVlRUlMqWLaupU6fq6aef1pIlS+wCHAAAAACYxanQFRcXp0cffVRPPPGEateunaNtHnnkEdWpU+e66zt27Gj3dVhYmHx8fDR8+HCdPn1aZcuW1SeffKLatWvrnXfekSQ1bNhQcXFxmjhxoi10nTp1SgsWLNCbb76pLl26SLo62ta8eXN9+eWX6tu3722cMQAAAADcGqcm0li/fr3efffdHAcuSbrnnnvUoEGDWzpOsWLFJElpaWlKTU3V5s2b7Ua0JCkyMlJHjx7ViRMnbLVlZmbatStWrJgaN26stWvX3tLxAQAAAOB2ORW6fH19lZSUpMzMzGzXZ2ZmKikpSRkZGbe874yMDF25ckV79+7VlClT1KJFC1WsWFF//PGH0tLSHO7LCgoKkiTFxMTY/i5ZsqSKFi3q0C6rDQAAAACYzanLCydPnqwZM2ZozZo1KlGihMP6ixcv6qGHHlK/fv00cODAW9p38+bNdfr0aUlSkyZNNHbsWElSfHy8JMnf39+ufdbXWesTEhKyvW/L39/f1uZ2GYah5ORkp/aRG1JTU+Xr66tMI1MZGYary7mpzP/L3ikpKTKMvF+vq6SkpNj9DfdB37on+tV90bfuiX51T67qV8MwZLFYbtrOqdD1yy+/qFGjRtkGLunqA5MffPBBrVq16pZD17Rp05SSkqIjR47ok08+0YABAzR79mxnys01aWlp2r9/v6vLkK+vr4oVK6bUK6lKTklzdTk35Wd4Syqq2NhYftDlwLFjx1xdAkxC37on+tV90bfuiX51T67oVx8fn5u2cXoijbCwsBu2CQgI0I4dO25539WqVZMkhYaGKiQkRB07dtTPP/+sqlWrSpISExPt2ickJEiS7XJCf39/JSUlOew3ISHB4ZLDW+Xt7W2rw5VSU1MlST4FfORn8XZxNTdXsICnpKvvCUa6ri8lJUXHjh1TlSpV5Ovr6+pykIvoW/dEv7ov+tY90a/uyVX9euTIkRy1cyp0paen52g47cqVK84cRsHBwfL29tYff/yhFi1ayNvbWzExMWrSpImtTdZ9Wln3egUGBurvv/9WfHy8XciKiYlx+jldFotFfn5+Tu0jN2S99h4WD3l6uriYHPDwvHoLIT/gcsbX1zdPvM+Q++hb90S/ui/61j3Rr+7pTvdrTrKQ5OREGpUrV9bmzZtv2Gbz5s2qWLGiM4fRrl27lJaWpooVK8rHx0dhYWH66aef7NpER0crKCjIdqzw8HB5eHho+fLltjbx8fFav369mjZt6lQ9AAAAAJBTTo10tW7dWlOmTNGECRM0cOBAeV4z3JKRkaFJkyZp//79ev7553O8z4EDB6pWrVoKDg5WwYIFdeDAAc2cOVPBwcFq2bKlJOm5555Tr1699NZbb6lt27bavHmzfvzxR40bN862n3LlyqlLly764IMP5OHhobJly+p///ufihQpom7dujlz2gAAAACQY06FrmeeeUZLlizR1KlTFR0drbCwMJUpU0ZnzpzR5s2b9ccffygoKEi9e/fO8T5r166t6OhoTZs2TYZhqEKFCuratav69Olju0mtXr16mjRpksaPH68FCxaofPnyGjVqlNq2bWu3rzfeeEOFChXS2LFjdenSJdWtW1ezZ8/OdlZDAAAAADCDU6GrUKFCmj9/vt566y39/PPPOn78uG2dh4eH2rRpozfffFOFChXK8T779eunfv363bRdRESEIiIibtjGx8dHQ4cO1dChQ3N8fAAAAADITU6FLunqtPATJ07U33//rT179igxMVH+/v6qVauWSpYsmRs1AgAAAEC+5XToylKqVCk99NBDubU7AAAAAHALTs1eCAAAAAC4MadHuo4cOaJ58+bp999/V2JiojIyMhzaWCwWrVixwtlDAQAAAEC+41To2rJli5599lmlpqbKy8tLJUuWtJs2PothGM4cBgAAAADyLadC19ixY5WRkaFRo0apc+fO2QYuAAAAALibORW6Dhw4oMjISHXp0iW36gEAAAAAt+LURBq+vr5MCw8AAAAAN+BU6GrWrJm2bduWW7UAAAAAgNtxKnT95z//UWJiokaNGqWUlJTcqgkAAAAA3IZT93QNGTJEfn5+mj9/vhYtWqQqVaqocOHCDu0sFos+++wzZw4FAAAAAPmS01PGZ0lOTta+ffuybWexWJw5DAAAAADkW07PXggAAAAAuD6n7ukCAAAAANyYUyNd17p06ZKOHTumlJQU1atXL7d2CwAAAAD5mtMjXSdOnNBzzz2nBg0aqEuXLurVq5dt3fbt2xUZGanNmzc7exgAAAAAyJecCl1//vmnnnjiCa1du1YRERGqU6eODMOwrb///vt14cIFLVmyxOlCAQAAACA/cip0TZo0SfHx8Zo7d64mTpyoxo0b26338vJSvXr1tGPHDqeKBAAAAID8yqnQtW7dOrVq1Up169a9bpvy5cvr9OnTzhwGAAAAAPItp0JXfHy8KlSocMM2hmEoNTXVmcMAAAAAQL7lVOgqVaqUjh8/fsM2hw4d0j333OPMYQAAAAAg33IqdD344INavXr1dR+SvG3bNv36669q1qyZM4cBAAAAgHzLqed0Pffcc/rpp5/Uo0cP9enTxzbqtWbNGu3cuVOffvqpihcvrj59+uRKsQAAAACQ3zgVuipWrKiZM2dqyJAhmjBhgiwWiwzD0IABA2QYhsqXL68JEyaoTJkyuVUvAAAAAOQrToUu6eqzuJYvX67Vq1dr165dio+PV+HChVW7dm1FRETIx8cnN+oEAAAAgHzJ6dAlXX0eV6tWrdSqVavc2B0AAAAAuA2nJtIAAAAAANyYUyNdkydPzlE7i8WiF154wZlDAQAAAEC+ZGroyppYg9AFAAAA4G7lVOiaM2dOtssTExO1b98+zZ07V40aNVL37t2dOQwAAAAA5FtOha4GDRpcd11ERIQeeeQRde7cWW3atHHmMAAAAACQb5k6kUaVKlXUqlUrTZs2zczDAAAAAECeZfrshSVLllRsbKzZhwEAAACAPMnU0JWamqp169apSJEiZh4GAAAAAPIsp+7p+vbbb7Ndnp6ertOnTys6OloxMTHq2bOnM4cBAAAAgHzLqdAVFRUli8XisNwwDElXp4xv166dXnnlFWcOAwAAAAD5llOha/To0dkut1gsKlq0qGrWrKkyZco4cwgAAAAAyNecCl2dO3fOrToAAAAAwC2ZPnshAAAAANzNnBrp2rp1621vW79+fWcODQAAAAD5glOhq2fPntlOpJET+/fvd+bQAAAAAJAvOBW6XnjhBe3atUvr16/Xvffeq7p166pUqVL6+++/tXPnTh07dkzh4eGqU6dOLpULAAAAAPmLU6GrUaNGmjZtmkaOHKkuXbrYjXoZhqGvv/5a7777rgYMGKB69eo5XSwAAAAA5DdOTaQxYcIEPfTQQ+ratavDZYYWi0VPPPGEmjZtqgkTJjhVJAAAAADkV06Frj179igwMPCGbYKCgrRnzx5nDgMAAAAA+ZZTocvHx+emE2Ls27dPPj4+zhwGAAAAAPItp0JX48aNtW7dOk2bNk2pqal261JTU/W///1P69evV3h4uFNFAgAAAEB+5dREGv/5z3+0bds2jRs3TnPmzFGtWrVUokQJnT9/Xnv27NG5c+dUpkwZvfrqq7lVLwAAAADkK06FrnLlymnhwoUaO3asli5dql9++cW2rkCBAurYsaNefvlllS5d2tk6AQAAACBfcip0SVLp0qU1ZswYjRw5UrGxsUpMTFSRIkVUpUoV7uUCAAAAcNdzOnRl8fb2ltVqza3dAQAAAIBbyJXQdfbsWS1fvlyxsbFKSUnRu+++K0k6f/68Tpw4IavVqoIFC+bGoQAAAAAgX3Fq9kJJmj9/viIiIjRy5EjNmzdPixYtsq07d+6cnnjiCX3//ffOHgYAAAAA8iWnQteqVas0cuRIWa1WffLJJ/rXv/5lt/6+++5TcHCwVqxY4VSRAAAAAJBfOXV54cyZM1W+fHnNmTNHfn5+2rt3r0Mbq9Wqbdu2OXMYAAAAAMi3nBrp2r9/v5o1ayY/P7/rtilbtqzOnTvnzGEAAAAAIN9yKnQZhiEvrxsPlp07d46p4wEAAADctZwKXQEBAdq+fft116enp2vbtm1MJQ8AAADgruVU6HrkkUe0b98+TZ482WFdRkaG3n//fcXFxalTp07OHAYAAAAA8i2nJtLo0aOHVq1apSlTpuiHH36wXUb40ksvac+ePTp58qQaN26sLl265EqxAAAAAJDfODXS5e3trZkzZ6pfv366ePGiDh8+LMMw9NNPPyk+Pl59+/bVJ598IovFklv1AgAAAEC+4tRIlyT5+PhoyJAhGjx4sGJiYhQfH6/ChQsrKChInp6euVEjAAAAAORbToWuiIgINW3aVG+++aYsFouCgoJyqy4AAAAAcAtOXV544cIFFS5cOLdqAQAAAAC341ToCg4O1rFjx3KpFAAAAABwP06Frr59+2r16tX69ddfc6seAAAAAHArTt3TlZCQoMaNG6tPnz6KiIhQSEiISpUqle1shTyrCwAAAMDdyKnQFRUVJYvFIsMwtHz5ci1fvlyS7EKXYRiyWCyELgAAAAB3pVsOXUlJSfLx8ZGPj49Gjx5tRk0AAAAA4DZuOXTVr19fAwcO1AsvvKDOnTtLknbt2qVdu3apV69euV4gAAAAAORntzyRhmEYMgzDbtm6desY9QIAAACAbDg1eyEAAAAA4MYIXQAAAABgIkIXAAAAAJiI0AUAAAAAJrqt53T98MMP2rVrl+3rP/74Q5LUt2/fbNtbLBZNmzYtR/teunSpvv/+e+3du1cJCQm699571bNnTz322GN2z//65ptvNGPGDP35558KCAjQkCFD1Lx5c7t9JSYmavTo0VqxYoXS0tLUpEkTvfHGGypTpsytnjIAAAAA3JbbCl3Hjx/X8ePHHZavW7cu2/bXhqWb+fTTT1WhQgVFRUWpePHi2rhxo4YPH65Tp05p4MCBkqQlS5Zo+PDhGjBggBo2bKjo6GgNHDhQ8+fPV506dWz7Gjx4sI4cOaK33npLBQoU0Pjx49W3b18tXLhQXl5OPRcaAAAAAHLklpPHypUrzajD5pNPPlGJEiVsXzdq1EgXL17U7Nmz9fzzz8vDw0MTJ05Uu3btNHjwYElSw4YNdejQIU2ZMkXTp0+XJO3cuVPr16/XzJkzFR4eLkkKCAhQZGSkli9frsjISFPPAwAAAACk2whdFSpUMKMOm2sDV5bq1avr66+/VnJysi5cuKBjx47p1VdftWsTGRmpDz74QKmpqfLx8dHatWvl7++vxo0b29oEBgaqevXqWrt2LaELAAAAwB2RLybS2L59u8qWLavChQsrJiZG0tVRq2sFBQUpLS1NcXFxkqSYmBgFBAQ4XNoYGBho2wcAAAAAmC3P39i0bds2RUdHa+jQoZKk+Ph4SZK/v79du6yvs9YnJCSoSJEiDvsrWrSo9uzZ41RNhmEoOTnZqX3khtTUVPn6+irTyFRGhuHqcm4qM+Pq3ykpKTKMvF+vq6SkpNj9DfdB37on+tV90bfuiX51T67qV8MwcjR/RZ4OXadOndKQIUMUFhamXr16ubocm7S0NO3fv9/VZcjX11fFihVT6pVUJaekubqcm/IzvCUVVWxsLD/ocuDYsWOuLgEmoW/dE/3qvuhb90S/uidX9KuPj89N2+TZ0JWQkKC+ffuqWLFimjRpkjw8rl4JWbRoUUlXp4MvXbq0Xftr1/v7++vUqVMO+42Pj7e1uV3e3t6qWrWqU/vIDampqZIknwI+8rN4u7iamytYwFPS1UtDGem6vpSUFB07dkxVqlSRr6+vq8tBLqJv3RP96r7oW/dEv7onV/XrkSNHctQuT4auy5cvq3///kpMTNRXX31ld5lgYGCgpKv3bGX9O+trb29vVapUydZu06ZNDkN+sbGxslqtTtVnsVjk5+fn1D5yQ9Z5eVg85Onp4mJywMPzanDmB1zO+Pr65on3GXIffeue6Ff3Rd+6J/rVPd3pfs3po7Hy3EQa6enpGjx4sGJiYjRjxgyVLVvWbn2lSpVUpUoVLVu2zG55dHS0GjVqZBvea9q0qeLj47Vp0yZbm9jYWO3bt09NmzY1/0QAAAAAQHlwpOvtt9/W6tWrFRUVpaSkJP3222+2dTVq1JCPj49efPFFvfLKK6pcubLCwsIUHR2t3bt3a968eba2oaGhCg8P17BhwzR06FAVKFBA48aNU3BwsFq3bu2CMwMAAABwN8pzoWvDhg2SpDFjxjisW7lypSpWrKj27dsrJSVF06dP17Rp0xQQEKDJkycrNDTUrv348eM1evRojRgxQunp6QoPD9cbb7whL688d9oAAAAA3FSeSx+rVq3KUbuuXbuqa9euN2xTpEgRvffee3rvvfdyozQAAAAAuGV57p4uAAAAAHAnhC4AAAAAMBGhCwAAAABMROgCAAAAABMRugAAAADARIQuAAAAADARoQsAAAAATEToAgAAAAATEboAAAAAwESELgAAAAAwEaELAAAAAExE6AIAAAAAExG6AAAAAMBEhC4AAAAAMBGhCwAAAABMROgCAAAAABMRugAAAADARIQuAAAAADARoQsAAAAATEToAgAAAAATEboAAAAAwESELtwxBT0tMgzD1WXckvxWLwAAAPIeL1cXgLuHj4dFFotFG08lKz41w9Xl3FRRH089WM7P1WUAAAAgnyN04Y6LT83QhSuZri4DAAAAuCO4vBAAAAAATEToAgAAAAATEboAAAAAwESELgAAAAAwEaELAAAAAExE6AIAAAAAExG6AAAAAMBEhC4AAAAAMBGhCwAAAABMROgCAAAAABMRugAAAADARIQuAAAAADARoQsAAAAATEToAgAAAAATEboAAAAAwESELgAAAAAwEaELAAAAAExE6AIAAAAAExG6AAAAAMBEhC4AAAAAMBGhCwAAAABMROgCAAAAABMRuoA8xmKxyNfXVxaLxdWlAAAAIBd4uboAIK8q6GmRYRh3PPz4+vqqRo0at7WtK+oFAADAjRG6gOvw8bDIYrFo46lkxadm3LHjZmZk6vLlyypYsKA8PHM+GF3Ux1MPlvMzsTIAAADcDkIXcBPxqRm6cCXzjh0vIyNDySlp8rN4y9Pzjh0WAAAAJuGeLgAAAAAwEaELAAAAAExE6AIAAAAAExG6AAAAAMBEhC4AAAAAMBGhCwAAAABMROgCAAAAABMRugAAAADARIQuAAAAADARoQsAAAAATEToAgAAAAATEboAAAAAwESELgAAAAAwEaELAAAAAExE6AIAAAAAExG6AAAAAMBEhC4AAAAAMBGhCwAAAABMROgCAAAAABMRugAAAADARIQuAAAAADARoQsAAAAATEToAgAAAAATEboAAAAAwESELgAAAAAwUZ4LXcePH9eIESPUsWNH1ahRQ+3bt8+23TfffKM2bdooJCREHTp00OrVqx3aJCYmatiwYWrQoIFCQ0M1aNAgnTlzxuxTAAAAAACbPBe6Dh8+rDVr1ujee+9VUFBQtm2WLFmi4cOHq23btpo+fbrq1KmjgQMH6rfffrNrN3jwYG3YsEFvvfWWPvzwQ8XGxqpv375KT0+/A2cCAAAAAJKXqwv4pxYtWqhly5aSpKioKO3Zs8ehzcSJE9WuXTsNHjxYktSwYUMdOnRIU6ZM0fTp0yVJO3fu1Pr16zVz5kyFh4dLkgICAhQZGanly5crMjLyzpwQAAAAgLtanhvp8vC4cUlxcXE6duyY2rZta7c8MjJSmzZtUmpqqiRp7dq18vf3V+PGjW1tAgMDVb16da1duzb3CwcAAACAbOS50HUzMTExkq6OWl0rKChIaWlpiouLs7ULCAiQxWKxaxcYGGjbBwAAAACYLc9dXngz8fHxkiR/f3+75VlfZ61PSEhQkSJFHLYvWrRotpcs3grDMJScnOzUPnJDamqqfH19lWlkKiPDcHU5N5WZeTXjG5mZysjIcHE1N+eqejMzM+3+zvF2/1diSkqKDCPvvx/uRikpKXZ/wz3Qr+6LvnVP9Kt7clW/GobhMMiTnXwXuvKCtLQ07d+/39VlyNfXV8WKFVPqlVQlp6S5upybSi3gK0m6fCVVyclXXFzNzbm63suXL99Sez/DW1JRxcbG8h9JHnfs2DFXlwAT0K/ui751T/Sre3JFv/r4+Ny0Tb4LXUWLFpV0dTr40qVL25YnJCTYrff399epU6ccto+Pj7e1uV3e3t6qWrWqU/vIDVn3r/kU8JGfxdvF1dycj8/VGgsW8JGf4eniam7OVfVmZmbq8uXLKliw4E3vcbxWwQJXawwICGCkK49KSUnRsWPHVKVKFfn6+rq6HOQS+tV90bfuiX51T67q1yNHjuSoXb4LXYGBgZKu3rOV9e+sr729vVWpUiVbu02bNjkM+cXGxspqtTpVg8VikZ+fn1P7yA1Z5+Vh8ZBn3s8wtgBh8aDenB7f8xYO7OF5tV7+A8n7fH1988TPEOQu+tV90bfuiX51T3e6X3NyaaGUDyfSqFSpkqpUqaJly5bZLY+OjlajRo1sw3tNmzZVfHy8Nm3aZGsTGxurffv2qWnTpne0ZgAAAAB3rzw30pWSkqI1a9ZIkk6ePKmkpCRbwGrQoIFKlCihF198Ua+88ooqV66ssLAwRUdHa/fu3Zo3b55tP6GhoQoPD9ewYcM0dOhQFShQQOPGjVNwcLBat27tknMDAAAAcPfJc6Hr3Llzeumll+yWZX09Z84chYWFqX379kpJSdH06dM1bdo0BQQEaPLkyQoNDbXbbvz48Ro9erRGjBih9PR0hYeH64033pCXV547bQAAAABuKs+lj4oVK+rgwYM3bde1a1d17dr1hm2KFCmi9957T++9915ulQcAAAAAtyTf3dMFAAAAAPkJoQsAAAAATEToAgAAAAATEboAAAAAwESELgAAAAAwEaELAAAAAExE6AIAAAAAExG6AAAAAMBEhC4AAAAAMBGhCwAAAABMROgCAAAAABMRugAAAADARIQuAAAAADARoQsAAAAATEToAgAAAAATEboAAAAAwESELgAAAAAwEaELAAAAAExE6AIAAAAAExG6ADdR0NMiwzBcXcYtyW/1AgAA3A4vVxcAIHf4eFhksVi08VSy4lMzXF3OTRX18dSD5fxcXQYAAIDpCF2Am4lPzdCFK5muLgMAAAD/h8sLAQAAAMBEhC4AAAAAMBGhCwAAAABMROgCAAAAABMRugAAAADARIQuAAAAADARoQsAAAAATEToAgAAAAATEboAAAAAwESELgAAAAAwEaELAAAAAExE6AIAAAAAExG6AAAAAMBEhC4AAAAAMBGhCwAAAABMROgCAAAAABMRugAAAADARIQuAAAAADARoQsAAAAATEToAgAAAAATEboAAAAAwESELgAAAAAwEaELAAAAAExE6AIAAAAAExG6AAAAAMBEhC4AAAAAMBGhCwAAAABMROgCAAAAABMRugAAAADARIQuAAAAADARoQsAAAAATEToAgAAAAATEboAwE0ZhuHqEm5JfqsXAICc8nJ1AQAAc1gsFm08laz41AxXl3JTRX089WA5P1eXAQCAKQhdAODG4lMzdOFKpqvLuKmCnhYZhiGLxeLqUnIsv9ULAHAdQhcAwOV8PCyMzAEA3BahCwCQZ+SXkTkAAG4FE2kAAAAAgIkIXQAAAABgIkIXAAAAAJiI0AUAAAAAJiJ0AQAAAICJCF0AAAAAYCJCFwAAAACYiNAFAAAAACYidAEAAACAiQhdAAAAAGAiQhcA3CEWi0W+vr6yWCyuLgUAANxBXq4uAADyC8MwnApMvr6+qlGjRi5WBAAA8gNCFwDkkMVi0cZTyYpPzbit7TMzMnX58mUVLFhQHp7mXmhQ3s9L95fyNfUYAAAgZwhdAFyioKfF6ZEjV4hPzdCFK5m3tW1GRoaSU9LkZ/GWp2cuF/YP/t63VyNuHZeNAgBuhtAFwCV8PCxOjxzdSYwc4VrX/tIgv1w2mh9/yQEA7oLQBcClnBk5upMYOcK1rv2lwYWUtDt22ejtKurjqQfL+bm6DAC4a7l96Dp69KhGjRqlnTt3qlChQurYsaMGDx4sHx8fV5cGAMjn4lMzdP7Knbts9Hblx8t581u9AHAjbh264uPj9dRTT6lKlSqaNGmSTp8+rTFjxujy5csaMWKEq8sDAOCOyG+X8zIyB8DduHXo+vLLL3Xp0iVNnjxZxYoVk3T1Rva3335b/fv3V9myZV1bIAAAd1B+uZwXANxN3rz4PJesXbtWjRo1sgUuSWrbtq0yMzO1YcMG1xUGAAAA4K7h1qErJiZGgYGBdsv8/f1VunRpxcTEuKgqAACQH9zK4wAMw7gDFeWezHxWb357fYF/cuvLCxMSEuTv7++wvGjRooqPj7+tfaalpckwDO3evdvZ8pxmGIY8PDxUJsNQqXzww8jrkkW//21RqQxDJaj3hgwZsqTc2g3kvL7myq16b6dvb8fd+vreKf+s90716+3Kb6+v5yWLfo/PO6/nkSNHctQuNdNQPnh55WGRvD0s+aZei+XqfYm5FbyyJmk5fPgwk7X8n/z6Olz7nsj6953u17S0tBwdz61DlxmyXtS88ObMqqGgp0WS6+vJKeo1F/Wai3rNRb3mym/15jc+Hvnrtc1v9ebWZy+LxSIPD7e+2Ouuce17wmKxuGR2covFQujy9/dXYmKiw/L4+HgVLVr0tvYZGhrqbFkAAAAA7iJuHfMDAwMd7t1KTEzU2bNnHe71AgAAAAAzuHXoatq0qTZu3KiEhATbsmXLlsnDw0ONGzd2YWUAAAAA7hYWw42ng4mPj1e7du0UEBCg/v372x6O/Mgjj/BwZAAAAAB3hFuHLkk6evSoRo4cqZ07d6pQoULq2LGjhgwZ4pIb7QAAAADcfdw+dAEAAACAK7n1PV0AAAAA4GqELgAAAAAwEaELAAAAAExE6AIAAAAAExG6AAAAAMBEhC4AAAAAMBGhCwAAAABMROjKh44ePapnnnlGderUUePGjfXBBx8oNTXV1WXhFixdulTPPfecmjZtqjp16qhjx45asGCB/vnYvG+++UZt2rRRSEiIOnTooNWrV7uoYtyOS5cuqWnTpgoODtbvv/9ut46+zZ8WL16sTp06KSQkRGFhYXr22Wd1+fJl2/pVq1apQ4cOCgkJUZs2bbRw4UIXVoucWLlypbp27arQ0FCFh4frpZdeUlxcnEM7vmfzruPHj2vEiBHq2LGjatSoofbt22fbLid9mJiYqGHDhqlBgwYKDQ3VoEGDdObMGbNPAddxs75NSkrSpEmT1KVLF9WrV08PPvigBgwYoIMHDzrsy9V9S+jKZ+Lj4/XUU08pLS1NkyZN0pAhQ/T1119rzJgxri4Nt+DTTz+Vr6+voqKi9Mknn6hp06YaPny4pkyZYmuzZMkSDR8+XG3bttX06dNVp04dDRw4UL/99pvrCsct+fjjj5WRkeGwnL7Nnz755BONHDlSkZGRmjlzpt555x1VrFjR1sfbtm3TwIEDVadOHU2fPl1t27bV66+/rmXLlrm4clzP5s2bNXDgQFWtWlVTpkzRsGHDdODAAfXu3dsuTPM9m7cdPnxYa9as0b333qugoKBs2+S0DwcPHqwNGzborbfe0ocffqjY2Fj17dtX6enpd+BM8E8369s///xTX331lRo3bqzx48dr5MiRSkxM1BNPPKGjR4/atXV53xrIV6ZOnWrUqVPHuHDhgm3Zl19+aVSvXt04deqU6wrDLTl37pzDsjfeeMOoW7eukZGRYRiGYbRu3dr497//bdfmiSeeMJ599tk7UiOcc+TIEaNOnTrGF198YVitVmP37t22dfRt/nP06FGjRo0axi+//HLdNr179zaeeOIJu2X//ve/jbZt25pdHm7T8OHDjRYtWhiZmZm2ZZs2bTKsVquxdetW2zK+Z/O2rP83DcMwhg4darRr186hTU76cMeOHYbVajXWrVtnW3b06FEjODjYWLJkiQmV42Zu1reXLl0ykpOT7ZYlJSUZDRo0MN555x3bsrzQt4x05TNr165Vo0aNVKxYMduytm3bKjMzUxs2bHBdYbglJUqUcFhWvXp1JSUlKTk5WXFxcTp27Jjatm1r1yYyMlKbNm3ictJ8YNSoUerWrZsCAgLsltO3+dOiRYtUsWJFNWvWLNv1qamp2rx5sx5++GG75ZGRkTp69KhOnDhxJ8rELUpPT1ehQoVksVhsy4oUKSJJtsu9+Z7N+zw8bvxxNqd9uHbtWvn7+6tx48a2NoGBgapevbrWrl2b+4Xjpm7Wt35+fvL19bVbVqhQIVWuXNnu0sG80LeErnwmJiZGgYGBdsv8/f1VunRpxcTEuKgq5Ibt27erbNmyKly4sK0v//mBPSgoSGlpadneb4C8Y9myZTp06JBeeOEFh3X0bf60a9cuWa1Wffzxx2rUqJFq1aqlbt26adeuXZKkP/74Q2lpaQ4/n7Muh+Hnc9706KOP6ujRo5o/f74SExMVFxenjz76SDVq1FDdunUl8T3rDnLahzExMQoICLAL4dLVD+d8D+cfCQkJOnz4sN3P47zQt4SufCYhIUH+/v4Oy4sWLar4+HgXVITcsG3bNkVHR6t3796SZOvLf/Z11tf0dd6VkpKiMWPGaMiQISpcuLDDevo2fzp79qzWr1+v7777Tm+++aamTJkii8Wi3r1769y5c/RrPlWvXj1NnjxZY8eOVb169dSyZUudO3dO06dPl6enpyS+Z91BTvswISHBNtJ5LT5j5S///e9/ZbFY9K9//cu2LC/0LaELcLFTp05pyJAhCgsLU69evVxdDpz0ySefqGTJknrsscdcXQpykWEYSk5O1oQJE/Twww+rWbNm+uSTT2QYhubNm+fq8nCbduzYof/85z96/PHH9dlnn2nChAnKzMxUv3797CbSAJA/LFy4UF9//bVGjBihcuXKubocO4SufMbf31+JiYkOy+Pj41W0aFEXVARnJCQkqG/fvipWrJgmTZpku3Y5qy//2dcJCQl265G3nDx5UrNmzdKgQYOUmJiohIQEJScnS5KSk5N16dIl+jaf8vf3V7FixVStWjXbsmLFiqlGjRo6cuQI/ZpPjRo1Sg0bNlRUVJQaNmyohx9+WNOmTdO+ffv03XffSeLnsTvIaR/6+/srKSnJYXs+Y+UPa9as0YgRI/T888+rc+fOduvyQt8SuvKZ7K49TUxM1NmzZx3uJUDedvnyZfXv31+JiYmaMWOG3bB3Vl/+s69jYmLk7e2tSpUq3dFakTMnTpxQWlqa+vXrp/r166t+/foaMGCAJKlXr1565pln6Nt8qmrVqtddd+XKFVWuXFne3t7Z9qskfj7nUUePHrUL0pJUrlw5FS9eXH/88Yckfh67g5z2YWBgoGJjYx2emRkbG8v3cB7322+/6aWXXlKnTp300ksvOazPC31L6MpnmjZtqo0bN9p+OyNdvWnfw8PDbkYW5G3p6ekaPHiwYmJiNGPGDJUtW9ZufaVKlVSlShWH5/tER0erUaNG8vHxuZPlIoeqV6+uOXPm2P157bXXJElvv/223nzzTfo2n2revLkuXryo/fv325ZduHBBe/fuVc2aNeXj46OwsDD99NNPdttFR0crKChIFStWvNMlIwfKly+vffv22S07efKkLly4oAoVKkji57E7yGkfNm3aVPHx8dq0aZOtTWxsrPbt26emTZve0ZqRc0eOHFH//v3VsGFDvf3229m2yQt963VHjoJc061bN82dO1cvvPCC+vfvr9OnT+uDDz5Qt27dHD64I+96++23tXr1akVFRSkpKcnu4Yw1atSQj4+PXnzxRb3yyiuqXLmywsLCFB0drd27d3P/SB7m7++vsLCwbNfVrFlTNWvWlCT6Nh9q2bKlQkJCNGjQIA0ZMkQFChTQtGnT5OPjoyeffFKS9Nxzz6lXr15666231LZtW23evFk//vijxo0b5+LqcT3dunXTe++9p1GjRqlFixa6ePGi7b7Ma6cX53s2b0tJSdGaNWskXQ3NSUlJtoDVoEEDlShRIkd9GBoaqvDwcA0bNkxDhw5VgQIFNG7cOAUHB6t169YuObe73c361jAM9enTRwUKFNBTTz2lPXv22LYtXLiw7SqFvNC3FuOf42zI844ePaqRI0dq586dKlSokDp27KghQ4bw27Z8pEWLFjp58mS261auXGn7rfg333yj6dOn688//1RAQID+/e9/q3nz5neyVDhp8+bN6tWrlxYsWKCQkBDbcvo2/zl//rxGjx6t1atXKy0tTfXq1dNrr71md+nhypUrNX78eMXGxqp8+fLq16+funTp4sKqcSOGYejLL7/UF198obi4OBUqVEh16tTRkCFDbNP9Z+F7Nu86ceKEIiIisl03Z84c2y/DctKHiYmJGj16tH7++Welp6crPDxcb7zxBr/YdpGb9a2k605C1qBBA82dO9f2tav7ltAFAAAAACbini4AAAAAMBGhCwAAAABMROgCAAAAABMRugAAAADARIQuAAAAADARoQsAAAAATEToAgAAAAATEboAADZnz57V0KFD1axZM1WvXl3BwcFKSEjI9eO0aNFCLVq0yPX93qrNmzcrODhYkyZNcnUppuvZs6eCg4MdliclJWnUqFFq0aKFatasqeDgYO3fv/+m6wAAOefl6gIAIC9KTk7WnDlz9NNPP+nYsWNKS0tTiRIlVLFiRT3wwAPq2rWrKleu7Ooyc11UVJQ2bNigdu3a6d5775XFYlGBAgVyvP1rr72mRYsWqVixYlq3bp18fHxMrDbviIqK0uLFi7Vy5UpVrFjR9ONk8fT0VKFChVS6dGlVr15drVq1UosWLW7pdf/ggw/01VdfqXnz5urQoYM8PT1VqlSpm64DAOQcoQsA/iEpKUlPPvmkDh48qHvvvVePPPKIihcvrgsXLmj37t2aNm2aKleu7HahKzU1VRs3btSDDz6osWPH3vL2SUlJWrZsmSwWiy5evKgVK1YoMjLShErRpUsXlStXToZhKCkpScePH9fq1av1448/KigoSB999JGqVatmt83777+vlJQUh3398ssvqlKliqZOnXpL6wAAOUfoAoB/+Oyzz3Tw4EF17dpVI0eOlMVisVsfFxen1NRUF1Vnnr///luZmZkqU6bMbW2/dOlSJScn65lnntFnn32mBQsWELpM0rVrV9WpU8duWVJSkiZNmqRPP/1Uffr00aJFi1S2bFnb+vLly2e7rzNnzqh+/fq3vA4AkHPc0wUA//Dbb79Jkrp37+4QuCSpUqVKCgoKslsWHBysnj17Zru/7O5fioqKUnBwsOLi4jRz5ky1adNGtWvXVmRkpJYsWSLp6sjTuHHj1KJFC4WEhOiRRx7RmjVrbulckpOTNXHiRD388MMKCQlRgwYN1K9fP23fvt2uXc+ePdW8eXNJ0uLFixUcHKzg4GBFRUXl+FgLFiyQl5eXnn32WYWFhWnTpk06efLkDbdJSEjQiBEj1LhxY4WEhKhTp0768ccfHdpduXJFs2bNUocOHfTAAw+oTp06atGihV566SUdOHDArm16erpmz56tDh06qHbt2nrggQfUs2dPrVq1Ksfnciv92aJFC9slfxEREbbX7p/bx8XF6fXXX9dDDz2kWrVqKTw8XFFRUTd9jXKqcOHCeu211/Too4/q77//1ieffGK3/p/3dGW9Bw3D0JYtW+zqvtG6a61YsUJPPfWU6tevr5CQELVv314zZ85URkaGXbtFixYpODhYixYt0qpVq9StWzeFhobavY6pqamaPXu2OnfurDp16ig0NFRPPvmkVq5c6XCu137/zJkzRw8//LBq1aql5s2ba/LkycrMzMz2NVqxYoV69+6tsLAwhYSEqEWLFnr11Vd16NAhu3a3UktiYqImTJigyMhIhYaGqm7dumrVqpWGDh2aa30LIP9jpAsA/qFYsWKSpNjYWFWvXt3UY40ePVq7d+9W8+bN5eHhoejoaL388svy9/fXvHnzdOTIETVr1kxXrlzRjz/+qBdeeEHR0dE5urTxypUreuqpp7R7927VrFlTTz31lM6dO6fo6GitX79eY8eOVdu2bSVJnTt3VrVq1TRnzhxVq1ZNLVu2lKQcn/+RI0f022+/qVmzZipVqpQ6deqkTZs2adGiRXrxxRez3SY1NVVPP/20kpOT1aFDB6WkpGjp0qV6+eWXdeHCBbsP+EOHDtXSpUsVHBysRx99VD4+Pjp16pQ2b96s33//3XYpnWEYGjRokFauXKkqVaqoe/fuSk5O1tKlS/Xcc8/ptdde09NPP52jc8qpXr16afHixTpw4IB69eolf39/SVKFChVsbXbt2qU+ffooJSVFDz30kO69916dPHlSP/zwg9auXauvvvpKlSpVypV6nn/+eS1atEhLly7Vm2++me0vDiSpZcuWqlChgiZPnqwKFSqoc+fOtrr9/f2vuy7L2LFjNW3aNJUtW1atWrVSkSJFtG3bNn3wwQfatWuXJk6c6HDMZcuWacOGDXrooYf05JNPKikpSdLV90KfPn20ZcsWVa9eXV26dFFaWprWrFmj559/XsOHD1ePHj0c9vff//5XW7ZsUfPmzRUeHq6VK1dq0qRJSktL05AhQ+zajhkzRrNnz1axYsUUERGhkiVL6q+//tKmTZtUs2ZNWa3WW67FMAz16dNHu3btUt26ddWkSRN5eHjo5MmTWrVqlTp27Gj3mgG4ixkAADsrVqwwrFarERoaaowZM8ZYt26dcf78+RtuY7VajR49emS7rnnz5kbz5s3tlg0dOtSwWq1G69atjXPnztmW79q1y7BarUa9evWMf/3rX8alS5ds65YsWWJYrVZj5MiROTqPSZMmGVar1Xj55ZeNzMxM2/K9e/caNWvWNOrVq2ckJibalsfFxRlWq9UYOnRojvZ/rdGjRxtWq9X48ccfDcMwjKSkJKNOnTrGQw89ZGRkZDi0b968uWG1Wo3u3bsbV65csS3/66+/jLCwMKNWrVrGqVOnDMMwjISEBCM4ONjo3LmzkZ6ebref9PR0Iz4+3vb14sWLbX1x7X5PnjxphIWFGTVq1DD++OMP2/Jff/3VsFqtxsSJE+32e7v9GRcX59A+NTXVaN68uREaGmrs3bvXbt3WrVuN6tWrG/3798/2WP+UdZydO3fesF2zZs0Mq9Vqd649evQwrFarQ9sbnev11q1fv96wWq1G79697d6jmZmZxogRIwyr1WosW7bMtnzhwoWG1Wo1qlWrZmzYsMFhfx999JFhtVqN8ePH271XExMTjUcffdSoWbOm7f1w7evQokUL4/Tp07bl586dM+rVq2eEhoba9f+qVasMq9VqtG/f3uF7OS0tzTh79uxt1XLgwAHDarUazz//vMM5XblyxUhKSnJYDuDuxOWFAPAPERERioqKkmEYmjVrlvr06aOGDRuqVatWeuedd3Ts2LFcO9Zzzz2nEiVK2L6uXbu2KlWqpISEBA0ZMkR+fn62dW3atJG3t7fD5XTX8+2338rb21uvvPKK3WhHjRo11LlzZyUkJGjFihVOn0NaWpq+++47FS5c2DZCVqhQIbVs2VJ//vmnNm7ceN1thwwZYjfTXrly5dSrVy+lpqbaLrO0WCwyDEMFChSQh4f9f1uenp62kSVJtsv8Xn31Vbv9li9fXk8//bTS09P1/fffO33Ot+KXX37RyZMn1adPH9WoUcNuXb169RQREaE1a9bYRn1yQ9Z9eRcuXMi1fV5r3rx5kqSRI0favUctFovt/ZbVf9eKiIjQgw8+aLcsMzNTX3zxhSpXrqxBgwbZvVcLFy6sF154QWlpafr5558d9vf888/b3YNYokQJRURE6NKlS4qNjbUt//zzzyVJr7/+uooXL263Dy8vL9uMjLdbS8GCBR1q8/HxUaFChRyWA7g7cXkhAGTjmWeeUdeuXbVu3Trt3LlTe/bs0e7duzV//nwtWLBA48aNU0REhNPH+ecMc5JUunRpxcXFOVza5+npqRIlSujMmTM33W9SUpLi4uIUFBSkcuXKOawPCwvT119/neMAdyMrV67U+fPn1aVLF7vp5Tt16qTvv/9eCxYsUHh4uMN2Xl5eCg0NdVher149SdK+ffskXf2w26xZM61Zs0adO3fWww8/rAYNGigkJETe3t522+7fv1++vr6qXbu2w37DwsIkKVfO+VZk3SMYGxub7fPAzp49q8zMTMXGxiokJOSO1na7du3aJT8/Py1cuDDb9QULFlRMTIzD8uz6JTY2VvHx8SpTpowmT57ssP78+fOSlO3+atas6bAsa/KQxMRE27Ldu3fLx8dHDRo0uM4Z3V4tQUFBCg4O1o8//qhTp06pZcuWatCggapXr+7wCwIAdzdCFwBcR+HChdW2bVvbfU+JiYn66KOP9Pnnn+v1119XkyZNnH4OVeHChR2WeXl53XBdenr6TfebNWpSsmTJbNeXLl3arp0zFixYIOlqyLpWo0aNVLZsWa1cuVIXL1603SuXpXjx4tl+MM2q+draJkyYoKlTp+rHH3/UuHHjJF19fR599FH9+9//lq+vr22b7EKmlLvnfCvi4+MlST/88MMN22U3nfvtygrm/xzVyS3x8fFKT0/PNphkSU5OdliW3fvx4sWLkqTDhw/r8OHD191fdq/Pjb5/rp3MIykpSWXLlr1pELrVWry8vPTZZ59p8uTJ+umnnzRmzBhJV0fcunfvrueee06enp43PCaAuwOhCwByqEiRIhoxYoTWrFmjkydP6tChQ6pVq5akq5dVXS8MJSYmqkiRIneyVNuH0XPnzmW7/u+//7Zrd7v++usvbdiwQZKyneggy/fff69evXrZLbtw4YIyMzMdPghn1Xxtbb6+vhoyZIiGDBmiuLg4bd68WV9++aXmzJmjK1eu6J133rFtkzUa8U+3cs652Z9Zx5s6dapthkgzxcXF6a+//rI9zNsMWee0efPmW9ouu0k9svbVpk2bbCffyA1FihSxjSjeKHjdTi3FixfX8OHD9cYbbygmJka//vqr5s6dq0mTJsnb21v9+/fPlXMAkL8x9g0At8BisdhGVa5VtGhRnT592mH5iRMnlJCQcCdKs1O4cGFVqlRJf/zxR7Z1ZX1Yzu7yxluxaNEiZWZm6oEHHlCXLl0c/mTNepc1Gnat9PR07dy502H5tm3bJMnh/qcslSpVUpcuXTRv3jz5+fnZTQVfvXp1paSkaPfu3Q7bbdmyRVLOzvlW+zPrg3x2U5VnXVKXdZmh2T7++GNJUmRk5HVnLnRW7dq1dfHixVy5vzEoKEiFCxfWnj17lJaW5nxx2ahdu7ZSU1Nt7wEzarFYLAoKClL37t01e/ZsSbqlxxQAcG+ELgD4hy+//DLbD+3S1ef8HD16VP7+/rYppiWpVq1aOnnypN2HutTUVNvlRq7QqVMnpaWlaezYsTIMw7b8wIEDWrx4sYoUKWKb+OJ2GIahRYsWyWKx6P3339e7777r8GfMmDEKDQ3VwYMH9fvvvzvsY9y4cXYPmj516pTmzJkjHx8ftWvXTtLV+2j++Rwl6eolbmlpaXaXeGaFvLFjx9p9aP7rr780e/ZseXl5qUOHDjc9t1vtz6JFi9qO808tW7ZU+fLlNXv2bG3dutVhfVpami1oOuPSpUsaM2aMFi1apNKlS5s6wpI1nf+wYcOynazj7NmzOnr0aI725eXlpX/96186efKk3n///WzDzqFDh647apsT3bt3lyS9++67tksIs6Snp9tGQW+1lhMnTujEiRMObbL25+zlxwDcB5cXAsA/rF27Vm+++abuvfde1a1bV2XKlFFycrL279+vbdu2ycPDQ2+++abdB6pnnnlGGzZsUL9+/dSuXTv5+vpqw4YN8vf3t91LdKf17dtXa9as0XfffaejR4+qUaNGOnfunJYuXaqMjAyNHDnSqcsLf/31V504cUINGjS44TOmHn30Ue3cuVMLFiywmyiidOnStmd0NW/e3PacrosXL+qNN96wTYhw+vRpderUSdWqVVNwcLDKli2rixcvauXKlUpLS1OfPn1s++zYsaOWL1+ulStXqkOHDnrooYfs9hsVFZWj52Hdan82bNhQs2bN0ogRI9S6dWv5+vqqfPny6tSpk3x8fDRhwgT17dtXPXr0UMOGDWW1WmWxWPTnn39q27ZtKlasmJYtW5bj1/6bb77RunXrZBiGLl26pOPHj2vLli26dOmS7rvvPn300Ud2s/rltqZNm+r555/Xxx9/rNatW6tJkyYqX768Ll68qOPHj2v79u0aPHiww0PEr2fQoEHat2+f5s6dqzVr1qhevXoqWbKkTp8+rUOHDunAgQP66quvrnuP4s00a9ZMvXv31qxZs9SmTRu1bNnStv9Nmzapd+/etue33UotBw4c0MCBA1W7dm0FBQWpdOnSOn36tFasWCEPD49cfyYcgPyL0AUA//DKK6+obt262rhxo7Zu3aqzZ89KujorWufOndWjRw/bvVxZwsPDNX78eE2ZMkXfffedihUrpocfflhDhgzRI4884orTUIECBfTZZ59p+vTpio6O1qeffipfX1/Vr19f/fv3t80SeLuyLhnMGl26nsjISL377rtasmSJXnvtNdv02j4+Ppo9e7bGjh2r77//XgkJCQoMDNTw4cPVvn172/YVKlTQiy++qF9//VUbN27UxYsXVbx4cdWoUUO9evVS06ZNbW0tFosmTpyoOXPmaPHixZo3b568vb1Vs2ZNPf300zmecfJW+7NZs2Z69dVX9c0332j27NlKS0tTgwYNbJOL1K5dW99//71mzJihtWvXaseOHfLx8VHZsmXVsmVL26heTmW99p6enipUqJDKlCmjFi1aqGXLloqIiHCY1dEML730kurXr685c+Zo06ZNSkxMVLFixVSxYkUNHDjwlt73Pj4+mj59uhYsWKBvv/1Wy5cvV2pqqkqVKqWgoCB169bNbmT5dgwdOlShoaGaN2+efvrpJ125ckWlS5dWw4YN1bhx49uqpVatWurbt6+2bNmiNWvWKCEhQaVLl9aDDz6oPn36qE6dOk7VDMB9WIxrrzkBAAAAAOQq7ukCAAAAABMRugAAAADARIQuAAAAADARoQsAAAAATEToAgAAAAATEboAAAAAwESELgAAAAAwEaELAAAAAExE6AIAAAAAExG6AAAAAMBEhC4AAAAAMBGhCwAAAABMROgCAAAAABP9P3Lcv+P6xgrMAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gFxNWRB1zuEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attack_params =  {'device':device, 'confidence':0.5, 'removal_array':removal_array,'max_iterations': 20000, 'max_learning_rate': 0.1, 'initial_const': 1 , 'binary_search_steps' : 7, 'cons_increase_factor' : 10}\n",
        "adv = adv_predict(test_loader, combined_model, CW_l1, **attack_params)"
      ],
      "metadata": {
        "id": "FSwfikjSCv_n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abd9cd7f-e0b1-47d8-fca8-7fc57a21f5c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0')\n",
            "Iteration 100: Current success : 21 \t All success : 22 \t mean(l0) : 0.727273 \t Current Learning Rate: 0.098\n",
            "Iteration 200: Current success : 21 \t All success : 23 \t mean(l0) : 0.739130 \t Current Learning Rate: 0.096\n",
            "Iteration 300: Current success : 21 \t All success : 24 \t mean(l0) : 1.041667 \t Current Learning Rate: 0.094\n",
            "Iteration 400: Current success : 21 \t All success : 24 \t mean(l0) : 1.041667 \t Current Learning Rate: 0.092\n",
            "Iteration 500: Current success : 21 \t All success : 24 \t mean(l0) : 1.041667 \t Current Learning Rate: 0.090\n",
            "Iteration 600: Current success : 22 \t All success : 32 \t mean(l0) : 1.875000 \t Current Learning Rate: 0.088\n",
            "Iteration 700: Current success : 21 \t All success : 32 \t mean(l0) : 1.875000 \t Current Learning Rate: 0.086\n",
            "Iteration 800: Current success : 21 \t All success : 33 \t mean(l0) : 1.848485 \t Current Learning Rate: 0.084\n",
            "Iteration 900: Current success : 21 \t All success : 33 \t mean(l0) : 1.848485 \t Current Learning Rate: 0.082\n",
            "Iteration 1000: Current success : 21 \t All success : 33 \t mean(l0) : 1.848485 \t Current Learning Rate: 0.080\n",
            "Iteration 1100: Current success : 21 \t All success : 33 \t mean(l0) : 1.848485 \t Current Learning Rate: 0.078\n",
            "Iteration 1200: Current success : 21 \t All success : 33 \t mean(l0) : 1.848485 \t Current Learning Rate: 0.076\n",
            "Iteration 1300: Current success : 21 \t All success : 33 \t mean(l0) : 1.848485 \t Current Learning Rate: 0.074\n",
            "Iteration 1400: Current success : 21 \t All success : 34 \t mean(l0) : 1.882353 \t Current Learning Rate: 0.072\n",
            "Iteration 1500: Current success : 21 \t All success : 34 \t mean(l0) : 1.882353 \t Current Learning Rate: 0.070\n",
            "Iteration 1600: Current success : 21 \t All success : 34 \t mean(l0) : 1.882353 \t Current Learning Rate: 0.068\n",
            "Iteration 1700: Current success : 21 \t All success : 37 \t mean(l0) : 2.135135 \t Current Learning Rate: 0.066\n",
            "Iteration 1800: Current success : 21 \t All success : 37 \t mean(l0) : 2.135135 \t Current Learning Rate: 0.064\n",
            "Iteration 1900: Current success : 21 \t All success : 37 \t mean(l0) : 2.135135 \t Current Learning Rate: 0.062\n",
            "Iteration 2000: Current success : 21 \t All success : 37 \t mean(l0) : 2.135135 \t Current Learning Rate: 0.060\n",
            "Iteration 2100: Current success : 21 \t All success : 37 \t mean(l0) : 2.135135 \t Current Learning Rate: 0.058\n",
            "Iteration 2200: Current success : 21 \t All success : 37 \t mean(l0) : 2.135135 \t Current Learning Rate: 0.056\n",
            "Iteration 2300: Current success : 21 \t All success : 37 \t mean(l0) : 2.135135 \t Current Learning Rate: 0.054\n",
            "Iteration 2400: Current success : 21 \t All success : 38 \t mean(l0) : 2.184211 \t Current Learning Rate: 0.052\n",
            "Iteration 2500: Current success : 21 \t All success : 38 \t mean(l0) : 2.184211 \t Current Learning Rate: 0.050\n",
            "Iteration 2600: Current success : 21 \t All success : 39 \t mean(l0) : 2.179487 \t Current Learning Rate: 0.048\n",
            "Iteration 2700: Current success : 23 \t All success : 42 \t mean(l0) : 2.214286 \t Current Learning Rate: 0.046\n",
            "Iteration 2800: Current success : 23 \t All success : 42 \t mean(l0) : 2.214286 \t Current Learning Rate: 0.044\n",
            "Iteration 2900: Current success : 23 \t All success : 42 \t mean(l0) : 2.214286 \t Current Learning Rate: 0.042\n",
            "Iteration 3000: Current success : 24 \t All success : 42 \t mean(l0) : 2.214286 \t Current Learning Rate: 0.040\n",
            "Iteration 3100: Current success : 24 \t All success : 42 \t mean(l0) : 2.214286 \t Current Learning Rate: 0.038\n",
            "Iteration 3200: Current success : 24 \t All success : 42 \t mean(l0) : 2.214286 \t Current Learning Rate: 0.036\n",
            "Iteration 3300: Current success : 24 \t All success : 42 \t mean(l0) : 2.214286 \t Current Learning Rate: 0.034\n",
            "Iteration 3400: Current success : 24 \t All success : 42 \t mean(l0) : 2.214286 \t Current Learning Rate: 0.032\n",
            "Iteration 3500: Current success : 24 \t All success : 42 \t mean(l0) : 2.214286 \t Current Learning Rate: 0.030\n",
            "Iteration 3600: Current success : 24 \t All success : 42 \t mean(l0) : 2.214286 \t Current Learning Rate: 0.028\n",
            "Iteration 3700: Current success : 24 \t All success : 42 \t mean(l0) : 2.214286 \t Current Learning Rate: 0.026\n",
            "Iteration 3800: Current success : 24 \t All success : 42 \t mean(l0) : 2.214286 \t Current Learning Rate: 0.024\n",
            "Iteration 3900: Current success : 24 \t All success : 42 \t mean(l0) : 2.214286 \t Current Learning Rate: 0.022\n",
            "Iteration 4000: Current success : 24 \t All success : 42 \t mean(l0) : 2.214286 \t Current Learning Rate: 0.020\n",
            "Iteration 4100: Current success : 24 \t All success : 42 \t mean(l0) : 2.214286 \t Current Learning Rate: 0.018\n",
            "Iteration 4200: Current success : 24 \t All success : 42 \t mean(l0) : 2.214286 \t Current Learning Rate: 0.016\n",
            "Iteration 4300: Current success : 24 \t All success : 42 \t mean(l0) : 2.214286 \t Current Learning Rate: 0.014\n",
            "Iteration 4400: Current success : 24 \t All success : 42 \t mean(l0) : 2.214286 \t Current Learning Rate: 0.012\n",
            "Iteration 4500: Current success : 24 \t All success : 42 \t mean(l0) : 2.214286 \t Current Learning Rate: 0.010\n",
            "Iteration 4600: Current success : 24 \t All success : 42 \t mean(l0) : 2.214286 \t Current Learning Rate: 0.008\n",
            "Iteration 4700: Current success : 24 \t All success : 42 \t mean(l0) : 2.214286 \t Current Learning Rate: 0.006\n",
            "Iteration 4800: Current success : 24 \t All success : 42 \t mean(l0) : 2.214286 \t Current Learning Rate: 0.004\n",
            "Iteration 4900: Current success : 24 \t All success : 42 \t mean(l0) : 2.214286 \t Current Learning Rate: 0.002\n",
            "Iteration 5000: Current success : 24 \t All success : 42 \t mean(l0) : 2.214286 \t Current Learning Rate: 0.000\n",
            "Iteration 5100: Current success : 37 \t All success : 46 \t mean(l0) : 2.282609 \t Current Learning Rate: 0.049\n",
            "Iteration 5200: Current success : 44 \t All success : 52 \t mean(l0) : 2.673077 \t Current Learning Rate: 0.048\n",
            "Iteration 5300: Current success : 44 \t All success : 52 \t mean(l0) : 2.673077 \t Current Learning Rate: 0.047\n",
            "Iteration 5400: Current success : 44 \t All success : 52 \t mean(l0) : 2.673077 \t Current Learning Rate: 0.046\n",
            "Iteration 5500: Current success : 44 \t All success : 52 \t mean(l0) : 2.673077 \t Current Learning Rate: 0.045\n",
            "Iteration 5600: Current success : 44 \t All success : 52 \t mean(l0) : 2.673077 \t Current Learning Rate: 0.044\n",
            "Iteration 5700: Current success : 42 \t All success : 52 \t mean(l0) : 2.673077 \t Current Learning Rate: 0.043\n",
            "Iteration 5800: Current success : 44 \t All success : 52 \t mean(l0) : 2.673077 \t Current Learning Rate: 0.042\n",
            "Iteration 5900: Current success : 41 \t All success : 52 \t mean(l0) : 2.673077 \t Current Learning Rate: 0.041\n",
            "Iteration 6000: Current success : 41 \t All success : 54 \t mean(l0) : 2.648148 \t Current Learning Rate: 0.040\n",
            "Iteration 6100: Current success : 41 \t All success : 54 \t mean(l0) : 2.648148 \t Current Learning Rate: 0.039\n",
            "Iteration 6200: Current success : 41 \t All success : 54 \t mean(l0) : 2.648148 \t Current Learning Rate: 0.038\n",
            "Iteration 6300: Current success : 41 \t All success : 54 \t mean(l0) : 2.648148 \t Current Learning Rate: 0.037\n",
            "Iteration 6400: Current success : 39 \t All success : 54 \t mean(l0) : 2.648148 \t Current Learning Rate: 0.036\n",
            "Iteration 6500: Current success : 38 \t All success : 54 \t mean(l0) : 2.648148 \t Current Learning Rate: 0.035\n",
            "Iteration 6600: Current success : 37 \t All success : 54 \t mean(l0) : 2.648148 \t Current Learning Rate: 0.034\n",
            "Iteration 6700: Current success : 38 \t All success : 54 \t mean(l0) : 2.648148 \t Current Learning Rate: 0.033\n",
            "Iteration 6800: Current success : 37 \t All success : 54 \t mean(l0) : 2.648148 \t Current Learning Rate: 0.032\n",
            "Iteration 6900: Current success : 37 \t All success : 54 \t mean(l0) : 2.648148 \t Current Learning Rate: 0.031\n",
            "Iteration 7000: Current success : 37 \t All success : 54 \t mean(l0) : 2.648148 \t Current Learning Rate: 0.030\n",
            "Iteration 7100: Current success : 38 \t All success : 54 \t mean(l0) : 2.648148 \t Current Learning Rate: 0.029\n",
            "Iteration 7200: Current success : 37 \t All success : 54 \t mean(l0) : 2.648148 \t Current Learning Rate: 0.028\n",
            "Iteration 7300: Current success : 37 \t All success : 54 \t mean(l0) : 2.648148 \t Current Learning Rate: 0.027\n",
            "Iteration 7400: Current success : 37 \t All success : 54 \t mean(l0) : 2.648148 \t Current Learning Rate: 0.026\n",
            "Iteration 7500: Current success : 37 \t All success : 54 \t mean(l0) : 2.648148 \t Current Learning Rate: 0.025\n",
            "Iteration 7600: Current success : 38 \t All success : 55 \t mean(l0) : 2.781818 \t Current Learning Rate: 0.024\n",
            "Iteration 7700: Current success : 42 \t All success : 55 \t mean(l0) : 2.781818 \t Current Learning Rate: 0.023\n",
            "Iteration 7800: Current success : 45 \t All success : 56 \t mean(l0) : 2.839286 \t Current Learning Rate: 0.022\n",
            "Iteration 7900: Current success : 45 \t All success : 56 \t mean(l0) : 2.839286 \t Current Learning Rate: 0.021\n",
            "Iteration 8000: Current success : 45 \t All success : 56 \t mean(l0) : 2.839286 \t Current Learning Rate: 0.020\n",
            "Iteration 8100: Current success : 45 \t All success : 56 \t mean(l0) : 2.839286 \t Current Learning Rate: 0.019\n",
            "Iteration 8200: Current success : 45 \t All success : 56 \t mean(l0) : 2.839286 \t Current Learning Rate: 0.018\n",
            "Iteration 8300: Current success : 45 \t All success : 56 \t mean(l0) : 2.839286 \t Current Learning Rate: 0.017\n",
            "Iteration 8400: Current success : 45 \t All success : 56 \t mean(l0) : 2.839286 \t Current Learning Rate: 0.016\n",
            "Iteration 8500: Current success : 45 \t All success : 56 \t mean(l0) : 2.839286 \t Current Learning Rate: 0.015\n",
            "Iteration 8600: Current success : 45 \t All success : 56 \t mean(l0) : 2.839286 \t Current Learning Rate: 0.014\n",
            "Iteration 8700: Current success : 45 \t All success : 56 \t mean(l0) : 2.839286 \t Current Learning Rate: 0.013\n",
            "Iteration 8800: Current success : 45 \t All success : 56 \t mean(l0) : 2.839286 \t Current Learning Rate: 0.012\n",
            "Iteration 8900: Current success : 45 \t All success : 56 \t mean(l0) : 2.839286 \t Current Learning Rate: 0.011\n",
            "Iteration 9000: Current success : 45 \t All success : 56 \t mean(l0) : 2.839286 \t Current Learning Rate: 0.010\n",
            "Iteration 9100: Current success : 45 \t All success : 56 \t mean(l0) : 2.839286 \t Current Learning Rate: 0.009\n",
            "Iteration 9200: Current success : 45 \t All success : 56 \t mean(l0) : 2.839286 \t Current Learning Rate: 0.008\n",
            "Iteration 9300: Current success : 45 \t All success : 56 \t mean(l0) : 2.839286 \t Current Learning Rate: 0.007\n",
            "Iteration 9400: Current success : 45 \t All success : 56 \t mean(l0) : 2.839286 \t Current Learning Rate: 0.006\n",
            "Iteration 9500: Current success : 45 \t All success : 56 \t mean(l0) : 2.839286 \t Current Learning Rate: 0.005\n",
            "Iteration 9600: Current success : 45 \t All success : 56 \t mean(l0) : 2.839286 \t Current Learning Rate: 0.004\n",
            "Iteration 9700: Current success : 45 \t All success : 56 \t mean(l0) : 2.839286 \t Current Learning Rate: 0.003\n",
            "Iteration 9800: Current success : 45 \t All success : 56 \t mean(l0) : 2.839286 \t Current Learning Rate: 0.002\n",
            "Iteration 9900: Current success : 45 \t All success : 56 \t mean(l0) : 2.839286 \t Current Learning Rate: 0.001\n",
            "Iteration 10000: Current success : 45 \t All success : 56 \t mean(l0) : 2.839286 \t Current Learning Rate: 0.000\n",
            "Iteration 10100: Current success : 38 \t All success : 56 \t mean(l0) : 2.821429 \t Current Learning Rate: 0.025\n",
            "Iteration 10200: Current success : 40 \t All success : 56 \t mean(l0) : 2.803571 \t Current Learning Rate: 0.024\n",
            "Iteration 10300: Current success : 40 \t All success : 56 \t mean(l0) : 2.732143 \t Current Learning Rate: 0.024\n",
            "Iteration 10400: Current success : 38 \t All success : 56 \t mean(l0) : 2.732143 \t Current Learning Rate: 0.023\n",
            "Iteration 10500: Current success : 40 \t All success : 56 \t mean(l0) : 2.732143 \t Current Learning Rate: 0.023\n",
            "Iteration 10600: Current success : 37 \t All success : 56 \t mean(l0) : 2.732143 \t Current Learning Rate: 0.022\n",
            "Iteration 10700: Current success : 39 \t All success : 57 \t mean(l0) : 2.736842 \t Current Learning Rate: 0.022\n",
            "Iteration 10800: Current success : 37 \t All success : 58 \t mean(l0) : 2.844828 \t Current Learning Rate: 0.021\n",
            "Iteration 10900: Current success : 38 \t All success : 58 \t mean(l0) : 2.844828 \t Current Learning Rate: 0.021\n",
            "Iteration 11000: Current success : 38 \t All success : 58 \t mean(l0) : 2.844828 \t Current Learning Rate: 0.020\n",
            "Iteration 11100: Current success : 38 \t All success : 58 \t mean(l0) : 2.844828 \t Current Learning Rate: 0.020\n",
            "Iteration 11200: Current success : 37 \t All success : 58 \t mean(l0) : 2.844828 \t Current Learning Rate: 0.019\n",
            "Iteration 11300: Current success : 37 \t All success : 58 \t mean(l0) : 2.793103 \t Current Learning Rate: 0.019\n",
            "Iteration 11400: Current success : 37 \t All success : 58 \t mean(l0) : 2.793103 \t Current Learning Rate: 0.018\n",
            "Iteration 11500: Current success : 37 \t All success : 58 \t mean(l0) : 2.793103 \t Current Learning Rate: 0.018\n",
            "Iteration 11600: Current success : 38 \t All success : 58 \t mean(l0) : 2.793103 \t Current Learning Rate: 0.017\n",
            "Iteration 11700: Current success : 37 \t All success : 58 \t mean(l0) : 2.793103 \t Current Learning Rate: 0.017\n",
            "Iteration 11800: Current success : 37 \t All success : 58 \t mean(l0) : 2.793103 \t Current Learning Rate: 0.016\n",
            "Iteration 11900: Current success : 37 \t All success : 58 \t mean(l0) : 2.793103 \t Current Learning Rate: 0.016\n",
            "Iteration 12000: Current success : 39 \t All success : 58 \t mean(l0) : 2.793103 \t Current Learning Rate: 0.015\n",
            "Iteration 12100: Current success : 37 \t All success : 59 \t mean(l0) : 2.847458 \t Current Learning Rate: 0.015\n",
            "Iteration 12200: Current success : 38 \t All success : 59 \t mean(l0) : 2.847458 \t Current Learning Rate: 0.014\n",
            "Iteration 12300: Current success : 37 \t All success : 59 \t mean(l0) : 2.847458 \t Current Learning Rate: 0.014\n",
            "Iteration 12400: Current success : 37 \t All success : 59 \t mean(l0) : 2.847458 \t Current Learning Rate: 0.013\n",
            "Iteration 12500: Current success : 38 \t All success : 59 \t mean(l0) : 2.847458 \t Current Learning Rate: 0.013\n",
            "Iteration 12600: Current success : 37 \t All success : 60 \t mean(l0) : 3.066667 \t Current Learning Rate: 0.012\n",
            "Iteration 12700: Current success : 37 \t All success : 60 \t mean(l0) : 3.066667 \t Current Learning Rate: 0.012\n",
            "Iteration 12800: Current success : 37 \t All success : 60 \t mean(l0) : 3.066667 \t Current Learning Rate: 0.011\n",
            "Iteration 12900: Current success : 43 \t All success : 60 \t mean(l0) : 3.050000 \t Current Learning Rate: 0.011\n",
            "Iteration 13000: Current success : 45 \t All success : 60 \t mean(l0) : 3.050000 \t Current Learning Rate: 0.010\n",
            "Iteration 13100: Current success : 45 \t All success : 60 \t mean(l0) : 3.050000 \t Current Learning Rate: 0.010\n",
            "Iteration 13200: Current success : 45 \t All success : 60 \t mean(l0) : 3.050000 \t Current Learning Rate: 0.009\n",
            "Iteration 13300: Current success : 45 \t All success : 60 \t mean(l0) : 3.050000 \t Current Learning Rate: 0.009\n",
            "Iteration 13400: Current success : 45 \t All success : 60 \t mean(l0) : 3.050000 \t Current Learning Rate: 0.008\n",
            "Iteration 13500: Current success : 45 \t All success : 60 \t mean(l0) : 3.050000 \t Current Learning Rate: 0.008\n",
            "Iteration 13600: Current success : 45 \t All success : 60 \t mean(l0) : 3.050000 \t Current Learning Rate: 0.007\n",
            "Iteration 13700: Current success : 45 \t All success : 60 \t mean(l0) : 3.050000 \t Current Learning Rate: 0.007\n",
            "Iteration 13800: Current success : 45 \t All success : 60 \t mean(l0) : 3.050000 \t Current Learning Rate: 0.006\n",
            "Iteration 13900: Current success : 45 \t All success : 60 \t mean(l0) : 3.050000 \t Current Learning Rate: 0.006\n",
            "Iteration 14000: Current success : 45 \t All success : 60 \t mean(l0) : 3.050000 \t Current Learning Rate: 0.005\n",
            "Iteration 14100: Current success : 45 \t All success : 60 \t mean(l0) : 3.050000 \t Current Learning Rate: 0.005\n",
            "Iteration 14200: Current success : 45 \t All success : 60 \t mean(l0) : 3.050000 \t Current Learning Rate: 0.004\n",
            "Iteration 14300: Current success : 45 \t All success : 60 \t mean(l0) : 3.050000 \t Current Learning Rate: 0.004\n",
            "Iteration 14400: Current success : 45 \t All success : 60 \t mean(l0) : 3.050000 \t Current Learning Rate: 0.003\n",
            "Iteration 14500: Current success : 45 \t All success : 60 \t mean(l0) : 3.050000 \t Current Learning Rate: 0.003\n",
            "Iteration 14600: Current success : 45 \t All success : 60 \t mean(l0) : 3.050000 \t Current Learning Rate: 0.002\n",
            "Iteration 14700: Current success : 45 \t All success : 60 \t mean(l0) : 3.050000 \t Current Learning Rate: 0.002\n",
            "Iteration 14800: Current success : 45 \t All success : 60 \t mean(l0) : 3.050000 \t Current Learning Rate: 0.001\n",
            "Iteration 14900: Current success : 45 \t All success : 60 \t mean(l0) : 3.050000 \t Current Learning Rate: 0.001\n",
            "Iteration 15000: Current success : 45 \t All success : 60 \t mean(l0) : 3.050000 \t Current Learning Rate: 0.000\n",
            "Iteration 15100: Current success : 38 \t All success : 60 \t mean(l0) : 3.016667 \t Current Learning Rate: 0.012\n",
            "Iteration 15200: Current success : 45 \t All success : 60 \t mean(l0) : 3.016667 \t Current Learning Rate: 0.012\n",
            "Iteration 15300: Current success : 46 \t All success : 60 \t mean(l0) : 3.016667 \t Current Learning Rate: 0.012\n",
            "Iteration 15400: Current success : 46 \t All success : 60 \t mean(l0) : 3.016667 \t Current Learning Rate: 0.012\n",
            "Iteration 15500: Current success : 45 \t All success : 60 \t mean(l0) : 3.016667 \t Current Learning Rate: 0.011\n",
            "Iteration 15600: Current success : 44 \t All success : 60 \t mean(l0) : 3.016667 \t Current Learning Rate: 0.011\n",
            "Iteration 15700: Current success : 44 \t All success : 60 \t mean(l0) : 3.016667 \t Current Learning Rate: 0.011\n",
            "Iteration 15800: Current success : 45 \t All success : 60 \t mean(l0) : 3.016667 \t Current Learning Rate: 0.011\n",
            "Iteration 15900: Current success : 44 \t All success : 60 \t mean(l0) : 3.016667 \t Current Learning Rate: 0.010\n",
            "Iteration 16000: Current success : 45 \t All success : 60 \t mean(l0) : 3.016667 \t Current Learning Rate: 0.010\n",
            "Iteration 16100: Current success : 43 \t All success : 60 \t mean(l0) : 3.016667 \t Current Learning Rate: 0.010\n",
            "Iteration 16200: Current success : 44 \t All success : 60 \t mean(l0) : 3.016667 \t Current Learning Rate: 0.010\n",
            "Iteration 16300: Current success : 43 \t All success : 60 \t mean(l0) : 3.016667 \t Current Learning Rate: 0.009\n",
            "Iteration 16400: Current success : 44 \t All success : 60 \t mean(l0) : 3.016667 \t Current Learning Rate: 0.009\n",
            "Iteration 16500: Current success : 42 \t All success : 60 \t mean(l0) : 3.016667 \t Current Learning Rate: 0.009\n",
            "Iteration 16600: Current success : 45 \t All success : 60 \t mean(l0) : 3.016667 \t Current Learning Rate: 0.009\n",
            "Iteration 16700: Current success : 44 \t All success : 60 \t mean(l0) : 3.016667 \t Current Learning Rate: 0.008\n",
            "Iteration 16800: Current success : 42 \t All success : 60 \t mean(l0) : 3.016667 \t Current Learning Rate: 0.008\n",
            "Iteration 16900: Current success : 44 \t All success : 60 \t mean(l0) : 3.016667 \t Current Learning Rate: 0.008\n",
            "Iteration 17000: Current success : 42 \t All success : 60 \t mean(l0) : 3.016667 \t Current Learning Rate: 0.008\n",
            "Iteration 17100: Current success : 43 \t All success : 60 \t mean(l0) : 3.016667 \t Current Learning Rate: 0.007\n",
            "Iteration 17200: Current success : 44 \t All success : 60 \t mean(l0) : 3.016667 \t Current Learning Rate: 0.007\n",
            "Iteration 17300: Current success : 42 \t All success : 60 \t mean(l0) : 3.016667 \t Current Learning Rate: 0.007\n",
            "Iteration 17400: Current success : 43 \t All success : 60 \t mean(l0) : 3.016667 \t Current Learning Rate: 0.007\n",
            "Iteration 17500: Current success : 44 \t All success : 60 \t mean(l0) : 3.016667 \t Current Learning Rate: 0.006\n",
            "Iteration 17600: Current success : 44 \t All success : 60 \t mean(l0) : 3.016667 \t Current Learning Rate: 0.006\n",
            "Iteration 17700: Current success : 42 \t All success : 60 \t mean(l0) : 3.016667 \t Current Learning Rate: 0.006\n",
            "Iteration 17800: Current success : 45 \t All success : 60 \t mean(l0) : 3.016667 \t Current Learning Rate: 0.006\n",
            "Iteration 17900: Current success : 45 \t All success : 60 \t mean(l0) : 3.016667 \t Current Learning Rate: 0.005\n",
            "Iteration 18000: Current success : 46 \t All success : 60 \t mean(l0) : 3.016667 \t Current Learning Rate: 0.005\n",
            "Iteration 18100: Current success : 46 \t All success : 60 \t mean(l0) : 3.016667 \t Current Learning Rate: 0.005\n",
            "Iteration 18200: Current success : 46 \t All success : 60 \t mean(l0) : 3.016667 \t Current Learning Rate: 0.005\n",
            "Iteration 18300: Current success : 46 \t All success : 60 \t mean(l0) : 3.016667 \t Current Learning Rate: 0.004\n",
            "Iteration 18400: Current success : 46 \t All success : 60 \t mean(l0) : 3.016667 \t Current Learning Rate: 0.004\n",
            "Iteration 18500: Current success : 46 \t All success : 60 \t mean(l0) : 3.016667 \t Current Learning Rate: 0.004\n",
            "Iteration 18600: Current success : 46 \t All success : 60 \t mean(l0) : 3.016667 \t Current Learning Rate: 0.004\n",
            "Iteration 18700: Current success : 46 \t All success : 60 \t mean(l0) : 3.016667 \t Current Learning Rate: 0.003\n",
            "Iteration 18800: Current success : 46 \t All success : 60 \t mean(l0) : 3.016667 \t Current Learning Rate: 0.003\n",
            "Iteration 18900: Current success : 46 \t All success : 60 \t mean(l0) : 3.016667 \t Current Learning Rate: 0.003\n",
            "Iteration 19000: Current success : 46 \t All success : 60 \t mean(l0) : 3.016667 \t Current Learning Rate: 0.003\n",
            "Iteration 19100: Current success : 46 \t All success : 60 \t mean(l0) : 3.016667 \t Current Learning Rate: 0.002\n",
            "Iteration 19200: Current success : 46 \t All success : 60 \t mean(l0) : 3.016667 \t Current Learning Rate: 0.002\n",
            "Iteration 19300: Current success : 46 \t All success : 60 \t mean(l0) : 3.016667 \t Current Learning Rate: 0.002\n",
            "Iteration 19400: Current success : 46 \t All success : 60 \t mean(l0) : 3.016667 \t Current Learning Rate: 0.002\n",
            "Iteration 19500: Current success : 46 \t All success : 60 \t mean(l0) : 3.016667 \t Current Learning Rate: 0.001\n",
            "Iteration 19600: Current success : 46 \t All success : 60 \t mean(l0) : 3.016667 \t Current Learning Rate: 0.001\n",
            "Iteration 19700: Current success : 46 \t All success : 60 \t mean(l0) : 3.016667 \t Current Learning Rate: 0.001\n",
            "Iteration 19800: Current success : 46 \t All success : 60 \t mean(l0) : 3.016667 \t Current Learning Rate: 0.001\n",
            "Iteration 19900: Current success : 46 \t All success : 60 \t mean(l0) : 3.016667 \t Current Learning Rate: 0.000\n",
            "Iteration 20000: Current success : 46 \t All success : 60 \t mean(l0) : 3.016667 \t Current Learning Rate: 0.000\n",
            "outer_step 1: all success : 60 \t mean(l0)(success) : 3.0167\n",
            "-------------------------------------------------------------\n",
            "tensor([10., 10., 10.,  ..., 10., 10., 10.], device='cuda:0')\n",
            "Iteration 100: Current success : 42 \t All success : 50 \t mean(l0) : 6.700000 \t Current Learning Rate: 0.098\n",
            "Iteration 200: Current success : 33 \t All success : 51 \t mean(l0) : 7.078432 \t Current Learning Rate: 0.096\n",
            "Iteration 300: Current success : 33 \t All success : 51 \t mean(l0) : 7.078432 \t Current Learning Rate: 0.094\n",
            "Iteration 400: Current success : 33 \t All success : 52 \t mean(l0) : 6.961539 \t Current Learning Rate: 0.092\n",
            "Iteration 500: Current success : 34 \t All success : 56 \t mean(l0) : 7.089286 \t Current Learning Rate: 0.090\n",
            "Iteration 600: Current success : 34 \t All success : 57 \t mean(l0) : 7.000000 \t Current Learning Rate: 0.088\n",
            "Iteration 700: Current success : 35 \t All success : 57 \t mean(l0) : 7.000000 \t Current Learning Rate: 0.086\n",
            "Iteration 800: Current success : 34 \t All success : 58 \t mean(l0) : 6.896552 \t Current Learning Rate: 0.084\n",
            "Iteration 900: Current success : 34 \t All success : 58 \t mean(l0) : 6.896552 \t Current Learning Rate: 0.082\n",
            "Iteration 1000: Current success : 33 \t All success : 58 \t mean(l0) : 6.896552 \t Current Learning Rate: 0.080\n",
            "Iteration 1100: Current success : 34 \t All success : 58 \t mean(l0) : 6.896552 \t Current Learning Rate: 0.078\n",
            "Iteration 1200: Current success : 35 \t All success : 58 \t mean(l0) : 6.896552 \t Current Learning Rate: 0.076\n",
            "Iteration 1300: Current success : 34 \t All success : 59 \t mean(l0) : 6.796610 \t Current Learning Rate: 0.074\n",
            "Iteration 1400: Current success : 34 \t All success : 59 \t mean(l0) : 6.796610 \t Current Learning Rate: 0.072\n",
            "Iteration 1500: Current success : 34 \t All success : 59 \t mean(l0) : 6.796610 \t Current Learning Rate: 0.070\n",
            "Iteration 1600: Current success : 34 \t All success : 59 \t mean(l0) : 6.796610 \t Current Learning Rate: 0.068\n",
            "Iteration 1700: Current success : 34 \t All success : 60 \t mean(l0) : 6.716667 \t Current Learning Rate: 0.066\n",
            "Iteration 1800: Current success : 35 \t All success : 60 \t mean(l0) : 6.716667 \t Current Learning Rate: 0.064\n",
            "Iteration 1900: Current success : 34 \t All success : 60 \t mean(l0) : 6.716667 \t Current Learning Rate: 0.062\n",
            "Iteration 2000: Current success : 34 \t All success : 60 \t mean(l0) : 6.716667 \t Current Learning Rate: 0.060\n",
            "Iteration 2100: Current success : 34 \t All success : 60 \t mean(l0) : 6.716667 \t Current Learning Rate: 0.058\n",
            "Iteration 2200: Current success : 34 \t All success : 60 \t mean(l0) : 6.716667 \t Current Learning Rate: 0.056\n",
            "Iteration 2300: Current success : 34 \t All success : 60 \t mean(l0) : 6.716667 \t Current Learning Rate: 0.054\n",
            "Iteration 2400: Current success : 34 \t All success : 60 \t mean(l0) : 6.716667 \t Current Learning Rate: 0.052\n",
            "Iteration 2500: Current success : 34 \t All success : 60 \t mean(l0) : 6.716667 \t Current Learning Rate: 0.050\n",
            "Iteration 2600: Current success : 37 \t All success : 61 \t mean(l0) : 6.639344 \t Current Learning Rate: 0.048\n",
            "Iteration 2700: Current success : 40 \t All success : 63 \t mean(l0) : 6.634921 \t Current Learning Rate: 0.046\n",
            "Iteration 2800: Current success : 43 \t All success : 63 \t mean(l0) : 6.619048 \t Current Learning Rate: 0.044\n",
            "Iteration 2900: Current success : 43 \t All success : 63 \t mean(l0) : 6.619048 \t Current Learning Rate: 0.042\n",
            "Iteration 3000: Current success : 44 \t All success : 63 \t mean(l0) : 6.619048 \t Current Learning Rate: 0.040\n",
            "Iteration 3100: Current success : 44 \t All success : 63 \t mean(l0) : 6.619048 \t Current Learning Rate: 0.038\n",
            "Iteration 3200: Current success : 44 \t All success : 63 \t mean(l0) : 6.619048 \t Current Learning Rate: 0.036\n",
            "Iteration 3300: Current success : 44 \t All success : 63 \t mean(l0) : 6.619048 \t Current Learning Rate: 0.034\n",
            "Iteration 3400: Current success : 44 \t All success : 63 \t mean(l0) : 6.619048 \t Current Learning Rate: 0.032\n",
            "Iteration 3500: Current success : 44 \t All success : 63 \t mean(l0) : 6.619048 \t Current Learning Rate: 0.030\n",
            "Iteration 3600: Current success : 44 \t All success : 63 \t mean(l0) : 6.619048 \t Current Learning Rate: 0.028\n",
            "Iteration 3700: Current success : 44 \t All success : 63 \t mean(l0) : 6.619048 \t Current Learning Rate: 0.026\n",
            "Iteration 3800: Current success : 44 \t All success : 63 \t mean(l0) : 6.619048 \t Current Learning Rate: 0.024\n",
            "Iteration 3900: Current success : 44 \t All success : 63 \t mean(l0) : 6.619048 \t Current Learning Rate: 0.022\n",
            "Iteration 4000: Current success : 44 \t All success : 63 \t mean(l0) : 6.619048 \t Current Learning Rate: 0.020\n",
            "Iteration 4100: Current success : 44 \t All success : 63 \t mean(l0) : 6.619048 \t Current Learning Rate: 0.018\n",
            "Iteration 4200: Current success : 44 \t All success : 63 \t mean(l0) : 6.619048 \t Current Learning Rate: 0.016\n",
            "Iteration 4300: Current success : 44 \t All success : 63 \t mean(l0) : 6.619048 \t Current Learning Rate: 0.014\n",
            "Iteration 4400: Current success : 44 \t All success : 63 \t mean(l0) : 6.619048 \t Current Learning Rate: 0.012\n",
            "Iteration 4500: Current success : 44 \t All success : 63 \t mean(l0) : 6.619048 \t Current Learning Rate: 0.010\n",
            "Iteration 4600: Current success : 44 \t All success : 63 \t mean(l0) : 6.619048 \t Current Learning Rate: 0.008\n",
            "Iteration 4700: Current success : 44 \t All success : 63 \t mean(l0) : 6.619048 \t Current Learning Rate: 0.006\n",
            "Iteration 4800: Current success : 44 \t All success : 63 \t mean(l0) : 6.619048 \t Current Learning Rate: 0.004\n",
            "Iteration 4900: Current success : 44 \t All success : 63 \t mean(l0) : 6.619048 \t Current Learning Rate: 0.002\n",
            "Iteration 5000: Current success : 44 \t All success : 63 \t mean(l0) : 6.619048 \t Current Learning Rate: 0.000\n",
            "Iteration 5100: Current success : 43 \t All success : 75 \t mean(l0) : 6.653334 \t Current Learning Rate: 0.049\n",
            "Iteration 5200: Current success : 53 \t All success : 77 \t mean(l0) : 6.597403 \t Current Learning Rate: 0.048\n",
            "Iteration 5300: Current success : 50 \t All success : 79 \t mean(l0) : 6.873418 \t Current Learning Rate: 0.047\n",
            "Iteration 5400: Current success : 50 \t All success : 79 \t mean(l0) : 6.873418 \t Current Learning Rate: 0.046\n",
            "Iteration 5500: Current success : 48 \t All success : 79 \t mean(l0) : 6.873418 \t Current Learning Rate: 0.045\n",
            "Iteration 5600: Current success : 48 \t All success : 79 \t mean(l0) : 6.873418 \t Current Learning Rate: 0.044\n",
            "Iteration 5700: Current success : 45 \t All success : 79 \t mean(l0) : 6.873418 \t Current Learning Rate: 0.043\n",
            "Iteration 5800: Current success : 52 \t All success : 79 \t mean(l0) : 6.873418 \t Current Learning Rate: 0.042\n",
            "Iteration 5900: Current success : 46 \t All success : 79 \t mean(l0) : 6.873418 \t Current Learning Rate: 0.041\n",
            "Iteration 6000: Current success : 44 \t All success : 79 \t mean(l0) : 6.873418 \t Current Learning Rate: 0.040\n",
            "Iteration 6100: Current success : 42 \t All success : 79 \t mean(l0) : 6.873418 \t Current Learning Rate: 0.039\n",
            "Iteration 6200: Current success : 43 \t All success : 79 \t mean(l0) : 6.873418 \t Current Learning Rate: 0.038\n",
            "Iteration 6300: Current success : 40 \t All success : 79 \t mean(l0) : 6.873418 \t Current Learning Rate: 0.037\n",
            "Iteration 6400: Current success : 40 \t All success : 79 \t mean(l0) : 6.873418 \t Current Learning Rate: 0.036\n",
            "Iteration 6500: Current success : 39 \t All success : 79 \t mean(l0) : 6.873418 \t Current Learning Rate: 0.035\n",
            "Iteration 6600: Current success : 39 \t All success : 79 \t mean(l0) : 6.873418 \t Current Learning Rate: 0.034\n",
            "Iteration 6700: Current success : 40 \t All success : 79 \t mean(l0) : 6.873418 \t Current Learning Rate: 0.033\n",
            "Iteration 6800: Current success : 41 \t All success : 79 \t mean(l0) : 6.873418 \t Current Learning Rate: 0.032\n",
            "Iteration 6900: Current success : 40 \t All success : 79 \t mean(l0) : 6.873418 \t Current Learning Rate: 0.031\n",
            "Iteration 7000: Current success : 40 \t All success : 79 \t mean(l0) : 6.873418 \t Current Learning Rate: 0.030\n",
            "Iteration 7100: Current success : 39 \t All success : 79 \t mean(l0) : 6.873418 \t Current Learning Rate: 0.029\n",
            "Iteration 7200: Current success : 39 \t All success : 79 \t mean(l0) : 6.873418 \t Current Learning Rate: 0.028\n",
            "Iteration 7300: Current success : 39 \t All success : 79 \t mean(l0) : 6.873418 \t Current Learning Rate: 0.027\n",
            "Iteration 7400: Current success : 39 \t All success : 79 \t mean(l0) : 6.873418 \t Current Learning Rate: 0.026\n",
            "Iteration 7500: Current success : 39 \t All success : 79 \t mean(l0) : 6.873418 \t Current Learning Rate: 0.025\n",
            "Iteration 7600: Current success : 41 \t All success : 79 \t mean(l0) : 6.873418 \t Current Learning Rate: 0.024\n",
            "Iteration 7700: Current success : 42 \t All success : 79 \t mean(l0) : 6.873418 \t Current Learning Rate: 0.023\n",
            "Iteration 7800: Current success : 50 \t All success : 81 \t mean(l0) : 6.802469 \t Current Learning Rate: 0.022\n",
            "Iteration 7900: Current success : 50 \t All success : 81 \t mean(l0) : 6.802469 \t Current Learning Rate: 0.021\n",
            "Iteration 8000: Current success : 50 \t All success : 81 \t mean(l0) : 6.802469 \t Current Learning Rate: 0.020\n",
            "Iteration 8100: Current success : 50 \t All success : 81 \t mean(l0) : 6.802469 \t Current Learning Rate: 0.019\n",
            "Iteration 8200: Current success : 50 \t All success : 81 \t mean(l0) : 6.802469 \t Current Learning Rate: 0.018\n",
            "Iteration 8300: Current success : 50 \t All success : 81 \t mean(l0) : 6.802469 \t Current Learning Rate: 0.017\n",
            "Iteration 8400: Current success : 50 \t All success : 81 \t mean(l0) : 6.802469 \t Current Learning Rate: 0.016\n",
            "Iteration 8500: Current success : 50 \t All success : 81 \t mean(l0) : 6.802469 \t Current Learning Rate: 0.015\n",
            "Iteration 8600: Current success : 50 \t All success : 81 \t mean(l0) : 6.802469 \t Current Learning Rate: 0.014\n",
            "Iteration 8700: Current success : 50 \t All success : 81 \t mean(l0) : 6.802469 \t Current Learning Rate: 0.013\n",
            "Iteration 8800: Current success : 50 \t All success : 81 \t mean(l0) : 6.802469 \t Current Learning Rate: 0.012\n",
            "Iteration 8900: Current success : 50 \t All success : 81 \t mean(l0) : 6.802469 \t Current Learning Rate: 0.011\n",
            "Iteration 9000: Current success : 50 \t All success : 81 \t mean(l0) : 6.802469 \t Current Learning Rate: 0.010\n",
            "Iteration 9100: Current success : 50 \t All success : 81 \t mean(l0) : 6.802469 \t Current Learning Rate: 0.009\n",
            "Iteration 9200: Current success : 50 \t All success : 81 \t mean(l0) : 6.802469 \t Current Learning Rate: 0.008\n",
            "Iteration 9300: Current success : 50 \t All success : 81 \t mean(l0) : 6.802469 \t Current Learning Rate: 0.007\n",
            "Iteration 9400: Current success : 50 \t All success : 81 \t mean(l0) : 6.802469 \t Current Learning Rate: 0.006\n",
            "Iteration 9500: Current success : 50 \t All success : 81 \t mean(l0) : 6.802469 \t Current Learning Rate: 0.005\n",
            "Iteration 9600: Current success : 50 \t All success : 81 \t mean(l0) : 6.802469 \t Current Learning Rate: 0.004\n",
            "Iteration 9700: Current success : 50 \t All success : 81 \t mean(l0) : 6.802469 \t Current Learning Rate: 0.003\n",
            "Iteration 9800: Current success : 50 \t All success : 81 \t mean(l0) : 6.802469 \t Current Learning Rate: 0.002\n",
            "Iteration 9900: Current success : 50 \t All success : 81 \t mean(l0) : 6.802469 \t Current Learning Rate: 0.001\n",
            "Iteration 10000: Current success : 50 \t All success : 81 \t mean(l0) : 6.802469 \t Current Learning Rate: 0.000\n",
            "Iteration 10100: Current success : 43 \t All success : 85 \t mean(l0) : 7.941176 \t Current Learning Rate: 0.025\n",
            "Iteration 10200: Current success : 55 \t All success : 85 \t mean(l0) : 7.894118 \t Current Learning Rate: 0.024\n",
            "Iteration 10300: Current success : 56 \t All success : 85 \t mean(l0) : 7.894118 \t Current Learning Rate: 0.024\n",
            "Iteration 10400: Current success : 56 \t All success : 85 \t mean(l0) : 7.894118 \t Current Learning Rate: 0.023\n",
            "Iteration 10500: Current success : 55 \t All success : 85 \t mean(l0) : 7.894118 \t Current Learning Rate: 0.023\n",
            "Iteration 10600: Current success : 55 \t All success : 85 \t mean(l0) : 7.882353 \t Current Learning Rate: 0.022\n",
            "Iteration 10700: Current success : 52 \t All success : 85 \t mean(l0) : 7.882353 \t Current Learning Rate: 0.022\n",
            "Iteration 10800: Current success : 54 \t All success : 85 \t mean(l0) : 7.882353 \t Current Learning Rate: 0.021\n",
            "Iteration 10900: Current success : 54 \t All success : 85 \t mean(l0) : 7.882353 \t Current Learning Rate: 0.021\n",
            "Iteration 11000: Current success : 54 \t All success : 85 \t mean(l0) : 7.882353 \t Current Learning Rate: 0.020\n",
            "Iteration 11100: Current success : 51 \t All success : 85 \t mean(l0) : 7.882353 \t Current Learning Rate: 0.020\n",
            "Iteration 11200: Current success : 51 \t All success : 85 \t mean(l0) : 7.882353 \t Current Learning Rate: 0.019\n",
            "Iteration 11300: Current success : 54 \t All success : 85 \t mean(l0) : 7.882353 \t Current Learning Rate: 0.019\n",
            "Iteration 11400: Current success : 49 \t All success : 85 \t mean(l0) : 7.882353 \t Current Learning Rate: 0.018\n",
            "Iteration 11500: Current success : 49 \t All success : 85 \t mean(l0) : 7.882353 \t Current Learning Rate: 0.018\n",
            "Iteration 11600: Current success : 51 \t All success : 85 \t mean(l0) : 7.882353 \t Current Learning Rate: 0.017\n",
            "Iteration 11700: Current success : 50 \t All success : 85 \t mean(l0) : 7.882353 \t Current Learning Rate: 0.017\n",
            "Iteration 11800: Current success : 49 \t All success : 85 \t mean(l0) : 7.882353 \t Current Learning Rate: 0.016\n",
            "Iteration 11900: Current success : 51 \t All success : 85 \t mean(l0) : 7.882353 \t Current Learning Rate: 0.016\n",
            "Iteration 12000: Current success : 49 \t All success : 85 \t mean(l0) : 7.882353 \t Current Learning Rate: 0.015\n",
            "Iteration 12100: Current success : 52 \t All success : 85 \t mean(l0) : 7.882353 \t Current Learning Rate: 0.015\n",
            "Iteration 12200: Current success : 52 \t All success : 85 \t mean(l0) : 7.870588 \t Current Learning Rate: 0.014\n",
            "Iteration 12300: Current success : 49 \t All success : 85 \t mean(l0) : 7.870588 \t Current Learning Rate: 0.014\n",
            "Iteration 12400: Current success : 47 \t All success : 85 \t mean(l0) : 7.870588 \t Current Learning Rate: 0.013\n",
            "Iteration 12500: Current success : 50 \t All success : 85 \t mean(l0) : 7.870588 \t Current Learning Rate: 0.013\n",
            "Iteration 12600: Current success : 50 \t All success : 85 \t mean(l0) : 7.870588 \t Current Learning Rate: 0.012\n",
            "Iteration 12700: Current success : 49 \t All success : 85 \t mean(l0) : 7.870588 \t Current Learning Rate: 0.012\n",
            "Iteration 12800: Current success : 53 \t All success : 85 \t mean(l0) : 7.870588 \t Current Learning Rate: 0.011\n",
            "Iteration 12900: Current success : 53 \t All success : 85 \t mean(l0) : 7.870588 \t Current Learning Rate: 0.011\n",
            "Iteration 13000: Current success : 55 \t All success : 85 \t mean(l0) : 7.870588 \t Current Learning Rate: 0.010\n",
            "Iteration 13100: Current success : 55 \t All success : 85 \t mean(l0) : 7.870588 \t Current Learning Rate: 0.010\n",
            "Iteration 13200: Current success : 55 \t All success : 85 \t mean(l0) : 7.870588 \t Current Learning Rate: 0.009\n",
            "Iteration 13300: Current success : 55 \t All success : 85 \t mean(l0) : 7.870588 \t Current Learning Rate: 0.009\n",
            "Iteration 13400: Current success : 55 \t All success : 85 \t mean(l0) : 7.870588 \t Current Learning Rate: 0.008\n",
            "Iteration 13500: Current success : 55 \t All success : 85 \t mean(l0) : 7.870588 \t Current Learning Rate: 0.008\n",
            "Iteration 13600: Current success : 55 \t All success : 85 \t mean(l0) : 7.870588 \t Current Learning Rate: 0.007\n",
            "Iteration 13700: Current success : 55 \t All success : 85 \t mean(l0) : 7.870588 \t Current Learning Rate: 0.007\n",
            "Iteration 13800: Current success : 55 \t All success : 85 \t mean(l0) : 7.870588 \t Current Learning Rate: 0.006\n",
            "Iteration 13900: Current success : 55 \t All success : 85 \t mean(l0) : 7.870588 \t Current Learning Rate: 0.006\n",
            "Iteration 14000: Current success : 55 \t All success : 85 \t mean(l0) : 7.870588 \t Current Learning Rate: 0.005\n",
            "Iteration 14100: Current success : 55 \t All success : 85 \t mean(l0) : 7.870588 \t Current Learning Rate: 0.005\n",
            "Iteration 14200: Current success : 55 \t All success : 85 \t mean(l0) : 7.870588 \t Current Learning Rate: 0.004\n",
            "Iteration 14300: Current success : 55 \t All success : 85 \t mean(l0) : 7.870588 \t Current Learning Rate: 0.004\n",
            "Iteration 14400: Current success : 55 \t All success : 85 \t mean(l0) : 7.870588 \t Current Learning Rate: 0.003\n",
            "Iteration 14500: Current success : 55 \t All success : 85 \t mean(l0) : 7.870588 \t Current Learning Rate: 0.003\n",
            "Iteration 14600: Current success : 55 \t All success : 85 \t mean(l0) : 7.870588 \t Current Learning Rate: 0.002\n",
            "Iteration 14700: Current success : 55 \t All success : 85 \t mean(l0) : 7.870588 \t Current Learning Rate: 0.002\n",
            "Iteration 14800: Current success : 55 \t All success : 85 \t mean(l0) : 7.870588 \t Current Learning Rate: 0.001\n",
            "Iteration 14900: Current success : 55 \t All success : 85 \t mean(l0) : 7.870588 \t Current Learning Rate: 0.001\n",
            "Iteration 15000: Current success : 55 \t All success : 85 \t mean(l0) : 7.870588 \t Current Learning Rate: 0.000\n",
            "Iteration 15100: Current success : 52 \t All success : 85 \t mean(l0) : 6.882353 \t Current Learning Rate: 0.012\n",
            "Iteration 15200: Current success : 63 \t All success : 85 \t mean(l0) : 6.882353 \t Current Learning Rate: 0.012\n",
            "Iteration 15300: Current success : 62 \t All success : 85 \t mean(l0) : 6.882353 \t Current Learning Rate: 0.012\n",
            "Iteration 15400: Current success : 62 \t All success : 85 \t mean(l0) : 6.882353 \t Current Learning Rate: 0.012\n",
            "Iteration 15500: Current success : 61 \t All success : 85 \t mean(l0) : 6.882353 \t Current Learning Rate: 0.011\n",
            "Iteration 15600: Current success : 62 \t All success : 85 \t mean(l0) : 6.882353 \t Current Learning Rate: 0.011\n",
            "Iteration 15700: Current success : 60 \t All success : 85 \t mean(l0) : 6.882353 \t Current Learning Rate: 0.011\n",
            "Iteration 15800: Current success : 61 \t All success : 85 \t mean(l0) : 6.882353 \t Current Learning Rate: 0.011\n",
            "Iteration 15900: Current success : 60 \t All success : 85 \t mean(l0) : 6.882353 \t Current Learning Rate: 0.010\n",
            "Iteration 16000: Current success : 62 \t All success : 85 \t mean(l0) : 6.882353 \t Current Learning Rate: 0.010\n",
            "Iteration 16100: Current success : 61 \t All success : 85 \t mean(l0) : 6.882353 \t Current Learning Rate: 0.010\n",
            "Iteration 16200: Current success : 61 \t All success : 86 \t mean(l0) : 7.209302 \t Current Learning Rate: 0.010\n",
            "Iteration 16300: Current success : 62 \t All success : 86 \t mean(l0) : 7.209302 \t Current Learning Rate: 0.009\n",
            "Iteration 16400: Current success : 59 \t All success : 86 \t mean(l0) : 7.209302 \t Current Learning Rate: 0.009\n",
            "Iteration 16500: Current success : 62 \t All success : 86 \t mean(l0) : 7.209302 \t Current Learning Rate: 0.009\n",
            "Iteration 16600: Current success : 60 \t All success : 86 \t mean(l0) : 7.209302 \t Current Learning Rate: 0.009\n",
            "Iteration 16700: Current success : 61 \t All success : 86 \t mean(l0) : 7.197674 \t Current Learning Rate: 0.008\n",
            "Iteration 16800: Current success : 62 \t All success : 86 \t mean(l0) : 7.197674 \t Current Learning Rate: 0.008\n",
            "Iteration 16900: Current success : 60 \t All success : 86 \t mean(l0) : 7.197674 \t Current Learning Rate: 0.008\n",
            "Iteration 17000: Current success : 63 \t All success : 86 \t mean(l0) : 7.197674 \t Current Learning Rate: 0.008\n",
            "Iteration 17100: Current success : 62 \t All success : 86 \t mean(l0) : 7.186047 \t Current Learning Rate: 0.007\n",
            "Iteration 17200: Current success : 61 \t All success : 86 \t mean(l0) : 7.186047 \t Current Learning Rate: 0.007\n",
            "Iteration 17300: Current success : 62 \t All success : 86 \t mean(l0) : 7.186047 \t Current Learning Rate: 0.007\n",
            "Iteration 17400: Current success : 61 \t All success : 86 \t mean(l0) : 7.186047 \t Current Learning Rate: 0.007\n",
            "Iteration 17500: Current success : 61 \t All success : 86 \t mean(l0) : 7.186047 \t Current Learning Rate: 0.006\n",
            "Iteration 17600: Current success : 63 \t All success : 86 \t mean(l0) : 7.186047 \t Current Learning Rate: 0.006\n",
            "Iteration 17700: Current success : 61 \t All success : 86 \t mean(l0) : 7.186047 \t Current Learning Rate: 0.006\n",
            "Iteration 17800: Current success : 61 \t All success : 86 \t mean(l0) : 7.186047 \t Current Learning Rate: 0.006\n",
            "Iteration 17900: Current success : 63 \t All success : 86 \t mean(l0) : 7.186047 \t Current Learning Rate: 0.005\n",
            "Iteration 18000: Current success : 62 \t All success : 86 \t mean(l0) : 7.186047 \t Current Learning Rate: 0.005\n",
            "Iteration 18100: Current success : 62 \t All success : 86 \t mean(l0) : 7.186047 \t Current Learning Rate: 0.005\n",
            "Iteration 18200: Current success : 62 \t All success : 86 \t mean(l0) : 7.186047 \t Current Learning Rate: 0.005\n",
            "Iteration 18300: Current success : 62 \t All success : 86 \t mean(l0) : 7.186047 \t Current Learning Rate: 0.004\n",
            "Iteration 18400: Current success : 62 \t All success : 86 \t mean(l0) : 7.186047 \t Current Learning Rate: 0.004\n",
            "Iteration 18500: Current success : 62 \t All success : 86 \t mean(l0) : 7.186047 \t Current Learning Rate: 0.004\n",
            "Iteration 18600: Current success : 62 \t All success : 86 \t mean(l0) : 7.186047 \t Current Learning Rate: 0.004\n",
            "Iteration 18700: Current success : 62 \t All success : 86 \t mean(l0) : 7.186047 \t Current Learning Rate: 0.003\n",
            "Iteration 18800: Current success : 62 \t All success : 86 \t mean(l0) : 7.186047 \t Current Learning Rate: 0.003\n",
            "Iteration 18900: Current success : 62 \t All success : 86 \t mean(l0) : 7.186047 \t Current Learning Rate: 0.003\n",
            "Iteration 19000: Current success : 62 \t All success : 86 \t mean(l0) : 7.186047 \t Current Learning Rate: 0.003\n",
            "Iteration 19100: Current success : 62 \t All success : 86 \t mean(l0) : 7.186047 \t Current Learning Rate: 0.002\n",
            "Iteration 19200: Current success : 62 \t All success : 86 \t mean(l0) : 7.186047 \t Current Learning Rate: 0.002\n",
            "Iteration 19300: Current success : 62 \t All success : 86 \t mean(l0) : 7.186047 \t Current Learning Rate: 0.002\n",
            "Iteration 19400: Current success : 62 \t All success : 86 \t mean(l0) : 7.186047 \t Current Learning Rate: 0.002\n",
            "Iteration 19500: Current success : 62 \t All success : 86 \t mean(l0) : 7.186047 \t Current Learning Rate: 0.001\n",
            "Iteration 19600: Current success : 62 \t All success : 86 \t mean(l0) : 7.186047 \t Current Learning Rate: 0.001\n",
            "Iteration 19700: Current success : 62 \t All success : 86 \t mean(l0) : 7.186047 \t Current Learning Rate: 0.001\n",
            "Iteration 19800: Current success : 62 \t All success : 86 \t mean(l0) : 7.186047 \t Current Learning Rate: 0.001\n",
            "Iteration 19900: Current success : 62 \t All success : 86 \t mean(l0) : 7.186047 \t Current Learning Rate: 0.000\n",
            "Iteration 20000: Current success : 62 \t All success : 86 \t mean(l0) : 7.186047 \t Current Learning Rate: 0.000\n",
            "outer_step 2: all success : 105 \t mean(l0)(success) : 6.8952\n",
            "-------------------------------------------------------------\n",
            "tensor([100., 100., 100.,  ..., 100., 100., 100.], device='cuda:0')\n",
            "Iteration 100: Current success : 68 \t All success : 74 \t mean(l0) : 10.918920 \t Current Learning Rate: 0.098\n",
            "Iteration 200: Current success : 69 \t All success : 76 \t mean(l0) : 10.881579 \t Current Learning Rate: 0.096\n",
            "Iteration 300: Current success : 72 \t All success : 80 \t mean(l0) : 11.537500 \t Current Learning Rate: 0.094\n",
            "Iteration 400: Current success : 70 \t All success : 80 \t mean(l0) : 11.462501 \t Current Learning Rate: 0.092\n",
            "Iteration 500: Current success : 69 \t All success : 82 \t mean(l0) : 11.500000 \t Current Learning Rate: 0.090\n",
            "Iteration 600: Current success : 71 \t All success : 84 \t mean(l0) : 12.440476 \t Current Learning Rate: 0.088\n",
            "Iteration 700: Current success : 75 \t All success : 89 \t mean(l0) : 13.853932 \t Current Learning Rate: 0.086\n",
            "Iteration 800: Current success : 74 \t All success : 90 \t mean(l0) : 13.600000 \t Current Learning Rate: 0.084\n",
            "Iteration 900: Current success : 75 \t All success : 92 \t mean(l0) : 14.043479 \t Current Learning Rate: 0.082\n",
            "Iteration 1000: Current success : 75 \t All success : 92 \t mean(l0) : 14.043479 \t Current Learning Rate: 0.080\n",
            "Iteration 1100: Current success : 76 \t All success : 94 \t mean(l0) : 14.276595 \t Current Learning Rate: 0.078\n",
            "Iteration 1200: Current success : 76 \t All success : 94 \t mean(l0) : 14.265957 \t Current Learning Rate: 0.076\n",
            "Iteration 1300: Current success : 76 \t All success : 94 \t mean(l0) : 14.244680 \t Current Learning Rate: 0.074\n",
            "Iteration 1400: Current success : 76 \t All success : 94 \t mean(l0) : 14.244680 \t Current Learning Rate: 0.072\n",
            "Iteration 1500: Current success : 76 \t All success : 94 \t mean(l0) : 14.244680 \t Current Learning Rate: 0.070\n",
            "Iteration 1600: Current success : 76 \t All success : 94 \t mean(l0) : 14.244680 \t Current Learning Rate: 0.068\n",
            "Iteration 1700: Current success : 76 \t All success : 94 \t mean(l0) : 14.244680 \t Current Learning Rate: 0.066\n",
            "Iteration 1800: Current success : 76 \t All success : 94 \t mean(l0) : 14.244680 \t Current Learning Rate: 0.064\n",
            "Iteration 1900: Current success : 76 \t All success : 94 \t mean(l0) : 14.244680 \t Current Learning Rate: 0.062\n",
            "Iteration 2000: Current success : 76 \t All success : 94 \t mean(l0) : 14.244680 \t Current Learning Rate: 0.060\n",
            "Iteration 2100: Current success : 76 \t All success : 94 \t mean(l0) : 13.648935 \t Current Learning Rate: 0.058\n",
            "Iteration 2200: Current success : 76 \t All success : 94 \t mean(l0) : 13.648935 \t Current Learning Rate: 0.056\n",
            "Iteration 2300: Current success : 76 \t All success : 94 \t mean(l0) : 13.648935 \t Current Learning Rate: 0.054\n",
            "Iteration 2400: Current success : 76 \t All success : 94 \t mean(l0) : 13.648935 \t Current Learning Rate: 0.052\n",
            "Iteration 2500: Current success : 76 \t All success : 94 \t mean(l0) : 13.648935 \t Current Learning Rate: 0.050\n",
            "Iteration 2600: Current success : 76 \t All success : 94 \t mean(l0) : 13.648935 \t Current Learning Rate: 0.048\n",
            "Iteration 2700: Current success : 83 \t All success : 98 \t mean(l0) : 13.336735 \t Current Learning Rate: 0.046\n",
            "Iteration 2800: Current success : 93 \t All success : 107 \t mean(l0) : 13.121495 \t Current Learning Rate: 0.044\n",
            "Iteration 2900: Current success : 94 \t All success : 111 \t mean(l0) : 13.297297 \t Current Learning Rate: 0.042\n",
            "Iteration 3000: Current success : 94 \t All success : 111 \t mean(l0) : 13.297297 \t Current Learning Rate: 0.040\n",
            "Iteration 3100: Current success : 95 \t All success : 112 \t mean(l0) : 13.223215 \t Current Learning Rate: 0.038\n",
            "Iteration 3200: Current success : 95 \t All success : 112 \t mean(l0) : 13.223215 \t Current Learning Rate: 0.036\n",
            "Iteration 3300: Current success : 95 \t All success : 112 \t mean(l0) : 13.223215 \t Current Learning Rate: 0.034\n",
            "Iteration 3400: Current success : 95 \t All success : 112 \t mean(l0) : 13.223215 \t Current Learning Rate: 0.032\n",
            "Iteration 3500: Current success : 95 \t All success : 112 \t mean(l0) : 13.223215 \t Current Learning Rate: 0.030\n",
            "Iteration 3600: Current success : 95 \t All success : 112 \t mean(l0) : 13.223215 \t Current Learning Rate: 0.028\n",
            "Iteration 3700: Current success : 95 \t All success : 112 \t mean(l0) : 13.223215 \t Current Learning Rate: 0.026\n",
            "Iteration 3800: Current success : 95 \t All success : 112 \t mean(l0) : 13.223215 \t Current Learning Rate: 0.024\n",
            "Iteration 3900: Current success : 95 \t All success : 112 \t mean(l0) : 13.223215 \t Current Learning Rate: 0.022\n",
            "Iteration 4000: Current success : 95 \t All success : 112 \t mean(l0) : 13.223215 \t Current Learning Rate: 0.020\n",
            "Iteration 4100: Current success : 95 \t All success : 112 \t mean(l0) : 13.223215 \t Current Learning Rate: 0.018\n",
            "Iteration 4200: Current success : 95 \t All success : 112 \t mean(l0) : 13.223215 \t Current Learning Rate: 0.016\n",
            "Iteration 4300: Current success : 95 \t All success : 112 \t mean(l0) : 13.223215 \t Current Learning Rate: 0.014\n",
            "Iteration 4400: Current success : 95 \t All success : 112 \t mean(l0) : 13.223215 \t Current Learning Rate: 0.012\n",
            "Iteration 4500: Current success : 95 \t All success : 112 \t mean(l0) : 13.223215 \t Current Learning Rate: 0.010\n",
            "Iteration 4600: Current success : 95 \t All success : 112 \t mean(l0) : 13.223215 \t Current Learning Rate: 0.008\n",
            "Iteration 4700: Current success : 95 \t All success : 112 \t mean(l0) : 13.223215 \t Current Learning Rate: 0.006\n",
            "Iteration 4800: Current success : 95 \t All success : 112 \t mean(l0) : 13.223215 \t Current Learning Rate: 0.004\n",
            "Iteration 4900: Current success : 95 \t All success : 112 \t mean(l0) : 13.223215 \t Current Learning Rate: 0.002\n",
            "Iteration 5000: Current success : 95 \t All success : 112 \t mean(l0) : 13.223215 \t Current Learning Rate: 0.000\n",
            "Iteration 5100: Current success : 98 \t All success : 137 \t mean(l0) : 16.167883 \t Current Learning Rate: 0.049\n",
            "Iteration 5200: Current success : 104 \t All success : 142 \t mean(l0) : 16.485916 \t Current Learning Rate: 0.048\n",
            "Iteration 5300: Current success : 102 \t All success : 142 \t mean(l0) : 16.450705 \t Current Learning Rate: 0.047\n",
            "Iteration 5400: Current success : 103 \t All success : 142 \t mean(l0) : 16.450705 \t Current Learning Rate: 0.046\n",
            "Iteration 5500: Current success : 103 \t All success : 142 \t mean(l0) : 16.443661 \t Current Learning Rate: 0.045\n",
            "Iteration 5600: Current success : 102 \t All success : 142 \t mean(l0) : 16.443661 \t Current Learning Rate: 0.044\n",
            "Iteration 5700: Current success : 104 \t All success : 143 \t mean(l0) : 16.363636 \t Current Learning Rate: 0.043\n",
            "Iteration 5800: Current success : 100 \t All success : 143 \t mean(l0) : 16.363636 \t Current Learning Rate: 0.042\n",
            "Iteration 5900: Current success : 99 \t All success : 143 \t mean(l0) : 16.363636 \t Current Learning Rate: 0.041\n",
            "Iteration 6000: Current success : 99 \t All success : 143 \t mean(l0) : 16.363636 \t Current Learning Rate: 0.040\n",
            "Iteration 6100: Current success : 97 \t All success : 143 \t mean(l0) : 16.363636 \t Current Learning Rate: 0.039\n",
            "Iteration 6200: Current success : 97 \t All success : 143 \t mean(l0) : 16.363636 \t Current Learning Rate: 0.038\n",
            "Iteration 6300: Current success : 95 \t All success : 143 \t mean(l0) : 16.363636 \t Current Learning Rate: 0.037\n",
            "Iteration 6400: Current success : 95 \t All success : 144 \t mean(l0) : 16.284723 \t Current Learning Rate: 0.036\n",
            "Iteration 6500: Current success : 97 \t All success : 144 \t mean(l0) : 16.284723 \t Current Learning Rate: 0.035\n",
            "Iteration 6600: Current success : 95 \t All success : 144 \t mean(l0) : 16.284723 \t Current Learning Rate: 0.034\n",
            "Iteration 6700: Current success : 95 \t All success : 144 \t mean(l0) : 16.270834 \t Current Learning Rate: 0.033\n",
            "Iteration 6800: Current success : 95 \t All success : 144 \t mean(l0) : 16.270834 \t Current Learning Rate: 0.032\n",
            "Iteration 6900: Current success : 95 \t All success : 144 \t mean(l0) : 16.270834 \t Current Learning Rate: 0.031\n",
            "Iteration 7000: Current success : 95 \t All success : 144 \t mean(l0) : 16.270834 \t Current Learning Rate: 0.030\n",
            "Iteration 7100: Current success : 95 \t All success : 144 \t mean(l0) : 16.270834 \t Current Learning Rate: 0.029\n",
            "Iteration 7200: Current success : 95 \t All success : 144 \t mean(l0) : 16.270834 \t Current Learning Rate: 0.028\n",
            "Iteration 7300: Current success : 95 \t All success : 144 \t mean(l0) : 16.270834 \t Current Learning Rate: 0.027\n",
            "Iteration 7400: Current success : 95 \t All success : 144 \t mean(l0) : 16.270834 \t Current Learning Rate: 0.026\n",
            "Iteration 7500: Current success : 95 \t All success : 144 \t mean(l0) : 16.270834 \t Current Learning Rate: 0.025\n",
            "Iteration 7600: Current success : 96 \t All success : 144 \t mean(l0) : 16.270834 \t Current Learning Rate: 0.024\n",
            "Iteration 7700: Current success : 98 \t All success : 144 \t mean(l0) : 16.270834 \t Current Learning Rate: 0.023\n",
            "Iteration 7800: Current success : 103 \t All success : 144 \t mean(l0) : 16.270834 \t Current Learning Rate: 0.022\n",
            "Iteration 7900: Current success : 103 \t All success : 144 \t mean(l0) : 16.270834 \t Current Learning Rate: 0.021\n",
            "Iteration 8000: Current success : 103 \t All success : 144 \t mean(l0) : 16.270834 \t Current Learning Rate: 0.020\n",
            "Iteration 8100: Current success : 103 \t All success : 144 \t mean(l0) : 16.270834 \t Current Learning Rate: 0.019\n",
            "Iteration 8200: Current success : 103 \t All success : 144 \t mean(l0) : 16.270834 \t Current Learning Rate: 0.018\n",
            "Iteration 8300: Current success : 103 \t All success : 144 \t mean(l0) : 16.270834 \t Current Learning Rate: 0.017\n",
            "Iteration 8400: Current success : 103 \t All success : 144 \t mean(l0) : 16.270834 \t Current Learning Rate: 0.016\n",
            "Iteration 8500: Current success : 103 \t All success : 144 \t mean(l0) : 16.270834 \t Current Learning Rate: 0.015\n",
            "Iteration 8600: Current success : 103 \t All success : 144 \t mean(l0) : 16.270834 \t Current Learning Rate: 0.014\n",
            "Iteration 8700: Current success : 103 \t All success : 144 \t mean(l0) : 16.270834 \t Current Learning Rate: 0.013\n",
            "Iteration 8800: Current success : 103 \t All success : 144 \t mean(l0) : 16.270834 \t Current Learning Rate: 0.012\n",
            "Iteration 8900: Current success : 103 \t All success : 144 \t mean(l0) : 16.270834 \t Current Learning Rate: 0.011\n",
            "Iteration 9000: Current success : 103 \t All success : 144 \t mean(l0) : 16.270834 \t Current Learning Rate: 0.010\n",
            "Iteration 9100: Current success : 103 \t All success : 144 \t mean(l0) : 16.270834 \t Current Learning Rate: 0.009\n",
            "Iteration 9200: Current success : 103 \t All success : 144 \t mean(l0) : 16.270834 \t Current Learning Rate: 0.008\n",
            "Iteration 9300: Current success : 103 \t All success : 144 \t mean(l0) : 16.270834 \t Current Learning Rate: 0.007\n",
            "Iteration 9400: Current success : 103 \t All success : 144 \t mean(l0) : 16.270834 \t Current Learning Rate: 0.006\n",
            "Iteration 9500: Current success : 103 \t All success : 144 \t mean(l0) : 16.270834 \t Current Learning Rate: 0.005\n",
            "Iteration 9600: Current success : 103 \t All success : 144 \t mean(l0) : 16.270834 \t Current Learning Rate: 0.004\n",
            "Iteration 9700: Current success : 103 \t All success : 144 \t mean(l0) : 16.243055 \t Current Learning Rate: 0.003\n",
            "Iteration 9800: Current success : 103 \t All success : 144 \t mean(l0) : 16.243055 \t Current Learning Rate: 0.002\n",
            "Iteration 9900: Current success : 103 \t All success : 144 \t mean(l0) : 16.243055 \t Current Learning Rate: 0.001\n",
            "Iteration 10000: Current success : 103 \t All success : 144 \t mean(l0) : 16.243055 \t Current Learning Rate: 0.000\n",
            "Iteration 10100: Current success : 66 \t All success : 144 \t mean(l0) : 16.159723 \t Current Learning Rate: 0.025\n",
            "Iteration 10200: Current success : 73 \t All success : 144 \t mean(l0) : 16.159723 \t Current Learning Rate: 0.024\n",
            "Iteration 10300: Current success : 76 \t All success : 148 \t mean(l0) : 16.094595 \t Current Learning Rate: 0.024\n",
            "Iteration 10400: Current success : 77 \t All success : 148 \t mean(l0) : 16.074326 \t Current Learning Rate: 0.023\n",
            "Iteration 10500: Current success : 77 \t All success : 148 \t mean(l0) : 16.074326 \t Current Learning Rate: 0.023\n",
            "Iteration 10600: Current success : 77 \t All success : 148 \t mean(l0) : 16.074326 \t Current Learning Rate: 0.022\n",
            "Iteration 10700: Current success : 76 \t All success : 148 \t mean(l0) : 16.074326 \t Current Learning Rate: 0.022\n",
            "Iteration 10800: Current success : 77 \t All success : 148 \t mean(l0) : 16.074326 \t Current Learning Rate: 0.021\n",
            "Iteration 10900: Current success : 76 \t All success : 148 \t mean(l0) : 16.074326 \t Current Learning Rate: 0.021\n",
            "Iteration 11000: Current success : 76 \t All success : 148 \t mean(l0) : 16.047298 \t Current Learning Rate: 0.020\n",
            "Iteration 11100: Current success : 76 \t All success : 148 \t mean(l0) : 16.020271 \t Current Learning Rate: 0.020\n",
            "Iteration 11200: Current success : 76 \t All success : 148 \t mean(l0) : 15.952703 \t Current Learning Rate: 0.019\n",
            "Iteration 11300: Current success : 76 \t All success : 148 \t mean(l0) : 15.932433 \t Current Learning Rate: 0.019\n",
            "Iteration 11400: Current success : 76 \t All success : 148 \t mean(l0) : 15.925676 \t Current Learning Rate: 0.018\n",
            "Iteration 11500: Current success : 78 \t All success : 148 \t mean(l0) : 15.898649 \t Current Learning Rate: 0.018\n",
            "Iteration 11600: Current success : 76 \t All success : 149 \t mean(l0) : 15.859060 \t Current Learning Rate: 0.017\n",
            "Iteration 11700: Current success : 76 \t All success : 149 \t mean(l0) : 15.859060 \t Current Learning Rate: 0.017\n",
            "Iteration 11800: Current success : 76 \t All success : 149 \t mean(l0) : 15.845637 \t Current Learning Rate: 0.016\n",
            "Iteration 11900: Current success : 77 \t All success : 149 \t mean(l0) : 15.845637 \t Current Learning Rate: 0.016\n",
            "Iteration 12000: Current success : 76 \t All success : 149 \t mean(l0) : 15.845637 \t Current Learning Rate: 0.015\n",
            "Iteration 12100: Current success : 76 \t All success : 149 \t mean(l0) : 15.845637 \t Current Learning Rate: 0.015\n",
            "Iteration 12200: Current success : 77 \t All success : 149 \t mean(l0) : 15.845637 \t Current Learning Rate: 0.014\n",
            "Iteration 12300: Current success : 76 \t All success : 149 \t mean(l0) : 15.845637 \t Current Learning Rate: 0.014\n",
            "Iteration 12400: Current success : 76 \t All success : 149 \t mean(l0) : 15.845637 \t Current Learning Rate: 0.013\n",
            "Iteration 12500: Current success : 76 \t All success : 149 \t mean(l0) : 15.845637 \t Current Learning Rate: 0.013\n",
            "Iteration 12600: Current success : 77 \t All success : 149 \t mean(l0) : 15.845637 \t Current Learning Rate: 0.012\n",
            "Iteration 12700: Current success : 77 \t All success : 149 \t mean(l0) : 15.845637 \t Current Learning Rate: 0.012\n",
            "Iteration 12800: Current success : 76 \t All success : 149 \t mean(l0) : 15.845637 \t Current Learning Rate: 0.011\n",
            "Iteration 12900: Current success : 77 \t All success : 149 \t mean(l0) : 15.845637 \t Current Learning Rate: 0.011\n",
            "Iteration 13000: Current success : 78 \t All success : 149 \t mean(l0) : 15.845637 \t Current Learning Rate: 0.010\n",
            "Iteration 13100: Current success : 78 \t All success : 149 \t mean(l0) : 15.845637 \t Current Learning Rate: 0.010\n",
            "Iteration 13200: Current success : 78 \t All success : 149 \t mean(l0) : 15.845637 \t Current Learning Rate: 0.009\n",
            "Iteration 13300: Current success : 78 \t All success : 149 \t mean(l0) : 15.845637 \t Current Learning Rate: 0.009\n",
            "Iteration 13400: Current success : 78 \t All success : 149 \t mean(l0) : 15.845637 \t Current Learning Rate: 0.008\n",
            "Iteration 13500: Current success : 78 \t All success : 149 \t mean(l0) : 15.845637 \t Current Learning Rate: 0.008\n",
            "Iteration 13600: Current success : 78 \t All success : 149 \t mean(l0) : 15.845637 \t Current Learning Rate: 0.007\n",
            "Iteration 13700: Current success : 78 \t All success : 149 \t mean(l0) : 15.845637 \t Current Learning Rate: 0.007\n",
            "Iteration 13800: Current success : 78 \t All success : 149 \t mean(l0) : 15.845637 \t Current Learning Rate: 0.006\n",
            "Iteration 13900: Current success : 78 \t All success : 149 \t mean(l0) : 15.845637 \t Current Learning Rate: 0.006\n",
            "Iteration 14000: Current success : 78 \t All success : 149 \t mean(l0) : 15.845637 \t Current Learning Rate: 0.005\n",
            "Iteration 14100: Current success : 78 \t All success : 149 \t mean(l0) : 15.845637 \t Current Learning Rate: 0.005\n",
            "Iteration 14200: Current success : 78 \t All success : 149 \t mean(l0) : 15.845637 \t Current Learning Rate: 0.004\n",
            "Iteration 14300: Current success : 78 \t All success : 149 \t mean(l0) : 15.845637 \t Current Learning Rate: 0.004\n",
            "Iteration 14400: Current success : 78 \t All success : 149 \t mean(l0) : 15.845637 \t Current Learning Rate: 0.003\n",
            "Iteration 14500: Current success : 78 \t All success : 149 \t mean(l0) : 15.845637 \t Current Learning Rate: 0.003\n",
            "Iteration 14600: Current success : 78 \t All success : 149 \t mean(l0) : 15.845637 \t Current Learning Rate: 0.002\n",
            "Iteration 14700: Current success : 78 \t All success : 149 \t mean(l0) : 15.845637 \t Current Learning Rate: 0.002\n",
            "Iteration 14800: Current success : 78 \t All success : 149 \t mean(l0) : 15.845637 \t Current Learning Rate: 0.001\n",
            "Iteration 14900: Current success : 78 \t All success : 149 \t mean(l0) : 15.845637 \t Current Learning Rate: 0.001\n",
            "Iteration 15000: Current success : 78 \t All success : 149 \t mean(l0) : 15.845637 \t Current Learning Rate: 0.000\n",
            "Iteration 15100: Current success : 69 \t All success : 150 \t mean(l0) : 15.680000 \t Current Learning Rate: 0.012\n",
            "Iteration 15200: Current success : 75 \t All success : 150 \t mean(l0) : 15.613334 \t Current Learning Rate: 0.012\n",
            "Iteration 15300: Current success : 76 \t All success : 150 \t mean(l0) : 15.593333 \t Current Learning Rate: 0.012\n",
            "Iteration 15400: Current success : 76 \t All success : 150 \t mean(l0) : 15.593333 \t Current Learning Rate: 0.012\n",
            "Iteration 15500: Current success : 76 \t All success : 150 \t mean(l0) : 15.593333 \t Current Learning Rate: 0.011\n",
            "Iteration 15600: Current success : 76 \t All success : 150 \t mean(l0) : 15.593333 \t Current Learning Rate: 0.011\n",
            "Iteration 15700: Current success : 76 \t All success : 150 \t mean(l0) : 15.593333 \t Current Learning Rate: 0.011\n",
            "Iteration 15800: Current success : 76 \t All success : 150 \t mean(l0) : 15.593333 \t Current Learning Rate: 0.011\n",
            "Iteration 15900: Current success : 76 \t All success : 150 \t mean(l0) : 15.593333 \t Current Learning Rate: 0.010\n",
            "Iteration 16000: Current success : 76 \t All success : 150 \t mean(l0) : 15.593333 \t Current Learning Rate: 0.010\n",
            "Iteration 16100: Current success : 76 \t All success : 150 \t mean(l0) : 15.593333 \t Current Learning Rate: 0.010\n",
            "Iteration 16200: Current success : 76 \t All success : 150 \t mean(l0) : 15.593333 \t Current Learning Rate: 0.010\n",
            "Iteration 16300: Current success : 76 \t All success : 150 \t mean(l0) : 15.593333 \t Current Learning Rate: 0.009\n",
            "Iteration 16400: Current success : 76 \t All success : 150 \t mean(l0) : 15.593333 \t Current Learning Rate: 0.009\n",
            "Iteration 16500: Current success : 76 \t All success : 150 \t mean(l0) : 15.593333 \t Current Learning Rate: 0.009\n",
            "Iteration 16600: Current success : 75 \t All success : 150 \t mean(l0) : 15.593333 \t Current Learning Rate: 0.009\n",
            "Iteration 16700: Current success : 76 \t All success : 150 \t mean(l0) : 15.593333 \t Current Learning Rate: 0.008\n",
            "Iteration 16800: Current success : 76 \t All success : 150 \t mean(l0) : 15.593333 \t Current Learning Rate: 0.008\n",
            "Iteration 16900: Current success : 76 \t All success : 150 \t mean(l0) : 15.593333 \t Current Learning Rate: 0.008\n",
            "Iteration 17000: Current success : 76 \t All success : 150 \t mean(l0) : 15.593333 \t Current Learning Rate: 0.008\n",
            "Iteration 17100: Current success : 75 \t All success : 150 \t mean(l0) : 15.593333 \t Current Learning Rate: 0.007\n",
            "Iteration 17200: Current success : 76 \t All success : 150 \t mean(l0) : 15.593333 \t Current Learning Rate: 0.007\n",
            "Iteration 17300: Current success : 76 \t All success : 150 \t mean(l0) : 15.593333 \t Current Learning Rate: 0.007\n",
            "Iteration 17400: Current success : 75 \t All success : 150 \t mean(l0) : 15.593333 \t Current Learning Rate: 0.007\n",
            "Iteration 17500: Current success : 76 \t All success : 150 \t mean(l0) : 15.593333 \t Current Learning Rate: 0.006\n",
            "Iteration 17600: Current success : 76 \t All success : 150 \t mean(l0) : 15.593333 \t Current Learning Rate: 0.006\n",
            "Iteration 17700: Current success : 76 \t All success : 150 \t mean(l0) : 15.593333 \t Current Learning Rate: 0.006\n",
            "Iteration 17800: Current success : 76 \t All success : 150 \t mean(l0) : 15.593333 \t Current Learning Rate: 0.006\n",
            "Iteration 17900: Current success : 76 \t All success : 150 \t mean(l0) : 15.593333 \t Current Learning Rate: 0.005\n",
            "Iteration 18000: Current success : 76 \t All success : 150 \t mean(l0) : 15.593333 \t Current Learning Rate: 0.005\n",
            "Iteration 18100: Current success : 76 \t All success : 150 \t mean(l0) : 15.593333 \t Current Learning Rate: 0.005\n",
            "Iteration 18200: Current success : 76 \t All success : 150 \t mean(l0) : 15.593333 \t Current Learning Rate: 0.005\n",
            "Iteration 18300: Current success : 76 \t All success : 150 \t mean(l0) : 15.593333 \t Current Learning Rate: 0.004\n",
            "Iteration 18400: Current success : 76 \t All success : 150 \t mean(l0) : 15.593333 \t Current Learning Rate: 0.004\n",
            "Iteration 18500: Current success : 76 \t All success : 150 \t mean(l0) : 15.593333 \t Current Learning Rate: 0.004\n",
            "Iteration 18600: Current success : 76 \t All success : 150 \t mean(l0) : 15.593333 \t Current Learning Rate: 0.004\n",
            "Iteration 18700: Current success : 76 \t All success : 150 \t mean(l0) : 15.593333 \t Current Learning Rate: 0.003\n",
            "Iteration 18800: Current success : 76 \t All success : 150 \t mean(l0) : 15.593333 \t Current Learning Rate: 0.003\n",
            "Iteration 18900: Current success : 76 \t All success : 150 \t mean(l0) : 15.593333 \t Current Learning Rate: 0.003\n",
            "Iteration 19000: Current success : 76 \t All success : 150 \t mean(l0) : 15.593333 \t Current Learning Rate: 0.003\n",
            "Iteration 19100: Current success : 76 \t All success : 150 \t mean(l0) : 15.593333 \t Current Learning Rate: 0.002\n",
            "Iteration 19200: Current success : 76 \t All success : 150 \t mean(l0) : 15.593333 \t Current Learning Rate: 0.002\n",
            "Iteration 19300: Current success : 76 \t All success : 150 \t mean(l0) : 15.593333 \t Current Learning Rate: 0.002\n",
            "Iteration 19400: Current success : 76 \t All success : 150 \t mean(l0) : 15.593333 \t Current Learning Rate: 0.002\n",
            "Iteration 19500: Current success : 76 \t All success : 150 \t mean(l0) : 15.593333 \t Current Learning Rate: 0.001\n",
            "Iteration 19600: Current success : 76 \t All success : 150 \t mean(l0) : 15.580000 \t Current Learning Rate: 0.001\n",
            "Iteration 19700: Current success : 76 \t All success : 150 \t mean(l0) : 15.580000 \t Current Learning Rate: 0.001\n",
            "Iteration 19800: Current success : 76 \t All success : 150 \t mean(l0) : 15.580000 \t Current Learning Rate: 0.001\n",
            "Iteration 19900: Current success : 76 \t All success : 150 \t mean(l0) : 15.580000 \t Current Learning Rate: 0.000\n",
            "Iteration 20000: Current success : 76 \t All success : 150 \t mean(l0) : 15.580000 \t Current Learning Rate: 0.000\n",
            "outer_step 3: all success : 184 \t mean(l0)(success) : 14.2500\n",
            "-------------------------------------------------------------\n",
            "tensor([  55., 1000., 1000.,  ..., 1000., 1000., 1000.], device='cuda:0')\n",
            "Iteration 100: Current success : 33 \t All success : 72 \t mean(l0) : 9.180555 \t Current Learning Rate: 0.098\n",
            "Iteration 200: Current success : 39 \t All success : 81 \t mean(l0) : 11.481482 \t Current Learning Rate: 0.096\n",
            "Iteration 300: Current success : 44 \t All success : 92 \t mean(l0) : 15.750000 \t Current Learning Rate: 0.094\n",
            "Iteration 400: Current success : 50 \t All success : 100 \t mean(l0) : 17.900000 \t Current Learning Rate: 0.092\n",
            "Iteration 500: Current success : 52 \t All success : 100 \t mean(l0) : 17.770000 \t Current Learning Rate: 0.090\n",
            "Iteration 600: Current success : 55 \t All success : 110 \t mean(l0) : 18.081818 \t Current Learning Rate: 0.088\n",
            "Iteration 700: Current success : 56 \t All success : 111 \t mean(l0) : 18.009010 \t Current Learning Rate: 0.086\n",
            "Iteration 800: Current success : 57 \t All success : 113 \t mean(l0) : 18.902655 \t Current Learning Rate: 0.084\n",
            "Iteration 900: Current success : 61 \t All success : 116 \t mean(l0) : 19.474138 \t Current Learning Rate: 0.082\n",
            "Iteration 1000: Current success : 61 \t All success : 117 \t mean(l0) : 19.692308 \t Current Learning Rate: 0.080\n",
            "Iteration 1100: Current success : 61 \t All success : 117 \t mean(l0) : 19.675215 \t Current Learning Rate: 0.078\n",
            "Iteration 1200: Current success : 66 \t All success : 120 \t mean(l0) : 20.133335 \t Current Learning Rate: 0.076\n",
            "Iteration 1300: Current success : 68 \t All success : 122 \t mean(l0) : 20.540983 \t Current Learning Rate: 0.074\n",
            "Iteration 1400: Current success : 68 \t All success : 122 \t mean(l0) : 20.540983 \t Current Learning Rate: 0.072\n",
            "Iteration 1500: Current success : 65 \t All success : 122 \t mean(l0) : 20.540983 \t Current Learning Rate: 0.070\n",
            "Iteration 1600: Current success : 68 \t All success : 122 \t mean(l0) : 20.532785 \t Current Learning Rate: 0.068\n",
            "Iteration 1700: Current success : 68 \t All success : 122 \t mean(l0) : 20.532785 \t Current Learning Rate: 0.066\n",
            "Iteration 1800: Current success : 67 \t All success : 122 \t mean(l0) : 20.532785 \t Current Learning Rate: 0.064\n",
            "Iteration 1900: Current success : 67 \t All success : 122 \t mean(l0) : 20.532785 \t Current Learning Rate: 0.062\n",
            "Iteration 2000: Current success : 67 \t All success : 122 \t mean(l0) : 20.467213 \t Current Learning Rate: 0.060\n",
            "Iteration 2100: Current success : 64 \t All success : 122 \t mean(l0) : 20.459015 \t Current Learning Rate: 0.058\n",
            "Iteration 2200: Current success : 67 \t All success : 122 \t mean(l0) : 20.459015 \t Current Learning Rate: 0.056\n",
            "Iteration 2300: Current success : 72 \t All success : 127 \t mean(l0) : 20.614174 \t Current Learning Rate: 0.054\n",
            "Iteration 2400: Current success : 72 \t All success : 127 \t mean(l0) : 20.385826 \t Current Learning Rate: 0.052\n",
            "Iteration 2500: Current success : 72 \t All success : 130 \t mean(l0) : 20.307692 \t Current Learning Rate: 0.050\n",
            "Iteration 2600: Current success : 72 \t All success : 130 \t mean(l0) : 20.169231 \t Current Learning Rate: 0.048\n",
            "Iteration 2700: Current success : 79 \t All success : 132 \t mean(l0) : 19.992424 \t Current Learning Rate: 0.046\n",
            "Iteration 2800: Current success : 82 \t All success : 135 \t mean(l0) : 19.666666 \t Current Learning Rate: 0.044\n",
            "Iteration 2900: Current success : 83 \t All success : 135 \t mean(l0) : 19.666666 \t Current Learning Rate: 0.042\n",
            "Iteration 3000: Current success : 83 \t All success : 135 \t mean(l0) : 19.666666 \t Current Learning Rate: 0.040\n",
            "Iteration 3100: Current success : 83 \t All success : 135 \t mean(l0) : 19.666666 \t Current Learning Rate: 0.038\n",
            "Iteration 3200: Current success : 83 \t All success : 135 \t mean(l0) : 19.666666 \t Current Learning Rate: 0.036\n",
            "Iteration 3300: Current success : 83 \t All success : 135 \t mean(l0) : 19.666666 \t Current Learning Rate: 0.034\n",
            "Iteration 3400: Current success : 83 \t All success : 135 \t mean(l0) : 19.666666 \t Current Learning Rate: 0.032\n",
            "Iteration 3500: Current success : 83 \t All success : 135 \t mean(l0) : 19.666666 \t Current Learning Rate: 0.030\n",
            "Iteration 3600: Current success : 83 \t All success : 135 \t mean(l0) : 19.666666 \t Current Learning Rate: 0.028\n",
            "Iteration 3700: Current success : 83 \t All success : 135 \t mean(l0) : 19.666666 \t Current Learning Rate: 0.026\n",
            "Iteration 3800: Current success : 83 \t All success : 135 \t mean(l0) : 19.666666 \t Current Learning Rate: 0.024\n",
            "Iteration 3900: Current success : 83 \t All success : 135 \t mean(l0) : 19.666666 \t Current Learning Rate: 0.022\n",
            "Iteration 4000: Current success : 83 \t All success : 135 \t mean(l0) : 19.666666 \t Current Learning Rate: 0.020\n",
            "Iteration 4100: Current success : 83 \t All success : 135 \t mean(l0) : 19.644444 \t Current Learning Rate: 0.018\n",
            "Iteration 4200: Current success : 83 \t All success : 135 \t mean(l0) : 19.644444 \t Current Learning Rate: 0.016\n",
            "Iteration 4300: Current success : 83 \t All success : 135 \t mean(l0) : 19.644444 \t Current Learning Rate: 0.014\n",
            "Iteration 4400: Current success : 83 \t All success : 135 \t mean(l0) : 19.644444 \t Current Learning Rate: 0.012\n",
            "Iteration 4500: Current success : 83 \t All success : 135 \t mean(l0) : 19.644444 \t Current Learning Rate: 0.010\n",
            "Iteration 4600: Current success : 83 \t All success : 135 \t mean(l0) : 19.644444 \t Current Learning Rate: 0.008\n",
            "Iteration 4700: Current success : 83 \t All success : 135 \t mean(l0) : 19.644444 \t Current Learning Rate: 0.006\n",
            "Iteration 4800: Current success : 83 \t All success : 135 \t mean(l0) : 19.644444 \t Current Learning Rate: 0.004\n",
            "Iteration 4900: Current success : 83 \t All success : 135 \t mean(l0) : 19.644444 \t Current Learning Rate: 0.002\n",
            "Iteration 5000: Current success : 83 \t All success : 135 \t mean(l0) : 19.644444 \t Current Learning Rate: 0.000\n",
            "Iteration 5100: Current success : 115 \t All success : 157 \t mean(l0) : 20.694267 \t Current Learning Rate: 0.049\n",
            "Iteration 5200: Current success : 117 \t All success : 161 \t mean(l0) : 21.000000 \t Current Learning Rate: 0.048\n",
            "Iteration 5300: Current success : 119 \t All success : 161 \t mean(l0) : 20.975155 \t Current Learning Rate: 0.047\n",
            "Iteration 5400: Current success : 118 \t All success : 161 \t mean(l0) : 20.975155 \t Current Learning Rate: 0.046\n",
            "Iteration 5500: Current success : 117 \t All success : 161 \t mean(l0) : 20.975155 \t Current Learning Rate: 0.045\n",
            "Iteration 5600: Current success : 117 \t All success : 161 \t mean(l0) : 20.975155 \t Current Learning Rate: 0.044\n",
            "Iteration 5700: Current success : 120 \t All success : 164 \t mean(l0) : 21.213413 \t Current Learning Rate: 0.043\n",
            "Iteration 5800: Current success : 118 \t All success : 164 \t mean(l0) : 21.201220 \t Current Learning Rate: 0.042\n",
            "Iteration 5900: Current success : 121 \t All success : 164 \t mean(l0) : 21.201220 \t Current Learning Rate: 0.041\n",
            "Iteration 6000: Current success : 116 \t All success : 165 \t mean(l0) : 21.133333 \t Current Learning Rate: 0.040\n",
            "Iteration 6100: Current success : 117 \t All success : 165 \t mean(l0) : 21.133333 \t Current Learning Rate: 0.039\n",
            "Iteration 6200: Current success : 116 \t All success : 165 \t mean(l0) : 21.127272 \t Current Learning Rate: 0.038\n",
            "Iteration 6300: Current success : 115 \t All success : 165 \t mean(l0) : 21.127272 \t Current Learning Rate: 0.037\n",
            "Iteration 6400: Current success : 112 \t All success : 165 \t mean(l0) : 21.127272 \t Current Learning Rate: 0.036\n",
            "Iteration 6500: Current success : 115 \t All success : 165 \t mean(l0) : 21.127272 \t Current Learning Rate: 0.035\n",
            "Iteration 6600: Current success : 114 \t All success : 165 \t mean(l0) : 21.115150 \t Current Learning Rate: 0.034\n",
            "Iteration 6700: Current success : 113 \t All success : 165 \t mean(l0) : 21.115150 \t Current Learning Rate: 0.033\n",
            "Iteration 6800: Current success : 111 \t All success : 165 \t mean(l0) : 21.115150 \t Current Learning Rate: 0.032\n",
            "Iteration 6900: Current success : 112 \t All success : 165 \t mean(l0) : 21.115150 \t Current Learning Rate: 0.031\n",
            "Iteration 7000: Current success : 110 \t All success : 165 \t mean(l0) : 21.115150 \t Current Learning Rate: 0.030\n",
            "Iteration 7100: Current success : 110 \t All success : 165 \t mean(l0) : 21.115150 \t Current Learning Rate: 0.029\n",
            "Iteration 7200: Current success : 110 \t All success : 165 \t mean(l0) : 21.115150 \t Current Learning Rate: 0.028\n",
            "Iteration 7300: Current success : 112 \t All success : 165 \t mean(l0) : 21.115150 \t Current Learning Rate: 0.027\n",
            "Iteration 7400: Current success : 110 \t All success : 165 \t mean(l0) : 21.115150 \t Current Learning Rate: 0.026\n",
            "Iteration 7500: Current success : 112 \t All success : 166 \t mean(l0) : 21.313251 \t Current Learning Rate: 0.025\n",
            "Iteration 7600: Current success : 113 \t All success : 166 \t mean(l0) : 21.313251 \t Current Learning Rate: 0.024\n",
            "Iteration 7700: Current success : 116 \t All success : 166 \t mean(l0) : 21.313251 \t Current Learning Rate: 0.023\n",
            "Iteration 7800: Current success : 119 \t All success : 166 \t mean(l0) : 21.307228 \t Current Learning Rate: 0.022\n",
            "Iteration 7900: Current success : 119 \t All success : 166 \t mean(l0) : 21.295179 \t Current Learning Rate: 0.021\n",
            "Iteration 8000: Current success : 119 \t All success : 166 \t mean(l0) : 21.295179 \t Current Learning Rate: 0.020\n",
            "Iteration 8100: Current success : 119 \t All success : 166 \t mean(l0) : 21.295179 \t Current Learning Rate: 0.019\n",
            "Iteration 8200: Current success : 119 \t All success : 166 \t mean(l0) : 21.295179 \t Current Learning Rate: 0.018\n",
            "Iteration 8300: Current success : 119 \t All success : 166 \t mean(l0) : 21.295179 \t Current Learning Rate: 0.017\n",
            "Iteration 8400: Current success : 119 \t All success : 166 \t mean(l0) : 21.295179 \t Current Learning Rate: 0.016\n",
            "Iteration 8500: Current success : 119 \t All success : 166 \t mean(l0) : 21.295179 \t Current Learning Rate: 0.015\n",
            "Iteration 8600: Current success : 119 \t All success : 166 \t mean(l0) : 21.295179 \t Current Learning Rate: 0.014\n",
            "Iteration 8700: Current success : 119 \t All success : 166 \t mean(l0) : 21.295179 \t Current Learning Rate: 0.013\n",
            "Iteration 8800: Current success : 119 \t All success : 166 \t mean(l0) : 21.295179 \t Current Learning Rate: 0.012\n",
            "Iteration 8900: Current success : 119 \t All success : 166 \t mean(l0) : 21.295179 \t Current Learning Rate: 0.011\n",
            "Iteration 9000: Current success : 119 \t All success : 166 \t mean(l0) : 21.283133 \t Current Learning Rate: 0.010\n",
            "Iteration 9100: Current success : 119 \t All success : 166 \t mean(l0) : 21.283133 \t Current Learning Rate: 0.009\n",
            "Iteration 9200: Current success : 119 \t All success : 166 \t mean(l0) : 21.283133 \t Current Learning Rate: 0.008\n",
            "Iteration 9300: Current success : 119 \t All success : 166 \t mean(l0) : 21.283133 \t Current Learning Rate: 0.007\n",
            "Iteration 9400: Current success : 119 \t All success : 166 \t mean(l0) : 21.283133 \t Current Learning Rate: 0.006\n",
            "Iteration 9500: Current success : 119 \t All success : 166 \t mean(l0) : 21.283133 \t Current Learning Rate: 0.005\n",
            "Iteration 9600: Current success : 119 \t All success : 166 \t mean(l0) : 21.283133 \t Current Learning Rate: 0.004\n",
            "Iteration 9700: Current success : 119 \t All success : 166 \t mean(l0) : 21.283133 \t Current Learning Rate: 0.003\n",
            "Iteration 9800: Current success : 119 \t All success : 166 \t mean(l0) : 21.283133 \t Current Learning Rate: 0.002\n",
            "Iteration 9900: Current success : 119 \t All success : 166 \t mean(l0) : 21.283133 \t Current Learning Rate: 0.001\n",
            "Iteration 10000: Current success : 119 \t All success : 166 \t mean(l0) : 21.283133 \t Current Learning Rate: 0.000\n",
            "Iteration 10100: Current success : 109 \t All success : 166 \t mean(l0) : 21.192770 \t Current Learning Rate: 0.025\n",
            "Iteration 10200: Current success : 117 \t All success : 166 \t mean(l0) : 21.168674 \t Current Learning Rate: 0.024\n",
            "Iteration 10300: Current success : 117 \t All success : 166 \t mean(l0) : 21.120481 \t Current Learning Rate: 0.024\n",
            "Iteration 10400: Current success : 117 \t All success : 168 \t mean(l0) : 21.059525 \t Current Learning Rate: 0.023\n",
            "Iteration 10500: Current success : 116 \t All success : 168 \t mean(l0) : 21.059525 \t Current Learning Rate: 0.023\n",
            "Iteration 10600: Current success : 117 \t All success : 168 \t mean(l0) : 21.059525 \t Current Learning Rate: 0.022\n",
            "Iteration 10700: Current success : 116 \t All success : 168 \t mean(l0) : 21.059525 \t Current Learning Rate: 0.022\n",
            "Iteration 10800: Current success : 116 \t All success : 168 \t mean(l0) : 21.059525 \t Current Learning Rate: 0.021\n",
            "Iteration 10900: Current success : 116 \t All success : 168 \t mean(l0) : 21.059525 \t Current Learning Rate: 0.021\n",
            "Iteration 11000: Current success : 115 \t All success : 168 \t mean(l0) : 21.059525 \t Current Learning Rate: 0.020\n",
            "Iteration 11100: Current success : 116 \t All success : 169 \t mean(l0) : 20.964497 \t Current Learning Rate: 0.020\n",
            "Iteration 11200: Current success : 116 \t All success : 169 \t mean(l0) : 20.964497 \t Current Learning Rate: 0.019\n",
            "Iteration 11300: Current success : 116 \t All success : 169 \t mean(l0) : 20.958580 \t Current Learning Rate: 0.019\n",
            "Iteration 11400: Current success : 116 \t All success : 170 \t mean(l0) : 20.852942 \t Current Learning Rate: 0.018\n",
            "Iteration 11500: Current success : 115 \t All success : 170 \t mean(l0) : 20.852942 \t Current Learning Rate: 0.018\n",
            "Iteration 11600: Current success : 117 \t All success : 171 \t mean(l0) : 20.748539 \t Current Learning Rate: 0.017\n",
            "Iteration 11700: Current success : 116 \t All success : 171 \t mean(l0) : 20.742691 \t Current Learning Rate: 0.017\n",
            "Iteration 11800: Current success : 116 \t All success : 171 \t mean(l0) : 20.742691 \t Current Learning Rate: 0.016\n",
            "Iteration 11900: Current success : 116 \t All success : 171 \t mean(l0) : 20.742691 \t Current Learning Rate: 0.016\n",
            "Iteration 12000: Current success : 116 \t All success : 172 \t mean(l0) : 20.651163 \t Current Learning Rate: 0.015\n",
            "Iteration 12100: Current success : 116 \t All success : 172 \t mean(l0) : 20.651163 \t Current Learning Rate: 0.015\n",
            "Iteration 12200: Current success : 115 \t All success : 172 \t mean(l0) : 20.651163 \t Current Learning Rate: 0.014\n",
            "Iteration 12300: Current success : 116 \t All success : 172 \t mean(l0) : 20.651163 \t Current Learning Rate: 0.014\n",
            "Iteration 12400: Current success : 116 \t All success : 172 \t mean(l0) : 20.651163 \t Current Learning Rate: 0.013\n",
            "Iteration 12500: Current success : 115 \t All success : 172 \t mean(l0) : 20.651163 \t Current Learning Rate: 0.013\n",
            "Iteration 12600: Current success : 117 \t All success : 172 \t mean(l0) : 20.651163 \t Current Learning Rate: 0.012\n",
            "Iteration 12700: Current success : 116 \t All success : 172 \t mean(l0) : 20.651163 \t Current Learning Rate: 0.012\n",
            "Iteration 12800: Current success : 117 \t All success : 172 \t mean(l0) : 20.651163 \t Current Learning Rate: 0.011\n",
            "Iteration 12900: Current success : 116 \t All success : 172 \t mean(l0) : 20.651163 \t Current Learning Rate: 0.011\n",
            "Iteration 13000: Current success : 118 \t All success : 172 \t mean(l0) : 20.651163 \t Current Learning Rate: 0.010\n",
            "Iteration 13100: Current success : 118 \t All success : 172 \t mean(l0) : 20.651163 \t Current Learning Rate: 0.010\n",
            "Iteration 13200: Current success : 118 \t All success : 172 \t mean(l0) : 20.651163 \t Current Learning Rate: 0.009\n",
            "Iteration 13300: Current success : 118 \t All success : 172 \t mean(l0) : 20.651163 \t Current Learning Rate: 0.009\n",
            "Iteration 13400: Current success : 118 \t All success : 172 \t mean(l0) : 20.651163 \t Current Learning Rate: 0.008\n",
            "Iteration 13500: Current success : 118 \t All success : 172 \t mean(l0) : 20.651163 \t Current Learning Rate: 0.008\n",
            "Iteration 13600: Current success : 118 \t All success : 172 \t mean(l0) : 20.651163 \t Current Learning Rate: 0.007\n",
            "Iteration 13700: Current success : 118 \t All success : 172 \t mean(l0) : 20.651163 \t Current Learning Rate: 0.007\n",
            "Iteration 13800: Current success : 118 \t All success : 172 \t mean(l0) : 20.651163 \t Current Learning Rate: 0.006\n",
            "Iteration 13900: Current success : 118 \t All success : 172 \t mean(l0) : 20.651163 \t Current Learning Rate: 0.006\n",
            "Iteration 14000: Current success : 118 \t All success : 172 \t mean(l0) : 20.651163 \t Current Learning Rate: 0.005\n",
            "Iteration 14100: Current success : 118 \t All success : 172 \t mean(l0) : 20.651163 \t Current Learning Rate: 0.005\n",
            "Iteration 14200: Current success : 118 \t All success : 172 \t mean(l0) : 20.651163 \t Current Learning Rate: 0.004\n",
            "Iteration 14300: Current success : 118 \t All success : 172 \t mean(l0) : 20.651163 \t Current Learning Rate: 0.004\n",
            "Iteration 14400: Current success : 118 \t All success : 172 \t mean(l0) : 20.651163 \t Current Learning Rate: 0.003\n",
            "Iteration 14500: Current success : 118 \t All success : 172 \t mean(l0) : 20.651163 \t Current Learning Rate: 0.003\n",
            "Iteration 14600: Current success : 118 \t All success : 172 \t mean(l0) : 20.651163 \t Current Learning Rate: 0.002\n",
            "Iteration 14700: Current success : 118 \t All success : 172 \t mean(l0) : 20.651163 \t Current Learning Rate: 0.002\n",
            "Iteration 14800: Current success : 118 \t All success : 172 \t mean(l0) : 20.651163 \t Current Learning Rate: 0.001\n",
            "Iteration 14900: Current success : 118 \t All success : 172 \t mean(l0) : 20.651163 \t Current Learning Rate: 0.001\n",
            "Iteration 15000: Current success : 118 \t All success : 172 \t mean(l0) : 20.651163 \t Current Learning Rate: 0.000\n",
            "Iteration 15100: Current success : 112 \t All success : 173 \t mean(l0) : 20.890173 \t Current Learning Rate: 0.012\n",
            "Iteration 15200: Current success : 120 \t All success : 174 \t mean(l0) : 20.804598 \t Current Learning Rate: 0.012\n",
            "Iteration 15300: Current success : 121 \t All success : 174 \t mean(l0) : 20.804598 \t Current Learning Rate: 0.012\n",
            "Iteration 15400: Current success : 123 \t All success : 177 \t mean(l0) : 21.519773 \t Current Learning Rate: 0.012\n",
            "Iteration 15500: Current success : 123 \t All success : 177 \t mean(l0) : 21.384180 \t Current Learning Rate: 0.011\n",
            "Iteration 15600: Current success : 123 \t All success : 177 \t mean(l0) : 21.350283 \t Current Learning Rate: 0.011\n",
            "Iteration 15700: Current success : 123 \t All success : 177 \t mean(l0) : 21.350283 \t Current Learning Rate: 0.011\n",
            "Iteration 15800: Current success : 123 \t All success : 177 \t mean(l0) : 21.350283 \t Current Learning Rate: 0.011\n",
            "Iteration 15900: Current success : 123 \t All success : 177 \t mean(l0) : 21.350283 \t Current Learning Rate: 0.010\n",
            "Iteration 16000: Current success : 123 \t All success : 177 \t mean(l0) : 21.350283 \t Current Learning Rate: 0.010\n",
            "Iteration 16100: Current success : 123 \t All success : 177 \t mean(l0) : 21.350283 \t Current Learning Rate: 0.010\n",
            "Iteration 16200: Current success : 123 \t All success : 177 \t mean(l0) : 21.350283 \t Current Learning Rate: 0.010\n",
            "Iteration 16300: Current success : 123 \t All success : 177 \t mean(l0) : 21.350283 \t Current Learning Rate: 0.009\n",
            "Iteration 16400: Current success : 122 \t All success : 177 \t mean(l0) : 21.350283 \t Current Learning Rate: 0.009\n",
            "Iteration 16500: Current success : 122 \t All success : 177 \t mean(l0) : 21.350283 \t Current Learning Rate: 0.009\n",
            "Iteration 16600: Current success : 124 \t All success : 177 \t mean(l0) : 21.350283 \t Current Learning Rate: 0.009\n",
            "Iteration 16700: Current success : 122 \t All success : 177 \t mean(l0) : 21.350283 \t Current Learning Rate: 0.008\n",
            "Iteration 16800: Current success : 123 \t All success : 177 \t mean(l0) : 21.350283 \t Current Learning Rate: 0.008\n",
            "Iteration 16900: Current success : 122 \t All success : 177 \t mean(l0) : 21.350283 \t Current Learning Rate: 0.008\n",
            "Iteration 17000: Current success : 123 \t All success : 177 \t mean(l0) : 21.350283 \t Current Learning Rate: 0.008\n",
            "Iteration 17100: Current success : 123 \t All success : 177 \t mean(l0) : 21.350283 \t Current Learning Rate: 0.007\n",
            "Iteration 17200: Current success : 122 \t All success : 177 \t mean(l0) : 21.350283 \t Current Learning Rate: 0.007\n",
            "Iteration 17300: Current success : 123 \t All success : 177 \t mean(l0) : 21.350283 \t Current Learning Rate: 0.007\n",
            "Iteration 17400: Current success : 122 \t All success : 177 \t mean(l0) : 21.344633 \t Current Learning Rate: 0.007\n",
            "Iteration 17500: Current success : 122 \t All success : 177 \t mean(l0) : 21.344633 \t Current Learning Rate: 0.006\n",
            "Iteration 17600: Current success : 123 \t All success : 177 \t mean(l0) : 21.344633 \t Current Learning Rate: 0.006\n",
            "Iteration 17700: Current success : 122 \t All success : 177 \t mean(l0) : 21.344633 \t Current Learning Rate: 0.006\n",
            "Iteration 17800: Current success : 124 \t All success : 177 \t mean(l0) : 21.344633 \t Current Learning Rate: 0.006\n",
            "Iteration 17900: Current success : 123 \t All success : 177 \t mean(l0) : 21.344633 \t Current Learning Rate: 0.005\n",
            "Iteration 18000: Current success : 124 \t All success : 177 \t mean(l0) : 21.344633 \t Current Learning Rate: 0.005\n",
            "Iteration 18100: Current success : 124 \t All success : 177 \t mean(l0) : 21.344633 \t Current Learning Rate: 0.005\n",
            "Iteration 18200: Current success : 124 \t All success : 177 \t mean(l0) : 21.344633 \t Current Learning Rate: 0.005\n",
            "Iteration 18300: Current success : 124 \t All success : 177 \t mean(l0) : 21.344633 \t Current Learning Rate: 0.004\n",
            "Iteration 18400: Current success : 124 \t All success : 177 \t mean(l0) : 21.344633 \t Current Learning Rate: 0.004\n",
            "Iteration 18500: Current success : 124 \t All success : 177 \t mean(l0) : 21.344633 \t Current Learning Rate: 0.004\n",
            "Iteration 18600: Current success : 124 \t All success : 177 \t mean(l0) : 21.344633 \t Current Learning Rate: 0.004\n",
            "Iteration 18700: Current success : 124 \t All success : 177 \t mean(l0) : 21.344633 \t Current Learning Rate: 0.003\n",
            "Iteration 18800: Current success : 124 \t All success : 177 \t mean(l0) : 21.344633 \t Current Learning Rate: 0.003\n",
            "Iteration 18900: Current success : 124 \t All success : 177 \t mean(l0) : 21.344633 \t Current Learning Rate: 0.003\n",
            "Iteration 19000: Current success : 124 \t All success : 177 \t mean(l0) : 21.344633 \t Current Learning Rate: 0.003\n",
            "Iteration 19100: Current success : 124 \t All success : 177 \t mean(l0) : 21.344633 \t Current Learning Rate: 0.002\n",
            "Iteration 19200: Current success : 124 \t All success : 177 \t mean(l0) : 21.344633 \t Current Learning Rate: 0.002\n",
            "Iteration 19300: Current success : 124 \t All success : 177 \t mean(l0) : 21.344633 \t Current Learning Rate: 0.002\n",
            "Iteration 19400: Current success : 124 \t All success : 177 \t mean(l0) : 21.344633 \t Current Learning Rate: 0.002\n",
            "Iteration 19500: Current success : 124 \t All success : 177 \t mean(l0) : 21.344633 \t Current Learning Rate: 0.001\n",
            "Iteration 19600: Current success : 124 \t All success : 177 \t mean(l0) : 21.344633 \t Current Learning Rate: 0.001\n",
            "Iteration 19700: Current success : 124 \t All success : 177 \t mean(l0) : 21.344633 \t Current Learning Rate: 0.001\n",
            "Iteration 19800: Current success : 124 \t All success : 177 \t mean(l0) : 21.344633 \t Current Learning Rate: 0.001\n",
            "Iteration 19900: Current success : 124 \t All success : 177 \t mean(l0) : 21.344633 \t Current Learning Rate: 0.000\n",
            "Iteration 20000: Current success : 124 \t All success : 177 \t mean(l0) : 21.344633 \t Current Learning Rate: 0.000\n",
            "outer_step 4: all success : 232 \t mean(l0)(success) : 20.1897\n",
            "-------------------------------------------------------------\n",
            "tensor([   32.5000, 10000.0000, 10000.0000,  ..., 10000.0000, 10000.0000,\n",
            "        10000.0000], device='cuda:0')\n",
            "Iteration 100: Current success : 68 \t All success : 74 \t mean(l0) : 8.851352 \t Current Learning Rate: 0.098\n",
            "Iteration 200: Current success : 36 \t All success : 83 \t mean(l0) : 11.578313 \t Current Learning Rate: 0.096\n",
            "Iteration 300: Current success : 39 \t All success : 88 \t mean(l0) : 13.693182 \t Current Learning Rate: 0.094\n",
            "Iteration 400: Current success : 43 \t All success : 93 \t mean(l0) : 16.311829 \t Current Learning Rate: 0.092\n",
            "Iteration 500: Current success : 42 \t All success : 93 \t mean(l0) : 16.172043 \t Current Learning Rate: 0.090\n",
            "Iteration 600: Current success : 43 \t All success : 95 \t mean(l0) : 16.631580 \t Current Learning Rate: 0.088\n",
            "Iteration 700: Current success : 46 \t All success : 97 \t mean(l0) : 16.804123 \t Current Learning Rate: 0.086\n",
            "Iteration 800: Current success : 45 \t All success : 97 \t mean(l0) : 16.793814 \t Current Learning Rate: 0.084\n",
            "Iteration 900: Current success : 47 \t All success : 99 \t mean(l0) : 16.838385 \t Current Learning Rate: 0.082\n",
            "Iteration 1000: Current success : 47 \t All success : 100 \t mean(l0) : 17.129999 \t Current Learning Rate: 0.080\n",
            "Iteration 1100: Current success : 48 \t All success : 102 \t mean(l0) : 18.088236 \t Current Learning Rate: 0.078\n",
            "Iteration 1200: Current success : 47 \t All success : 104 \t mean(l0) : 17.759617 \t Current Learning Rate: 0.076\n",
            "Iteration 1300: Current success : 47 \t All success : 105 \t mean(l0) : 17.580954 \t Current Learning Rate: 0.074\n",
            "Iteration 1400: Current success : 49 \t All success : 105 \t mean(l0) : 17.580954 \t Current Learning Rate: 0.072\n",
            "Iteration 1500: Current success : 50 \t All success : 108 \t mean(l0) : 18.157408 \t Current Learning Rate: 0.070\n",
            "Iteration 1600: Current success : 49 \t All success : 108 \t mean(l0) : 18.157408 \t Current Learning Rate: 0.068\n",
            "Iteration 1700: Current success : 49 \t All success : 108 \t mean(l0) : 18.157408 \t Current Learning Rate: 0.066\n",
            "Iteration 1800: Current success : 47 \t All success : 108 \t mean(l0) : 18.157408 \t Current Learning Rate: 0.064\n",
            "Iteration 1900: Current success : 49 \t All success : 110 \t mean(l0) : 18.063635 \t Current Learning Rate: 0.062\n",
            "Iteration 2000: Current success : 49 \t All success : 110 \t mean(l0) : 18.063635 \t Current Learning Rate: 0.060\n",
            "Iteration 2100: Current success : 49 \t All success : 110 \t mean(l0) : 18.045454 \t Current Learning Rate: 0.058\n",
            "Iteration 2200: Current success : 49 \t All success : 110 \t mean(l0) : 18.045454 \t Current Learning Rate: 0.056\n",
            "Iteration 2300: Current success : 49 \t All success : 110 \t mean(l0) : 18.045454 \t Current Learning Rate: 0.054\n",
            "Iteration 2400: Current success : 49 \t All success : 110 \t mean(l0) : 18.045454 \t Current Learning Rate: 0.052\n",
            "Iteration 2500: Current success : 49 \t All success : 110 \t mean(l0) : 18.045454 \t Current Learning Rate: 0.050\n",
            "Iteration 2600: Current success : 51 \t All success : 112 \t mean(l0) : 18.214287 \t Current Learning Rate: 0.048\n",
            "Iteration 2700: Current success : 56 \t All success : 114 \t mean(l0) : 18.429825 \t Current Learning Rate: 0.046\n",
            "Iteration 2800: Current success : 58 \t All success : 117 \t mean(l0) : 18.068377 \t Current Learning Rate: 0.044\n",
            "Iteration 2900: Current success : 59 \t All success : 118 \t mean(l0) : 18.194916 \t Current Learning Rate: 0.042\n",
            "Iteration 3000: Current success : 59 \t All success : 118 \t mean(l0) : 18.177967 \t Current Learning Rate: 0.040\n",
            "Iteration 3100: Current success : 59 \t All success : 118 \t mean(l0) : 18.169491 \t Current Learning Rate: 0.038\n",
            "Iteration 3200: Current success : 59 \t All success : 118 \t mean(l0) : 18.169491 \t Current Learning Rate: 0.036\n",
            "Iteration 3300: Current success : 59 \t All success : 118 \t mean(l0) : 18.169491 \t Current Learning Rate: 0.034\n",
            "Iteration 3400: Current success : 59 \t All success : 118 \t mean(l0) : 18.169491 \t Current Learning Rate: 0.032\n",
            "Iteration 3500: Current success : 59 \t All success : 118 \t mean(l0) : 18.169491 \t Current Learning Rate: 0.030\n",
            "Iteration 3600: Current success : 59 \t All success : 118 \t mean(l0) : 18.169491 \t Current Learning Rate: 0.028\n",
            "Iteration 3700: Current success : 59 \t All success : 118 \t mean(l0) : 18.169491 \t Current Learning Rate: 0.026\n",
            "Iteration 3800: Current success : 59 \t All success : 118 \t mean(l0) : 18.169491 \t Current Learning Rate: 0.024\n",
            "Iteration 3900: Current success : 59 \t All success : 118 \t mean(l0) : 18.169491 \t Current Learning Rate: 0.022\n",
            "Iteration 4000: Current success : 59 \t All success : 118 \t mean(l0) : 18.169491 \t Current Learning Rate: 0.020\n",
            "Iteration 4100: Current success : 59 \t All success : 118 \t mean(l0) : 18.169491 \t Current Learning Rate: 0.018\n",
            "Iteration 4200: Current success : 60 \t All success : 118 \t mean(l0) : 18.169491 \t Current Learning Rate: 0.016\n",
            "Iteration 4300: Current success : 59 \t All success : 118 \t mean(l0) : 18.169491 \t Current Learning Rate: 0.014\n",
            "Iteration 4400: Current success : 59 \t All success : 118 \t mean(l0) : 18.169491 \t Current Learning Rate: 0.012\n",
            "Iteration 4500: Current success : 59 \t All success : 118 \t mean(l0) : 18.169491 \t Current Learning Rate: 0.010\n",
            "Iteration 4600: Current success : 59 \t All success : 118 \t mean(l0) : 18.169491 \t Current Learning Rate: 0.008\n",
            "Iteration 4700: Current success : 59 \t All success : 118 \t mean(l0) : 18.169491 \t Current Learning Rate: 0.006\n",
            "Iteration 4800: Current success : 59 \t All success : 118 \t mean(l0) : 18.169491 \t Current Learning Rate: 0.004\n",
            "Iteration 4900: Current success : 59 \t All success : 118 \t mean(l0) : 18.169491 \t Current Learning Rate: 0.002\n",
            "Iteration 5000: Current success : 59 \t All success : 118 \t mean(l0) : 18.169491 \t Current Learning Rate: 0.000\n",
            "Iteration 5100: Current success : 75 \t All success : 153 \t mean(l0) : 19.496733 \t Current Learning Rate: 0.049\n",
            "Iteration 5200: Current success : 88 \t All success : 164 \t mean(l0) : 19.079268 \t Current Learning Rate: 0.048\n",
            "Iteration 5300: Current success : 96 \t All success : 167 \t mean(l0) : 18.748505 \t Current Learning Rate: 0.047\n",
            "Iteration 5400: Current success : 97 \t All success : 167 \t mean(l0) : 18.742516 \t Current Learning Rate: 0.046\n",
            "Iteration 5500: Current success : 94 \t All success : 167 \t mean(l0) : 18.742516 \t Current Learning Rate: 0.045\n",
            "Iteration 5600: Current success : 94 \t All success : 167 \t mean(l0) : 18.688623 \t Current Learning Rate: 0.044\n",
            "Iteration 5700: Current success : 97 \t All success : 170 \t mean(l0) : 18.658823 \t Current Learning Rate: 0.043\n",
            "Iteration 5800: Current success : 96 \t All success : 170 \t mean(l0) : 18.658823 \t Current Learning Rate: 0.042\n",
            "Iteration 5900: Current success : 89 \t All success : 170 \t mean(l0) : 18.658823 \t Current Learning Rate: 0.041\n",
            "Iteration 6000: Current success : 91 \t All success : 170 \t mean(l0) : 18.658823 \t Current Learning Rate: 0.040\n",
            "Iteration 6100: Current success : 90 \t All success : 170 \t mean(l0) : 18.658823 \t Current Learning Rate: 0.039\n",
            "Iteration 6200: Current success : 91 \t All success : 173 \t mean(l0) : 19.254335 \t Current Learning Rate: 0.038\n",
            "Iteration 6300: Current success : 89 \t All success : 175 \t mean(l0) : 19.205713 \t Current Learning Rate: 0.037\n",
            "Iteration 6400: Current success : 88 \t All success : 177 \t mean(l0) : 19.463278 \t Current Learning Rate: 0.036\n",
            "Iteration 6500: Current success : 88 \t All success : 177 \t mean(l0) : 19.463278 \t Current Learning Rate: 0.035\n",
            "Iteration 6600: Current success : 87 \t All success : 177 \t mean(l0) : 19.463278 \t Current Learning Rate: 0.034\n",
            "Iteration 6700: Current success : 91 \t All success : 177 \t mean(l0) : 19.463278 \t Current Learning Rate: 0.033\n",
            "Iteration 6800: Current success : 89 \t All success : 177 \t mean(l0) : 19.463278 \t Current Learning Rate: 0.032\n",
            "Iteration 6900: Current success : 90 \t All success : 177 \t mean(l0) : 19.463278 \t Current Learning Rate: 0.031\n",
            "Iteration 7000: Current success : 88 \t All success : 177 \t mean(l0) : 19.463278 \t Current Learning Rate: 0.030\n",
            "Iteration 7100: Current success : 90 \t All success : 177 \t mean(l0) : 19.463278 \t Current Learning Rate: 0.029\n",
            "Iteration 7200: Current success : 91 \t All success : 177 \t mean(l0) : 19.463278 \t Current Learning Rate: 0.028\n",
            "Iteration 7300: Current success : 90 \t All success : 177 \t mean(l0) : 19.463278 \t Current Learning Rate: 0.027\n",
            "Iteration 7400: Current success : 90 \t All success : 177 \t mean(l0) : 19.451977 \t Current Learning Rate: 0.026\n",
            "Iteration 7500: Current success : 91 \t All success : 177 \t mean(l0) : 19.451977 \t Current Learning Rate: 0.025\n",
            "Iteration 7600: Current success : 92 \t All success : 177 \t mean(l0) : 19.451977 \t Current Learning Rate: 0.024\n",
            "Iteration 7700: Current success : 97 \t All success : 177 \t mean(l0) : 19.451977 \t Current Learning Rate: 0.023\n",
            "Iteration 7800: Current success : 105 \t All success : 179 \t mean(l0) : 19.905027 \t Current Learning Rate: 0.022\n",
            "Iteration 7900: Current success : 105 \t All success : 179 \t mean(l0) : 19.905027 \t Current Learning Rate: 0.021\n",
            "Iteration 8000: Current success : 105 \t All success : 179 \t mean(l0) : 19.905027 \t Current Learning Rate: 0.020\n",
            "Iteration 8100: Current success : 105 \t All success : 179 \t mean(l0) : 19.905027 \t Current Learning Rate: 0.019\n",
            "Iteration 8200: Current success : 105 \t All success : 179 \t mean(l0) : 19.905027 \t Current Learning Rate: 0.018\n",
            "Iteration 8300: Current success : 105 \t All success : 179 \t mean(l0) : 19.905027 \t Current Learning Rate: 0.017\n",
            "Iteration 8400: Current success : 104 \t All success : 179 \t mean(l0) : 19.905027 \t Current Learning Rate: 0.016\n",
            "Iteration 8500: Current success : 104 \t All success : 179 \t mean(l0) : 19.905027 \t Current Learning Rate: 0.015\n",
            "Iteration 8600: Current success : 104 \t All success : 179 \t mean(l0) : 19.905027 \t Current Learning Rate: 0.014\n",
            "Iteration 8700: Current success : 104 \t All success : 179 \t mean(l0) : 19.905027 \t Current Learning Rate: 0.013\n",
            "Iteration 8800: Current success : 105 \t All success : 179 \t mean(l0) : 19.905027 \t Current Learning Rate: 0.012\n",
            "Iteration 8900: Current success : 105 \t All success : 179 \t mean(l0) : 19.905027 \t Current Learning Rate: 0.011\n",
            "Iteration 9000: Current success : 105 \t All success : 179 \t mean(l0) : 19.905027 \t Current Learning Rate: 0.010\n",
            "Iteration 9100: Current success : 105 \t All success : 179 \t mean(l0) : 19.905027 \t Current Learning Rate: 0.009\n",
            "Iteration 9200: Current success : 105 \t All success : 179 \t mean(l0) : 19.905027 \t Current Learning Rate: 0.008\n",
            "Iteration 9300: Current success : 105 \t All success : 179 \t mean(l0) : 19.905027 \t Current Learning Rate: 0.007\n",
            "Iteration 9400: Current success : 102 \t All success : 179 \t mean(l0) : 19.905027 \t Current Learning Rate: 0.006\n",
            "Iteration 9500: Current success : 102 \t All success : 179 \t mean(l0) : 19.905027 \t Current Learning Rate: 0.005\n",
            "Iteration 9600: Current success : 102 \t All success : 179 \t mean(l0) : 19.905027 \t Current Learning Rate: 0.004\n",
            "Iteration 9700: Current success : 102 \t All success : 179 \t mean(l0) : 19.905027 \t Current Learning Rate: 0.003\n",
            "Iteration 9800: Current success : 105 \t All success : 179 \t mean(l0) : 19.905027 \t Current Learning Rate: 0.002\n",
            "Iteration 9900: Current success : 105 \t All success : 179 \t mean(l0) : 19.905027 \t Current Learning Rate: 0.001\n",
            "Iteration 10000: Current success : 105 \t All success : 179 \t mean(l0) : 19.905027 \t Current Learning Rate: 0.000\n",
            "Iteration 10100: Current success : 90 \t All success : 183 \t mean(l0) : 20.469944 \t Current Learning Rate: 0.025\n",
            "Iteration 10200: Current success : 99 \t All success : 183 \t mean(l0) : 20.415300 \t Current Learning Rate: 0.024\n",
            "Iteration 10300: Current success : 102 \t All success : 183 \t mean(l0) : 20.393442 \t Current Learning Rate: 0.024\n",
            "Iteration 10400: Current success : 102 \t All success : 183 \t mean(l0) : 20.344261 \t Current Learning Rate: 0.023\n",
            "Iteration 10500: Current success : 103 \t All success : 183 \t mean(l0) : 20.344261 \t Current Learning Rate: 0.023\n",
            "Iteration 10600: Current success : 101 \t All success : 183 \t mean(l0) : 20.344261 \t Current Learning Rate: 0.022\n",
            "Iteration 10700: Current success : 101 \t All success : 184 \t mean(l0) : 20.304348 \t Current Learning Rate: 0.022\n",
            "Iteration 10800: Current success : 101 \t All success : 188 \t mean(l0) : 20.893616 \t Current Learning Rate: 0.021\n",
            "Iteration 10900: Current success : 101 \t All success : 188 \t mean(l0) : 20.888298 \t Current Learning Rate: 0.021\n",
            "Iteration 11000: Current success : 106 \t All success : 188 \t mean(l0) : 20.888298 \t Current Learning Rate: 0.020\n",
            "Iteration 11100: Current success : 104 \t All success : 188 \t mean(l0) : 20.888298 \t Current Learning Rate: 0.020\n",
            "Iteration 11200: Current success : 105 \t All success : 188 \t mean(l0) : 20.888298 \t Current Learning Rate: 0.019\n",
            "Iteration 11300: Current success : 102 \t All success : 188 \t mean(l0) : 20.867022 \t Current Learning Rate: 0.019\n",
            "Iteration 11400: Current success : 102 \t All success : 188 \t mean(l0) : 20.867022 \t Current Learning Rate: 0.018\n",
            "Iteration 11500: Current success : 106 \t All success : 188 \t mean(l0) : 20.867022 \t Current Learning Rate: 0.018\n",
            "Iteration 11600: Current success : 106 \t All success : 188 \t mean(l0) : 20.867022 \t Current Learning Rate: 0.017\n",
            "Iteration 11700: Current success : 108 \t All success : 188 \t mean(l0) : 20.867022 \t Current Learning Rate: 0.017\n",
            "Iteration 11800: Current success : 103 \t All success : 188 \t mean(l0) : 20.867022 \t Current Learning Rate: 0.016\n",
            "Iteration 11900: Current success : 104 \t All success : 188 \t mean(l0) : 20.867022 \t Current Learning Rate: 0.016\n",
            "Iteration 12000: Current success : 105 \t All success : 188 \t mean(l0) : 20.867022 \t Current Learning Rate: 0.015\n",
            "Iteration 12100: Current success : 103 \t All success : 188 \t mean(l0) : 20.867022 \t Current Learning Rate: 0.015\n",
            "Iteration 12200: Current success : 105 \t All success : 188 \t mean(l0) : 20.867022 \t Current Learning Rate: 0.014\n",
            "Iteration 12300: Current success : 105 \t All success : 188 \t mean(l0) : 20.867022 \t Current Learning Rate: 0.014\n",
            "Iteration 12400: Current success : 104 \t All success : 188 \t mean(l0) : 20.867022 \t Current Learning Rate: 0.013\n",
            "Iteration 12500: Current success : 104 \t All success : 188 \t mean(l0) : 20.867022 \t Current Learning Rate: 0.013\n",
            "Iteration 12600: Current success : 105 \t All success : 188 \t mean(l0) : 20.867022 \t Current Learning Rate: 0.012\n",
            "Iteration 12700: Current success : 102 \t All success : 188 \t mean(l0) : 20.867022 \t Current Learning Rate: 0.012\n",
            "Iteration 12800: Current success : 105 \t All success : 188 \t mean(l0) : 20.867022 \t Current Learning Rate: 0.011\n",
            "Iteration 12900: Current success : 106 \t All success : 188 \t mean(l0) : 20.867022 \t Current Learning Rate: 0.011\n",
            "Iteration 13000: Current success : 112 \t All success : 188 \t mean(l0) : 20.867022 \t Current Learning Rate: 0.010\n",
            "Iteration 13100: Current success : 112 \t All success : 188 \t mean(l0) : 20.867022 \t Current Learning Rate: 0.010\n",
            "Iteration 13200: Current success : 112 \t All success : 188 \t mean(l0) : 20.867022 \t Current Learning Rate: 0.009\n",
            "Iteration 13300: Current success : 112 \t All success : 188 \t mean(l0) : 20.867022 \t Current Learning Rate: 0.009\n",
            "Iteration 13400: Current success : 112 \t All success : 188 \t mean(l0) : 20.867022 \t Current Learning Rate: 0.008\n",
            "Iteration 13500: Current success : 112 \t All success : 188 \t mean(l0) : 20.867022 \t Current Learning Rate: 0.008\n",
            "Iteration 13600: Current success : 112 \t All success : 188 \t mean(l0) : 20.867022 \t Current Learning Rate: 0.007\n",
            "Iteration 13700: Current success : 112 \t All success : 188 \t mean(l0) : 20.867022 \t Current Learning Rate: 0.007\n",
            "Iteration 13800: Current success : 112 \t All success : 188 \t mean(l0) : 20.867022 \t Current Learning Rate: 0.006\n",
            "Iteration 13900: Current success : 112 \t All success : 188 \t mean(l0) : 20.867022 \t Current Learning Rate: 0.006\n",
            "Iteration 14000: Current success : 112 \t All success : 188 \t mean(l0) : 20.867022 \t Current Learning Rate: 0.005\n",
            "Iteration 14100: Current success : 112 \t All success : 188 \t mean(l0) : 20.867022 \t Current Learning Rate: 0.005\n",
            "Iteration 14200: Current success : 112 \t All success : 188 \t mean(l0) : 20.867022 \t Current Learning Rate: 0.004\n",
            "Iteration 14300: Current success : 112 \t All success : 188 \t mean(l0) : 20.867022 \t Current Learning Rate: 0.004\n",
            "Iteration 14400: Current success : 112 \t All success : 188 \t mean(l0) : 20.867022 \t Current Learning Rate: 0.003\n",
            "Iteration 14500: Current success : 112 \t All success : 188 \t mean(l0) : 20.867022 \t Current Learning Rate: 0.003\n",
            "Iteration 14600: Current success : 112 \t All success : 188 \t mean(l0) : 20.867022 \t Current Learning Rate: 0.002\n",
            "Iteration 14700: Current success : 112 \t All success : 188 \t mean(l0) : 20.867022 \t Current Learning Rate: 0.002\n",
            "Iteration 14800: Current success : 112 \t All success : 188 \t mean(l0) : 20.867022 \t Current Learning Rate: 0.001\n",
            "Iteration 14900: Current success : 112 \t All success : 188 \t mean(l0) : 20.867022 \t Current Learning Rate: 0.001\n",
            "Iteration 15000: Current success : 112 \t All success : 188 \t mean(l0) : 20.867022 \t Current Learning Rate: 0.000\n",
            "Iteration 15100: Current success : 95 \t All success : 192 \t mean(l0) : 20.979168 \t Current Learning Rate: 0.012\n",
            "Iteration 15200: Current success : 111 \t All success : 193 \t mean(l0) : 20.922279 \t Current Learning Rate: 0.012\n",
            "Iteration 15300: Current success : 112 \t All success : 193 \t mean(l0) : 20.880829 \t Current Learning Rate: 0.012\n",
            "Iteration 15400: Current success : 114 \t All success : 193 \t mean(l0) : 20.870466 \t Current Learning Rate: 0.012\n",
            "Iteration 15500: Current success : 116 \t All success : 193 \t mean(l0) : 20.870466 \t Current Learning Rate: 0.011\n",
            "Iteration 15600: Current success : 115 \t All success : 193 \t mean(l0) : 20.865284 \t Current Learning Rate: 0.011\n",
            "Iteration 15700: Current success : 118 \t All success : 193 \t mean(l0) : 20.844559 \t Current Learning Rate: 0.011\n",
            "Iteration 15800: Current success : 116 \t All success : 193 \t mean(l0) : 20.844559 \t Current Learning Rate: 0.011\n",
            "Iteration 15900: Current success : 116 \t All success : 193 \t mean(l0) : 20.834196 \t Current Learning Rate: 0.010\n",
            "Iteration 16000: Current success : 114 \t All success : 193 \t mean(l0) : 20.834196 \t Current Learning Rate: 0.010\n",
            "Iteration 16100: Current success : 114 \t All success : 193 \t mean(l0) : 20.834196 \t Current Learning Rate: 0.010\n",
            "Iteration 16200: Current success : 113 \t All success : 193 \t mean(l0) : 20.834196 \t Current Learning Rate: 0.010\n",
            "Iteration 16300: Current success : 114 \t All success : 193 \t mean(l0) : 20.834196 \t Current Learning Rate: 0.009\n",
            "Iteration 16400: Current success : 114 \t All success : 193 \t mean(l0) : 20.834196 \t Current Learning Rate: 0.009\n",
            "Iteration 16500: Current success : 114 \t All success : 193 \t mean(l0) : 20.829016 \t Current Learning Rate: 0.009\n",
            "Iteration 16600: Current success : 110 \t All success : 193 \t mean(l0) : 20.829016 \t Current Learning Rate: 0.009\n",
            "Iteration 16700: Current success : 114 \t All success : 193 \t mean(l0) : 20.829016 \t Current Learning Rate: 0.008\n",
            "Iteration 16800: Current success : 111 \t All success : 193 \t mean(l0) : 20.766838 \t Current Learning Rate: 0.008\n",
            "Iteration 16900: Current success : 114 \t All success : 193 \t mean(l0) : 20.715025 \t Current Learning Rate: 0.008\n",
            "Iteration 17000: Current success : 110 \t All success : 193 \t mean(l0) : 20.715025 \t Current Learning Rate: 0.008\n",
            "Iteration 17100: Current success : 114 \t All success : 193 \t mean(l0) : 20.715025 \t Current Learning Rate: 0.007\n",
            "Iteration 17200: Current success : 113 \t All success : 193 \t mean(l0) : 20.715025 \t Current Learning Rate: 0.007\n",
            "Iteration 17300: Current success : 114 \t All success : 193 \t mean(l0) : 20.709845 \t Current Learning Rate: 0.007\n",
            "Iteration 17400: Current success : 114 \t All success : 193 \t mean(l0) : 20.709845 \t Current Learning Rate: 0.007\n",
            "Iteration 17500: Current success : 111 \t All success : 193 \t mean(l0) : 20.709845 \t Current Learning Rate: 0.006\n",
            "Iteration 17600: Current success : 114 \t All success : 193 \t mean(l0) : 20.709845 \t Current Learning Rate: 0.006\n",
            "Iteration 17700: Current success : 113 \t All success : 193 \t mean(l0) : 20.709845 \t Current Learning Rate: 0.006\n",
            "Iteration 17800: Current success : 116 \t All success : 194 \t mean(l0) : 20.953608 \t Current Learning Rate: 0.006\n",
            "Iteration 17900: Current success : 116 \t All success : 194 \t mean(l0) : 20.943298 \t Current Learning Rate: 0.005\n",
            "Iteration 18000: Current success : 117 \t All success : 194 \t mean(l0) : 20.943298 \t Current Learning Rate: 0.005\n",
            "Iteration 18100: Current success : 117 \t All success : 194 \t mean(l0) : 20.943298 \t Current Learning Rate: 0.005\n",
            "Iteration 18200: Current success : 117 \t All success : 194 \t mean(l0) : 20.938143 \t Current Learning Rate: 0.005\n",
            "Iteration 18300: Current success : 117 \t All success : 194 \t mean(l0) : 20.938143 \t Current Learning Rate: 0.004\n",
            "Iteration 18400: Current success : 117 \t All success : 194 \t mean(l0) : 20.938143 \t Current Learning Rate: 0.004\n",
            "Iteration 18500: Current success : 117 \t All success : 194 \t mean(l0) : 20.938143 \t Current Learning Rate: 0.004\n",
            "Iteration 18600: Current success : 117 \t All success : 194 \t mean(l0) : 20.938143 \t Current Learning Rate: 0.004\n",
            "Iteration 18700: Current success : 117 \t All success : 194 \t mean(l0) : 20.938143 \t Current Learning Rate: 0.003\n",
            "Iteration 18800: Current success : 117 \t All success : 194 \t mean(l0) : 20.938143 \t Current Learning Rate: 0.003\n",
            "Iteration 18900: Current success : 117 \t All success : 194 \t mean(l0) : 20.938143 \t Current Learning Rate: 0.003\n",
            "Iteration 19000: Current success : 117 \t All success : 194 \t mean(l0) : 20.938143 \t Current Learning Rate: 0.003\n",
            "Iteration 19100: Current success : 117 \t All success : 194 \t mean(l0) : 20.938143 \t Current Learning Rate: 0.002\n",
            "Iteration 19200: Current success : 117 \t All success : 194 \t mean(l0) : 20.938143 \t Current Learning Rate: 0.002\n",
            "Iteration 19300: Current success : 117 \t All success : 194 \t mean(l0) : 20.938143 \t Current Learning Rate: 0.002\n",
            "Iteration 19400: Current success : 117 \t All success : 194 \t mean(l0) : 20.938143 \t Current Learning Rate: 0.002\n",
            "Iteration 19500: Current success : 117 \t All success : 194 \t mean(l0) : 20.938143 \t Current Learning Rate: 0.001\n",
            "Iteration 19600: Current success : 117 \t All success : 194 \t mean(l0) : 20.938143 \t Current Learning Rate: 0.001\n",
            "Iteration 19700: Current success : 117 \t All success : 194 \t mean(l0) : 20.938143 \t Current Learning Rate: 0.001\n",
            "Iteration 19800: Current success : 117 \t All success : 194 \t mean(l0) : 20.938143 \t Current Learning Rate: 0.001\n",
            "Iteration 19900: Current success : 117 \t All success : 194 \t mean(l0) : 20.938143 \t Current Learning Rate: 0.000\n",
            "Iteration 20000: Current success : 117 \t All success : 194 \t mean(l0) : 20.938143 \t Current Learning Rate: 0.000\n",
            "outer_step 5: all success : 255 \t mean(l0)(success) : 21.5686\n",
            "-------------------------------------------------------------\n",
            "tensor([2.1250e+01, 1.0000e+05, 1.0000e+05,  ..., 1.0000e+05, 1.0000e+05,\n",
            "        1.0000e+05], device='cuda:0')\n",
            "Iteration 100: Current success : 33 \t All success : 42 \t mean(l0) : 8.500000 \t Current Learning Rate: 0.098\n",
            "Iteration 200: Current success : 28 \t All success : 47 \t mean(l0) : 11.361702 \t Current Learning Rate: 0.096\n",
            "Iteration 300: Current success : 38 \t All success : 57 \t mean(l0) : 17.912281 \t Current Learning Rate: 0.094\n",
            "Iteration 400: Current success : 37 \t All success : 60 \t mean(l0) : 17.866667 \t Current Learning Rate: 0.092\n",
            "Iteration 500: Current success : 38 \t All success : 62 \t mean(l0) : 18.822580 \t Current Learning Rate: 0.090\n",
            "Iteration 600: Current success : 40 \t All success : 64 \t mean(l0) : 20.000000 \t Current Learning Rate: 0.088\n",
            "Iteration 700: Current success : 40 \t All success : 65 \t mean(l0) : 20.261538 \t Current Learning Rate: 0.086\n",
            "Iteration 800: Current success : 40 \t All success : 65 \t mean(l0) : 20.215385 \t Current Learning Rate: 0.084\n",
            "Iteration 900: Current success : 41 \t All success : 65 \t mean(l0) : 20.215385 \t Current Learning Rate: 0.082\n",
            "Iteration 1000: Current success : 41 \t All success : 66 \t mean(l0) : 19.924244 \t Current Learning Rate: 0.080\n",
            "Iteration 1100: Current success : 42 \t All success : 66 \t mean(l0) : 19.924244 \t Current Learning Rate: 0.078\n",
            "Iteration 1200: Current success : 41 \t All success : 66 \t mean(l0) : 19.924244 \t Current Learning Rate: 0.076\n",
            "Iteration 1300: Current success : 42 \t All success : 68 \t mean(l0) : 19.779411 \t Current Learning Rate: 0.074\n",
            "Iteration 1400: Current success : 41 \t All success : 70 \t mean(l0) : 19.885714 \t Current Learning Rate: 0.072\n",
            "Iteration 1500: Current success : 41 \t All success : 70 \t mean(l0) : 19.885714 \t Current Learning Rate: 0.070\n",
            "Iteration 1600: Current success : 41 \t All success : 71 \t mean(l0) : 20.056337 \t Current Learning Rate: 0.068\n",
            "Iteration 1700: Current success : 42 \t All success : 71 \t mean(l0) : 20.056337 \t Current Learning Rate: 0.066\n",
            "Iteration 1800: Current success : 41 \t All success : 71 \t mean(l0) : 20.056337 \t Current Learning Rate: 0.064\n",
            "Iteration 1900: Current success : 41 \t All success : 71 \t mean(l0) : 20.056337 \t Current Learning Rate: 0.062\n",
            "Iteration 2000: Current success : 41 \t All success : 71 \t mean(l0) : 20.056337 \t Current Learning Rate: 0.060\n",
            "Iteration 2100: Current success : 41 \t All success : 71 \t mean(l0) : 20.056337 \t Current Learning Rate: 0.058\n",
            "Iteration 2200: Current success : 41 \t All success : 71 \t mean(l0) : 20.056337 \t Current Learning Rate: 0.056\n",
            "Iteration 2300: Current success : 43 \t All success : 73 \t mean(l0) : 21.095890 \t Current Learning Rate: 0.054\n",
            "Iteration 2400: Current success : 43 \t All success : 73 \t mean(l0) : 21.095890 \t Current Learning Rate: 0.052\n",
            "Iteration 2500: Current success : 43 \t All success : 73 \t mean(l0) : 21.095890 \t Current Learning Rate: 0.050\n",
            "Iteration 2600: Current success : 43 \t All success : 74 \t mean(l0) : 20.837839 \t Current Learning Rate: 0.048\n",
            "Iteration 2700: Current success : 49 \t All success : 77 \t mean(l0) : 20.298700 \t Current Learning Rate: 0.046\n",
            "Iteration 2800: Current success : 51 \t All success : 78 \t mean(l0) : 20.192308 \t Current Learning Rate: 0.044\n",
            "Iteration 2900: Current success : 51 \t All success : 78 \t mean(l0) : 20.192308 \t Current Learning Rate: 0.042\n",
            "Iteration 3000: Current success : 52 \t All success : 79 \t mean(l0) : 20.468355 \t Current Learning Rate: 0.040\n",
            "Iteration 3100: Current success : 52 \t All success : 79 \t mean(l0) : 20.468355 \t Current Learning Rate: 0.038\n",
            "Iteration 3200: Current success : 52 \t All success : 79 \t mean(l0) : 20.468355 \t Current Learning Rate: 0.036\n",
            "Iteration 3300: Current success : 52 \t All success : 79 \t mean(l0) : 20.468355 \t Current Learning Rate: 0.034\n",
            "Iteration 3400: Current success : 52 \t All success : 79 \t mean(l0) : 20.468355 \t Current Learning Rate: 0.032\n",
            "Iteration 3500: Current success : 49 \t All success : 79 \t mean(l0) : 20.468355 \t Current Learning Rate: 0.030\n",
            "Iteration 3600: Current success : 50 \t All success : 79 \t mean(l0) : 20.468355 \t Current Learning Rate: 0.028\n",
            "Iteration 3700: Current success : 50 \t All success : 79 \t mean(l0) : 20.468355 \t Current Learning Rate: 0.026\n",
            "Iteration 3800: Current success : 50 \t All success : 79 \t mean(l0) : 20.468355 \t Current Learning Rate: 0.024\n",
            "Iteration 3900: Current success : 50 \t All success : 79 \t mean(l0) : 20.468355 \t Current Learning Rate: 0.022\n",
            "Iteration 4000: Current success : 50 \t All success : 79 \t mean(l0) : 20.468355 \t Current Learning Rate: 0.020\n",
            "Iteration 4100: Current success : 50 \t All success : 79 \t mean(l0) : 20.468355 \t Current Learning Rate: 0.018\n",
            "Iteration 4200: Current success : 52 \t All success : 79 \t mean(l0) : 20.468355 \t Current Learning Rate: 0.016\n",
            "Iteration 4300: Current success : 50 \t All success : 79 \t mean(l0) : 20.468355 \t Current Learning Rate: 0.014\n",
            "Iteration 4400: Current success : 50 \t All success : 79 \t mean(l0) : 20.468355 \t Current Learning Rate: 0.012\n",
            "Iteration 4500: Current success : 50 \t All success : 79 \t mean(l0) : 20.468355 \t Current Learning Rate: 0.010\n",
            "Iteration 4600: Current success : 50 \t All success : 79 \t mean(l0) : 20.468355 \t Current Learning Rate: 0.008\n",
            "Iteration 4700: Current success : 50 \t All success : 79 \t mean(l0) : 20.468355 \t Current Learning Rate: 0.006\n",
            "Iteration 4800: Current success : 50 \t All success : 79 \t mean(l0) : 20.468355 \t Current Learning Rate: 0.004\n",
            "Iteration 4900: Current success : 50 \t All success : 79 \t mean(l0) : 20.468355 \t Current Learning Rate: 0.002\n",
            "Iteration 5000: Current success : 50 \t All success : 79 \t mean(l0) : 20.468355 \t Current Learning Rate: 0.000\n",
            "Iteration 5100: Current success : 70 \t All success : 119 \t mean(l0) : 22.294119 \t Current Learning Rate: 0.049\n",
            "Iteration 5200: Current success : 86 \t All success : 131 \t mean(l0) : 22.618320 \t Current Learning Rate: 0.048\n",
            "Iteration 5300: Current success : 82 \t All success : 132 \t mean(l0) : 22.454546 \t Current Learning Rate: 0.047\n",
            "Iteration 5400: Current success : 82 \t All success : 132 \t mean(l0) : 22.431818 \t Current Learning Rate: 0.046\n",
            "Iteration 5500: Current success : 79 \t All success : 132 \t mean(l0) : 22.325758 \t Current Learning Rate: 0.045\n",
            "Iteration 5600: Current success : 81 \t All success : 132 \t mean(l0) : 22.325758 \t Current Learning Rate: 0.044\n",
            "Iteration 5700: Current success : 80 \t All success : 132 \t mean(l0) : 22.325758 \t Current Learning Rate: 0.043\n",
            "Iteration 5800: Current success : 82 \t All success : 132 \t mean(l0) : 22.325758 \t Current Learning Rate: 0.042\n",
            "Iteration 5900: Current success : 76 \t All success : 132 \t mean(l0) : 22.325758 \t Current Learning Rate: 0.041\n",
            "Iteration 6000: Current success : 80 \t All success : 132 \t mean(l0) : 22.325758 \t Current Learning Rate: 0.040\n",
            "Iteration 6100: Current success : 77 \t All success : 132 \t mean(l0) : 22.310606 \t Current Learning Rate: 0.039\n",
            "Iteration 6200: Current success : 77 \t All success : 132 \t mean(l0) : 22.303032 \t Current Learning Rate: 0.038\n",
            "Iteration 6300: Current success : 76 \t All success : 132 \t mean(l0) : 22.303032 \t Current Learning Rate: 0.037\n",
            "Iteration 6400: Current success : 79 \t All success : 132 \t mean(l0) : 22.303032 \t Current Learning Rate: 0.036\n",
            "Iteration 6500: Current success : 76 \t All success : 132 \t mean(l0) : 22.303032 \t Current Learning Rate: 0.035\n",
            "Iteration 6600: Current success : 76 \t All success : 132 \t mean(l0) : 22.303032 \t Current Learning Rate: 0.034\n",
            "Iteration 6700: Current success : 76 \t All success : 132 \t mean(l0) : 22.303032 \t Current Learning Rate: 0.033\n",
            "Iteration 6800: Current success : 75 \t All success : 132 \t mean(l0) : 22.303032 \t Current Learning Rate: 0.032\n",
            "Iteration 6900: Current success : 76 \t All success : 132 \t mean(l0) : 22.303032 \t Current Learning Rate: 0.031\n",
            "Iteration 7000: Current success : 75 \t All success : 132 \t mean(l0) : 22.303032 \t Current Learning Rate: 0.030\n",
            "Iteration 7100: Current success : 76 \t All success : 132 \t mean(l0) : 22.303032 \t Current Learning Rate: 0.029\n",
            "Iteration 7200: Current success : 77 \t All success : 132 \t mean(l0) : 22.295456 \t Current Learning Rate: 0.028\n",
            "Iteration 7300: Current success : 75 \t All success : 132 \t mean(l0) : 22.295456 \t Current Learning Rate: 0.027\n",
            "Iteration 7400: Current success : 75 \t All success : 132 \t mean(l0) : 22.295456 \t Current Learning Rate: 0.026\n",
            "Iteration 7500: Current success : 77 \t All success : 132 \t mean(l0) : 22.295456 \t Current Learning Rate: 0.025\n",
            "Iteration 7600: Current success : 76 \t All success : 132 \t mean(l0) : 22.295456 \t Current Learning Rate: 0.024\n",
            "Iteration 7700: Current success : 81 \t All success : 132 \t mean(l0) : 22.295456 \t Current Learning Rate: 0.023\n",
            "Iteration 7800: Current success : 84 \t All success : 132 \t mean(l0) : 22.295456 \t Current Learning Rate: 0.022\n",
            "Iteration 7900: Current success : 84 \t All success : 132 \t mean(l0) : 22.295456 \t Current Learning Rate: 0.021\n",
            "Iteration 8000: Current success : 84 \t All success : 132 \t mean(l0) : 22.295456 \t Current Learning Rate: 0.020\n",
            "Iteration 8100: Current success : 84 \t All success : 132 \t mean(l0) : 22.295456 \t Current Learning Rate: 0.019\n",
            "Iteration 8200: Current success : 84 \t All success : 132 \t mean(l0) : 22.295456 \t Current Learning Rate: 0.018\n",
            "Iteration 8300: Current success : 84 \t All success : 132 \t mean(l0) : 22.295456 \t Current Learning Rate: 0.017\n",
            "Iteration 8400: Current success : 84 \t All success : 132 \t mean(l0) : 22.295456 \t Current Learning Rate: 0.016\n",
            "Iteration 8500: Current success : 84 \t All success : 132 \t mean(l0) : 22.295456 \t Current Learning Rate: 0.015\n",
            "Iteration 8600: Current success : 84 \t All success : 132 \t mean(l0) : 22.295456 \t Current Learning Rate: 0.014\n",
            "Iteration 8700: Current success : 84 \t All success : 132 \t mean(l0) : 22.295456 \t Current Learning Rate: 0.013\n",
            "Iteration 8800: Current success : 84 \t All success : 132 \t mean(l0) : 22.295456 \t Current Learning Rate: 0.012\n",
            "Iteration 8900: Current success : 84 \t All success : 132 \t mean(l0) : 22.295456 \t Current Learning Rate: 0.011\n",
            "Iteration 9000: Current success : 84 \t All success : 132 \t mean(l0) : 22.295456 \t Current Learning Rate: 0.010\n",
            "Iteration 9100: Current success : 84 \t All success : 132 \t mean(l0) : 22.295456 \t Current Learning Rate: 0.009\n",
            "Iteration 9200: Current success : 84 \t All success : 132 \t mean(l0) : 22.295456 \t Current Learning Rate: 0.008\n",
            "Iteration 9300: Current success : 84 \t All success : 132 \t mean(l0) : 22.295456 \t Current Learning Rate: 0.007\n",
            "Iteration 9400: Current success : 84 \t All success : 132 \t mean(l0) : 22.295456 \t Current Learning Rate: 0.006\n",
            "Iteration 9500: Current success : 84 \t All success : 132 \t mean(l0) : 22.295456 \t Current Learning Rate: 0.005\n",
            "Iteration 9600: Current success : 84 \t All success : 132 \t mean(l0) : 22.295456 \t Current Learning Rate: 0.004\n",
            "Iteration 9700: Current success : 84 \t All success : 132 \t mean(l0) : 22.295456 \t Current Learning Rate: 0.003\n",
            "Iteration 9800: Current success : 84 \t All success : 132 \t mean(l0) : 22.295456 \t Current Learning Rate: 0.002\n",
            "Iteration 9900: Current success : 84 \t All success : 132 \t mean(l0) : 22.295456 \t Current Learning Rate: 0.001\n",
            "Iteration 10000: Current success : 84 \t All success : 132 \t mean(l0) : 22.295456 \t Current Learning Rate: 0.000\n",
            "Iteration 10100: Current success : 68 \t All success : 132 \t mean(l0) : 22.295456 \t Current Learning Rate: 0.025\n",
            "Iteration 10200: Current success : 77 \t All success : 132 \t mean(l0) : 22.295456 \t Current Learning Rate: 0.024\n",
            "Iteration 10300: Current success : 78 \t All success : 132 \t mean(l0) : 22.295456 \t Current Learning Rate: 0.024\n",
            "Iteration 10400: Current success : 79 \t All success : 132 \t mean(l0) : 22.295456 \t Current Learning Rate: 0.023\n",
            "Iteration 10500: Current success : 79 \t All success : 132 \t mean(l0) : 22.295456 \t Current Learning Rate: 0.023\n",
            "Iteration 10600: Current success : 77 \t All success : 132 \t mean(l0) : 22.295456 \t Current Learning Rate: 0.022\n",
            "Iteration 10700: Current success : 79 \t All success : 132 \t mean(l0) : 22.295456 \t Current Learning Rate: 0.022\n",
            "Iteration 10800: Current success : 79 \t All success : 132 \t mean(l0) : 22.295456 \t Current Learning Rate: 0.021\n",
            "Iteration 10900: Current success : 77 \t All success : 132 \t mean(l0) : 22.287880 \t Current Learning Rate: 0.021\n",
            "Iteration 11000: Current success : 78 \t All success : 136 \t mean(l0) : 21.779411 \t Current Learning Rate: 0.020\n",
            "Iteration 11100: Current success : 79 \t All success : 136 \t mean(l0) : 21.779411 \t Current Learning Rate: 0.020\n",
            "Iteration 11200: Current success : 78 \t All success : 136 \t mean(l0) : 21.779411 \t Current Learning Rate: 0.019\n",
            "Iteration 11300: Current success : 78 \t All success : 136 \t mean(l0) : 21.779411 \t Current Learning Rate: 0.019\n",
            "Iteration 11400: Current success : 78 \t All success : 136 \t mean(l0) : 21.779411 \t Current Learning Rate: 0.018\n",
            "Iteration 11500: Current success : 78 \t All success : 136 \t mean(l0) : 21.779411 \t Current Learning Rate: 0.018\n",
            "Iteration 11600: Current success : 77 \t All success : 136 \t mean(l0) : 21.779411 \t Current Learning Rate: 0.017\n",
            "Iteration 11700: Current success : 78 \t All success : 136 \t mean(l0) : 21.779411 \t Current Learning Rate: 0.017\n",
            "Iteration 11800: Current success : 81 \t All success : 136 \t mean(l0) : 21.779411 \t Current Learning Rate: 0.016\n",
            "Iteration 11900: Current success : 78 \t All success : 136 \t mean(l0) : 21.779411 \t Current Learning Rate: 0.016\n",
            "Iteration 12000: Current success : 77 \t All success : 136 \t mean(l0) : 21.779411 \t Current Learning Rate: 0.015\n",
            "Iteration 12100: Current success : 77 \t All success : 136 \t mean(l0) : 21.779411 \t Current Learning Rate: 0.015\n",
            "Iteration 12200: Current success : 77 \t All success : 136 \t mean(l0) : 21.779411 \t Current Learning Rate: 0.014\n",
            "Iteration 12300: Current success : 82 \t All success : 136 \t mean(l0) : 21.779411 \t Current Learning Rate: 0.014\n",
            "Iteration 12400: Current success : 77 \t All success : 136 \t mean(l0) : 21.779411 \t Current Learning Rate: 0.013\n",
            "Iteration 12500: Current success : 77 \t All success : 136 \t mean(l0) : 21.779411 \t Current Learning Rate: 0.013\n",
            "Iteration 12600: Current success : 78 \t All success : 136 \t mean(l0) : 21.779411 \t Current Learning Rate: 0.012\n",
            "Iteration 12700: Current success : 81 \t All success : 136 \t mean(l0) : 21.779411 \t Current Learning Rate: 0.012\n",
            "Iteration 12800: Current success : 79 \t All success : 136 \t mean(l0) : 21.779411 \t Current Learning Rate: 0.011\n",
            "Iteration 12900: Current success : 80 \t All success : 136 \t mean(l0) : 21.779411 \t Current Learning Rate: 0.011\n",
            "Iteration 13000: Current success : 82 \t All success : 136 \t mean(l0) : 21.779411 \t Current Learning Rate: 0.010\n",
            "Iteration 13100: Current success : 82 \t All success : 136 \t mean(l0) : 21.779411 \t Current Learning Rate: 0.010\n",
            "Iteration 13200: Current success : 82 \t All success : 136 \t mean(l0) : 21.779411 \t Current Learning Rate: 0.009\n",
            "Iteration 13300: Current success : 82 \t All success : 136 \t mean(l0) : 21.779411 \t Current Learning Rate: 0.009\n",
            "Iteration 13400: Current success : 82 \t All success : 136 \t mean(l0) : 21.779411 \t Current Learning Rate: 0.008\n",
            "Iteration 13500: Current success : 82 \t All success : 136 \t mean(l0) : 21.779411 \t Current Learning Rate: 0.008\n",
            "Iteration 13600: Current success : 82 \t All success : 136 \t mean(l0) : 21.779411 \t Current Learning Rate: 0.007\n",
            "Iteration 13700: Current success : 82 \t All success : 136 \t mean(l0) : 21.779411 \t Current Learning Rate: 0.007\n",
            "Iteration 13800: Current success : 82 \t All success : 136 \t mean(l0) : 21.779411 \t Current Learning Rate: 0.006\n",
            "Iteration 13900: Current success : 82 \t All success : 136 \t mean(l0) : 21.779411 \t Current Learning Rate: 0.006\n",
            "Iteration 14000: Current success : 82 \t All success : 136 \t mean(l0) : 21.779411 \t Current Learning Rate: 0.005\n",
            "Iteration 14100: Current success : 82 \t All success : 136 \t mean(l0) : 21.779411 \t Current Learning Rate: 0.005\n",
            "Iteration 14200: Current success : 82 \t All success : 136 \t mean(l0) : 21.779411 \t Current Learning Rate: 0.004\n",
            "Iteration 14300: Current success : 82 \t All success : 136 \t mean(l0) : 21.779411 \t Current Learning Rate: 0.004\n",
            "Iteration 14400: Current success : 82 \t All success : 136 \t mean(l0) : 21.779411 \t Current Learning Rate: 0.003\n",
            "Iteration 14500: Current success : 82 \t All success : 136 \t mean(l0) : 21.779411 \t Current Learning Rate: 0.003\n",
            "Iteration 14600: Current success : 82 \t All success : 136 \t mean(l0) : 21.779411 \t Current Learning Rate: 0.002\n",
            "Iteration 14700: Current success : 82 \t All success : 136 \t mean(l0) : 21.779411 \t Current Learning Rate: 0.002\n",
            "Iteration 14800: Current success : 82 \t All success : 136 \t mean(l0) : 21.779411 \t Current Learning Rate: 0.001\n",
            "Iteration 14900: Current success : 82 \t All success : 136 \t mean(l0) : 21.779411 \t Current Learning Rate: 0.001\n",
            "Iteration 15000: Current success : 82 \t All success : 136 \t mean(l0) : 21.779411 \t Current Learning Rate: 0.000\n",
            "Iteration 15100: Current success : 72 \t All success : 140 \t mean(l0) : 21.371429 \t Current Learning Rate: 0.012\n",
            "Iteration 15200: Current success : 84 \t All success : 140 \t mean(l0) : 21.364286 \t Current Learning Rate: 0.012\n",
            "Iteration 15300: Current success : 84 \t All success : 140 \t mean(l0) : 21.364286 \t Current Learning Rate: 0.012\n",
            "Iteration 15400: Current success : 84 \t All success : 140 \t mean(l0) : 21.364286 \t Current Learning Rate: 0.012\n",
            "Iteration 15500: Current success : 84 \t All success : 140 \t mean(l0) : 21.364286 \t Current Learning Rate: 0.011\n",
            "Iteration 15600: Current success : 84 \t All success : 140 \t mean(l0) : 21.364286 \t Current Learning Rate: 0.011\n",
            "Iteration 15700: Current success : 84 \t All success : 140 \t mean(l0) : 21.335714 \t Current Learning Rate: 0.011\n",
            "Iteration 15800: Current success : 84 \t All success : 140 \t mean(l0) : 21.335714 \t Current Learning Rate: 0.011\n",
            "Iteration 15900: Current success : 84 \t All success : 140 \t mean(l0) : 21.335714 \t Current Learning Rate: 0.010\n",
            "Iteration 16000: Current success : 84 \t All success : 140 \t mean(l0) : 21.335714 \t Current Learning Rate: 0.010\n",
            "Iteration 16100: Current success : 84 \t All success : 140 \t mean(l0) : 21.335714 \t Current Learning Rate: 0.010\n",
            "Iteration 16200: Current success : 84 \t All success : 140 \t mean(l0) : 21.335714 \t Current Learning Rate: 0.010\n",
            "Iteration 16300: Current success : 84 \t All success : 140 \t mean(l0) : 21.335714 \t Current Learning Rate: 0.009\n",
            "Iteration 16400: Current success : 84 \t All success : 140 \t mean(l0) : 21.335714 \t Current Learning Rate: 0.009\n",
            "Iteration 16500: Current success : 84 \t All success : 140 \t mean(l0) : 21.335714 \t Current Learning Rate: 0.009\n",
            "Iteration 16600: Current success : 84 \t All success : 140 \t mean(l0) : 21.335714 \t Current Learning Rate: 0.009\n",
            "Iteration 16700: Current success : 83 \t All success : 140 \t mean(l0) : 21.335714 \t Current Learning Rate: 0.008\n",
            "Iteration 16800: Current success : 83 \t All success : 143 \t mean(l0) : 21.748251 \t Current Learning Rate: 0.008\n",
            "Iteration 16900: Current success : 84 \t All success : 143 \t mean(l0) : 21.748251 \t Current Learning Rate: 0.008\n",
            "Iteration 17000: Current success : 83 \t All success : 143 \t mean(l0) : 21.748251 \t Current Learning Rate: 0.008\n",
            "Iteration 17100: Current success : 84 \t All success : 143 \t mean(l0) : 21.748251 \t Current Learning Rate: 0.007\n",
            "Iteration 17200: Current success : 83 \t All success : 143 \t mean(l0) : 21.748251 \t Current Learning Rate: 0.007\n",
            "Iteration 17300: Current success : 83 \t All success : 143 \t mean(l0) : 21.748251 \t Current Learning Rate: 0.007\n",
            "Iteration 17400: Current success : 83 \t All success : 143 \t mean(l0) : 21.748251 \t Current Learning Rate: 0.007\n",
            "Iteration 17500: Current success : 84 \t All success : 143 \t mean(l0) : 21.748251 \t Current Learning Rate: 0.006\n",
            "Iteration 17600: Current success : 84 \t All success : 143 \t mean(l0) : 21.748251 \t Current Learning Rate: 0.006\n",
            "Iteration 17700: Current success : 83 \t All success : 143 \t mean(l0) : 21.748251 \t Current Learning Rate: 0.006\n",
            "Iteration 17800: Current success : 84 \t All success : 143 \t mean(l0) : 21.748251 \t Current Learning Rate: 0.006\n",
            "Iteration 17900: Current success : 84 \t All success : 143 \t mean(l0) : 21.748251 \t Current Learning Rate: 0.005\n",
            "Iteration 18000: Current success : 85 \t All success : 143 \t mean(l0) : 21.734266 \t Current Learning Rate: 0.005\n",
            "Iteration 18100: Current success : 85 \t All success : 143 \t mean(l0) : 21.720280 \t Current Learning Rate: 0.005\n",
            "Iteration 18200: Current success : 85 \t All success : 143 \t mean(l0) : 21.720280 \t Current Learning Rate: 0.005\n",
            "Iteration 18300: Current success : 85 \t All success : 143 \t mean(l0) : 21.720280 \t Current Learning Rate: 0.004\n",
            "Iteration 18400: Current success : 85 \t All success : 143 \t mean(l0) : 21.720280 \t Current Learning Rate: 0.004\n",
            "Iteration 18500: Current success : 85 \t All success : 143 \t mean(l0) : 21.720280 \t Current Learning Rate: 0.004\n",
            "Iteration 18600: Current success : 85 \t All success : 143 \t mean(l0) : 21.720280 \t Current Learning Rate: 0.004\n",
            "Iteration 18700: Current success : 85 \t All success : 143 \t mean(l0) : 21.720280 \t Current Learning Rate: 0.003\n",
            "Iteration 18800: Current success : 85 \t All success : 143 \t mean(l0) : 21.720280 \t Current Learning Rate: 0.003\n",
            "Iteration 18900: Current success : 85 \t All success : 143 \t mean(l0) : 21.720280 \t Current Learning Rate: 0.003\n",
            "Iteration 19000: Current success : 85 \t All success : 143 \t mean(l0) : 21.720280 \t Current Learning Rate: 0.003\n",
            "Iteration 19100: Current success : 85 \t All success : 143 \t mean(l0) : 21.720280 \t Current Learning Rate: 0.002\n",
            "Iteration 19200: Current success : 85 \t All success : 143 \t mean(l0) : 21.720280 \t Current Learning Rate: 0.002\n",
            "Iteration 19300: Current success : 85 \t All success : 143 \t mean(l0) : 21.720280 \t Current Learning Rate: 0.002\n",
            "Iteration 19400: Current success : 85 \t All success : 143 \t mean(l0) : 21.720280 \t Current Learning Rate: 0.002\n",
            "Iteration 19500: Current success : 85 \t All success : 143 \t mean(l0) : 21.720280 \t Current Learning Rate: 0.001\n",
            "Iteration 19600: Current success : 85 \t All success : 143 \t mean(l0) : 21.720280 \t Current Learning Rate: 0.001\n",
            "Iteration 19700: Current success : 85 \t All success : 143 \t mean(l0) : 21.720280 \t Current Learning Rate: 0.001\n",
            "Iteration 19800: Current success : 85 \t All success : 143 \t mean(l0) : 21.720280 \t Current Learning Rate: 0.001\n",
            "Iteration 19900: Current success : 85 \t All success : 143 \t mean(l0) : 21.720280 \t Current Learning Rate: 0.000\n",
            "Iteration 20000: Current success : 85 \t All success : 143 \t mean(l0) : 21.720280 \t Current Learning Rate: 0.000\n",
            "outer_step 6: all success : 260 \t mean(l0)(success) : 21.4462\n",
            "-------------------------------------------------------------\n",
            "tensor([1.5625e+01, 1.0000e+06, 1.0000e+06,  ..., 1.0000e+06, 1.0000e+06,\n",
            "        1.0000e+06], device='cuda:0')\n",
            "Iteration 100: Current success : 119 \t All success : 121 \t mean(l0) : 9.553719 \t Current Learning Rate: 0.098\n",
            "Iteration 200: Current success : 116 \t All success : 132 \t mean(l0) : 11.553031 \t Current Learning Rate: 0.096\n",
            "Iteration 300: Current success : 126 \t All success : 180 \t mean(l0) : 13.488889 \t Current Learning Rate: 0.094\n",
            "Iteration 400: Current success : 161 \t All success : 185 \t mean(l0) : 13.827027 \t Current Learning Rate: 0.092\n",
            "Iteration 500: Current success : 164 \t All success : 187 \t mean(l0) : 13.989305 \t Current Learning Rate: 0.090\n",
            "Iteration 600: Current success : 165 \t All success : 188 \t mean(l0) : 14.000000 \t Current Learning Rate: 0.088\n",
            "Iteration 700: Current success : 169 \t All success : 192 \t mean(l0) : 14.458334 \t Current Learning Rate: 0.086\n",
            "Iteration 800: Current success : 170 \t All success : 192 \t mean(l0) : 14.458334 \t Current Learning Rate: 0.084\n",
            "Iteration 900: Current success : 166 \t All success : 192 \t mean(l0) : 14.453125 \t Current Learning Rate: 0.082\n",
            "Iteration 1000: Current success : 166 \t All success : 192 \t mean(l0) : 14.453125 \t Current Learning Rate: 0.080\n",
            "Iteration 1100: Current success : 169 \t All success : 192 \t mean(l0) : 14.453125 \t Current Learning Rate: 0.078\n",
            "Iteration 1200: Current success : 169 \t All success : 192 \t mean(l0) : 14.453125 \t Current Learning Rate: 0.076\n",
            "Iteration 1300: Current success : 168 \t All success : 192 \t mean(l0) : 14.437500 \t Current Learning Rate: 0.074\n",
            "Iteration 1400: Current success : 169 \t All success : 193 \t mean(l0) : 14.709845 \t Current Learning Rate: 0.072\n",
            "Iteration 1500: Current success : 170 \t All success : 193 \t mean(l0) : 14.709845 \t Current Learning Rate: 0.070\n",
            "Iteration 1600: Current success : 170 \t All success : 193 \t mean(l0) : 14.709845 \t Current Learning Rate: 0.068\n",
            "Iteration 1700: Current success : 170 \t All success : 193 \t mean(l0) : 14.709845 \t Current Learning Rate: 0.066\n",
            "Iteration 1800: Current success : 172 \t All success : 195 \t mean(l0) : 15.143590 \t Current Learning Rate: 0.064\n",
            "Iteration 1900: Current success : 172 \t All success : 195 \t mean(l0) : 15.143590 \t Current Learning Rate: 0.062\n",
            "Iteration 2000: Current success : 172 \t All success : 195 \t mean(l0) : 15.143590 \t Current Learning Rate: 0.060\n",
            "Iteration 2100: Current success : 172 \t All success : 195 \t mean(l0) : 15.143590 \t Current Learning Rate: 0.058\n",
            "Iteration 2200: Current success : 172 \t All success : 195 \t mean(l0) : 15.143590 \t Current Learning Rate: 0.056\n",
            "Iteration 2300: Current success : 172 \t All success : 195 \t mean(l0) : 15.143590 \t Current Learning Rate: 0.054\n",
            "Iteration 2400: Current success : 174 \t All success : 197 \t mean(l0) : 15.304568 \t Current Learning Rate: 0.052\n",
            "Iteration 2500: Current success : 174 \t All success : 197 \t mean(l0) : 15.279187 \t Current Learning Rate: 0.050\n",
            "Iteration 2600: Current success : 174 \t All success : 197 \t mean(l0) : 15.279187 \t Current Learning Rate: 0.048\n",
            "Iteration 2700: Current success : 174 \t All success : 197 \t mean(l0) : 15.279187 \t Current Learning Rate: 0.046\n",
            "Iteration 2800: Current success : 174 \t All success : 197 \t mean(l0) : 15.279187 \t Current Learning Rate: 0.044\n",
            "Iteration 2900: Current success : 174 \t All success : 197 \t mean(l0) : 15.279187 \t Current Learning Rate: 0.042\n",
            "Iteration 3000: Current success : 174 \t All success : 197 \t mean(l0) : 15.279187 \t Current Learning Rate: 0.040\n",
            "Iteration 3100: Current success : 174 \t All success : 197 \t mean(l0) : 15.279187 \t Current Learning Rate: 0.038\n",
            "Iteration 3200: Current success : 175 \t All success : 198 \t mean(l0) : 15.363636 \t Current Learning Rate: 0.036\n",
            "Iteration 3300: Current success : 175 \t All success : 198 \t mean(l0) : 15.363636 \t Current Learning Rate: 0.034\n",
            "Iteration 3400: Current success : 174 \t All success : 198 \t mean(l0) : 15.363636 \t Current Learning Rate: 0.032\n",
            "Iteration 3500: Current success : 175 \t All success : 198 \t mean(l0) : 15.363636 \t Current Learning Rate: 0.030\n",
            "Iteration 3600: Current success : 175 \t All success : 198 \t mean(l0) : 15.363636 \t Current Learning Rate: 0.028\n",
            "Iteration 3700: Current success : 175 \t All success : 198 \t mean(l0) : 15.363636 \t Current Learning Rate: 0.026\n",
            "Iteration 3800: Current success : 175 \t All success : 198 \t mean(l0) : 15.363636 \t Current Learning Rate: 0.024\n",
            "Iteration 3900: Current success : 175 \t All success : 198 \t mean(l0) : 15.363636 \t Current Learning Rate: 0.022\n",
            "Iteration 4000: Current success : 175 \t All success : 198 \t mean(l0) : 15.363636 \t Current Learning Rate: 0.020\n",
            "Iteration 4100: Current success : 175 \t All success : 198 \t mean(l0) : 15.363636 \t Current Learning Rate: 0.018\n",
            "Iteration 4200: Current success : 175 \t All success : 198 \t mean(l0) : 15.363636 \t Current Learning Rate: 0.016\n",
            "Iteration 4300: Current success : 175 \t All success : 198 \t mean(l0) : 15.363636 \t Current Learning Rate: 0.014\n",
            "Iteration 4400: Current success : 175 \t All success : 198 \t mean(l0) : 15.363636 \t Current Learning Rate: 0.012\n",
            "Iteration 4500: Current success : 175 \t All success : 198 \t mean(l0) : 15.363636 \t Current Learning Rate: 0.010\n",
            "Iteration 4600: Current success : 175 \t All success : 198 \t mean(l0) : 15.363636 \t Current Learning Rate: 0.008\n",
            "Iteration 4700: Current success : 175 \t All success : 198 \t mean(l0) : 15.363636 \t Current Learning Rate: 0.006\n",
            "Iteration 4800: Current success : 175 \t All success : 198 \t mean(l0) : 15.363636 \t Current Learning Rate: 0.004\n",
            "Iteration 4900: Current success : 175 \t All success : 198 \t mean(l0) : 15.363636 \t Current Learning Rate: 0.002\n",
            "Iteration 5000: Current success : 175 \t All success : 198 \t mean(l0) : 15.363636 \t Current Learning Rate: 0.000\n",
            "Iteration 5100: Current success : 198 \t All success : 213 \t mean(l0) : 16.582159 \t Current Learning Rate: 0.049\n",
            "Iteration 5200: Current success : 198 \t All success : 219 \t mean(l0) : 17.255707 \t Current Learning Rate: 0.048\n",
            "Iteration 5300: Current success : 198 \t All success : 219 \t mean(l0) : 17.246574 \t Current Learning Rate: 0.047\n",
            "Iteration 5400: Current success : 201 \t All success : 220 \t mean(l0) : 17.600000 \t Current Learning Rate: 0.046\n",
            "Iteration 5500: Current success : 201 \t All success : 221 \t mean(l0) : 17.583712 \t Current Learning Rate: 0.045\n",
            "Iteration 5600: Current success : 200 \t All success : 221 \t mean(l0) : 17.583712 \t Current Learning Rate: 0.044\n",
            "Iteration 5700: Current success : 200 \t All success : 221 \t mean(l0) : 17.583712 \t Current Learning Rate: 0.043\n",
            "Iteration 5800: Current success : 201 \t All success : 221 \t mean(l0) : 17.583712 \t Current Learning Rate: 0.042\n",
            "Iteration 5900: Current success : 201 \t All success : 221 \t mean(l0) : 17.583712 \t Current Learning Rate: 0.041\n",
            "Iteration 6000: Current success : 201 \t All success : 221 \t mean(l0) : 17.583712 \t Current Learning Rate: 0.040\n",
            "Iteration 6100: Current success : 201 \t All success : 221 \t mean(l0) : 17.583712 \t Current Learning Rate: 0.039\n",
            "Iteration 6200: Current success : 201 \t All success : 221 \t mean(l0) : 17.583712 \t Current Learning Rate: 0.038\n",
            "Iteration 6300: Current success : 201 \t All success : 221 \t mean(l0) : 17.583712 \t Current Learning Rate: 0.037\n",
            "Iteration 6400: Current success : 201 \t All success : 221 \t mean(l0) : 17.574661 \t Current Learning Rate: 0.036\n",
            "Iteration 6500: Current success : 201 \t All success : 221 \t mean(l0) : 17.574661 \t Current Learning Rate: 0.035\n",
            "Iteration 6600: Current success : 201 \t All success : 221 \t mean(l0) : 17.574661 \t Current Learning Rate: 0.034\n",
            "Iteration 6700: Current success : 201 \t All success : 221 \t mean(l0) : 17.574661 \t Current Learning Rate: 0.033\n",
            "Iteration 6800: Current success : 201 \t All success : 221 \t mean(l0) : 17.574661 \t Current Learning Rate: 0.032\n",
            "Iteration 6900: Current success : 201 \t All success : 221 \t mean(l0) : 17.574661 \t Current Learning Rate: 0.031\n",
            "Iteration 7000: Current success : 201 \t All success : 221 \t mean(l0) : 17.574661 \t Current Learning Rate: 0.030\n",
            "Iteration 7100: Current success : 201 \t All success : 221 \t mean(l0) : 17.574661 \t Current Learning Rate: 0.029\n",
            "Iteration 7200: Current success : 201 \t All success : 221 \t mean(l0) : 17.574661 \t Current Learning Rate: 0.028\n",
            "Iteration 7300: Current success : 201 \t All success : 221 \t mean(l0) : 17.574661 \t Current Learning Rate: 0.027\n",
            "Iteration 7400: Current success : 201 \t All success : 221 \t mean(l0) : 17.574661 \t Current Learning Rate: 0.026\n",
            "Iteration 7500: Current success : 201 \t All success : 221 \t mean(l0) : 17.574661 \t Current Learning Rate: 0.025\n",
            "Iteration 7600: Current success : 201 \t All success : 221 \t mean(l0) : 17.574661 \t Current Learning Rate: 0.024\n",
            "Iteration 7700: Current success : 201 \t All success : 221 \t mean(l0) : 17.574661 \t Current Learning Rate: 0.023\n",
            "Iteration 7800: Current success : 201 \t All success : 221 \t mean(l0) : 17.574661 \t Current Learning Rate: 0.022\n",
            "Iteration 7900: Current success : 201 \t All success : 221 \t mean(l0) : 17.574661 \t Current Learning Rate: 0.021\n",
            "Iteration 8000: Current success : 201 \t All success : 221 \t mean(l0) : 17.574661 \t Current Learning Rate: 0.020\n",
            "Iteration 8100: Current success : 201 \t All success : 221 \t mean(l0) : 17.574661 \t Current Learning Rate: 0.019\n",
            "Iteration 8200: Current success : 201 \t All success : 221 \t mean(l0) : 17.574661 \t Current Learning Rate: 0.018\n",
            "Iteration 8300: Current success : 201 \t All success : 221 \t mean(l0) : 17.574661 \t Current Learning Rate: 0.017\n",
            "Iteration 8400: Current success : 201 \t All success : 221 \t mean(l0) : 17.574661 \t Current Learning Rate: 0.016\n",
            "Iteration 8500: Current success : 201 \t All success : 221 \t mean(l0) : 17.574661 \t Current Learning Rate: 0.015\n",
            "Iteration 8600: Current success : 201 \t All success : 221 \t mean(l0) : 17.574661 \t Current Learning Rate: 0.014\n",
            "Iteration 8700: Current success : 201 \t All success : 221 \t mean(l0) : 17.574661 \t Current Learning Rate: 0.013\n",
            "Iteration 8800: Current success : 201 \t All success : 221 \t mean(l0) : 17.574661 \t Current Learning Rate: 0.012\n",
            "Iteration 8900: Current success : 201 \t All success : 221 \t mean(l0) : 17.574661 \t Current Learning Rate: 0.011\n",
            "Iteration 9000: Current success : 201 \t All success : 221 \t mean(l0) : 17.574661 \t Current Learning Rate: 0.010\n",
            "Iteration 9100: Current success : 201 \t All success : 221 \t mean(l0) : 17.574661 \t Current Learning Rate: 0.009\n",
            "Iteration 9200: Current success : 201 \t All success : 221 \t mean(l0) : 17.574661 \t Current Learning Rate: 0.008\n",
            "Iteration 9300: Current success : 201 \t All success : 221 \t mean(l0) : 17.574661 \t Current Learning Rate: 0.007\n",
            "Iteration 9400: Current success : 201 \t All success : 221 \t mean(l0) : 17.574661 \t Current Learning Rate: 0.006\n",
            "Iteration 9500: Current success : 201 \t All success : 221 \t mean(l0) : 17.574661 \t Current Learning Rate: 0.005\n",
            "Iteration 9600: Current success : 201 \t All success : 221 \t mean(l0) : 17.574661 \t Current Learning Rate: 0.004\n",
            "Iteration 9700: Current success : 201 \t All success : 221 \t mean(l0) : 17.574661 \t Current Learning Rate: 0.003\n",
            "Iteration 9800: Current success : 201 \t All success : 221 \t mean(l0) : 17.574661 \t Current Learning Rate: 0.002\n",
            "Iteration 9900: Current success : 201 \t All success : 221 \t mean(l0) : 17.574661 \t Current Learning Rate: 0.001\n",
            "Iteration 10000: Current success : 201 \t All success : 221 \t mean(l0) : 17.574661 \t Current Learning Rate: 0.000\n",
            "Iteration 10100: Current success : 197 \t All success : 221 \t mean(l0) : 17.570137 \t Current Learning Rate: 0.025\n",
            "Iteration 10200: Current success : 198 \t All success : 221 \t mean(l0) : 17.542988 \t Current Learning Rate: 0.024\n",
            "Iteration 10300: Current success : 199 \t All success : 221 \t mean(l0) : 17.538462 \t Current Learning Rate: 0.024\n",
            "Iteration 10400: Current success : 199 \t All success : 221 \t mean(l0) : 17.538462 \t Current Learning Rate: 0.023\n",
            "Iteration 10500: Current success : 199 \t All success : 221 \t mean(l0) : 17.538462 \t Current Learning Rate: 0.023\n",
            "Iteration 10600: Current success : 199 \t All success : 221 \t mean(l0) : 17.538462 \t Current Learning Rate: 0.022\n",
            "Iteration 10700: Current success : 200 \t All success : 222 \t mean(l0) : 17.486486 \t Current Learning Rate: 0.022\n",
            "Iteration 10800: Current success : 200 \t All success : 222 \t mean(l0) : 17.486486 \t Current Learning Rate: 0.021\n",
            "Iteration 10900: Current success : 200 \t All success : 222 \t mean(l0) : 17.486486 \t Current Learning Rate: 0.021\n",
            "Iteration 11000: Current success : 200 \t All success : 222 \t mean(l0) : 17.486486 \t Current Learning Rate: 0.020\n",
            "Iteration 11100: Current success : 200 \t All success : 222 \t mean(l0) : 17.486486 \t Current Learning Rate: 0.020\n",
            "Iteration 11200: Current success : 200 \t All success : 222 \t mean(l0) : 17.481983 \t Current Learning Rate: 0.019\n",
            "Iteration 11300: Current success : 200 \t All success : 222 \t mean(l0) : 17.481983 \t Current Learning Rate: 0.019\n",
            "Iteration 11400: Current success : 200 \t All success : 223 \t mean(l0) : 17.434978 \t Current Learning Rate: 0.018\n",
            "Iteration 11500: Current success : 200 \t All success : 223 \t mean(l0) : 17.434978 \t Current Learning Rate: 0.018\n",
            "Iteration 11600: Current success : 200 \t All success : 223 \t mean(l0) : 17.434978 \t Current Learning Rate: 0.017\n",
            "Iteration 11700: Current success : 200 \t All success : 223 \t mean(l0) : 17.434978 \t Current Learning Rate: 0.017\n",
            "Iteration 11800: Current success : 200 \t All success : 223 \t mean(l0) : 17.434978 \t Current Learning Rate: 0.016\n",
            "Iteration 11900: Current success : 200 \t All success : 223 \t mean(l0) : 17.434978 \t Current Learning Rate: 0.016\n",
            "Iteration 12000: Current success : 200 \t All success : 223 \t mean(l0) : 17.434978 \t Current Learning Rate: 0.015\n",
            "Iteration 12100: Current success : 200 \t All success : 223 \t mean(l0) : 17.434978 \t Current Learning Rate: 0.015\n",
            "Iteration 12200: Current success : 201 \t All success : 223 \t mean(l0) : 17.434978 \t Current Learning Rate: 0.014\n",
            "Iteration 12300: Current success : 201 \t All success : 223 \t mean(l0) : 17.434978 \t Current Learning Rate: 0.014\n",
            "Iteration 12400: Current success : 200 \t All success : 223 \t mean(l0) : 17.434978 \t Current Learning Rate: 0.013\n",
            "Iteration 12500: Current success : 201 \t All success : 223 \t mean(l0) : 17.434978 \t Current Learning Rate: 0.013\n",
            "Iteration 12600: Current success : 201 \t All success : 223 \t mean(l0) : 17.434978 \t Current Learning Rate: 0.012\n",
            "Iteration 12700: Current success : 201 \t All success : 223 \t mean(l0) : 17.434978 \t Current Learning Rate: 0.012\n",
            "Iteration 12800: Current success : 201 \t All success : 223 \t mean(l0) : 17.434978 \t Current Learning Rate: 0.011\n",
            "Iteration 12900: Current success : 201 \t All success : 223 \t mean(l0) : 17.434978 \t Current Learning Rate: 0.011\n",
            "Iteration 13000: Current success : 200 \t All success : 223 \t mean(l0) : 17.434978 \t Current Learning Rate: 0.010\n",
            "Iteration 13100: Current success : 200 \t All success : 223 \t mean(l0) : 17.434978 \t Current Learning Rate: 0.010\n",
            "Iteration 13200: Current success : 200 \t All success : 223 \t mean(l0) : 17.434978 \t Current Learning Rate: 0.009\n",
            "Iteration 13300: Current success : 200 \t All success : 223 \t mean(l0) : 17.434978 \t Current Learning Rate: 0.009\n",
            "Iteration 13400: Current success : 200 \t All success : 223 \t mean(l0) : 17.434978 \t Current Learning Rate: 0.008\n",
            "Iteration 13500: Current success : 200 \t All success : 223 \t mean(l0) : 17.434978 \t Current Learning Rate: 0.008\n",
            "Iteration 13600: Current success : 200 \t All success : 223 \t mean(l0) : 17.434978 \t Current Learning Rate: 0.007\n",
            "Iteration 13700: Current success : 200 \t All success : 223 \t mean(l0) : 17.434978 \t Current Learning Rate: 0.007\n",
            "Iteration 13800: Current success : 200 \t All success : 223 \t mean(l0) : 17.434978 \t Current Learning Rate: 0.006\n",
            "Iteration 13900: Current success : 200 \t All success : 223 \t mean(l0) : 17.434978 \t Current Learning Rate: 0.006\n",
            "Iteration 14000: Current success : 200 \t All success : 223 \t mean(l0) : 17.434978 \t Current Learning Rate: 0.005\n",
            "Iteration 14100: Current success : 200 \t All success : 223 \t mean(l0) : 17.434978 \t Current Learning Rate: 0.005\n",
            "Iteration 14200: Current success : 200 \t All success : 223 \t mean(l0) : 17.434978 \t Current Learning Rate: 0.004\n",
            "Iteration 14300: Current success : 200 \t All success : 223 \t mean(l0) : 17.434978 \t Current Learning Rate: 0.004\n",
            "Iteration 14400: Current success : 200 \t All success : 223 \t mean(l0) : 17.434978 \t Current Learning Rate: 0.003\n",
            "Iteration 14500: Current success : 200 \t All success : 223 \t mean(l0) : 17.434978 \t Current Learning Rate: 0.003\n",
            "Iteration 14600: Current success : 200 \t All success : 223 \t mean(l0) : 17.434978 \t Current Learning Rate: 0.002\n",
            "Iteration 14700: Current success : 200 \t All success : 223 \t mean(l0) : 17.434978 \t Current Learning Rate: 0.002\n",
            "Iteration 14800: Current success : 200 \t All success : 223 \t mean(l0) : 17.434978 \t Current Learning Rate: 0.001\n",
            "Iteration 14900: Current success : 200 \t All success : 223 \t mean(l0) : 17.434978 \t Current Learning Rate: 0.001\n",
            "Iteration 15000: Current success : 200 \t All success : 223 \t mean(l0) : 17.434978 \t Current Learning Rate: 0.000\n",
            "Iteration 15100: Current success : 201 \t All success : 223 \t mean(l0) : 17.434978 \t Current Learning Rate: 0.012\n",
            "Iteration 15200: Current success : 202 \t All success : 223 \t mean(l0) : 17.434978 \t Current Learning Rate: 0.012\n",
            "Iteration 15300: Current success : 201 \t All success : 223 \t mean(l0) : 17.434978 \t Current Learning Rate: 0.012\n",
            "Iteration 15400: Current success : 201 \t All success : 224 \t mean(l0) : 17.656250 \t Current Learning Rate: 0.012\n",
            "Iteration 15500: Current success : 201 \t All success : 224 \t mean(l0) : 17.651787 \t Current Learning Rate: 0.011\n",
            "Iteration 15600: Current success : 201 \t All success : 224 \t mean(l0) : 17.651787 \t Current Learning Rate: 0.011\n",
            "Iteration 15700: Current success : 201 \t All success : 224 \t mean(l0) : 17.651787 \t Current Learning Rate: 0.011\n",
            "Iteration 15800: Current success : 202 \t All success : 224 \t mean(l0) : 17.651787 \t Current Learning Rate: 0.011\n",
            "Iteration 15900: Current success : 202 \t All success : 224 \t mean(l0) : 17.651787 \t Current Learning Rate: 0.010\n",
            "Iteration 16000: Current success : 203 \t All success : 224 \t mean(l0) : 17.642859 \t Current Learning Rate: 0.010\n",
            "Iteration 16100: Current success : 203 \t All success : 224 \t mean(l0) : 17.642859 \t Current Learning Rate: 0.010\n",
            "Iteration 16200: Current success : 202 \t All success : 224 \t mean(l0) : 17.642859 \t Current Learning Rate: 0.010\n",
            "Iteration 16300: Current success : 203 \t All success : 224 \t mean(l0) : 17.642859 \t Current Learning Rate: 0.009\n",
            "Iteration 16400: Current success : 203 \t All success : 224 \t mean(l0) : 17.642859 \t Current Learning Rate: 0.009\n",
            "Iteration 16500: Current success : 203 \t All success : 224 \t mean(l0) : 17.642859 \t Current Learning Rate: 0.009\n",
            "Iteration 16600: Current success : 203 \t All success : 224 \t mean(l0) : 17.642859 \t Current Learning Rate: 0.009\n",
            "Iteration 16700: Current success : 203 \t All success : 224 \t mean(l0) : 17.642859 \t Current Learning Rate: 0.008\n",
            "Iteration 16800: Current success : 203 \t All success : 224 \t mean(l0) : 17.642859 \t Current Learning Rate: 0.008\n",
            "Iteration 16900: Current success : 203 \t All success : 224 \t mean(l0) : 17.642859 \t Current Learning Rate: 0.008\n",
            "Iteration 17000: Current success : 203 \t All success : 224 \t mean(l0) : 17.642859 \t Current Learning Rate: 0.008\n",
            "Iteration 17100: Current success : 203 \t All success : 224 \t mean(l0) : 17.642859 \t Current Learning Rate: 0.007\n",
            "Iteration 17200: Current success : 203 \t All success : 224 \t mean(l0) : 17.642859 \t Current Learning Rate: 0.007\n",
            "Iteration 17300: Current success : 202 \t All success : 224 \t mean(l0) : 17.642859 \t Current Learning Rate: 0.007\n",
            "Iteration 17400: Current success : 202 \t All success : 224 \t mean(l0) : 17.642859 \t Current Learning Rate: 0.007\n",
            "Iteration 17500: Current success : 202 \t All success : 224 \t mean(l0) : 17.642859 \t Current Learning Rate: 0.006\n",
            "Iteration 17600: Current success : 202 \t All success : 224 \t mean(l0) : 17.642859 \t Current Learning Rate: 0.006\n",
            "Iteration 17700: Current success : 202 \t All success : 224 \t mean(l0) : 17.642859 \t Current Learning Rate: 0.006\n",
            "Iteration 17800: Current success : 202 \t All success : 224 \t mean(l0) : 17.642859 \t Current Learning Rate: 0.006\n",
            "Iteration 17900: Current success : 202 \t All success : 224 \t mean(l0) : 17.642859 \t Current Learning Rate: 0.005\n",
            "Iteration 18000: Current success : 202 \t All success : 224 \t mean(l0) : 17.642859 \t Current Learning Rate: 0.005\n",
            "Iteration 18100: Current success : 202 \t All success : 224 \t mean(l0) : 17.642859 \t Current Learning Rate: 0.005\n",
            "Iteration 18200: Current success : 203 \t All success : 224 \t mean(l0) : 17.642859 \t Current Learning Rate: 0.005\n",
            "Iteration 18300: Current success : 203 \t All success : 224 \t mean(l0) : 17.642859 \t Current Learning Rate: 0.004\n",
            "Iteration 18400: Current success : 203 \t All success : 224 \t mean(l0) : 17.642859 \t Current Learning Rate: 0.004\n",
            "Iteration 18500: Current success : 202 \t All success : 224 \t mean(l0) : 17.642859 \t Current Learning Rate: 0.004\n",
            "Iteration 18600: Current success : 202 \t All success : 224 \t mean(l0) : 17.642859 \t Current Learning Rate: 0.004\n",
            "Iteration 18700: Current success : 203 \t All success : 224 \t mean(l0) : 17.642859 \t Current Learning Rate: 0.003\n",
            "Iteration 18800: Current success : 202 \t All success : 224 \t mean(l0) : 17.642859 \t Current Learning Rate: 0.003\n",
            "Iteration 18900: Current success : 202 \t All success : 224 \t mean(l0) : 17.642859 \t Current Learning Rate: 0.003\n",
            "Iteration 19000: Current success : 202 \t All success : 224 \t mean(l0) : 17.642859 \t Current Learning Rate: 0.003\n",
            "Iteration 19100: Current success : 201 \t All success : 224 \t mean(l0) : 17.642859 \t Current Learning Rate: 0.002\n",
            "Iteration 19200: Current success : 201 \t All success : 224 \t mean(l0) : 17.642859 \t Current Learning Rate: 0.002\n",
            "Iteration 19300: Current success : 201 \t All success : 224 \t mean(l0) : 17.642859 \t Current Learning Rate: 0.002\n",
            "Iteration 19400: Current success : 201 \t All success : 224 \t mean(l0) : 17.642859 \t Current Learning Rate: 0.002\n",
            "Iteration 19500: Current success : 201 \t All success : 224 \t mean(l0) : 17.642859 \t Current Learning Rate: 0.001\n",
            "Iteration 19600: Current success : 201 \t All success : 224 \t mean(l0) : 17.642859 \t Current Learning Rate: 0.001\n",
            "Iteration 19700: Current success : 201 \t All success : 224 \t mean(l0) : 17.642859 \t Current Learning Rate: 0.001\n",
            "Iteration 19800: Current success : 201 \t All success : 224 \t mean(l0) : 17.642859 \t Current Learning Rate: 0.001\n",
            "Iteration 19900: Current success : 201 \t All success : 224 \t mean(l0) : 17.642859 \t Current Learning Rate: 0.000\n",
            "Iteration 20000: Current success : 201 \t All success : 224 \t mean(l0) : 17.642859 \t Current Learning Rate: 0.000\n",
            "1130\n",
            "264\n",
            "Accuracy of just malwares (without attack): 98.85% | Under attack: 76.64%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VPSU7ISViK4E"
      },
      "outputs": [],
      "source": [
        "for x_test, y_test in test_loader:\n",
        "    x_test, y_test = x_test.to(torch.float32).to(device), y_test.to(device)\n",
        "    mal_x_batch, mal_y_batch = x_test[y_test.squeeze() == 1], y_test[y_test.squeeze() == 1]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation on rounded adversarial examples\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "combined_model.eval()\n",
        "\n",
        "# Apply the final rounding solution to adversarial examples\n",
        "adv_rounded = final_rounding(mal_x_batch, adv, combined_model,removal_array)\n",
        "\n",
        "# Get the model's predictions on the rounded adversarial examples\n",
        "outputs = combined_model(adv_rounded)\n",
        "y_pred = outputs.argmax(dim=1)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "num_malwares = len(mal_y_batch)\n",
        "num_successful_attacks = (y_pred == 0).sum().item()\n",
        "total_difference = (torch.abs(adv_rounded[y_pred == 0] - mal_x_batch[y_pred == 0])).sum().item()\n",
        "accuracy = (y_pred == 1).sum().item() / num_malwares * 100\n",
        "\n",
        "# Display the results with clear formatting\n",
        "print(f\"Evaluation Results on Rounded Adversarial Examples\")\n",
        "print(f\"-----------------------------------------------\")\n",
        "print(f\"Number of malware samples: {num_malwares}\")\n",
        "print(f\"Number of successful attacks: {num_successful_attacks}\")\n",
        "print(f\"Total L1 norm difference (successful adversarial examples vs. original): {total_difference}\")\n",
        "print(f\"Accuracy of the model on malware under attack: {accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "id": "OjwkOtQPiK4E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5b1c5a0-728b-4f96-842a-311041dc2e2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Results on Rounded Adversarial Examples\n",
            "-----------------------------------------------\n",
            "Number of malware samples: 1130\n",
            "Number of successful attacks: 264\n",
            "Total L1 norm difference (successful adversarial examples vs. original): 5543.0\n",
            "Accuracy of the model on malware under attack: 76.64%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure that fixed elements are not changed\n",
        "\n",
        "# Ensure `removal_array` is expanded to match the batch size\n",
        "expanded_removal_array = removal_array.expand(mal_x_batch.shape[0], -1)\n",
        "\n",
        "# Create a mask where non-fixed features are set to 1\n",
        "non_fixed_features_mask = torch.bitwise_or(expanded_removal_array, 1 - mal_x_batch.to(torch.uint8))\n",
        "\n",
        "# Calculate indices where the mask is 0 (fixed) or 1 (non-fixed)\n",
        "mask_fixed = non_fixed_features_mask == 0\n",
        "mask_non_fixed = non_fixed_features_mask == 1\n",
        "\n",
        "# Calculate and print the differences\n",
        "diff_adv_fixed = (torch.abs(adv - mal_x_batch) * mask_fixed).sum().item()\n",
        "diff_adv_non_fixed = (torch.abs(adv - mal_x_batch) * mask_non_fixed).sum().item()\n",
        "diff_adv_rounded_fixed = (torch.abs(adv_rounded - mal_x_batch) * mask_fixed).sum().item()\n",
        "diff_adv_rounded_non_fixed = (torch.abs(adv_rounded - mal_x_batch) * mask_non_fixed).sum().item()\n",
        "\n",
        "# Display results with clear formatting\n",
        "print(f\"Differences when mask is 0 (fixed features):\")\n",
        "print(f\"  Adv vs. Original: {diff_adv_fixed}\")\n",
        "print(f\"  Rounded Adv vs. Original: {diff_adv_rounded_fixed}\")\n",
        "print(f\"Differences when mask is 1 (non-fixed features):\")\n",
        "print(f\"  Adv vs. Original: {diff_adv_non_fixed}\")\n",
        "print(f\"  Rounded Adv vs. Original: {diff_adv_rounded_non_fixed}\")\n"
      ],
      "metadata": {
        "id": "U7YUn80SiK4F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25da9df1-f3df-4270-ff00-9a19c906f8e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Differences when mask is 0 (fixed features):\n",
            "  Adv vs. Original: 0.0\n",
            "  Rounded Adv vs. Original: 0.0\n",
            "Differences when mask is 1 (non-fixed features):\n",
            "  Adv vs. Original: 62308.578125\n",
            "  Rounded Adv vs. Original: 70699.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming differences is already computed as in your code\n",
        "differences = (torch.abs(adv_rounded[y_pred == 0] - mal_x_batch[y_pred == 0])).sum(1)\n",
        "\n",
        "# Convert differences to a numpy array for plotting\n",
        "differences_np = differences.cpu().numpy()\n",
        "\n",
        "# Set the style of the plot\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# Create a histogram with Seaborn\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(differences_np, bins=16, kde=False, color='skyblue')\n",
        "#sns.histplot(differences_np, bins=30, kde=True, color='skyblue')\n",
        "\n",
        "# Set the title and labels\n",
        "plt.title('Abundance of Differences', fontsize=16)\n",
        "plt.xlabel('Sum of Absolute Differences', fontsize=14)\n",
        "plt.ylabel('Frequency', fontsize=14)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 576
        },
        "outputId": "52428dd9-6ea7-4115-f73f-1150be332ee1",
        "id": "_AVNcvDudsfH"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "adv[y_pred==0][0][:200]- mal_x_batch[y_pred==0][0][:200]"
      ],
      "metadata": {
        "id": "jfaY6s7liK4F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8328dd7-64b8-405f-a7b0-fdbaef84bb1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  1.0000e+00,  2.8399e-02,  0.0000e+00,\n",
              "         5.9605e-07,  0.0000e+00,  0.0000e+00,  0.0000e+00,  8.8948e-01,\n",
              "         0.0000e+00,  1.1921e-07,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         2.6058e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  3.9572e-04,  0.0000e+00,  0.0000e+00,  2.2962e-03,\n",
              "         0.0000e+00,  0.0000e+00,  1.4901e-07,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  1.9697e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  2.9206e-06,\n",
              "         1.0000e+00,  0.0000e+00,  0.0000e+00,  2.9802e-08,  0.0000e+00,\n",
              "         1.0000e+00,  5.9605e-08,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00, -3.6955e-06,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  1.7364e-03,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  8.8513e-06,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  1.8775e-06,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  1.4901e-07,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         1.1921e-06,  0.0000e+00,  3.5763e-07,  0.0000e+00,  2.2054e-06,\n",
              "         7.3016e-06,  0.0000e+00,  1.0645e-04,  4.3809e-06,  2.9802e-08,\n",
              "         0.0000e+00,  5.3644e-07,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "adv[y_pred==0][1][:200] - mal_x_batch[y_pred==0][1][:200]"
      ],
      "metadata": {
        "id": "QPZS5zwuiK4G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c35f5efe-eb81-4c34-e5fa-f10b5d96878f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.0000e+00,  1.0000e+00,  0.0000e+00,  0.0000e+00,  4.7386e-06,\n",
              "         0.0000e+00,  6.6370e-05,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         4.0435e-03,  1.0000e+00,  7.3716e-04,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  9.9986e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  5.3458e-02,  9.9902e-01,  1.0000e+00,  1.0133e-06,\n",
              "         0.0000e+00,  1.5241e-04,  0.0000e+00,  0.0000e+00,  9.9999e-01,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  9.9263e-01, -1.1683e-04,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  2.9802e-08,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  6.6340e-05,\n",
              "         0.0000e+00,  0.0000e+00, -4.6888e-01,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  3.7909e-05,\n",
              "        -9.9988e-01,  3.2783e-07,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,  1.1027e-05,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00, -5.1332e-04,  0.0000e+00,\n",
              "         0.0000e+00, -1.0000e+00,  2.3550e-03, -6.5565e-07, -2.6703e-01,\n",
              "         4.9865e-04,  3.7094e-01,  3.2783e-07,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  2.6107e-05,  5.8413e-06,  9.1636e-01,\n",
              "         0.0000e+00,  1.6502e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  3.2187e-06,  1.0000e+00,\n",
              "         0.0000e+00,  9.9901e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         5.9605e-08,  0.0000e+00,  5.9605e-08,  1.0000e+00,  0.0000e+00,\n",
              "         2.9802e-08,  1.2904e-05,  9.2387e-07,  0.0000e+00,  0.0000e+00,\n",
              "         5.5114e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  2.7418e-06,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  9.9979e-01,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  2.9802e-08,  1.9372e-06,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.1921e-07,\n",
              "         0.0000e+00,  1.6361e-05,  0.0000e+00,  0.0000e+00,  1.0000e+00,\n",
              "         4.5505e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6WRs7FNvRZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# pertubation ** 2"
      ],
      "metadata": {
        "id": "DO94hzD6R7KY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "from torch.optim.lr_scheduler import CyclicLR\n",
        "\n",
        "def compare(output, target, confidence):\n",
        "    if not isinstance(output, (float, int, torch.LongTensor, torch.cuda.LongTensor)):\n",
        "        output = torch.clone(output).detach()\n",
        "        output[target] -= confidence  # Slightly reduce the confidence for targeted attacks\n",
        "        output = torch.argmax(output, dim=1)\n",
        "    return output == target\n",
        "\n",
        "def compare_round(mal_x_batch,newimg, model,removal_array):\n",
        "\n",
        "    adv_rounded = final_rounding(mal_x_batch, newimg, model,removal_array)\n",
        "    #l0dist = torch.sum((mal_x_batch - newimg)!=0, dim=1)\n",
        "    l0dist = torch.sum(torch.abs(adv_rounded - mal_x_batch), dim=1)\n",
        "    logits = model(adv_rounded)\n",
        "    output = torch.argmax(logits, dim=1)\n",
        "\n",
        "    return output == 0, l0dist\n",
        "\n",
        "def sin_func_torch(input_tensor):\n",
        "    \"\"\"\n",
        "    A sine-based transformation applied to the input tensor.\n",
        "\n",
        "    The function applies a sine transformation and rescales the output to [0, 1].\n",
        "    \"\"\"\n",
        "    return (torch.sin((input_tensor - 0.5) * torch.pi) + 1) / 2.0\n",
        "\n",
        "def CW_l1(imgs, y, model, removal_array, max_iterations=1000, max_learning_rate=0.1,\n",
        "       binary_search_steps=3, confidence=0.01, initial_const=0.1,\n",
        "       cons_increase_factor=2, device='cuda'):\n",
        "    \"\"\"\n",
        "    Implements the Carlini & Wagner (CW) L1 attack.\n",
        "\n",
        "    Parameters:\n",
        "    - imgs: The original images to be attacked.\n",
        "    - y: The true labels of the images.\n",
        "    - model: The target model to attack.\n",
        "    - removal_array: A mask array specifying which features are fixed.\n",
        "    - max_iterations: The maximum number of iterations for the attack.\n",
        "    - learning_rate: The learning rate for the optimizer.\n",
        "    - binary_search_steps: The number of steps for the binary search over the constant.\n",
        "    - confidence: Confidence value for the attack.\n",
        "    - initial_const: The initial constant for the attack loss.\n",
        "    - cons_increase_factor: The factor by which the constant is increased during binary search.\n",
        "    - device: The device on which to perform computations.\n",
        "\n",
        "    Returns:\n",
        "    - final_output: The perturbed images after the attack.\n",
        "    \"\"\"\n",
        "\n",
        "    labs = torch.zeros_like(y.squeeze(), dtype=torch.int64)\n",
        "    batch_size = imgs.shape[0]\n",
        "\n",
        "    # Expand removal array to match the batch size\n",
        "    expanded_removal_array = removal_array.expand(imgs.shape[0], -1)\n",
        "    non_fixed_features_mask = torch.bitwise_or(expanded_removal_array, 1 - imgs.to(torch.uint8))\n",
        "\n",
        "    mask_0 = non_fixed_features_mask == 0\n",
        "    mask_1 = non_fixed_features_mask == 1\n",
        "\n",
        "    imgs_tensor = imgs.clone().detach().to(dtype=torch.float32, device=device)\n",
        "    labs_tensor = labs.clone().detach().to(dtype=torch.int64, device=device)\n",
        "\n",
        "    lower_bound = torch.zeros(batch_size, dtype=torch.float32, device=device)\n",
        "    CONST = torch.full((batch_size,), initial_const, dtype=torch.float32, device=device)\n",
        "    upper_bound = torch.full((batch_size,), 1e10, dtype=torch.float32, device=device)\n",
        "\n",
        "    o_bestl0 = torch.full((batch_size,), 1e10, device=device)\n",
        "    o_bestattack = imgs.clone().detach()\n",
        "\n",
        "    o_best_loss1 = torch.full((batch_size,), float('inf'), device=device)\n",
        "    o_best_unsuccessfull_attack = imgs.clone().detach()\n",
        "    o_success_attack = torch.zeros(batch_size, dtype=torch.bool, device=device)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    for outer_step in range(binary_search_steps):\n",
        "        print(CONST)\n",
        "\n",
        "        modifier = torch.zeros_like(imgs_tensor, requires_grad=True).to(device)\n",
        "        optimizer = torch.optim.Adam([modifier], lr=max_learning_rate)\n",
        "\n",
        "        def grad_hook(grad):\n",
        "            return grad * non_fixed_features_mask\n",
        "        modifier.register_hook(grad_hook)\n",
        "\n",
        "        scheduler = CyclicLR(optimizer, base_lr=0.0001, max_lr=max_learning_rate, step_size_up=5,\n",
        "                             step_size_down=4995, mode='triangular2')\n",
        "\n",
        "        best_loss = torch.full((batch_size,), float('inf'), device=device)\n",
        "        no_improvement_count = torch.zeros(batch_size, dtype=torch.int, device=device)\n",
        "        active_mask = torch.ones(batch_size, dtype=torch.bool, device=device)\n",
        "\n",
        "        bestl0 = torch.full((batch_size,), 1e10, device=device)\n",
        "        bestscore = torch.full((batch_size,), -1, device=device)\n",
        "        success_mask = torch.zeros(batch_size, dtype=torch.bool, device=device)\n",
        "\n",
        "        for iteration in range(max_iterations):\n",
        "\n",
        "            active_indices = torch.where(active_mask)[0]\n",
        "            active_modifier = modifier[active_mask]\n",
        "            active_imgs_tensor = imgs_tensor[active_mask]\n",
        "            active_labs_tensor = labs_tensor[active_mask]\n",
        "            active_CONST = CONST[active_mask]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            modified_input = active_imgs_tensor + active_modifier\n",
        "            newimg = sin_func_torch(modified_input)\n",
        "            output = model(newimg)\n",
        "\n",
        "            #eps = 1e-2\n",
        "            #perturbation = torch.where((active_imgs_tensor==1.), (active_imgs_tensor - newimg), (newimg - active_imgs_tensor))\n",
        "\n",
        "            #l2dist = torch.sum((1 - (perturbation - 1)**2 + eps)**0.5 - eps**0.5 , dim=1)\n",
        "            l2dist = torch.sum((newimg - active_imgs_tensor) ** 2, dim=1)\n",
        "\n",
        "            real = output[torch.arange(active_indices.size(0)), active_labs_tensor]\n",
        "            other = output[torch.arange(active_indices.size(0)), active_labs_tensor + 1]\n",
        "\n",
        "            loss1 = torch.clamp((other - real + confidence), min=0.0)\n",
        "            #loss2 = l2dist\n",
        "\n",
        "            if outer_step != (binary_search_steps - 1):\n",
        "                loss = (active_CONST * loss1) + l2dist\n",
        "            else:\n",
        "                loss = loss1\n",
        "\n",
        "            total_loss = torch.sum(loss)\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            if (iteration + 1) % 10 == 0:\n",
        "                with torch.no_grad():\n",
        "                    success_mask_inloop, l0dist = compare_round(active_imgs_tensor,newimg, model,removal_array)\n",
        "                    success_mask  = torch.logical_or(success_mask_inloop, success_mask)\n",
        "                    current_lr = scheduler.get_last_lr()[0]\n",
        "\n",
        "                    improved_mask2 = (loss1 < o_best_loss1[active_mask])\n",
        "                    o_best_loss1[active_mask] = torch.where(improved_mask2, loss1, o_best_loss1[active_mask])\n",
        "                    o_best_unsuccessfull_attack[active_mask] = torch.where(improved_mask2.unsqueeze(-1), newimg, o_best_unsuccessfull_attack[active_mask])\n",
        "\n",
        "                    update_mask = (l0dist < bestl0[active_mask]) & success_mask_inloop\n",
        "                    bestl0[active_mask] = torch.where(update_mask, l0dist, bestl0[active_mask])\n",
        "                    bestscore[active_mask] = torch.where(update_mask, torch.argmax(output, dim=1), bestscore[active_mask])\n",
        "\n",
        "                    update_mask2 = (l0dist < o_bestl0[active_mask]) & success_mask_inloop\n",
        "                    o_bestl0[active_mask] = torch.where(update_mask2, l0dist, o_bestl0[active_mask])\n",
        "                    o_bestattack[active_mask] = torch.where(update_mask2.unsqueeze(-1), newimg, o_bestattack[active_mask])\n",
        "\n",
        "                    # Skip the rest of the loop for samples that haven't improved in 50 iterations\n",
        "                    #active_mask[active_mask.clone()] = no_improvement_count[active_mask.clone()] < 50\n",
        "\n",
        "                    if (iteration + 1) % 100 == 0:\n",
        "                        print(f\"Iteration {iteration + 1}: Current success : {success_mask_inloop.sum()} \\t All success : {success_mask.sum()} \\t mean(l0) : {o_bestl0[success_mask].mean():.6f} \\t Current Learning Rate: {current_lr:.3f}\")\n",
        "\n",
        "        if outer_step != (binary_search_steps - 1):\n",
        "            #o_success_attack  = torch.logical_or(o_success_attack, success_mask)\n",
        "            o_success_attack, o_l0dist = compare_round(imgs,o_bestattack, model,removal_array)\n",
        "            print(f\"outer_step {outer_step + 1}: all success : {o_success_attack.sum()} \\t mean(l0)(success) : {o_l0dist[o_success_attack].mean():.4f}\")\n",
        "            #print('o_success_attack ', o_success_attack.sum())\n",
        "            print('-------------------------------------------------------------')\n",
        "            failure_mask = ~success_mask\n",
        "            upper_bound[success_mask] = torch.minimum(upper_bound[success_mask], CONST[success_mask])\n",
        "            lower_bound[failure_mask] = torch.maximum(lower_bound[failure_mask], CONST[failure_mask])\n",
        "\n",
        "            upper_limit_mask = upper_bound < 1e10\n",
        "            CONST[upper_limit_mask] = (lower_bound[upper_limit_mask] + upper_bound[upper_limit_mask]) / 2\n",
        "            CONST[failure_mask & ~upper_limit_mask] *= cons_increase_factor\n",
        "\n",
        "    success_attack_mask = (o_bestl0 != 1e10)\n",
        "    final_output = torch.where(success_attack_mask.unsqueeze(-1), o_bestattack, o_best_unsuccessfull_attack)\n",
        "\n",
        "    return final_output\n"
      ],
      "metadata": {
        "id": "av7WuVQaRefb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#l2dist = torch.sum(perturbation ** 2, dim=1)\n",
        "attack_params =  {'device':device, 'confidence':0.5, 'removal_array':removal_array,'max_iterations': 20000, 'max_learning_rate': 0.1, 'initial_const': 1 , 'binary_search_steps' : 7, 'cons_increase_factor' : 10}\n",
        "adv = adv_predict(test_loader, combined_model, CW_l1, **attack_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45c71ea1-1d2f-4950-f5e0-5e46f3fd2a07",
        "id": "kBlhm5sqR0xx"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0')\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-15-86a2713630d1>:43: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at ../aten/src/ATen/native/TensorCompare.cpp:530.)\n",
            "  final[active_indices, indices] = torch.where(non_fixed_features_mask[active_indices, indices],\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 100: Current success : 82 \t All success : 95 \t mean(l0) : 6.136842 \t Current Learning Rate: 0.098\n",
            "Iteration 200: Current success : 80 \t All success : 106 \t mean(l0) : 6.754717 \t Current Learning Rate: 0.096\n",
            "Iteration 300: Current success : 84 \t All success : 108 \t mean(l0) : 6.703704 \t Current Learning Rate: 0.094\n",
            "Iteration 400: Current success : 81 \t All success : 108 \t mean(l0) : 6.666667 \t Current Learning Rate: 0.092\n",
            "Iteration 500: Current success : 87 \t All success : 109 \t mean(l0) : 6.614678 \t Current Learning Rate: 0.090\n",
            "Iteration 600: Current success : 83 \t All success : 109 \t mean(l0) : 6.596330 \t Current Learning Rate: 0.088\n",
            "Iteration 700: Current success : 78 \t All success : 109 \t mean(l0) : 6.504587 \t Current Learning Rate: 0.086\n",
            "Iteration 800: Current success : 81 \t All success : 109 \t mean(l0) : 6.403669 \t Current Learning Rate: 0.084\n",
            "Iteration 900: Current success : 86 \t All success : 111 \t mean(l0) : 6.423424 \t Current Learning Rate: 0.082\n",
            "Iteration 1000: Current success : 84 \t All success : 111 \t mean(l0) : 6.387388 \t Current Learning Rate: 0.080\n",
            "Iteration 1100: Current success : 85 \t All success : 111 \t mean(l0) : 6.387388 \t Current Learning Rate: 0.078\n",
            "Iteration 1200: Current success : 80 \t All success : 111 \t mean(l0) : 6.261261 \t Current Learning Rate: 0.076\n",
            "Iteration 1300: Current success : 81 \t All success : 111 \t mean(l0) : 6.261261 \t Current Learning Rate: 0.074\n",
            "Iteration 1400: Current success : 79 \t All success : 111 \t mean(l0) : 6.261261 \t Current Learning Rate: 0.072\n",
            "Iteration 1500: Current success : 81 \t All success : 111 \t mean(l0) : 6.261261 \t Current Learning Rate: 0.070\n",
            "Iteration 1600: Current success : 81 \t All success : 111 \t mean(l0) : 6.261261 \t Current Learning Rate: 0.068\n",
            "Iteration 1700: Current success : 80 \t All success : 111 \t mean(l0) : 6.261261 \t Current Learning Rate: 0.066\n",
            "Iteration 1800: Current success : 81 \t All success : 111 \t mean(l0) : 6.261261 \t Current Learning Rate: 0.064\n",
            "Iteration 1900: Current success : 80 \t All success : 111 \t mean(l0) : 6.261261 \t Current Learning Rate: 0.062\n",
            "Iteration 2000: Current success : 79 \t All success : 111 \t mean(l0) : 6.261261 \t Current Learning Rate: 0.060\n",
            "Iteration 2100: Current success : 79 \t All success : 111 \t mean(l0) : 6.261261 \t Current Learning Rate: 0.058\n",
            "Iteration 2200: Current success : 81 \t All success : 111 \t mean(l0) : 6.261261 \t Current Learning Rate: 0.056\n",
            "Iteration 2300: Current success : 79 \t All success : 111 \t mean(l0) : 6.252253 \t Current Learning Rate: 0.054\n",
            "Iteration 2400: Current success : 79 \t All success : 111 \t mean(l0) : 6.252253 \t Current Learning Rate: 0.052\n",
            "Iteration 2500: Current success : 82 \t All success : 111 \t mean(l0) : 6.252253 \t Current Learning Rate: 0.050\n",
            "Iteration 2600: Current success : 81 \t All success : 111 \t mean(l0) : 6.252253 \t Current Learning Rate: 0.048\n",
            "Iteration 2700: Current success : 84 \t All success : 111 \t mean(l0) : 6.252253 \t Current Learning Rate: 0.046\n",
            "Iteration 2800: Current success : 80 \t All success : 111 \t mean(l0) : 6.252253 \t Current Learning Rate: 0.044\n",
            "Iteration 2900: Current success : 80 \t All success : 111 \t mean(l0) : 6.252253 \t Current Learning Rate: 0.042\n",
            "Iteration 3000: Current success : 80 \t All success : 111 \t mean(l0) : 6.252253 \t Current Learning Rate: 0.040\n",
            "Iteration 3100: Current success : 81 \t All success : 111 \t mean(l0) : 6.252253 \t Current Learning Rate: 0.038\n",
            "Iteration 3200: Current success : 80 \t All success : 111 \t mean(l0) : 6.252253 \t Current Learning Rate: 0.036\n",
            "Iteration 3300: Current success : 82 \t All success : 111 \t mean(l0) : 6.252253 \t Current Learning Rate: 0.034\n",
            "Iteration 3400: Current success : 82 \t All success : 111 \t mean(l0) : 6.252253 \t Current Learning Rate: 0.032\n",
            "Iteration 3500: Current success : 80 \t All success : 111 \t mean(l0) : 6.252253 \t Current Learning Rate: 0.030\n",
            "Iteration 3600: Current success : 82 \t All success : 111 \t mean(l0) : 6.252253 \t Current Learning Rate: 0.028\n",
            "Iteration 3700: Current success : 80 \t All success : 111 \t mean(l0) : 6.252253 \t Current Learning Rate: 0.026\n",
            "Iteration 3800: Current success : 80 \t All success : 111 \t mean(l0) : 6.252253 \t Current Learning Rate: 0.024\n",
            "Iteration 3900: Current success : 80 \t All success : 111 \t mean(l0) : 6.252253 \t Current Learning Rate: 0.022\n",
            "Iteration 4000: Current success : 80 \t All success : 111 \t mean(l0) : 6.252253 \t Current Learning Rate: 0.020\n",
            "Iteration 4100: Current success : 80 \t All success : 111 \t mean(l0) : 6.252253 \t Current Learning Rate: 0.018\n",
            "Iteration 4200: Current success : 81 \t All success : 111 \t mean(l0) : 6.252253 \t Current Learning Rate: 0.016\n",
            "Iteration 4300: Current success : 80 \t All success : 111 \t mean(l0) : 6.252253 \t Current Learning Rate: 0.014\n",
            "Iteration 4400: Current success : 80 \t All success : 111 \t mean(l0) : 6.252253 \t Current Learning Rate: 0.012\n",
            "Iteration 4500: Current success : 80 \t All success : 111 \t mean(l0) : 6.252253 \t Current Learning Rate: 0.010\n",
            "Iteration 4600: Current success : 80 \t All success : 111 \t mean(l0) : 6.252253 \t Current Learning Rate: 0.008\n",
            "Iteration 4700: Current success : 80 \t All success : 111 \t mean(l0) : 6.252253 \t Current Learning Rate: 0.006\n",
            "Iteration 4800: Current success : 80 \t All success : 111 \t mean(l0) : 6.252253 \t Current Learning Rate: 0.004\n",
            "Iteration 4900: Current success : 80 \t All success : 111 \t mean(l0) : 6.252253 \t Current Learning Rate: 0.002\n",
            "Iteration 5000: Current success : 80 \t All success : 111 \t mean(l0) : 6.252253 \t Current Learning Rate: 0.000\n",
            "Iteration 5100: Current success : 76 \t All success : 120 \t mean(l0) : 7.033334 \t Current Learning Rate: 0.049\n",
            "Iteration 5200: Current success : 86 \t All success : 120 \t mean(l0) : 7.016667 \t Current Learning Rate: 0.048\n",
            "Iteration 5300: Current success : 79 \t All success : 120 \t mean(l0) : 7.016667 \t Current Learning Rate: 0.047\n",
            "Iteration 5400: Current success : 70 \t All success : 120 \t mean(l0) : 7.016667 \t Current Learning Rate: 0.046\n",
            "Iteration 5500: Current success : 72 \t All success : 120 \t mean(l0) : 7.016667 \t Current Learning Rate: 0.045\n",
            "Iteration 5600: Current success : 74 \t All success : 120 \t mean(l0) : 7.016667 \t Current Learning Rate: 0.044\n",
            "Iteration 5700: Current success : 74 \t All success : 120 \t mean(l0) : 7.008334 \t Current Learning Rate: 0.043\n",
            "Iteration 5800: Current success : 76 \t All success : 120 \t mean(l0) : 7.008334 \t Current Learning Rate: 0.042\n",
            "Iteration 5900: Current success : 76 \t All success : 120 \t mean(l0) : 7.008334 \t Current Learning Rate: 0.041\n",
            "Iteration 6000: Current success : 75 \t All success : 120 \t mean(l0) : 7.008334 \t Current Learning Rate: 0.040\n",
            "Iteration 6100: Current success : 71 \t All success : 120 \t mean(l0) : 7.008334 \t Current Learning Rate: 0.039\n",
            "Iteration 6200: Current success : 79 \t All success : 120 \t mean(l0) : 7.008334 \t Current Learning Rate: 0.038\n",
            "Iteration 6300: Current success : 76 \t All success : 120 \t mean(l0) : 7.008334 \t Current Learning Rate: 0.037\n",
            "Iteration 6400: Current success : 76 \t All success : 120 \t mean(l0) : 7.008334 \t Current Learning Rate: 0.036\n",
            "Iteration 6500: Current success : 74 \t All success : 120 \t mean(l0) : 7.008334 \t Current Learning Rate: 0.035\n",
            "Iteration 6600: Current success : 74 \t All success : 120 \t mean(l0) : 7.008334 \t Current Learning Rate: 0.034\n",
            "Iteration 6700: Current success : 73 \t All success : 120 \t mean(l0) : 7.008334 \t Current Learning Rate: 0.033\n",
            "Iteration 6800: Current success : 73 \t All success : 120 \t mean(l0) : 7.008334 \t Current Learning Rate: 0.032\n",
            "Iteration 6900: Current success : 76 \t All success : 120 \t mean(l0) : 7.008334 \t Current Learning Rate: 0.031\n",
            "Iteration 7000: Current success : 74 \t All success : 120 \t mean(l0) : 7.008334 \t Current Learning Rate: 0.030\n",
            "Iteration 7100: Current success : 75 \t All success : 120 \t mean(l0) : 7.008334 \t Current Learning Rate: 0.029\n",
            "Iteration 7200: Current success : 75 \t All success : 120 \t mean(l0) : 7.008334 \t Current Learning Rate: 0.028\n",
            "Iteration 7300: Current success : 77 \t All success : 120 \t mean(l0) : 7.008334 \t Current Learning Rate: 0.027\n",
            "Iteration 7400: Current success : 75 \t All success : 120 \t mean(l0) : 7.008334 \t Current Learning Rate: 0.026\n",
            "Iteration 7500: Current success : 75 \t All success : 120 \t mean(l0) : 7.008334 \t Current Learning Rate: 0.025\n",
            "Iteration 7600: Current success : 77 \t All success : 120 \t mean(l0) : 7.008334 \t Current Learning Rate: 0.024\n",
            "Iteration 7700: Current success : 77 \t All success : 120 \t mean(l0) : 7.008334 \t Current Learning Rate: 0.023\n",
            "Iteration 7800: Current success : 78 \t All success : 120 \t mean(l0) : 7.008334 \t Current Learning Rate: 0.022\n",
            "Iteration 7900: Current success : 77 \t All success : 120 \t mean(l0) : 7.008334 \t Current Learning Rate: 0.021\n",
            "Iteration 8000: Current success : 77 \t All success : 120 \t mean(l0) : 7.008334 \t Current Learning Rate: 0.020\n",
            "Iteration 8100: Current success : 75 \t All success : 120 \t mean(l0) : 7.008334 \t Current Learning Rate: 0.019\n",
            "Iteration 8200: Current success : 75 \t All success : 120 \t mean(l0) : 7.008334 \t Current Learning Rate: 0.018\n",
            "Iteration 8300: Current success : 75 \t All success : 120 \t mean(l0) : 7.008334 \t Current Learning Rate: 0.017\n",
            "Iteration 8400: Current success : 78 \t All success : 120 \t mean(l0) : 7.008334 \t Current Learning Rate: 0.016\n",
            "Iteration 8500: Current success : 76 \t All success : 120 \t mean(l0) : 7.008334 \t Current Learning Rate: 0.015\n",
            "Iteration 8600: Current success : 78 \t All success : 120 \t mean(l0) : 7.008334 \t Current Learning Rate: 0.014\n",
            "Iteration 8700: Current success : 78 \t All success : 120 \t mean(l0) : 7.008334 \t Current Learning Rate: 0.013\n",
            "Iteration 8800: Current success : 79 \t All success : 120 \t mean(l0) : 7.008334 \t Current Learning Rate: 0.012\n",
            "Iteration 8900: Current success : 78 \t All success : 120 \t mean(l0) : 7.008334 \t Current Learning Rate: 0.011\n",
            "Iteration 9000: Current success : 78 \t All success : 120 \t mean(l0) : 7.008334 \t Current Learning Rate: 0.010\n",
            "Iteration 9100: Current success : 78 \t All success : 120 \t mean(l0) : 7.008334 \t Current Learning Rate: 0.009\n",
            "Iteration 9200: Current success : 78 \t All success : 120 \t mean(l0) : 7.008334 \t Current Learning Rate: 0.008\n",
            "Iteration 9300: Current success : 78 \t All success : 120 \t mean(l0) : 7.008334 \t Current Learning Rate: 0.007\n",
            "Iteration 9400: Current success : 78 \t All success : 120 \t mean(l0) : 7.008334 \t Current Learning Rate: 0.006\n",
            "Iteration 9500: Current success : 78 \t All success : 120 \t mean(l0) : 7.008334 \t Current Learning Rate: 0.005\n",
            "Iteration 9600: Current success : 78 \t All success : 120 \t mean(l0) : 7.008334 \t Current Learning Rate: 0.004\n",
            "Iteration 9700: Current success : 78 \t All success : 120 \t mean(l0) : 7.008334 \t Current Learning Rate: 0.003\n",
            "Iteration 9800: Current success : 78 \t All success : 120 \t mean(l0) : 7.008334 \t Current Learning Rate: 0.002\n",
            "Iteration 9900: Current success : 78 \t All success : 120 \t mean(l0) : 7.008334 \t Current Learning Rate: 0.001\n",
            "Iteration 10000: Current success : 78 \t All success : 120 \t mean(l0) : 7.008334 \t Current Learning Rate: 0.000\n",
            "Iteration 10100: Current success : 73 \t All success : 120 \t mean(l0) : 6.975000 \t Current Learning Rate: 0.025\n",
            "Iteration 10200: Current success : 77 \t All success : 120 \t mean(l0) : 6.966667 \t Current Learning Rate: 0.024\n",
            "Iteration 10300: Current success : 81 \t All success : 120 \t mean(l0) : 6.966667 \t Current Learning Rate: 0.024\n",
            "Iteration 10400: Current success : 79 \t All success : 120 \t mean(l0) : 6.966667 \t Current Learning Rate: 0.023\n",
            "Iteration 10500: Current success : 79 \t All success : 120 \t mean(l0) : 6.966667 \t Current Learning Rate: 0.023\n",
            "Iteration 10600: Current success : 77 \t All success : 120 \t mean(l0) : 6.966667 \t Current Learning Rate: 0.022\n",
            "Iteration 10700: Current success : 79 \t All success : 120 \t mean(l0) : 6.966667 \t Current Learning Rate: 0.022\n",
            "Iteration 10800: Current success : 79 \t All success : 120 \t mean(l0) : 6.966667 \t Current Learning Rate: 0.021\n",
            "Iteration 10900: Current success : 79 \t All success : 120 \t mean(l0) : 6.966667 \t Current Learning Rate: 0.021\n",
            "Iteration 11000: Current success : 79 \t All success : 120 \t mean(l0) : 6.966667 \t Current Learning Rate: 0.020\n",
            "Iteration 11100: Current success : 79 \t All success : 120 \t mean(l0) : 6.966667 \t Current Learning Rate: 0.020\n",
            "Iteration 11200: Current success : 77 \t All success : 120 \t mean(l0) : 6.966667 \t Current Learning Rate: 0.019\n",
            "Iteration 11300: Current success : 77 \t All success : 120 \t mean(l0) : 6.966667 \t Current Learning Rate: 0.019\n",
            "Iteration 11400: Current success : 79 \t All success : 120 \t mean(l0) : 6.966667 \t Current Learning Rate: 0.018\n",
            "Iteration 11500: Current success : 79 \t All success : 120 \t mean(l0) : 6.966667 \t Current Learning Rate: 0.018\n",
            "Iteration 11600: Current success : 79 \t All success : 120 \t mean(l0) : 6.966667 \t Current Learning Rate: 0.017\n",
            "Iteration 11700: Current success : 79 \t All success : 120 \t mean(l0) : 6.966667 \t Current Learning Rate: 0.017\n",
            "Iteration 11800: Current success : 77 \t All success : 120 \t mean(l0) : 6.966667 \t Current Learning Rate: 0.016\n",
            "Iteration 11900: Current success : 79 \t All success : 120 \t mean(l0) : 6.966667 \t Current Learning Rate: 0.016\n",
            "Iteration 12000: Current success : 79 \t All success : 120 \t mean(l0) : 6.966667 \t Current Learning Rate: 0.015\n",
            "Iteration 12100: Current success : 79 \t All success : 120 \t mean(l0) : 6.966667 \t Current Learning Rate: 0.015\n",
            "Iteration 12200: Current success : 79 \t All success : 120 \t mean(l0) : 6.966667 \t Current Learning Rate: 0.014\n",
            "Iteration 12300: Current success : 79 \t All success : 120 \t mean(l0) : 6.966667 \t Current Learning Rate: 0.014\n",
            "Iteration 12400: Current success : 79 \t All success : 120 \t mean(l0) : 6.966667 \t Current Learning Rate: 0.013\n",
            "Iteration 12500: Current success : 79 \t All success : 120 \t mean(l0) : 6.966667 \t Current Learning Rate: 0.013\n",
            "Iteration 12600: Current success : 77 \t All success : 120 \t mean(l0) : 6.966667 \t Current Learning Rate: 0.012\n",
            "Iteration 12700: Current success : 79 \t All success : 120 \t mean(l0) : 6.966667 \t Current Learning Rate: 0.012\n",
            "Iteration 12800: Current success : 79 \t All success : 120 \t mean(l0) : 6.966667 \t Current Learning Rate: 0.011\n",
            "Iteration 12900: Current success : 79 \t All success : 120 \t mean(l0) : 6.966667 \t Current Learning Rate: 0.011\n",
            "Iteration 13000: Current success : 79 \t All success : 120 \t mean(l0) : 6.966667 \t Current Learning Rate: 0.010\n",
            "Iteration 13100: Current success : 79 \t All success : 120 \t mean(l0) : 6.966667 \t Current Learning Rate: 0.010\n",
            "Iteration 13200: Current success : 79 \t All success : 120 \t mean(l0) : 6.966667 \t Current Learning Rate: 0.009\n",
            "Iteration 13300: Current success : 79 \t All success : 120 \t mean(l0) : 6.966667 \t Current Learning Rate: 0.009\n",
            "Iteration 13400: Current success : 79 \t All success : 120 \t mean(l0) : 6.966667 \t Current Learning Rate: 0.008\n",
            "Iteration 13500: Current success : 79 \t All success : 120 \t mean(l0) : 6.966667 \t Current Learning Rate: 0.008\n",
            "Iteration 13600: Current success : 79 \t All success : 120 \t mean(l0) : 6.966667 \t Current Learning Rate: 0.007\n",
            "Iteration 13700: Current success : 79 \t All success : 120 \t mean(l0) : 6.966667 \t Current Learning Rate: 0.007\n",
            "Iteration 13800: Current success : 79 \t All success : 120 \t mean(l0) : 6.966667 \t Current Learning Rate: 0.006\n",
            "Iteration 13900: Current success : 80 \t All success : 120 \t mean(l0) : 6.966667 \t Current Learning Rate: 0.006\n",
            "Iteration 14000: Current success : 80 \t All success : 120 \t mean(l0) : 6.966667 \t Current Learning Rate: 0.005\n",
            "Iteration 14100: Current success : 80 \t All success : 120 \t mean(l0) : 6.966667 \t Current Learning Rate: 0.005\n",
            "Iteration 14200: Current success : 80 \t All success : 120 \t mean(l0) : 6.966667 \t Current Learning Rate: 0.004\n",
            "Iteration 14300: Current success : 80 \t All success : 120 \t mean(l0) : 6.966667 \t Current Learning Rate: 0.004\n",
            "Iteration 14400: Current success : 80 \t All success : 120 \t mean(l0) : 6.966667 \t Current Learning Rate: 0.003\n",
            "Iteration 14500: Current success : 80 \t All success : 120 \t mean(l0) : 6.966667 \t Current Learning Rate: 0.003\n",
            "Iteration 14600: Current success : 80 \t All success : 120 \t mean(l0) : 6.966667 \t Current Learning Rate: 0.002\n",
            "Iteration 14700: Current success : 80 \t All success : 120 \t mean(l0) : 6.966667 \t Current Learning Rate: 0.002\n",
            "Iteration 14800: Current success : 80 \t All success : 120 \t mean(l0) : 6.966667 \t Current Learning Rate: 0.001\n",
            "Iteration 14900: Current success : 80 \t All success : 120 \t mean(l0) : 6.966667 \t Current Learning Rate: 0.001\n",
            "Iteration 15000: Current success : 80 \t All success : 120 \t mean(l0) : 6.966667 \t Current Learning Rate: 0.000\n",
            "Iteration 15100: Current success : 75 \t All success : 120 \t mean(l0) : 6.958333 \t Current Learning Rate: 0.012\n",
            "Iteration 15200: Current success : 77 \t All success : 120 \t mean(l0) : 6.958333 \t Current Learning Rate: 0.012\n",
            "Iteration 15300: Current success : 77 \t All success : 120 \t mean(l0) : 6.958333 \t Current Learning Rate: 0.012\n",
            "Iteration 15400: Current success : 75 \t All success : 120 \t mean(l0) : 6.958333 \t Current Learning Rate: 0.012\n",
            "Iteration 15500: Current success : 74 \t All success : 120 \t mean(l0) : 6.958333 \t Current Learning Rate: 0.011\n",
            "Iteration 15600: Current success : 77 \t All success : 120 \t mean(l0) : 6.958333 \t Current Learning Rate: 0.011\n",
            "Iteration 15700: Current success : 75 \t All success : 120 \t mean(l0) : 6.958333 \t Current Learning Rate: 0.011\n",
            "Iteration 15800: Current success : 77 \t All success : 120 \t mean(l0) : 6.958333 \t Current Learning Rate: 0.011\n",
            "Iteration 15900: Current success : 76 \t All success : 120 \t mean(l0) : 6.958333 \t Current Learning Rate: 0.010\n",
            "Iteration 16000: Current success : 76 \t All success : 120 \t mean(l0) : 6.958333 \t Current Learning Rate: 0.010\n",
            "Iteration 16100: Current success : 76 \t All success : 120 \t mean(l0) : 6.958333 \t Current Learning Rate: 0.010\n",
            "Iteration 16200: Current success : 76 \t All success : 120 \t mean(l0) : 6.958333 \t Current Learning Rate: 0.010\n",
            "Iteration 16300: Current success : 75 \t All success : 120 \t mean(l0) : 6.958333 \t Current Learning Rate: 0.009\n",
            "Iteration 16400: Current success : 76 \t All success : 120 \t mean(l0) : 6.958333 \t Current Learning Rate: 0.009\n",
            "Iteration 16500: Current success : 75 \t All success : 120 \t mean(l0) : 6.958333 \t Current Learning Rate: 0.009\n",
            "Iteration 16600: Current success : 77 \t All success : 120 \t mean(l0) : 6.958333 \t Current Learning Rate: 0.009\n",
            "Iteration 16700: Current success : 76 \t All success : 120 \t mean(l0) : 6.958333 \t Current Learning Rate: 0.008\n",
            "Iteration 16800: Current success : 77 \t All success : 120 \t mean(l0) : 6.958333 \t Current Learning Rate: 0.008\n",
            "Iteration 16900: Current success : 77 \t All success : 120 \t mean(l0) : 6.958333 \t Current Learning Rate: 0.008\n",
            "Iteration 17000: Current success : 76 \t All success : 120 \t mean(l0) : 6.958333 \t Current Learning Rate: 0.008\n",
            "Iteration 17100: Current success : 75 \t All success : 120 \t mean(l0) : 6.958333 \t Current Learning Rate: 0.007\n",
            "Iteration 17200: Current success : 75 \t All success : 120 \t mean(l0) : 6.958333 \t Current Learning Rate: 0.007\n",
            "Iteration 17300: Current success : 76 \t All success : 120 \t mean(l0) : 6.958333 \t Current Learning Rate: 0.007\n",
            "Iteration 17400: Current success : 78 \t All success : 120 \t mean(l0) : 6.958333 \t Current Learning Rate: 0.007\n",
            "Iteration 17500: Current success : 76 \t All success : 120 \t mean(l0) : 6.958333 \t Current Learning Rate: 0.006\n",
            "Iteration 17600: Current success : 75 \t All success : 120 \t mean(l0) : 6.958333 \t Current Learning Rate: 0.006\n",
            "Iteration 17700: Current success : 75 \t All success : 120 \t mean(l0) : 6.958333 \t Current Learning Rate: 0.006\n",
            "Iteration 17800: Current success : 76 \t All success : 120 \t mean(l0) : 6.958333 \t Current Learning Rate: 0.006\n",
            "Iteration 17900: Current success : 74 \t All success : 120 \t mean(l0) : 6.958333 \t Current Learning Rate: 0.005\n",
            "Iteration 18000: Current success : 76 \t All success : 120 \t mean(l0) : 6.958333 \t Current Learning Rate: 0.005\n",
            "Iteration 18100: Current success : 76 \t All success : 120 \t mean(l0) : 6.958333 \t Current Learning Rate: 0.005\n",
            "Iteration 18200: Current success : 75 \t All success : 120 \t mean(l0) : 6.958333 \t Current Learning Rate: 0.005\n",
            "Iteration 18300: Current success : 76 \t All success : 120 \t mean(l0) : 6.958333 \t Current Learning Rate: 0.004\n",
            "Iteration 18400: Current success : 76 \t All success : 120 \t mean(l0) : 6.958333 \t Current Learning Rate: 0.004\n",
            "Iteration 18500: Current success : 74 \t All success : 120 \t mean(l0) : 6.958333 \t Current Learning Rate: 0.004\n",
            "Iteration 18600: Current success : 76 \t All success : 120 \t mean(l0) : 6.958333 \t Current Learning Rate: 0.004\n",
            "Iteration 18700: Current success : 76 \t All success : 120 \t mean(l0) : 6.958333 \t Current Learning Rate: 0.003\n",
            "Iteration 18800: Current success : 76 \t All success : 120 \t mean(l0) : 6.958333 \t Current Learning Rate: 0.003\n",
            "Iteration 18900: Current success : 76 \t All success : 120 \t mean(l0) : 6.958333 \t Current Learning Rate: 0.003\n",
            "Iteration 19000: Current success : 76 \t All success : 120 \t mean(l0) : 6.958333 \t Current Learning Rate: 0.003\n",
            "Iteration 19100: Current success : 76 \t All success : 120 \t mean(l0) : 6.958333 \t Current Learning Rate: 0.002\n",
            "Iteration 19200: Current success : 76 \t All success : 120 \t mean(l0) : 6.958333 \t Current Learning Rate: 0.002\n",
            "Iteration 19300: Current success : 76 \t All success : 120 \t mean(l0) : 6.958333 \t Current Learning Rate: 0.002\n",
            "Iteration 19400: Current success : 76 \t All success : 120 \t mean(l0) : 6.958333 \t Current Learning Rate: 0.002\n",
            "Iteration 19500: Current success : 76 \t All success : 120 \t mean(l0) : 6.958333 \t Current Learning Rate: 0.001\n",
            "Iteration 19600: Current success : 76 \t All success : 120 \t mean(l0) : 6.958333 \t Current Learning Rate: 0.001\n",
            "Iteration 19700: Current success : 76 \t All success : 120 \t mean(l0) : 6.958333 \t Current Learning Rate: 0.001\n",
            "Iteration 19800: Current success : 76 \t All success : 120 \t mean(l0) : 6.958333 \t Current Learning Rate: 0.001\n",
            "Iteration 19900: Current success : 76 \t All success : 120 \t mean(l0) : 6.958333 \t Current Learning Rate: 0.000\n",
            "Iteration 20000: Current success : 76 \t All success : 120 \t mean(l0) : 6.958333 \t Current Learning Rate: 0.000\n",
            "outer_step 1: all success : 120 \t mean(l0)(success) : 6.9583\n",
            "-------------------------------------------------------------\n",
            "tensor([ 0.5000, 10.0000, 10.0000,  ..., 10.0000, 10.0000, 10.0000],\n",
            "       device='cuda:0')\n",
            "Iteration 100: Current success : 76 \t All success : 109 \t mean(l0) : 8.137614 \t Current Learning Rate: 0.098\n",
            "Iteration 200: Current success : 73 \t All success : 120 \t mean(l0) : 7.991667 \t Current Learning Rate: 0.096\n",
            "Iteration 300: Current success : 77 \t All success : 160 \t mean(l0) : 10.656250 \t Current Learning Rate: 0.094\n",
            "Iteration 400: Current success : 90 \t All success : 162 \t mean(l0) : 10.407408 \t Current Learning Rate: 0.092\n",
            "Iteration 500: Current success : 115 \t All success : 162 \t mean(l0) : 9.141975 \t Current Learning Rate: 0.090\n",
            "Iteration 600: Current success : 113 \t All success : 163 \t mean(l0) : 9.116564 \t Current Learning Rate: 0.088\n",
            "Iteration 700: Current success : 120 \t All success : 167 \t mean(l0) : 9.053892 \t Current Learning Rate: 0.086\n",
            "Iteration 800: Current success : 114 \t All success : 168 \t mean(l0) : 9.285714 \t Current Learning Rate: 0.084\n",
            "Iteration 900: Current success : 122 \t All success : 169 \t mean(l0) : 9.461538 \t Current Learning Rate: 0.082\n",
            "Iteration 1000: Current success : 119 \t All success : 170 \t mean(l0) : 9.629412 \t Current Learning Rate: 0.080\n",
            "Iteration 1100: Current success : 125 \t All success : 170 \t mean(l0) : 9.576470 \t Current Learning Rate: 0.078\n",
            "Iteration 1200: Current success : 123 \t All success : 174 \t mean(l0) : 10.252873 \t Current Learning Rate: 0.076\n",
            "Iteration 1300: Current success : 116 \t All success : 174 \t mean(l0) : 10.137931 \t Current Learning Rate: 0.074\n",
            "Iteration 1400: Current success : 119 \t All success : 174 \t mean(l0) : 10.097701 \t Current Learning Rate: 0.072\n",
            "Iteration 1500: Current success : 118 \t All success : 174 \t mean(l0) : 10.086206 \t Current Learning Rate: 0.070\n",
            "Iteration 1600: Current success : 127 \t All success : 174 \t mean(l0) : 10.086206 \t Current Learning Rate: 0.068\n",
            "Iteration 1700: Current success : 123 \t All success : 174 \t mean(l0) : 10.086206 \t Current Learning Rate: 0.066\n",
            "Iteration 1800: Current success : 125 \t All success : 174 \t mean(l0) : 10.086206 \t Current Learning Rate: 0.064\n",
            "Iteration 1900: Current success : 126 \t All success : 174 \t mean(l0) : 10.086206 \t Current Learning Rate: 0.062\n",
            "Iteration 2000: Current success : 121 \t All success : 174 \t mean(l0) : 10.086206 \t Current Learning Rate: 0.060\n",
            "Iteration 2100: Current success : 116 \t All success : 174 \t mean(l0) : 10.074713 \t Current Learning Rate: 0.058\n",
            "Iteration 2200: Current success : 124 \t All success : 175 \t mean(l0) : 10.097143 \t Current Learning Rate: 0.056\n",
            "Iteration 2300: Current success : 122 \t All success : 175 \t mean(l0) : 10.085714 \t Current Learning Rate: 0.054\n",
            "Iteration 2400: Current success : 129 \t All success : 175 \t mean(l0) : 10.085714 \t Current Learning Rate: 0.052\n",
            "Iteration 2500: Current success : 120 \t All success : 175 \t mean(l0) : 10.085714 \t Current Learning Rate: 0.050\n",
            "Iteration 2600: Current success : 130 \t All success : 175 \t mean(l0) : 10.085714 \t Current Learning Rate: 0.048\n",
            "Iteration 2700: Current success : 126 \t All success : 175 \t mean(l0) : 10.085714 \t Current Learning Rate: 0.046\n",
            "Iteration 2800: Current success : 120 \t All success : 175 \t mean(l0) : 10.085714 \t Current Learning Rate: 0.044\n",
            "Iteration 2900: Current success : 127 \t All success : 175 \t mean(l0) : 10.085714 \t Current Learning Rate: 0.042\n",
            "Iteration 3000: Current success : 120 \t All success : 175 \t mean(l0) : 10.085714 \t Current Learning Rate: 0.040\n",
            "Iteration 3100: Current success : 127 \t All success : 175 \t mean(l0) : 10.085714 \t Current Learning Rate: 0.038\n",
            "Iteration 3200: Current success : 126 \t All success : 175 \t mean(l0) : 10.085714 \t Current Learning Rate: 0.036\n",
            "Iteration 3300: Current success : 126 \t All success : 175 \t mean(l0) : 10.085714 \t Current Learning Rate: 0.034\n",
            "Iteration 3400: Current success : 130 \t All success : 175 \t mean(l0) : 10.085714 \t Current Learning Rate: 0.032\n",
            "Iteration 3500: Current success : 126 \t All success : 175 \t mean(l0) : 10.085714 \t Current Learning Rate: 0.030\n",
            "Iteration 3600: Current success : 128 \t All success : 175 \t mean(l0) : 10.062857 \t Current Learning Rate: 0.028\n",
            "Iteration 3700: Current success : 129 \t All success : 175 \t mean(l0) : 10.062857 \t Current Learning Rate: 0.026\n",
            "Iteration 3800: Current success : 126 \t All success : 175 \t mean(l0) : 10.062857 \t Current Learning Rate: 0.024\n",
            "Iteration 3900: Current success : 132 \t All success : 175 \t mean(l0) : 10.062857 \t Current Learning Rate: 0.022\n",
            "Iteration 4000: Current success : 126 \t All success : 175 \t mean(l0) : 10.062857 \t Current Learning Rate: 0.020\n",
            "Iteration 4100: Current success : 131 \t All success : 175 \t mean(l0) : 10.062857 \t Current Learning Rate: 0.018\n",
            "Iteration 4200: Current success : 133 \t All success : 175 \t mean(l0) : 10.062857 \t Current Learning Rate: 0.016\n",
            "Iteration 4300: Current success : 132 \t All success : 175 \t mean(l0) : 10.062857 \t Current Learning Rate: 0.014\n",
            "Iteration 4400: Current success : 127 \t All success : 175 \t mean(l0) : 10.062857 \t Current Learning Rate: 0.012\n",
            "Iteration 4500: Current success : 129 \t All success : 175 \t mean(l0) : 10.062857 \t Current Learning Rate: 0.010\n",
            "Iteration 4600: Current success : 133 \t All success : 175 \t mean(l0) : 10.062857 \t Current Learning Rate: 0.008\n",
            "Iteration 4700: Current success : 133 \t All success : 175 \t mean(l0) : 10.062857 \t Current Learning Rate: 0.006\n",
            "Iteration 4800: Current success : 133 \t All success : 175 \t mean(l0) : 10.062857 \t Current Learning Rate: 0.004\n",
            "Iteration 4900: Current success : 132 \t All success : 175 \t mean(l0) : 10.062857 \t Current Learning Rate: 0.002\n",
            "Iteration 5000: Current success : 133 \t All success : 175 \t mean(l0) : 10.062857 \t Current Learning Rate: 0.000\n",
            "Iteration 5100: Current success : 125 \t All success : 183 \t mean(l0) : 10.786885 \t Current Learning Rate: 0.049\n",
            "Iteration 5200: Current success : 136 \t All success : 186 \t mean(l0) : 11.096774 \t Current Learning Rate: 0.048\n",
            "Iteration 5300: Current success : 129 \t All success : 187 \t mean(l0) : 11.016043 \t Current Learning Rate: 0.047\n",
            "Iteration 5400: Current success : 124 \t All success : 189 \t mean(l0) : 11.349206 \t Current Learning Rate: 0.046\n",
            "Iteration 5500: Current success : 130 \t All success : 189 \t mean(l0) : 11.328042 \t Current Learning Rate: 0.045\n",
            "Iteration 5600: Current success : 126 \t All success : 189 \t mean(l0) : 11.227513 \t Current Learning Rate: 0.044\n",
            "Iteration 5700: Current success : 123 \t All success : 189 \t mean(l0) : 11.222222 \t Current Learning Rate: 0.043\n",
            "Iteration 5800: Current success : 125 \t All success : 189 \t mean(l0) : 11.222222 \t Current Learning Rate: 0.042\n",
            "Iteration 5900: Current success : 130 \t All success : 189 \t mean(l0) : 11.222222 \t Current Learning Rate: 0.041\n",
            "Iteration 6000: Current success : 125 \t All success : 189 \t mean(l0) : 11.222222 \t Current Learning Rate: 0.040\n",
            "Iteration 6100: Current success : 128 \t All success : 189 \t mean(l0) : 11.222222 \t Current Learning Rate: 0.039\n",
            "Iteration 6200: Current success : 127 \t All success : 189 \t mean(l0) : 11.206348 \t Current Learning Rate: 0.038\n",
            "Iteration 6300: Current success : 130 \t All success : 189 \t mean(l0) : 11.206348 \t Current Learning Rate: 0.037\n",
            "Iteration 6400: Current success : 131 \t All success : 189 \t mean(l0) : 11.206348 \t Current Learning Rate: 0.036\n",
            "Iteration 6500: Current success : 129 \t All success : 189 \t mean(l0) : 11.206348 \t Current Learning Rate: 0.035\n",
            "Iteration 6600: Current success : 129 \t All success : 189 \t mean(l0) : 11.185184 \t Current Learning Rate: 0.034\n",
            "Iteration 6700: Current success : 132 \t All success : 189 \t mean(l0) : 11.185184 \t Current Learning Rate: 0.033\n",
            "Iteration 6800: Current success : 129 \t All success : 189 \t mean(l0) : 11.179893 \t Current Learning Rate: 0.032\n",
            "Iteration 6900: Current success : 126 \t All success : 189 \t mean(l0) : 11.179893 \t Current Learning Rate: 0.031\n",
            "Iteration 7000: Current success : 129 \t All success : 189 \t mean(l0) : 11.179893 \t Current Learning Rate: 0.030\n",
            "Iteration 7100: Current success : 130 \t All success : 189 \t mean(l0) : 11.179893 \t Current Learning Rate: 0.029\n",
            "Iteration 7200: Current success : 129 \t All success : 189 \t mean(l0) : 11.174603 \t Current Learning Rate: 0.028\n",
            "Iteration 7300: Current success : 129 \t All success : 189 \t mean(l0) : 11.174603 \t Current Learning Rate: 0.027\n",
            "Iteration 7400: Current success : 132 \t All success : 189 \t mean(l0) : 11.174603 \t Current Learning Rate: 0.026\n",
            "Iteration 7500: Current success : 129 \t All success : 189 \t mean(l0) : 11.174603 \t Current Learning Rate: 0.025\n",
            "Iteration 7600: Current success : 130 \t All success : 189 \t mean(l0) : 11.174603 \t Current Learning Rate: 0.024\n",
            "Iteration 7700: Current success : 130 \t All success : 189 \t mean(l0) : 11.174603 \t Current Learning Rate: 0.023\n",
            "Iteration 7800: Current success : 130 \t All success : 189 \t mean(l0) : 11.174603 \t Current Learning Rate: 0.022\n",
            "Iteration 7900: Current success : 126 \t All success : 189 \t mean(l0) : 11.174603 \t Current Learning Rate: 0.021\n",
            "Iteration 8000: Current success : 130 \t All success : 189 \t mean(l0) : 11.174603 \t Current Learning Rate: 0.020\n",
            "Iteration 8100: Current success : 126 \t All success : 189 \t mean(l0) : 11.174603 \t Current Learning Rate: 0.019\n",
            "Iteration 8200: Current success : 129 \t All success : 189 \t mean(l0) : 11.174603 \t Current Learning Rate: 0.018\n",
            "Iteration 8300: Current success : 125 \t All success : 189 \t mean(l0) : 11.174603 \t Current Learning Rate: 0.017\n",
            "Iteration 8400: Current success : 130 \t All success : 189 \t mean(l0) : 11.174603 \t Current Learning Rate: 0.016\n",
            "Iteration 8500: Current success : 129 \t All success : 189 \t mean(l0) : 11.174603 \t Current Learning Rate: 0.015\n",
            "Iteration 8600: Current success : 130 \t All success : 189 \t mean(l0) : 11.174603 \t Current Learning Rate: 0.014\n",
            "Iteration 8700: Current success : 131 \t All success : 189 \t mean(l0) : 11.174603 \t Current Learning Rate: 0.013\n",
            "Iteration 8800: Current success : 129 \t All success : 189 \t mean(l0) : 11.174603 \t Current Learning Rate: 0.012\n",
            "Iteration 8900: Current success : 130 \t All success : 189 \t mean(l0) : 11.174603 \t Current Learning Rate: 0.011\n",
            "Iteration 9000: Current success : 130 \t All success : 189 \t mean(l0) : 11.174603 \t Current Learning Rate: 0.010\n",
            "Iteration 9100: Current success : 130 \t All success : 189 \t mean(l0) : 11.174603 \t Current Learning Rate: 0.009\n",
            "Iteration 9200: Current success : 130 \t All success : 189 \t mean(l0) : 11.174603 \t Current Learning Rate: 0.008\n",
            "Iteration 9300: Current success : 129 \t All success : 189 \t mean(l0) : 11.174603 \t Current Learning Rate: 0.007\n",
            "Iteration 9400: Current success : 130 \t All success : 189 \t mean(l0) : 11.174603 \t Current Learning Rate: 0.006\n",
            "Iteration 9500: Current success : 130 \t All success : 189 \t mean(l0) : 11.174603 \t Current Learning Rate: 0.005\n",
            "Iteration 9600: Current success : 129 \t All success : 189 \t mean(l0) : 11.174603 \t Current Learning Rate: 0.004\n",
            "Iteration 9700: Current success : 130 \t All success : 189 \t mean(l0) : 11.174603 \t Current Learning Rate: 0.003\n",
            "Iteration 9800: Current success : 130 \t All success : 189 \t mean(l0) : 11.174603 \t Current Learning Rate: 0.002\n",
            "Iteration 9900: Current success : 130 \t All success : 189 \t mean(l0) : 11.174603 \t Current Learning Rate: 0.001\n",
            "Iteration 10000: Current success : 130 \t All success : 189 \t mean(l0) : 11.174603 \t Current Learning Rate: 0.000\n",
            "Iteration 10100: Current success : 92 \t All success : 189 \t mean(l0) : 11.153439 \t Current Learning Rate: 0.025\n",
            "Iteration 10200: Current success : 95 \t All success : 189 \t mean(l0) : 11.137566 \t Current Learning Rate: 0.024\n",
            "Iteration 10300: Current success : 93 \t All success : 189 \t mean(l0) : 11.137566 \t Current Learning Rate: 0.024\n",
            "Iteration 10400: Current success : 97 \t All success : 189 \t mean(l0) : 11.137566 \t Current Learning Rate: 0.023\n",
            "Iteration 10500: Current success : 95 \t All success : 189 \t mean(l0) : 11.137566 \t Current Learning Rate: 0.023\n",
            "Iteration 10600: Current success : 94 \t All success : 189 \t mean(l0) : 11.137566 \t Current Learning Rate: 0.022\n",
            "Iteration 10700: Current success : 92 \t All success : 189 \t mean(l0) : 11.137566 \t Current Learning Rate: 0.022\n",
            "Iteration 10800: Current success : 94 \t All success : 189 \t mean(l0) : 11.137566 \t Current Learning Rate: 0.021\n",
            "Iteration 10900: Current success : 93 \t All success : 189 \t mean(l0) : 11.137566 \t Current Learning Rate: 0.021\n",
            "Iteration 11000: Current success : 97 \t All success : 189 \t mean(l0) : 11.137566 \t Current Learning Rate: 0.020\n",
            "Iteration 11100: Current success : 96 \t All success : 189 \t mean(l0) : 11.137566 \t Current Learning Rate: 0.020\n",
            "Iteration 11200: Current success : 97 \t All success : 189 \t mean(l0) : 11.132275 \t Current Learning Rate: 0.019\n",
            "Iteration 11300: Current success : 96 \t All success : 189 \t mean(l0) : 11.132275 \t Current Learning Rate: 0.019\n",
            "Iteration 11400: Current success : 93 \t All success : 189 \t mean(l0) : 11.132275 \t Current Learning Rate: 0.018\n",
            "Iteration 11500: Current success : 96 \t All success : 189 \t mean(l0) : 11.132275 \t Current Learning Rate: 0.018\n",
            "Iteration 11600: Current success : 95 \t All success : 189 \t mean(l0) : 11.132275 \t Current Learning Rate: 0.017\n",
            "Iteration 11700: Current success : 96 \t All success : 189 \t mean(l0) : 11.132275 \t Current Learning Rate: 0.017\n",
            "Iteration 11800: Current success : 96 \t All success : 189 \t mean(l0) : 11.132275 \t Current Learning Rate: 0.016\n",
            "Iteration 11900: Current success : 93 \t All success : 189 \t mean(l0) : 11.132275 \t Current Learning Rate: 0.016\n",
            "Iteration 12000: Current success : 96 \t All success : 189 \t mean(l0) : 11.132275 \t Current Learning Rate: 0.015\n",
            "Iteration 12100: Current success : 96 \t All success : 189 \t mean(l0) : 11.132275 \t Current Learning Rate: 0.015\n",
            "Iteration 12200: Current success : 96 \t All success : 189 \t mean(l0) : 11.132275 \t Current Learning Rate: 0.014\n",
            "Iteration 12300: Current success : 95 \t All success : 189 \t mean(l0) : 11.132275 \t Current Learning Rate: 0.014\n",
            "Iteration 12400: Current success : 96 \t All success : 189 \t mean(l0) : 11.132275 \t Current Learning Rate: 0.013\n",
            "Iteration 12500: Current success : 96 \t All success : 189 \t mean(l0) : 11.132275 \t Current Learning Rate: 0.013\n",
            "Iteration 12600: Current success : 96 \t All success : 189 \t mean(l0) : 11.132275 \t Current Learning Rate: 0.012\n",
            "Iteration 12700: Current success : 96 \t All success : 189 \t mean(l0) : 11.132275 \t Current Learning Rate: 0.012\n",
            "Iteration 12800: Current success : 96 \t All success : 189 \t mean(l0) : 11.132275 \t Current Learning Rate: 0.011\n",
            "Iteration 12900: Current success : 96 \t All success : 189 \t mean(l0) : 11.132275 \t Current Learning Rate: 0.011\n",
            "Iteration 13000: Current success : 96 \t All success : 189 \t mean(l0) : 11.132275 \t Current Learning Rate: 0.010\n",
            "Iteration 13100: Current success : 96 \t All success : 189 \t mean(l0) : 11.132275 \t Current Learning Rate: 0.010\n",
            "Iteration 13200: Current success : 96 \t All success : 189 \t mean(l0) : 11.132275 \t Current Learning Rate: 0.009\n",
            "Iteration 13300: Current success : 96 \t All success : 189 \t mean(l0) : 11.132275 \t Current Learning Rate: 0.009\n",
            "Iteration 13400: Current success : 95 \t All success : 189 \t mean(l0) : 11.132275 \t Current Learning Rate: 0.008\n",
            "Iteration 13500: Current success : 96 \t All success : 189 \t mean(l0) : 11.132275 \t Current Learning Rate: 0.008\n",
            "Iteration 13600: Current success : 96 \t All success : 189 \t mean(l0) : 11.132275 \t Current Learning Rate: 0.007\n",
            "Iteration 13700: Current success : 97 \t All success : 189 \t mean(l0) : 11.132275 \t Current Learning Rate: 0.007\n",
            "Iteration 13800: Current success : 96 \t All success : 189 \t mean(l0) : 11.132275 \t Current Learning Rate: 0.006\n",
            "Iteration 13900: Current success : 96 \t All success : 189 \t mean(l0) : 11.132275 \t Current Learning Rate: 0.006\n",
            "Iteration 14000: Current success : 96 \t All success : 189 \t mean(l0) : 11.132275 \t Current Learning Rate: 0.005\n",
            "Iteration 14100: Current success : 96 \t All success : 189 \t mean(l0) : 11.132275 \t Current Learning Rate: 0.005\n",
            "Iteration 14200: Current success : 97 \t All success : 189 \t mean(l0) : 11.132275 \t Current Learning Rate: 0.004\n",
            "Iteration 14300: Current success : 96 \t All success : 189 \t mean(l0) : 11.132275 \t Current Learning Rate: 0.004\n",
            "Iteration 14400: Current success : 96 \t All success : 189 \t mean(l0) : 11.132275 \t Current Learning Rate: 0.003\n",
            "Iteration 14500: Current success : 96 \t All success : 189 \t mean(l0) : 11.132275 \t Current Learning Rate: 0.003\n",
            "Iteration 14600: Current success : 96 \t All success : 189 \t mean(l0) : 11.132275 \t Current Learning Rate: 0.002\n",
            "Iteration 14700: Current success : 96 \t All success : 189 \t mean(l0) : 11.132275 \t Current Learning Rate: 0.002\n",
            "Iteration 14800: Current success : 96 \t All success : 189 \t mean(l0) : 11.132275 \t Current Learning Rate: 0.001\n",
            "Iteration 14900: Current success : 96 \t All success : 189 \t mean(l0) : 11.132275 \t Current Learning Rate: 0.001\n",
            "Iteration 15000: Current success : 96 \t All success : 189 \t mean(l0) : 11.132275 \t Current Learning Rate: 0.000\n",
            "Iteration 15100: Current success : 99 \t All success : 189 \t mean(l0) : 11.132275 \t Current Learning Rate: 0.012\n",
            "Iteration 15200: Current success : 101 \t All success : 189 \t mean(l0) : 11.132275 \t Current Learning Rate: 0.012\n",
            "Iteration 15300: Current success : 101 \t All success : 189 \t mean(l0) : 11.132275 \t Current Learning Rate: 0.012\n",
            "Iteration 15400: Current success : 101 \t All success : 189 \t mean(l0) : 11.132275 \t Current Learning Rate: 0.012\n",
            "Iteration 15500: Current success : 101 \t All success : 189 \t mean(l0) : 11.132275 \t Current Learning Rate: 0.011\n",
            "Iteration 15600: Current success : 101 \t All success : 189 \t mean(l0) : 11.132275 \t Current Learning Rate: 0.011\n",
            "Iteration 15700: Current success : 101 \t All success : 189 \t mean(l0) : 11.132275 \t Current Learning Rate: 0.011\n",
            "Iteration 15800: Current success : 101 \t All success : 189 \t mean(l0) : 11.132275 \t Current Learning Rate: 0.011\n",
            "Iteration 15900: Current success : 101 \t All success : 189 \t mean(l0) : 11.132275 \t Current Learning Rate: 0.010\n",
            "Iteration 16000: Current success : 101 \t All success : 189 \t mean(l0) : 11.132275 \t Current Learning Rate: 0.010\n",
            "Iteration 16100: Current success : 101 \t All success : 189 \t mean(l0) : 11.132275 \t Current Learning Rate: 0.010\n",
            "Iteration 16200: Current success : 101 \t All success : 189 \t mean(l0) : 11.132275 \t Current Learning Rate: 0.010\n",
            "Iteration 16300: Current success : 101 \t All success : 189 \t mean(l0) : 11.132275 \t Current Learning Rate: 0.009\n",
            "Iteration 16400: Current success : 101 \t All success : 189 \t mean(l0) : 11.132275 \t Current Learning Rate: 0.009\n",
            "Iteration 16500: Current success : 100 \t All success : 189 \t mean(l0) : 11.132275 \t Current Learning Rate: 0.009\n",
            "Iteration 16600: Current success : 101 \t All success : 189 \t mean(l0) : 11.132275 \t Current Learning Rate: 0.009\n",
            "Iteration 16700: Current success : 101 \t All success : 189 \t mean(l0) : 11.132275 \t Current Learning Rate: 0.008\n",
            "Iteration 16800: Current success : 101 \t All success : 189 \t mean(l0) : 11.132275 \t Current Learning Rate: 0.008\n",
            "Iteration 16900: Current success : 101 \t All success : 189 \t mean(l0) : 11.132275 \t Current Learning Rate: 0.008\n",
            "Iteration 17000: Current success : 101 \t All success : 189 \t mean(l0) : 11.132275 \t Current Learning Rate: 0.008\n",
            "Iteration 17100: Current success : 101 \t All success : 189 \t mean(l0) : 11.132275 \t Current Learning Rate: 0.007\n",
            "Iteration 17200: Current success : 101 \t All success : 189 \t mean(l0) : 11.132275 \t Current Learning Rate: 0.007\n",
            "Iteration 17300: Current success : 101 \t All success : 189 \t mean(l0) : 11.132275 \t Current Learning Rate: 0.007\n",
            "Iteration 17400: Current success : 101 \t All success : 189 \t mean(l0) : 11.132275 \t Current Learning Rate: 0.007\n",
            "Iteration 17500: Current success : 101 \t All success : 189 \t mean(l0) : 11.132275 \t Current Learning Rate: 0.006\n",
            "Iteration 17600: Current success : 101 \t All success : 189 \t mean(l0) : 11.132275 \t Current Learning Rate: 0.006\n",
            "Iteration 17700: Current success : 101 \t All success : 189 \t mean(l0) : 11.132275 \t Current Learning Rate: 0.006\n",
            "Iteration 17800: Current success : 101 \t All success : 189 \t mean(l0) : 11.132275 \t Current Learning Rate: 0.006\n",
            "Iteration 17900: Current success : 101 \t All success : 189 \t mean(l0) : 11.132275 \t Current Learning Rate: 0.005\n",
            "Iteration 18000: Current success : 101 \t All success : 189 \t mean(l0) : 11.132275 \t Current Learning Rate: 0.005\n",
            "Iteration 18100: Current success : 101 \t All success : 189 \t mean(l0) : 11.132275 \t Current Learning Rate: 0.005\n",
            "Iteration 18200: Current success : 101 \t All success : 189 \t mean(l0) : 11.132275 \t Current Learning Rate: 0.005\n",
            "Iteration 18300: Current success : 101 \t All success : 189 \t mean(l0) : 11.132275 \t Current Learning Rate: 0.004\n",
            "Iteration 18400: Current success : 100 \t All success : 189 \t mean(l0) : 11.132275 \t Current Learning Rate: 0.004\n",
            "Iteration 18500: Current success : 101 \t All success : 189 \t mean(l0) : 11.132275 \t Current Learning Rate: 0.004\n",
            "Iteration 18600: Current success : 101 \t All success : 189 \t mean(l0) : 11.132275 \t Current Learning Rate: 0.004\n",
            "Iteration 18700: Current success : 101 \t All success : 189 \t mean(l0) : 11.132275 \t Current Learning Rate: 0.003\n",
            "Iteration 18800: Current success : 101 \t All success : 189 \t mean(l0) : 11.132275 \t Current Learning Rate: 0.003\n",
            "Iteration 18900: Current success : 101 \t All success : 189 \t mean(l0) : 11.132275 \t Current Learning Rate: 0.003\n",
            "Iteration 19000: Current success : 101 \t All success : 189 \t mean(l0) : 11.132275 \t Current Learning Rate: 0.003\n",
            "Iteration 19100: Current success : 101 \t All success : 189 \t mean(l0) : 11.132275 \t Current Learning Rate: 0.002\n",
            "Iteration 19200: Current success : 101 \t All success : 189 \t mean(l0) : 11.132275 \t Current Learning Rate: 0.002\n",
            "Iteration 19300: Current success : 101 \t All success : 189 \t mean(l0) : 11.132275 \t Current Learning Rate: 0.002\n",
            "Iteration 19400: Current success : 101 \t All success : 189 \t mean(l0) : 11.132275 \t Current Learning Rate: 0.002\n",
            "Iteration 19500: Current success : 101 \t All success : 189 \t mean(l0) : 11.132275 \t Current Learning Rate: 0.001\n",
            "Iteration 19600: Current success : 101 \t All success : 189 \t mean(l0) : 11.132275 \t Current Learning Rate: 0.001\n",
            "Iteration 19700: Current success : 101 \t All success : 189 \t mean(l0) : 11.132275 \t Current Learning Rate: 0.001\n",
            "Iteration 19800: Current success : 101 \t All success : 189 \t mean(l0) : 11.132275 \t Current Learning Rate: 0.001\n",
            "Iteration 19900: Current success : 101 \t All success : 189 \t mean(l0) : 11.132275 \t Current Learning Rate: 0.000\n",
            "Iteration 20000: Current success : 101 \t All success : 189 \t mean(l0) : 11.132275 \t Current Learning Rate: 0.000\n",
            "outer_step 2: all success : 203 \t mean(l0)(success) : 11.9064\n",
            "-------------------------------------------------------------\n",
            "tensor([  0.2500, 100.0000, 100.0000,  ..., 100.0000, 100.0000, 100.0000],\n",
            "       device='cuda:0')\n",
            "Iteration 100: Current success : 71 \t All success : 88 \t mean(l0) : 6.613636 \t Current Learning Rate: 0.098\n",
            "Iteration 200: Current success : 65 \t All success : 106 \t mean(l0) : 8.811321 \t Current Learning Rate: 0.096\n",
            "Iteration 300: Current success : 112 \t All success : 145 \t mean(l0) : 10.089655 \t Current Learning Rate: 0.094\n",
            "Iteration 400: Current success : 108 \t All success : 150 \t mean(l0) : 10.433333 \t Current Learning Rate: 0.092\n",
            "Iteration 500: Current success : 119 \t All success : 152 \t mean(l0) : 10.802631 \t Current Learning Rate: 0.090\n",
            "Iteration 600: Current success : 119 \t All success : 157 \t mean(l0) : 11.267516 \t Current Learning Rate: 0.088\n",
            "Iteration 700: Current success : 117 \t All success : 157 \t mean(l0) : 11.235669 \t Current Learning Rate: 0.086\n",
            "Iteration 800: Current success : 109 \t All success : 157 \t mean(l0) : 11.229300 \t Current Learning Rate: 0.084\n",
            "Iteration 900: Current success : 113 \t All success : 158 \t mean(l0) : 11.170886 \t Current Learning Rate: 0.082\n",
            "Iteration 1000: Current success : 108 \t All success : 158 \t mean(l0) : 11.158228 \t Current Learning Rate: 0.080\n",
            "Iteration 1100: Current success : 110 \t All success : 158 \t mean(l0) : 11.145570 \t Current Learning Rate: 0.078\n",
            "Iteration 1200: Current success : 114 \t All success : 158 \t mean(l0) : 11.132912 \t Current Learning Rate: 0.076\n",
            "Iteration 1300: Current success : 116 \t All success : 158 \t mean(l0) : 11.126582 \t Current Learning Rate: 0.074\n",
            "Iteration 1400: Current success : 115 \t All success : 158 \t mean(l0) : 11.126582 \t Current Learning Rate: 0.072\n",
            "Iteration 1500: Current success : 114 \t All success : 158 \t mean(l0) : 11.126582 \t Current Learning Rate: 0.070\n",
            "Iteration 1600: Current success : 115 \t All success : 160 \t mean(l0) : 11.175000 \t Current Learning Rate: 0.068\n",
            "Iteration 1700: Current success : 118 \t All success : 160 \t mean(l0) : 11.175000 \t Current Learning Rate: 0.066\n",
            "Iteration 1800: Current success : 120 \t All success : 160 \t mean(l0) : 11.175000 \t Current Learning Rate: 0.064\n",
            "Iteration 1900: Current success : 118 \t All success : 160 \t mean(l0) : 11.175000 \t Current Learning Rate: 0.062\n",
            "Iteration 2000: Current success : 119 \t All success : 160 \t mean(l0) : 11.175000 \t Current Learning Rate: 0.060\n",
            "Iteration 2100: Current success : 121 \t All success : 160 \t mean(l0) : 11.175000 \t Current Learning Rate: 0.058\n",
            "Iteration 2200: Current success : 117 \t All success : 160 \t mean(l0) : 11.175000 \t Current Learning Rate: 0.056\n",
            "Iteration 2300: Current success : 119 \t All success : 160 \t mean(l0) : 11.175000 \t Current Learning Rate: 0.054\n",
            "Iteration 2400: Current success : 119 \t All success : 160 \t mean(l0) : 11.175000 \t Current Learning Rate: 0.052\n",
            "Iteration 2500: Current success : 119 \t All success : 160 \t mean(l0) : 11.175000 \t Current Learning Rate: 0.050\n",
            "Iteration 2600: Current success : 119 \t All success : 160 \t mean(l0) : 11.175000 \t Current Learning Rate: 0.048\n",
            "Iteration 2700: Current success : 114 \t All success : 160 \t mean(l0) : 11.175000 \t Current Learning Rate: 0.046\n",
            "Iteration 2800: Current success : 119 \t All success : 160 \t mean(l0) : 11.175000 \t Current Learning Rate: 0.044\n",
            "Iteration 2900: Current success : 118 \t All success : 160 \t mean(l0) : 11.175000 \t Current Learning Rate: 0.042\n",
            "Iteration 3000: Current success : 118 \t All success : 160 \t mean(l0) : 11.175000 \t Current Learning Rate: 0.040\n",
            "Iteration 3100: Current success : 119 \t All success : 161 \t mean(l0) : 11.142858 \t Current Learning Rate: 0.038\n",
            "Iteration 3200: Current success : 118 \t All success : 161 \t mean(l0) : 11.142858 \t Current Learning Rate: 0.036\n",
            "Iteration 3300: Current success : 118 \t All success : 161 \t mean(l0) : 11.142858 \t Current Learning Rate: 0.034\n",
            "Iteration 3400: Current success : 118 \t All success : 161 \t mean(l0) : 11.142858 \t Current Learning Rate: 0.032\n",
            "Iteration 3500: Current success : 119 \t All success : 161 \t mean(l0) : 11.142858 \t Current Learning Rate: 0.030\n",
            "Iteration 3600: Current success : 118 \t All success : 161 \t mean(l0) : 11.142858 \t Current Learning Rate: 0.028\n",
            "Iteration 3700: Current success : 118 \t All success : 161 \t mean(l0) : 11.142858 \t Current Learning Rate: 0.026\n",
            "Iteration 3800: Current success : 118 \t All success : 161 \t mean(l0) : 11.142858 \t Current Learning Rate: 0.024\n",
            "Iteration 3900: Current success : 118 \t All success : 161 \t mean(l0) : 11.142858 \t Current Learning Rate: 0.022\n",
            "Iteration 4000: Current success : 118 \t All success : 161 \t mean(l0) : 11.142858 \t Current Learning Rate: 0.020\n",
            "Iteration 4100: Current success : 118 \t All success : 161 \t mean(l0) : 11.142858 \t Current Learning Rate: 0.018\n",
            "Iteration 4200: Current success : 121 \t All success : 161 \t mean(l0) : 11.142858 \t Current Learning Rate: 0.016\n",
            "Iteration 4300: Current success : 118 \t All success : 161 \t mean(l0) : 11.142858 \t Current Learning Rate: 0.014\n",
            "Iteration 4400: Current success : 118 \t All success : 161 \t mean(l0) : 11.142858 \t Current Learning Rate: 0.012\n",
            "Iteration 4500: Current success : 121 \t All success : 161 \t mean(l0) : 11.142858 \t Current Learning Rate: 0.010\n",
            "Iteration 4600: Current success : 118 \t All success : 161 \t mean(l0) : 11.142858 \t Current Learning Rate: 0.008\n",
            "Iteration 4700: Current success : 118 \t All success : 161 \t mean(l0) : 11.142858 \t Current Learning Rate: 0.006\n",
            "Iteration 4800: Current success : 118 \t All success : 161 \t mean(l0) : 11.142858 \t Current Learning Rate: 0.004\n",
            "Iteration 4900: Current success : 118 \t All success : 161 \t mean(l0) : 11.142858 \t Current Learning Rate: 0.002\n",
            "Iteration 5000: Current success : 118 \t All success : 161 \t mean(l0) : 11.142858 \t Current Learning Rate: 0.000\n",
            "Iteration 5100: Current success : 125 \t All success : 175 \t mean(l0) : 12.571428 \t Current Learning Rate: 0.049\n",
            "Iteration 5200: Current success : 121 \t All success : 177 \t mean(l0) : 12.435028 \t Current Learning Rate: 0.048\n",
            "Iteration 5300: Current success : 118 \t All success : 178 \t mean(l0) : 12.426967 \t Current Learning Rate: 0.047\n",
            "Iteration 5400: Current success : 116 \t All success : 178 \t mean(l0) : 12.426967 \t Current Learning Rate: 0.046\n",
            "Iteration 5500: Current success : 119 \t All success : 179 \t mean(l0) : 12.385474 \t Current Learning Rate: 0.045\n",
            "Iteration 5600: Current success : 119 \t All success : 179 \t mean(l0) : 12.379888 \t Current Learning Rate: 0.044\n",
            "Iteration 5700: Current success : 117 \t All success : 179 \t mean(l0) : 12.379888 \t Current Learning Rate: 0.043\n",
            "Iteration 5800: Current success : 116 \t All success : 179 \t mean(l0) : 12.379888 \t Current Learning Rate: 0.042\n",
            "Iteration 5900: Current success : 118 \t All success : 179 \t mean(l0) : 12.379888 \t Current Learning Rate: 0.041\n",
            "Iteration 6000: Current success : 119 \t All success : 179 \t mean(l0) : 12.374301 \t Current Learning Rate: 0.040\n",
            "Iteration 6100: Current success : 119 \t All success : 179 \t mean(l0) : 12.374301 \t Current Learning Rate: 0.039\n",
            "Iteration 6200: Current success : 116 \t All success : 179 \t mean(l0) : 12.374301 \t Current Learning Rate: 0.038\n",
            "Iteration 6300: Current success : 116 \t All success : 179 \t mean(l0) : 12.374301 \t Current Learning Rate: 0.037\n",
            "Iteration 6400: Current success : 116 \t All success : 179 \t mean(l0) : 12.374301 \t Current Learning Rate: 0.036\n",
            "Iteration 6500: Current success : 118 \t All success : 179 \t mean(l0) : 12.374301 \t Current Learning Rate: 0.035\n",
            "Iteration 6600: Current success : 116 \t All success : 179 \t mean(l0) : 12.374301 \t Current Learning Rate: 0.034\n",
            "Iteration 6700: Current success : 119 \t All success : 179 \t mean(l0) : 12.374301 \t Current Learning Rate: 0.033\n",
            "Iteration 6800: Current success : 117 \t All success : 179 \t mean(l0) : 12.374301 \t Current Learning Rate: 0.032\n",
            "Iteration 6900: Current success : 116 \t All success : 179 \t mean(l0) : 12.374301 \t Current Learning Rate: 0.031\n",
            "Iteration 7000: Current success : 118 \t All success : 179 \t mean(l0) : 12.374301 \t Current Learning Rate: 0.030\n",
            "Iteration 7100: Current success : 120 \t All success : 179 \t mean(l0) : 12.374301 \t Current Learning Rate: 0.029\n",
            "Iteration 7200: Current success : 118 \t All success : 179 \t mean(l0) : 12.374301 \t Current Learning Rate: 0.028\n",
            "Iteration 7300: Current success : 118 \t All success : 179 \t mean(l0) : 12.374301 \t Current Learning Rate: 0.027\n",
            "Iteration 7400: Current success : 119 \t All success : 179 \t mean(l0) : 12.374301 \t Current Learning Rate: 0.026\n",
            "Iteration 7500: Current success : 117 \t All success : 179 \t mean(l0) : 12.374301 \t Current Learning Rate: 0.025\n",
            "Iteration 7600: Current success : 119 \t All success : 179 \t mean(l0) : 12.374301 \t Current Learning Rate: 0.024\n",
            "Iteration 7700: Current success : 118 \t All success : 179 \t mean(l0) : 12.374301 \t Current Learning Rate: 0.023\n",
            "Iteration 7800: Current success : 118 \t All success : 179 \t mean(l0) : 12.374301 \t Current Learning Rate: 0.022\n",
            "Iteration 7900: Current success : 118 \t All success : 179 \t mean(l0) : 12.374301 \t Current Learning Rate: 0.021\n",
            "Iteration 8000: Current success : 116 \t All success : 179 \t mean(l0) : 12.374301 \t Current Learning Rate: 0.020\n",
            "Iteration 8100: Current success : 118 \t All success : 179 \t mean(l0) : 12.368714 \t Current Learning Rate: 0.019\n",
            "Iteration 8200: Current success : 116 \t All success : 179 \t mean(l0) : 12.368714 \t Current Learning Rate: 0.018\n",
            "Iteration 8300: Current success : 118 \t All success : 179 \t mean(l0) : 12.368714 \t Current Learning Rate: 0.017\n",
            "Iteration 8400: Current success : 118 \t All success : 179 \t mean(l0) : 12.368714 \t Current Learning Rate: 0.016\n",
            "Iteration 8500: Current success : 118 \t All success : 179 \t mean(l0) : 12.368714 \t Current Learning Rate: 0.015\n",
            "Iteration 8600: Current success : 116 \t All success : 179 \t mean(l0) : 12.368714 \t Current Learning Rate: 0.014\n",
            "Iteration 8700: Current success : 116 \t All success : 179 \t mean(l0) : 12.368714 \t Current Learning Rate: 0.013\n",
            "Iteration 8800: Current success : 116 \t All success : 179 \t mean(l0) : 12.368714 \t Current Learning Rate: 0.012\n",
            "Iteration 8900: Current success : 118 \t All success : 179 \t mean(l0) : 12.368714 \t Current Learning Rate: 0.011\n",
            "Iteration 9000: Current success : 116 \t All success : 179 \t mean(l0) : 12.368714 \t Current Learning Rate: 0.010\n",
            "Iteration 9100: Current success : 116 \t All success : 179 \t mean(l0) : 12.368714 \t Current Learning Rate: 0.009\n",
            "Iteration 9200: Current success : 116 \t All success : 179 \t mean(l0) : 12.368714 \t Current Learning Rate: 0.008\n",
            "Iteration 9300: Current success : 118 \t All success : 179 \t mean(l0) : 12.368714 \t Current Learning Rate: 0.007\n",
            "Iteration 9400: Current success : 116 \t All success : 179 \t mean(l0) : 12.368714 \t Current Learning Rate: 0.006\n",
            "Iteration 9500: Current success : 116 \t All success : 179 \t mean(l0) : 12.368714 \t Current Learning Rate: 0.005\n",
            "Iteration 9600: Current success : 116 \t All success : 179 \t mean(l0) : 12.368714 \t Current Learning Rate: 0.004\n",
            "Iteration 9700: Current success : 116 \t All success : 179 \t mean(l0) : 12.368714 \t Current Learning Rate: 0.003\n",
            "Iteration 9800: Current success : 116 \t All success : 179 \t mean(l0) : 12.368714 \t Current Learning Rate: 0.002\n",
            "Iteration 9900: Current success : 116 \t All success : 179 \t mean(l0) : 12.368714 \t Current Learning Rate: 0.001\n",
            "Iteration 10000: Current success : 116 \t All success : 179 \t mean(l0) : 12.368714 \t Current Learning Rate: 0.000\n",
            "Iteration 10100: Current success : 116 \t All success : 180 \t mean(l0) : 12.338889 \t Current Learning Rate: 0.025\n",
            "Iteration 10200: Current success : 119 \t All success : 180 \t mean(l0) : 12.333334 \t Current Learning Rate: 0.024\n",
            "Iteration 10300: Current success : 123 \t All success : 180 \t mean(l0) : 12.333334 \t Current Learning Rate: 0.024\n",
            "Iteration 10400: Current success : 121 \t All success : 183 \t mean(l0) : 12.590163 \t Current Learning Rate: 0.023\n",
            "Iteration 10500: Current success : 122 \t All success : 183 \t mean(l0) : 12.590163 \t Current Learning Rate: 0.023\n",
            "Iteration 10600: Current success : 123 \t All success : 183 \t mean(l0) : 12.590163 \t Current Learning Rate: 0.022\n",
            "Iteration 10700: Current success : 121 \t All success : 183 \t mean(l0) : 12.590163 \t Current Learning Rate: 0.022\n",
            "Iteration 10800: Current success : 124 \t All success : 183 \t mean(l0) : 12.590163 \t Current Learning Rate: 0.021\n",
            "Iteration 10900: Current success : 121 \t All success : 183 \t mean(l0) : 12.584699 \t Current Learning Rate: 0.021\n",
            "Iteration 11000: Current success : 123 \t All success : 183 \t mean(l0) : 12.584699 \t Current Learning Rate: 0.020\n",
            "Iteration 11100: Current success : 124 \t All success : 183 \t mean(l0) : 12.584699 \t Current Learning Rate: 0.020\n",
            "Iteration 11200: Current success : 128 \t All success : 183 \t mean(l0) : 12.584699 \t Current Learning Rate: 0.019\n",
            "Iteration 11300: Current success : 125 \t All success : 183 \t mean(l0) : 12.584699 \t Current Learning Rate: 0.019\n",
            "Iteration 11400: Current success : 123 \t All success : 183 \t mean(l0) : 12.584699 \t Current Learning Rate: 0.018\n",
            "Iteration 11500: Current success : 120 \t All success : 183 \t mean(l0) : 12.584699 \t Current Learning Rate: 0.018\n",
            "Iteration 11600: Current success : 123 \t All success : 183 \t mean(l0) : 12.584699 \t Current Learning Rate: 0.017\n",
            "Iteration 11700: Current success : 123 \t All success : 183 \t mean(l0) : 12.584699 \t Current Learning Rate: 0.017\n",
            "Iteration 11800: Current success : 125 \t All success : 183 \t mean(l0) : 12.579235 \t Current Learning Rate: 0.016\n",
            "Iteration 11900: Current success : 123 \t All success : 183 \t mean(l0) : 12.579235 \t Current Learning Rate: 0.016\n",
            "Iteration 12000: Current success : 118 \t All success : 183 \t mean(l0) : 12.579235 \t Current Learning Rate: 0.015\n",
            "Iteration 12100: Current success : 121 \t All success : 183 \t mean(l0) : 12.579235 \t Current Learning Rate: 0.015\n",
            "Iteration 12200: Current success : 123 \t All success : 183 \t mean(l0) : 12.579235 \t Current Learning Rate: 0.014\n",
            "Iteration 12300: Current success : 123 \t All success : 183 \t mean(l0) : 12.579235 \t Current Learning Rate: 0.014\n",
            "Iteration 12400: Current success : 125 \t All success : 183 \t mean(l0) : 12.579235 \t Current Learning Rate: 0.013\n",
            "Iteration 12500: Current success : 126 \t All success : 183 \t mean(l0) : 12.579235 \t Current Learning Rate: 0.013\n",
            "Iteration 12600: Current success : 121 \t All success : 183 \t mean(l0) : 12.579235 \t Current Learning Rate: 0.012\n",
            "Iteration 12700: Current success : 120 \t All success : 183 \t mean(l0) : 12.579235 \t Current Learning Rate: 0.012\n",
            "Iteration 12800: Current success : 123 \t All success : 183 \t mean(l0) : 12.579235 \t Current Learning Rate: 0.011\n",
            "Iteration 12900: Current success : 124 \t All success : 183 \t mean(l0) : 12.579235 \t Current Learning Rate: 0.011\n",
            "Iteration 13000: Current success : 124 \t All success : 183 \t mean(l0) : 12.579235 \t Current Learning Rate: 0.010\n",
            "Iteration 13100: Current success : 123 \t All success : 183 \t mean(l0) : 12.579235 \t Current Learning Rate: 0.010\n",
            "Iteration 13200: Current success : 121 \t All success : 183 \t mean(l0) : 12.579235 \t Current Learning Rate: 0.009\n",
            "Iteration 13300: Current success : 123 \t All success : 183 \t mean(l0) : 12.579235 \t Current Learning Rate: 0.009\n",
            "Iteration 13400: Current success : 124 \t All success : 183 \t mean(l0) : 12.579235 \t Current Learning Rate: 0.008\n",
            "Iteration 13500: Current success : 121 \t All success : 183 \t mean(l0) : 12.579235 \t Current Learning Rate: 0.008\n",
            "Iteration 13600: Current success : 123 \t All success : 183 \t mean(l0) : 12.579235 \t Current Learning Rate: 0.007\n",
            "Iteration 13700: Current success : 120 \t All success : 183 \t mean(l0) : 12.579235 \t Current Learning Rate: 0.007\n",
            "Iteration 13800: Current success : 125 \t All success : 183 \t mean(l0) : 12.579235 \t Current Learning Rate: 0.006\n",
            "Iteration 13900: Current success : 121 \t All success : 183 \t mean(l0) : 12.579235 \t Current Learning Rate: 0.006\n",
            "Iteration 14000: Current success : 123 \t All success : 183 \t mean(l0) : 12.579235 \t Current Learning Rate: 0.005\n",
            "Iteration 14100: Current success : 126 \t All success : 183 \t mean(l0) : 12.579235 \t Current Learning Rate: 0.005\n",
            "Iteration 14200: Current success : 123 \t All success : 183 \t mean(l0) : 12.579235 \t Current Learning Rate: 0.004\n",
            "Iteration 14300: Current success : 126 \t All success : 183 \t mean(l0) : 12.579235 \t Current Learning Rate: 0.004\n",
            "Iteration 14400: Current success : 123 \t All success : 183 \t mean(l0) : 12.579235 \t Current Learning Rate: 0.003\n",
            "Iteration 14500: Current success : 123 \t All success : 183 \t mean(l0) : 12.579235 \t Current Learning Rate: 0.003\n",
            "Iteration 14600: Current success : 123 \t All success : 183 \t mean(l0) : 12.579235 \t Current Learning Rate: 0.002\n",
            "Iteration 14700: Current success : 126 \t All success : 183 \t mean(l0) : 12.579235 \t Current Learning Rate: 0.002\n",
            "Iteration 14800: Current success : 126 \t All success : 183 \t mean(l0) : 12.579235 \t Current Learning Rate: 0.001\n",
            "Iteration 14900: Current success : 126 \t All success : 183 \t mean(l0) : 12.579235 \t Current Learning Rate: 0.001\n",
            "Iteration 15000: Current success : 126 \t All success : 183 \t mean(l0) : 12.579235 \t Current Learning Rate: 0.000\n",
            "Iteration 15100: Current success : 121 \t All success : 184 \t mean(l0) : 12.777174 \t Current Learning Rate: 0.012\n",
            "Iteration 15200: Current success : 120 \t All success : 184 \t mean(l0) : 12.706522 \t Current Learning Rate: 0.012\n",
            "Iteration 15300: Current success : 124 \t All success : 184 \t mean(l0) : 12.695652 \t Current Learning Rate: 0.012\n",
            "Iteration 15400: Current success : 126 \t All success : 184 \t mean(l0) : 12.695652 \t Current Learning Rate: 0.012\n",
            "Iteration 15500: Current success : 123 \t All success : 184 \t mean(l0) : 12.695652 \t Current Learning Rate: 0.011\n",
            "Iteration 15600: Current success : 123 \t All success : 184 \t mean(l0) : 12.695652 \t Current Learning Rate: 0.011\n",
            "Iteration 15700: Current success : 126 \t All success : 184 \t mean(l0) : 12.695652 \t Current Learning Rate: 0.011\n",
            "Iteration 15800: Current success : 123 \t All success : 184 \t mean(l0) : 12.695652 \t Current Learning Rate: 0.011\n",
            "Iteration 15900: Current success : 121 \t All success : 184 \t mean(l0) : 12.695652 \t Current Learning Rate: 0.010\n",
            "Iteration 16000: Current success : 123 \t All success : 184 \t mean(l0) : 12.695652 \t Current Learning Rate: 0.010\n",
            "Iteration 16100: Current success : 123 \t All success : 184 \t mean(l0) : 12.695652 \t Current Learning Rate: 0.010\n",
            "Iteration 16200: Current success : 118 \t All success : 184 \t mean(l0) : 12.695652 \t Current Learning Rate: 0.010\n",
            "Iteration 16300: Current success : 122 \t All success : 184 \t mean(l0) : 12.695652 \t Current Learning Rate: 0.009\n",
            "Iteration 16400: Current success : 121 \t All success : 184 \t mean(l0) : 12.695652 \t Current Learning Rate: 0.009\n",
            "Iteration 16500: Current success : 121 \t All success : 184 \t mean(l0) : 12.695652 \t Current Learning Rate: 0.009\n",
            "Iteration 16600: Current success : 121 \t All success : 184 \t mean(l0) : 12.695652 \t Current Learning Rate: 0.009\n",
            "Iteration 16700: Current success : 123 \t All success : 184 \t mean(l0) : 12.695652 \t Current Learning Rate: 0.008\n",
            "Iteration 16800: Current success : 121 \t All success : 184 \t mean(l0) : 12.695652 \t Current Learning Rate: 0.008\n",
            "Iteration 16900: Current success : 123 \t All success : 184 \t mean(l0) : 12.695652 \t Current Learning Rate: 0.008\n",
            "Iteration 17000: Current success : 118 \t All success : 184 \t mean(l0) : 12.695652 \t Current Learning Rate: 0.008\n",
            "Iteration 17100: Current success : 121 \t All success : 184 \t mean(l0) : 12.695652 \t Current Learning Rate: 0.007\n",
            "Iteration 17200: Current success : 123 \t All success : 184 \t mean(l0) : 12.695652 \t Current Learning Rate: 0.007\n",
            "Iteration 17300: Current success : 123 \t All success : 184 \t mean(l0) : 12.695652 \t Current Learning Rate: 0.007\n",
            "Iteration 17400: Current success : 123 \t All success : 184 \t mean(l0) : 12.695652 \t Current Learning Rate: 0.007\n",
            "Iteration 17500: Current success : 121 \t All success : 184 \t mean(l0) : 12.695652 \t Current Learning Rate: 0.006\n",
            "Iteration 17600: Current success : 121 \t All success : 184 \t mean(l0) : 12.695652 \t Current Learning Rate: 0.006\n",
            "Iteration 17700: Current success : 123 \t All success : 184 \t mean(l0) : 12.695652 \t Current Learning Rate: 0.006\n",
            "Iteration 17800: Current success : 123 \t All success : 184 \t mean(l0) : 12.695652 \t Current Learning Rate: 0.006\n",
            "Iteration 17900: Current success : 123 \t All success : 184 \t mean(l0) : 12.695652 \t Current Learning Rate: 0.005\n",
            "Iteration 18000: Current success : 120 \t All success : 184 \t mean(l0) : 12.695652 \t Current Learning Rate: 0.005\n",
            "Iteration 18100: Current success : 123 \t All success : 184 \t mean(l0) : 12.695652 \t Current Learning Rate: 0.005\n",
            "Iteration 18200: Current success : 120 \t All success : 184 \t mean(l0) : 12.695652 \t Current Learning Rate: 0.005\n",
            "Iteration 18300: Current success : 123 \t All success : 184 \t mean(l0) : 12.695652 \t Current Learning Rate: 0.004\n",
            "Iteration 18400: Current success : 123 \t All success : 184 \t mean(l0) : 12.695652 \t Current Learning Rate: 0.004\n",
            "Iteration 18500: Current success : 123 \t All success : 184 \t mean(l0) : 12.695652 \t Current Learning Rate: 0.004\n",
            "Iteration 18600: Current success : 123 \t All success : 184 \t mean(l0) : 12.695652 \t Current Learning Rate: 0.004\n",
            "Iteration 18700: Current success : 123 \t All success : 184 \t mean(l0) : 12.695652 \t Current Learning Rate: 0.003\n",
            "Iteration 18800: Current success : 123 \t All success : 184 \t mean(l0) : 12.695652 \t Current Learning Rate: 0.003\n",
            "Iteration 18900: Current success : 123 \t All success : 184 \t mean(l0) : 12.695652 \t Current Learning Rate: 0.003\n",
            "Iteration 19000: Current success : 123 \t All success : 184 \t mean(l0) : 12.695652 \t Current Learning Rate: 0.003\n",
            "Iteration 19100: Current success : 120 \t All success : 184 \t mean(l0) : 12.695652 \t Current Learning Rate: 0.002\n",
            "Iteration 19200: Current success : 123 \t All success : 184 \t mean(l0) : 12.695652 \t Current Learning Rate: 0.002\n",
            "Iteration 19300: Current success : 123 \t All success : 184 \t mean(l0) : 12.690218 \t Current Learning Rate: 0.002\n",
            "Iteration 19400: Current success : 123 \t All success : 184 \t mean(l0) : 12.690218 \t Current Learning Rate: 0.002\n",
            "Iteration 19500: Current success : 123 \t All success : 184 \t mean(l0) : 12.690218 \t Current Learning Rate: 0.001\n",
            "Iteration 19600: Current success : 123 \t All success : 184 \t mean(l0) : 12.690218 \t Current Learning Rate: 0.001\n",
            "Iteration 19700: Current success : 123 \t All success : 184 \t mean(l0) : 12.690218 \t Current Learning Rate: 0.001\n",
            "Iteration 19800: Current success : 123 \t All success : 184 \t mean(l0) : 12.690218 \t Current Learning Rate: 0.001\n",
            "Iteration 19900: Current success : 123 \t All success : 184 \t mean(l0) : 12.690218 \t Current Learning Rate: 0.000\n",
            "Iteration 20000: Current success : 123 \t All success : 184 \t mean(l0) : 12.690218 \t Current Learning Rate: 0.000\n",
            "outer_step 3: all success : 230 \t mean(l0)(success) : 14.1391\n",
            "-------------------------------------------------------------\n",
            "tensor([3.7500e-01, 1.0000e+03, 1.0000e+03,  ..., 1.0000e+03, 1.0000e+03,\n",
            "        1.0000e+03], device='cuda:0')\n",
            "Iteration 100: Current success : 53 \t All success : 71 \t mean(l0) : 7.084507 \t Current Learning Rate: 0.098\n",
            "Iteration 200: Current success : 67 \t All success : 83 \t mean(l0) : 7.746988 \t Current Learning Rate: 0.096\n",
            "Iteration 300: Current success : 77 \t All success : 97 \t mean(l0) : 11.721649 \t Current Learning Rate: 0.094\n",
            "Iteration 400: Current success : 82 \t All success : 112 \t mean(l0) : 13.169643 \t Current Learning Rate: 0.092\n",
            "Iteration 500: Current success : 86 \t All success : 113 \t mean(l0) : 13.203540 \t Current Learning Rate: 0.090\n",
            "Iteration 600: Current success : 86 \t All success : 114 \t mean(l0) : 13.543859 \t Current Learning Rate: 0.088\n",
            "Iteration 700: Current success : 83 \t All success : 117 \t mean(l0) : 14.820514 \t Current Learning Rate: 0.086\n",
            "Iteration 800: Current success : 81 \t All success : 117 \t mean(l0) : 14.777779 \t Current Learning Rate: 0.084\n",
            "Iteration 900: Current success : 78 \t All success : 118 \t mean(l0) : 14.677966 \t Current Learning Rate: 0.082\n",
            "Iteration 1000: Current success : 77 \t All success : 118 \t mean(l0) : 14.677966 \t Current Learning Rate: 0.080\n",
            "Iteration 1100: Current success : 76 \t All success : 118 \t mean(l0) : 14.677966 \t Current Learning Rate: 0.078\n",
            "Iteration 1200: Current success : 82 \t All success : 118 \t mean(l0) : 14.652542 \t Current Learning Rate: 0.076\n",
            "Iteration 1300: Current success : 77 \t All success : 118 \t mean(l0) : 14.652542 \t Current Learning Rate: 0.074\n",
            "Iteration 1400: Current success : 80 \t All success : 118 \t mean(l0) : 14.652542 \t Current Learning Rate: 0.072\n",
            "Iteration 1500: Current success : 80 \t All success : 120 \t mean(l0) : 15.408334 \t Current Learning Rate: 0.070\n",
            "Iteration 1600: Current success : 82 \t All success : 120 \t mean(l0) : 15.408334 \t Current Learning Rate: 0.068\n",
            "Iteration 1700: Current success : 81 \t All success : 149 \t mean(l0) : 13.953020 \t Current Learning Rate: 0.066\n",
            "Iteration 1800: Current success : 79 \t All success : 149 \t mean(l0) : 13.926174 \t Current Learning Rate: 0.064\n",
            "Iteration 1900: Current success : 83 \t All success : 149 \t mean(l0) : 13.926174 \t Current Learning Rate: 0.062\n",
            "Iteration 2000: Current success : 79 \t All success : 149 \t mean(l0) : 13.926174 \t Current Learning Rate: 0.060\n",
            "Iteration 2100: Current success : 82 \t All success : 149 \t mean(l0) : 13.926174 \t Current Learning Rate: 0.058\n",
            "Iteration 2200: Current success : 82 \t All success : 150 \t mean(l0) : 13.980000 \t Current Learning Rate: 0.056\n",
            "Iteration 2300: Current success : 79 \t All success : 150 \t mean(l0) : 13.980000 \t Current Learning Rate: 0.054\n",
            "Iteration 2400: Current success : 84 \t All success : 151 \t mean(l0) : 14.125828 \t Current Learning Rate: 0.052\n",
            "Iteration 2500: Current success : 82 \t All success : 152 \t mean(l0) : 14.072369 \t Current Learning Rate: 0.050\n",
            "Iteration 2600: Current success : 83 \t All success : 152 \t mean(l0) : 14.072369 \t Current Learning Rate: 0.048\n",
            "Iteration 2700: Current success : 84 \t All success : 152 \t mean(l0) : 14.072369 \t Current Learning Rate: 0.046\n",
            "Iteration 2800: Current success : 82 \t All success : 152 \t mean(l0) : 14.072369 \t Current Learning Rate: 0.044\n",
            "Iteration 2900: Current success : 84 \t All success : 152 \t mean(l0) : 14.072369 \t Current Learning Rate: 0.042\n",
            "Iteration 3000: Current success : 83 \t All success : 152 \t mean(l0) : 14.072369 \t Current Learning Rate: 0.040\n",
            "Iteration 3100: Current success : 87 \t All success : 152 \t mean(l0) : 14.072369 \t Current Learning Rate: 0.038\n",
            "Iteration 3200: Current success : 82 \t All success : 152 \t mean(l0) : 14.072369 \t Current Learning Rate: 0.036\n",
            "Iteration 3300: Current success : 82 \t All success : 152 \t mean(l0) : 14.072369 \t Current Learning Rate: 0.034\n",
            "Iteration 3400: Current success : 86 \t All success : 152 \t mean(l0) : 14.072369 \t Current Learning Rate: 0.032\n",
            "Iteration 3500: Current success : 85 \t All success : 152 \t mean(l0) : 14.072369 \t Current Learning Rate: 0.030\n",
            "Iteration 3600: Current success : 83 \t All success : 152 \t mean(l0) : 14.072369 \t Current Learning Rate: 0.028\n",
            "Iteration 3700: Current success : 85 \t All success : 152 \t mean(l0) : 14.072369 \t Current Learning Rate: 0.026\n",
            "Iteration 3800: Current success : 82 \t All success : 152 \t mean(l0) : 14.072369 \t Current Learning Rate: 0.024\n",
            "Iteration 3900: Current success : 86 \t All success : 152 \t mean(l0) : 14.072369 \t Current Learning Rate: 0.022\n",
            "Iteration 4000: Current success : 86 \t All success : 152 \t mean(l0) : 14.072369 \t Current Learning Rate: 0.020\n",
            "Iteration 4100: Current success : 87 \t All success : 152 \t mean(l0) : 14.072369 \t Current Learning Rate: 0.018\n",
            "Iteration 4200: Current success : 85 \t All success : 152 \t mean(l0) : 14.072369 \t Current Learning Rate: 0.016\n",
            "Iteration 4300: Current success : 82 \t All success : 152 \t mean(l0) : 14.072369 \t Current Learning Rate: 0.014\n",
            "Iteration 4400: Current success : 82 \t All success : 152 \t mean(l0) : 14.072369 \t Current Learning Rate: 0.012\n",
            "Iteration 4500: Current success : 86 \t All success : 152 \t mean(l0) : 14.072369 \t Current Learning Rate: 0.010\n",
            "Iteration 4600: Current success : 86 \t All success : 152 \t mean(l0) : 14.072369 \t Current Learning Rate: 0.008\n",
            "Iteration 4700: Current success : 81 \t All success : 152 \t mean(l0) : 14.072369 \t Current Learning Rate: 0.006\n",
            "Iteration 4800: Current success : 81 \t All success : 152 \t mean(l0) : 14.072369 \t Current Learning Rate: 0.004\n",
            "Iteration 4900: Current success : 81 \t All success : 152 \t mean(l0) : 14.072369 \t Current Learning Rate: 0.002\n",
            "Iteration 5000: Current success : 81 \t All success : 152 \t mean(l0) : 14.072369 \t Current Learning Rate: 0.000\n",
            "Iteration 5100: Current success : 72 \t All success : 167 \t mean(l0) : 14.808384 \t Current Learning Rate: 0.049\n",
            "Iteration 5200: Current success : 69 \t All success : 169 \t mean(l0) : 14.668639 \t Current Learning Rate: 0.048\n",
            "Iteration 5300: Current success : 71 \t All success : 169 \t mean(l0) : 14.662722 \t Current Learning Rate: 0.047\n",
            "Iteration 5400: Current success : 75 \t All success : 170 \t mean(l0) : 15.011765 \t Current Learning Rate: 0.046\n",
            "Iteration 5500: Current success : 73 \t All success : 170 \t mean(l0) : 14.982353 \t Current Learning Rate: 0.045\n",
            "Iteration 5600: Current success : 73 \t All success : 170 \t mean(l0) : 14.964706 \t Current Learning Rate: 0.044\n",
            "Iteration 5700: Current success : 75 \t All success : 171 \t mean(l0) : 15.327485 \t Current Learning Rate: 0.043\n",
            "Iteration 5800: Current success : 79 \t All success : 171 \t mean(l0) : 15.327485 \t Current Learning Rate: 0.042\n",
            "Iteration 5900: Current success : 74 \t All success : 171 \t mean(l0) : 15.327485 \t Current Learning Rate: 0.041\n",
            "Iteration 6000: Current success : 77 \t All success : 171 \t mean(l0) : 15.327485 \t Current Learning Rate: 0.040\n",
            "Iteration 6100: Current success : 72 \t All success : 171 \t mean(l0) : 15.309941 \t Current Learning Rate: 0.039\n",
            "Iteration 6200: Current success : 74 \t All success : 171 \t mean(l0) : 15.309941 \t Current Learning Rate: 0.038\n",
            "Iteration 6300: Current success : 74 \t All success : 171 \t mean(l0) : 15.304093 \t Current Learning Rate: 0.037\n",
            "Iteration 6400: Current success : 76 \t All success : 171 \t mean(l0) : 15.304093 \t Current Learning Rate: 0.036\n",
            "Iteration 6500: Current success : 74 \t All success : 171 \t mean(l0) : 15.304093 \t Current Learning Rate: 0.035\n",
            "Iteration 6600: Current success : 75 \t All success : 171 \t mean(l0) : 15.304093 \t Current Learning Rate: 0.034\n",
            "Iteration 6700: Current success : 74 \t All success : 171 \t mean(l0) : 15.304093 \t Current Learning Rate: 0.033\n",
            "Iteration 6800: Current success : 74 \t All success : 171 \t mean(l0) : 15.304093 \t Current Learning Rate: 0.032\n",
            "Iteration 6900: Current success : 74 \t All success : 171 \t mean(l0) : 15.304093 \t Current Learning Rate: 0.031\n",
            "Iteration 7000: Current success : 74 \t All success : 171 \t mean(l0) : 15.304093 \t Current Learning Rate: 0.030\n",
            "Iteration 7100: Current success : 78 \t All success : 171 \t mean(l0) : 15.304093 \t Current Learning Rate: 0.029\n",
            "Iteration 7200: Current success : 74 \t All success : 171 \t mean(l0) : 15.304093 \t Current Learning Rate: 0.028\n",
            "Iteration 7300: Current success : 74 \t All success : 171 \t mean(l0) : 15.304093 \t Current Learning Rate: 0.027\n",
            "Iteration 7400: Current success : 77 \t All success : 171 \t mean(l0) : 15.304093 \t Current Learning Rate: 0.026\n",
            "Iteration 7500: Current success : 81 \t All success : 171 \t mean(l0) : 15.304093 \t Current Learning Rate: 0.025\n",
            "Iteration 7600: Current success : 76 \t All success : 171 \t mean(l0) : 15.304093 \t Current Learning Rate: 0.024\n",
            "Iteration 7700: Current success : 78 \t All success : 171 \t mean(l0) : 15.304093 \t Current Learning Rate: 0.023\n",
            "Iteration 7800: Current success : 75 \t All success : 171 \t mean(l0) : 15.304093 \t Current Learning Rate: 0.022\n",
            "Iteration 7900: Current success : 75 \t All success : 171 \t mean(l0) : 15.304093 \t Current Learning Rate: 0.021\n",
            "Iteration 8000: Current success : 78 \t All success : 171 \t mean(l0) : 15.304093 \t Current Learning Rate: 0.020\n",
            "Iteration 8100: Current success : 74 \t All success : 171 \t mean(l0) : 15.304093 \t Current Learning Rate: 0.019\n",
            "Iteration 8200: Current success : 78 \t All success : 171 \t mean(l0) : 15.304093 \t Current Learning Rate: 0.018\n",
            "Iteration 8300: Current success : 75 \t All success : 171 \t mean(l0) : 15.304093 \t Current Learning Rate: 0.017\n",
            "Iteration 8400: Current success : 74 \t All success : 171 \t mean(l0) : 15.304093 \t Current Learning Rate: 0.016\n",
            "Iteration 8500: Current success : 78 \t All success : 171 \t mean(l0) : 15.304093 \t Current Learning Rate: 0.015\n",
            "Iteration 8600: Current success : 74 \t All success : 171 \t mean(l0) : 15.304093 \t Current Learning Rate: 0.014\n",
            "Iteration 8700: Current success : 74 \t All success : 171 \t mean(l0) : 15.304093 \t Current Learning Rate: 0.013\n",
            "Iteration 8800: Current success : 74 \t All success : 171 \t mean(l0) : 15.304093 \t Current Learning Rate: 0.012\n",
            "Iteration 8900: Current success : 74 \t All success : 171 \t mean(l0) : 15.304093 \t Current Learning Rate: 0.011\n",
            "Iteration 9000: Current success : 74 \t All success : 171 \t mean(l0) : 15.304093 \t Current Learning Rate: 0.010\n",
            "Iteration 9100: Current success : 74 \t All success : 171 \t mean(l0) : 15.304093 \t Current Learning Rate: 0.009\n",
            "Iteration 9200: Current success : 74 \t All success : 171 \t mean(l0) : 15.304093 \t Current Learning Rate: 0.008\n",
            "Iteration 9300: Current success : 74 \t All success : 171 \t mean(l0) : 15.304093 \t Current Learning Rate: 0.007\n",
            "Iteration 9400: Current success : 78 \t All success : 171 \t mean(l0) : 15.304093 \t Current Learning Rate: 0.006\n",
            "Iteration 9500: Current success : 74 \t All success : 171 \t mean(l0) : 15.304093 \t Current Learning Rate: 0.005\n",
            "Iteration 9600: Current success : 74 \t All success : 171 \t mean(l0) : 15.304093 \t Current Learning Rate: 0.004\n",
            "Iteration 9700: Current success : 74 \t All success : 171 \t mean(l0) : 15.304093 \t Current Learning Rate: 0.003\n",
            "Iteration 9800: Current success : 74 \t All success : 171 \t mean(l0) : 15.304093 \t Current Learning Rate: 0.002\n",
            "Iteration 9900: Current success : 74 \t All success : 171 \t mean(l0) : 15.304093 \t Current Learning Rate: 0.001\n",
            "Iteration 10000: Current success : 74 \t All success : 171 \t mean(l0) : 15.304093 \t Current Learning Rate: 0.000\n",
            "Iteration 10100: Current success : 78 \t All success : 176 \t mean(l0) : 16.267046 \t Current Learning Rate: 0.025\n",
            "Iteration 10200: Current success : 80 \t All success : 176 \t mean(l0) : 16.255682 \t Current Learning Rate: 0.024\n",
            "Iteration 10300: Current success : 86 \t All success : 176 \t mean(l0) : 16.255682 \t Current Learning Rate: 0.024\n",
            "Iteration 10400: Current success : 87 \t All success : 176 \t mean(l0) : 16.238636 \t Current Learning Rate: 0.023\n",
            "Iteration 10500: Current success : 86 \t All success : 176 \t mean(l0) : 16.238636 \t Current Learning Rate: 0.023\n",
            "Iteration 10600: Current success : 86 \t All success : 176 \t mean(l0) : 16.221592 \t Current Learning Rate: 0.022\n",
            "Iteration 10700: Current success : 81 \t All success : 176 \t mean(l0) : 16.221592 \t Current Learning Rate: 0.022\n",
            "Iteration 10800: Current success : 86 \t All success : 176 \t mean(l0) : 16.221592 \t Current Learning Rate: 0.021\n",
            "Iteration 10900: Current success : 85 \t All success : 176 \t mean(l0) : 16.221592 \t Current Learning Rate: 0.021\n",
            "Iteration 11000: Current success : 86 \t All success : 176 \t mean(l0) : 16.204546 \t Current Learning Rate: 0.020\n",
            "Iteration 11100: Current success : 86 \t All success : 176 \t mean(l0) : 16.204546 \t Current Learning Rate: 0.020\n",
            "Iteration 11200: Current success : 88 \t All success : 176 \t mean(l0) : 16.204546 \t Current Learning Rate: 0.019\n",
            "Iteration 11300: Current success : 80 \t All success : 176 \t mean(l0) : 16.204546 \t Current Learning Rate: 0.019\n",
            "Iteration 11400: Current success : 87 \t All success : 176 \t mean(l0) : 16.204546 \t Current Learning Rate: 0.018\n",
            "Iteration 11500: Current success : 88 \t All success : 176 \t mean(l0) : 16.204546 \t Current Learning Rate: 0.018\n",
            "Iteration 11600: Current success : 85 \t All success : 176 \t mean(l0) : 16.204546 \t Current Learning Rate: 0.017\n",
            "Iteration 11700: Current success : 86 \t All success : 176 \t mean(l0) : 16.204546 \t Current Learning Rate: 0.017\n",
            "Iteration 11800: Current success : 86 \t All success : 176 \t mean(l0) : 16.204546 \t Current Learning Rate: 0.016\n",
            "Iteration 11900: Current success : 86 \t All success : 176 \t mean(l0) : 16.204546 \t Current Learning Rate: 0.016\n",
            "Iteration 12000: Current success : 86 \t All success : 176 \t mean(l0) : 16.204546 \t Current Learning Rate: 0.015\n",
            "Iteration 12100: Current success : 81 \t All success : 176 \t mean(l0) : 16.204546 \t Current Learning Rate: 0.015\n",
            "Iteration 12200: Current success : 87 \t All success : 176 \t mean(l0) : 16.204546 \t Current Learning Rate: 0.014\n",
            "Iteration 12300: Current success : 80 \t All success : 176 \t mean(l0) : 16.204546 \t Current Learning Rate: 0.014\n",
            "Iteration 12400: Current success : 87 \t All success : 176 \t mean(l0) : 16.204546 \t Current Learning Rate: 0.013\n",
            "Iteration 12500: Current success : 87 \t All success : 176 \t mean(l0) : 16.204546 \t Current Learning Rate: 0.013\n",
            "Iteration 12600: Current success : 80 \t All success : 176 \t mean(l0) : 16.204546 \t Current Learning Rate: 0.012\n",
            "Iteration 12700: Current success : 80 \t All success : 176 \t mean(l0) : 16.204546 \t Current Learning Rate: 0.012\n",
            "Iteration 12800: Current success : 81 \t All success : 176 \t mean(l0) : 16.204546 \t Current Learning Rate: 0.011\n",
            "Iteration 12900: Current success : 80 \t All success : 176 \t mean(l0) : 16.204546 \t Current Learning Rate: 0.011\n",
            "Iteration 13000: Current success : 87 \t All success : 176 \t mean(l0) : 16.204546 \t Current Learning Rate: 0.010\n",
            "Iteration 13100: Current success : 87 \t All success : 176 \t mean(l0) : 16.204546 \t Current Learning Rate: 0.010\n",
            "Iteration 13200: Current success : 80 \t All success : 176 \t mean(l0) : 16.204546 \t Current Learning Rate: 0.009\n",
            "Iteration 13300: Current success : 79 \t All success : 176 \t mean(l0) : 16.204546 \t Current Learning Rate: 0.009\n",
            "Iteration 13400: Current success : 81 \t All success : 176 \t mean(l0) : 16.204546 \t Current Learning Rate: 0.008\n",
            "Iteration 13500: Current success : 80 \t All success : 176 \t mean(l0) : 16.204546 \t Current Learning Rate: 0.008\n",
            "Iteration 13600: Current success : 80 \t All success : 176 \t mean(l0) : 16.204546 \t Current Learning Rate: 0.007\n",
            "Iteration 13700: Current success : 80 \t All success : 176 \t mean(l0) : 16.204546 \t Current Learning Rate: 0.007\n",
            "Iteration 13800: Current success : 80 \t All success : 176 \t mean(l0) : 16.204546 \t Current Learning Rate: 0.006\n",
            "Iteration 13900: Current success : 80 \t All success : 176 \t mean(l0) : 16.204546 \t Current Learning Rate: 0.006\n",
            "Iteration 14000: Current success : 80 \t All success : 176 \t mean(l0) : 16.204546 \t Current Learning Rate: 0.005\n",
            "Iteration 14100: Current success : 80 \t All success : 176 \t mean(l0) : 16.204546 \t Current Learning Rate: 0.005\n",
            "Iteration 14200: Current success : 80 \t All success : 176 \t mean(l0) : 16.204546 \t Current Learning Rate: 0.004\n",
            "Iteration 14300: Current success : 80 \t All success : 176 \t mean(l0) : 16.204546 \t Current Learning Rate: 0.004\n",
            "Iteration 14400: Current success : 80 \t All success : 176 \t mean(l0) : 16.204546 \t Current Learning Rate: 0.003\n",
            "Iteration 14500: Current success : 80 \t All success : 176 \t mean(l0) : 16.204546 \t Current Learning Rate: 0.003\n",
            "Iteration 14600: Current success : 80 \t All success : 176 \t mean(l0) : 16.204546 \t Current Learning Rate: 0.002\n",
            "Iteration 14700: Current success : 80 \t All success : 176 \t mean(l0) : 16.204546 \t Current Learning Rate: 0.002\n",
            "Iteration 14800: Current success : 80 \t All success : 176 \t mean(l0) : 16.204546 \t Current Learning Rate: 0.001\n",
            "Iteration 14900: Current success : 80 \t All success : 176 \t mean(l0) : 16.204546 \t Current Learning Rate: 0.001\n",
            "Iteration 15000: Current success : 80 \t All success : 176 \t mean(l0) : 16.204546 \t Current Learning Rate: 0.000\n",
            "Iteration 15100: Current success : 83 \t All success : 176 \t mean(l0) : 16.204546 \t Current Learning Rate: 0.012\n",
            "Iteration 15200: Current success : 87 \t All success : 176 \t mean(l0) : 16.187500 \t Current Learning Rate: 0.012\n",
            "Iteration 15300: Current success : 86 \t All success : 176 \t mean(l0) : 16.159092 \t Current Learning Rate: 0.012\n",
            "Iteration 15400: Current success : 85 \t All success : 176 \t mean(l0) : 16.153410 \t Current Learning Rate: 0.012\n",
            "Iteration 15500: Current success : 86 \t All success : 176 \t mean(l0) : 16.153410 \t Current Learning Rate: 0.011\n",
            "Iteration 15600: Current success : 86 \t All success : 176 \t mean(l0) : 16.153410 \t Current Learning Rate: 0.011\n",
            "Iteration 15700: Current success : 87 \t All success : 176 \t mean(l0) : 16.153410 \t Current Learning Rate: 0.011\n",
            "Iteration 15800: Current success : 87 \t All success : 176 \t mean(l0) : 16.153410 \t Current Learning Rate: 0.011\n",
            "Iteration 15900: Current success : 88 \t All success : 176 \t mean(l0) : 16.153410 \t Current Learning Rate: 0.010\n",
            "Iteration 16000: Current success : 86 \t All success : 176 \t mean(l0) : 16.153410 \t Current Learning Rate: 0.010\n",
            "Iteration 16100: Current success : 80 \t All success : 176 \t mean(l0) : 16.153410 \t Current Learning Rate: 0.010\n",
            "Iteration 16200: Current success : 86 \t All success : 176 \t mean(l0) : 16.153410 \t Current Learning Rate: 0.010\n",
            "Iteration 16300: Current success : 80 \t All success : 176 \t mean(l0) : 16.153410 \t Current Learning Rate: 0.009\n",
            "Iteration 16400: Current success : 81 \t All success : 176 \t mean(l0) : 16.153410 \t Current Learning Rate: 0.009\n",
            "Iteration 16500: Current success : 86 \t All success : 176 \t mean(l0) : 16.153410 \t Current Learning Rate: 0.009\n",
            "Iteration 16600: Current success : 80 \t All success : 176 \t mean(l0) : 16.153410 \t Current Learning Rate: 0.009\n",
            "Iteration 16700: Current success : 80 \t All success : 176 \t mean(l0) : 16.153410 \t Current Learning Rate: 0.008\n",
            "Iteration 16800: Current success : 80 \t All success : 176 \t mean(l0) : 16.153410 \t Current Learning Rate: 0.008\n",
            "Iteration 16900: Current success : 89 \t All success : 176 \t mean(l0) : 16.142046 \t Current Learning Rate: 0.008\n",
            "Iteration 17000: Current success : 83 \t All success : 176 \t mean(l0) : 16.130682 \t Current Learning Rate: 0.008\n",
            "Iteration 17100: Current success : 89 \t All success : 176 \t mean(l0) : 16.125000 \t Current Learning Rate: 0.007\n",
            "Iteration 17200: Current success : 89 \t All success : 176 \t mean(l0) : 16.125000 \t Current Learning Rate: 0.007\n",
            "Iteration 17300: Current success : 90 \t All success : 176 \t mean(l0) : 16.125000 \t Current Learning Rate: 0.007\n",
            "Iteration 17400: Current success : 89 \t All success : 176 \t mean(l0) : 16.125000 \t Current Learning Rate: 0.007\n",
            "Iteration 17500: Current success : 90 \t All success : 176 \t mean(l0) : 16.119318 \t Current Learning Rate: 0.006\n",
            "Iteration 17600: Current success : 83 \t All success : 176 \t mean(l0) : 16.119318 \t Current Learning Rate: 0.006\n",
            "Iteration 17700: Current success : 89 \t All success : 176 \t mean(l0) : 16.119318 \t Current Learning Rate: 0.006\n",
            "Iteration 17800: Current success : 83 \t All success : 176 \t mean(l0) : 16.119318 \t Current Learning Rate: 0.006\n",
            "Iteration 17900: Current success : 84 \t All success : 176 \t mean(l0) : 16.119318 \t Current Learning Rate: 0.005\n",
            "Iteration 18000: Current success : 83 \t All success : 176 \t mean(l0) : 16.119318 \t Current Learning Rate: 0.005\n",
            "Iteration 18100: Current success : 85 \t All success : 176 \t mean(l0) : 16.119318 \t Current Learning Rate: 0.005\n",
            "Iteration 18200: Current success : 83 \t All success : 176 \t mean(l0) : 16.119318 \t Current Learning Rate: 0.005\n",
            "Iteration 18300: Current success : 89 \t All success : 176 \t mean(l0) : 16.113636 \t Current Learning Rate: 0.004\n",
            "Iteration 18400: Current success : 89 \t All success : 176 \t mean(l0) : 16.113636 \t Current Learning Rate: 0.004\n",
            "Iteration 18500: Current success : 91 \t All success : 176 \t mean(l0) : 16.113636 \t Current Learning Rate: 0.004\n",
            "Iteration 18600: Current success : 83 \t All success : 176 \t mean(l0) : 16.113636 \t Current Learning Rate: 0.004\n",
            "Iteration 18700: Current success : 89 \t All success : 176 \t mean(l0) : 16.113636 \t Current Learning Rate: 0.003\n",
            "Iteration 18800: Current success : 83 \t All success : 176 \t mean(l0) : 16.113636 \t Current Learning Rate: 0.003\n",
            "Iteration 18900: Current success : 89 \t All success : 176 \t mean(l0) : 16.113636 \t Current Learning Rate: 0.003\n",
            "Iteration 19000: Current success : 83 \t All success : 176 \t mean(l0) : 16.113636 \t Current Learning Rate: 0.003\n",
            "Iteration 19100: Current success : 83 \t All success : 176 \t mean(l0) : 16.113636 \t Current Learning Rate: 0.002\n",
            "Iteration 19200: Current success : 83 \t All success : 176 \t mean(l0) : 16.113636 \t Current Learning Rate: 0.002\n",
            "Iteration 19300: Current success : 83 \t All success : 176 \t mean(l0) : 16.113636 \t Current Learning Rate: 0.002\n",
            "Iteration 19400: Current success : 83 \t All success : 176 \t mean(l0) : 16.113636 \t Current Learning Rate: 0.002\n",
            "Iteration 19500: Current success : 83 \t All success : 176 \t mean(l0) : 16.113636 \t Current Learning Rate: 0.001\n",
            "Iteration 19600: Current success : 89 \t All success : 176 \t mean(l0) : 16.113636 \t Current Learning Rate: 0.001\n",
            "Iteration 19700: Current success : 83 \t All success : 176 \t mean(l0) : 16.113636 \t Current Learning Rate: 0.001\n",
            "Iteration 19800: Current success : 89 \t All success : 176 \t mean(l0) : 16.113636 \t Current Learning Rate: 0.001\n",
            "Iteration 19900: Current success : 83 \t All success : 176 \t mean(l0) : 16.113636 \t Current Learning Rate: 0.000\n",
            "Iteration 20000: Current success : 83 \t All success : 176 \t mean(l0) : 16.113636 \t Current Learning Rate: 0.000\n",
            "outer_step 4: all success : 248 \t mean(l0)(success) : 16.4113\n",
            "-------------------------------------------------------------\n",
            "tensor([4.3750e-01, 1.0000e+04, 1.0000e+04,  ..., 1.0000e+04, 1.0000e+04,\n",
            "        1.0000e+04], device='cuda:0')\n",
            "Iteration 100: Current success : 52 \t All success : 85 \t mean(l0) : 7.741177 \t Current Learning Rate: 0.098\n",
            "Iteration 200: Current success : 68 \t All success : 102 \t mean(l0) : 9.450981 \t Current Learning Rate: 0.096\n",
            "Iteration 300: Current success : 70 \t All success : 112 \t mean(l0) : 11.232143 \t Current Learning Rate: 0.094\n",
            "Iteration 400: Current success : 64 \t All success : 119 \t mean(l0) : 11.327732 \t Current Learning Rate: 0.092\n",
            "Iteration 500: Current success : 70 \t All success : 129 \t mean(l0) : 11.093023 \t Current Learning Rate: 0.090\n",
            "Iteration 600: Current success : 74 \t All success : 135 \t mean(l0) : 12.007407 \t Current Learning Rate: 0.088\n",
            "Iteration 700: Current success : 72 \t All success : 137 \t mean(l0) : 12.218978 \t Current Learning Rate: 0.086\n",
            "Iteration 800: Current success : 82 \t All success : 138 \t mean(l0) : 12.282609 \t Current Learning Rate: 0.084\n",
            "Iteration 900: Current success : 82 \t All success : 138 \t mean(l0) : 12.282609 \t Current Learning Rate: 0.082\n",
            "Iteration 1000: Current success : 83 \t All success : 143 \t mean(l0) : 12.265734 \t Current Learning Rate: 0.080\n",
            "Iteration 1100: Current success : 85 \t All success : 144 \t mean(l0) : 12.291667 \t Current Learning Rate: 0.078\n",
            "Iteration 1200: Current success : 82 \t All success : 145 \t mean(l0) : 12.248276 \t Current Learning Rate: 0.076\n",
            "Iteration 1300: Current success : 83 \t All success : 147 \t mean(l0) : 12.625850 \t Current Learning Rate: 0.074\n",
            "Iteration 1400: Current success : 83 \t All success : 147 \t mean(l0) : 12.625850 \t Current Learning Rate: 0.072\n",
            "Iteration 1500: Current success : 86 \t All success : 147 \t mean(l0) : 12.625850 \t Current Learning Rate: 0.070\n",
            "Iteration 1600: Current success : 88 \t All success : 147 \t mean(l0) : 12.625850 \t Current Learning Rate: 0.068\n",
            "Iteration 1700: Current success : 87 \t All success : 147 \t mean(l0) : 12.605442 \t Current Learning Rate: 0.066\n",
            "Iteration 1800: Current success : 85 \t All success : 147 \t mean(l0) : 12.605442 \t Current Learning Rate: 0.064\n",
            "Iteration 1900: Current success : 86 \t All success : 147 \t mean(l0) : 12.605442 \t Current Learning Rate: 0.062\n",
            "Iteration 2000: Current success : 88 \t All success : 147 \t mean(l0) : 12.605442 \t Current Learning Rate: 0.060\n",
            "Iteration 2100: Current success : 88 \t All success : 147 \t mean(l0) : 12.605442 \t Current Learning Rate: 0.058\n",
            "Iteration 2200: Current success : 85 \t All success : 147 \t mean(l0) : 12.605442 \t Current Learning Rate: 0.056\n",
            "Iteration 2300: Current success : 86 \t All success : 147 \t mean(l0) : 12.605442 \t Current Learning Rate: 0.054\n",
            "Iteration 2400: Current success : 87 \t All success : 147 \t mean(l0) : 12.605442 \t Current Learning Rate: 0.052\n",
            "Iteration 2500: Current success : 87 \t All success : 147 \t mean(l0) : 12.591837 \t Current Learning Rate: 0.050\n",
            "Iteration 2600: Current success : 86 \t All success : 147 \t mean(l0) : 12.591837 \t Current Learning Rate: 0.048\n",
            "Iteration 2700: Current success : 85 \t All success : 147 \t mean(l0) : 12.591837 \t Current Learning Rate: 0.046\n",
            "Iteration 2800: Current success : 89 \t All success : 147 \t mean(l0) : 12.591837 \t Current Learning Rate: 0.044\n",
            "Iteration 2900: Current success : 85 \t All success : 147 \t mean(l0) : 12.591837 \t Current Learning Rate: 0.042\n",
            "Iteration 3000: Current success : 85 \t All success : 147 \t mean(l0) : 12.591837 \t Current Learning Rate: 0.040\n",
            "Iteration 3100: Current success : 85 \t All success : 147 \t mean(l0) : 12.591837 \t Current Learning Rate: 0.038\n",
            "Iteration 3200: Current success : 87 \t All success : 147 \t mean(l0) : 12.591837 \t Current Learning Rate: 0.036\n",
            "Iteration 3300: Current success : 86 \t All success : 147 \t mean(l0) : 12.591837 \t Current Learning Rate: 0.034\n",
            "Iteration 3400: Current success : 87 \t All success : 147 \t mean(l0) : 12.591837 \t Current Learning Rate: 0.032\n",
            "Iteration 3500: Current success : 86 \t All success : 147 \t mean(l0) : 12.591837 \t Current Learning Rate: 0.030\n",
            "Iteration 3600: Current success : 87 \t All success : 147 \t mean(l0) : 12.591837 \t Current Learning Rate: 0.028\n",
            "Iteration 3700: Current success : 86 \t All success : 147 \t mean(l0) : 12.591837 \t Current Learning Rate: 0.026\n",
            "Iteration 3800: Current success : 87 \t All success : 147 \t mean(l0) : 12.591837 \t Current Learning Rate: 0.024\n",
            "Iteration 3900: Current success : 87 \t All success : 147 \t mean(l0) : 12.591837 \t Current Learning Rate: 0.022\n",
            "Iteration 4000: Current success : 86 \t All success : 147 \t mean(l0) : 12.591837 \t Current Learning Rate: 0.020\n",
            "Iteration 4100: Current success : 86 \t All success : 147 \t mean(l0) : 12.591837 \t Current Learning Rate: 0.018\n",
            "Iteration 4200: Current success : 85 \t All success : 147 \t mean(l0) : 12.591837 \t Current Learning Rate: 0.016\n",
            "Iteration 4300: Current success : 85 \t All success : 147 \t mean(l0) : 12.591837 \t Current Learning Rate: 0.014\n",
            "Iteration 4400: Current success : 84 \t All success : 147 \t mean(l0) : 12.591837 \t Current Learning Rate: 0.012\n",
            "Iteration 4500: Current success : 85 \t All success : 147 \t mean(l0) : 12.591837 \t Current Learning Rate: 0.010\n",
            "Iteration 4600: Current success : 85 \t All success : 147 \t mean(l0) : 12.591837 \t Current Learning Rate: 0.008\n",
            "Iteration 4700: Current success : 85 \t All success : 147 \t mean(l0) : 12.591837 \t Current Learning Rate: 0.006\n",
            "Iteration 4800: Current success : 85 \t All success : 147 \t mean(l0) : 12.591837 \t Current Learning Rate: 0.004\n",
            "Iteration 4900: Current success : 85 \t All success : 147 \t mean(l0) : 12.591837 \t Current Learning Rate: 0.002\n",
            "Iteration 5000: Current success : 85 \t All success : 147 \t mean(l0) : 12.591837 \t Current Learning Rate: 0.000\n",
            "Iteration 5100: Current success : 72 \t All success : 157 \t mean(l0) : 13.222930 \t Current Learning Rate: 0.049\n",
            "Iteration 5200: Current success : 72 \t All success : 159 \t mean(l0) : 13.559748 \t Current Learning Rate: 0.048\n",
            "Iteration 5300: Current success : 79 \t All success : 159 \t mean(l0) : 13.547170 \t Current Learning Rate: 0.047\n",
            "Iteration 5400: Current success : 87 \t All success : 159 \t mean(l0) : 13.547170 \t Current Learning Rate: 0.046\n",
            "Iteration 5500: Current success : 85 \t All success : 159 \t mean(l0) : 13.547170 \t Current Learning Rate: 0.045\n",
            "Iteration 5600: Current success : 84 \t All success : 159 \t mean(l0) : 13.547170 \t Current Learning Rate: 0.044\n",
            "Iteration 5700: Current success : 86 \t All success : 160 \t mean(l0) : 13.731250 \t Current Learning Rate: 0.043\n",
            "Iteration 5800: Current success : 85 \t All success : 160 \t mean(l0) : 13.731250 \t Current Learning Rate: 0.042\n",
            "Iteration 5900: Current success : 86 \t All success : 160 \t mean(l0) : 13.731250 \t Current Learning Rate: 0.041\n",
            "Iteration 6000: Current success : 86 \t All success : 160 \t mean(l0) : 13.731250 \t Current Learning Rate: 0.040\n",
            "Iteration 6100: Current success : 86 \t All success : 160 \t mean(l0) : 13.731250 \t Current Learning Rate: 0.039\n",
            "Iteration 6200: Current success : 86 \t All success : 160 \t mean(l0) : 13.731250 \t Current Learning Rate: 0.038\n",
            "Iteration 6300: Current success : 88 \t All success : 160 \t mean(l0) : 13.731250 \t Current Learning Rate: 0.037\n",
            "Iteration 6400: Current success : 87 \t All success : 160 \t mean(l0) : 13.731250 \t Current Learning Rate: 0.036\n",
            "Iteration 6500: Current success : 86 \t All success : 160 \t mean(l0) : 13.731250 \t Current Learning Rate: 0.035\n",
            "Iteration 6600: Current success : 86 \t All success : 160 \t mean(l0) : 13.731250 \t Current Learning Rate: 0.034\n",
            "Iteration 6700: Current success : 87 \t All success : 160 \t mean(l0) : 13.731250 \t Current Learning Rate: 0.033\n",
            "Iteration 6800: Current success : 88 \t All success : 160 \t mean(l0) : 13.731250 \t Current Learning Rate: 0.032\n",
            "Iteration 6900: Current success : 87 \t All success : 160 \t mean(l0) : 13.731250 \t Current Learning Rate: 0.031\n",
            "Iteration 7000: Current success : 87 \t All success : 160 \t mean(l0) : 13.731250 \t Current Learning Rate: 0.030\n",
            "Iteration 7100: Current success : 86 \t All success : 160 \t mean(l0) : 13.731250 \t Current Learning Rate: 0.029\n",
            "Iteration 7200: Current success : 85 \t All success : 160 \t mean(l0) : 13.731250 \t Current Learning Rate: 0.028\n",
            "Iteration 7300: Current success : 86 \t All success : 160 \t mean(l0) : 13.731250 \t Current Learning Rate: 0.027\n",
            "Iteration 7400: Current success : 88 \t All success : 160 \t mean(l0) : 13.731250 \t Current Learning Rate: 0.026\n",
            "Iteration 7500: Current success : 86 \t All success : 160 \t mean(l0) : 13.731250 \t Current Learning Rate: 0.025\n",
            "Iteration 7600: Current success : 88 \t All success : 160 \t mean(l0) : 13.731250 \t Current Learning Rate: 0.024\n",
            "Iteration 7700: Current success : 85 \t All success : 160 \t mean(l0) : 13.731250 \t Current Learning Rate: 0.023\n",
            "Iteration 7800: Current success : 88 \t All success : 160 \t mean(l0) : 13.731250 \t Current Learning Rate: 0.022\n",
            "Iteration 7900: Current success : 88 \t All success : 160 \t mean(l0) : 13.731250 \t Current Learning Rate: 0.021\n",
            "Iteration 8000: Current success : 87 \t All success : 160 \t mean(l0) : 13.731250 \t Current Learning Rate: 0.020\n",
            "Iteration 8100: Current success : 88 \t All success : 160 \t mean(l0) : 13.731250 \t Current Learning Rate: 0.019\n",
            "Iteration 8200: Current success : 88 \t All success : 160 \t mean(l0) : 13.731250 \t Current Learning Rate: 0.018\n",
            "Iteration 8300: Current success : 88 \t All success : 160 \t mean(l0) : 13.731250 \t Current Learning Rate: 0.017\n",
            "Iteration 8400: Current success : 88 \t All success : 160 \t mean(l0) : 13.731250 \t Current Learning Rate: 0.016\n",
            "Iteration 8500: Current success : 88 \t All success : 160 \t mean(l0) : 13.731250 \t Current Learning Rate: 0.015\n",
            "Iteration 8600: Current success : 88 \t All success : 160 \t mean(l0) : 13.731250 \t Current Learning Rate: 0.014\n",
            "Iteration 8700: Current success : 88 \t All success : 160 \t mean(l0) : 13.731250 \t Current Learning Rate: 0.013\n",
            "Iteration 8800: Current success : 87 \t All success : 160 \t mean(l0) : 13.731250 \t Current Learning Rate: 0.012\n",
            "Iteration 8900: Current success : 87 \t All success : 160 \t mean(l0) : 13.731250 \t Current Learning Rate: 0.011\n",
            "Iteration 9000: Current success : 87 \t All success : 160 \t mean(l0) : 13.731250 \t Current Learning Rate: 0.010\n",
            "Iteration 9100: Current success : 87 \t All success : 160 \t mean(l0) : 13.731250 \t Current Learning Rate: 0.009\n",
            "Iteration 9200: Current success : 87 \t All success : 160 \t mean(l0) : 13.731250 \t Current Learning Rate: 0.008\n",
            "Iteration 9300: Current success : 87 \t All success : 160 \t mean(l0) : 13.731250 \t Current Learning Rate: 0.007\n",
            "Iteration 9400: Current success : 88 \t All success : 160 \t mean(l0) : 13.731250 \t Current Learning Rate: 0.006\n",
            "Iteration 9500: Current success : 88 \t All success : 160 \t mean(l0) : 13.731250 \t Current Learning Rate: 0.005\n",
            "Iteration 9600: Current success : 88 \t All success : 160 \t mean(l0) : 13.731250 \t Current Learning Rate: 0.004\n",
            "Iteration 9700: Current success : 88 \t All success : 160 \t mean(l0) : 13.731250 \t Current Learning Rate: 0.003\n",
            "Iteration 9800: Current success : 88 \t All success : 160 \t mean(l0) : 13.731250 \t Current Learning Rate: 0.002\n",
            "Iteration 9900: Current success : 88 \t All success : 160 \t mean(l0) : 13.731250 \t Current Learning Rate: 0.001\n",
            "Iteration 10000: Current success : 88 \t All success : 160 \t mean(l0) : 13.731250 \t Current Learning Rate: 0.000\n",
            "Iteration 10100: Current success : 79 \t All success : 161 \t mean(l0) : 13.658385 \t Current Learning Rate: 0.025\n",
            "Iteration 10200: Current success : 77 \t All success : 162 \t mean(l0) : 13.629630 \t Current Learning Rate: 0.024\n",
            "Iteration 10300: Current success : 85 \t All success : 162 \t mean(l0) : 13.623457 \t Current Learning Rate: 0.024\n",
            "Iteration 10400: Current success : 86 \t All success : 162 \t mean(l0) : 13.623457 \t Current Learning Rate: 0.023\n",
            "Iteration 10500: Current success : 86 \t All success : 162 \t mean(l0) : 13.623457 \t Current Learning Rate: 0.023\n",
            "Iteration 10600: Current success : 85 \t All success : 162 \t mean(l0) : 13.623457 \t Current Learning Rate: 0.022\n",
            "Iteration 10700: Current success : 86 \t All success : 162 \t mean(l0) : 13.623457 \t Current Learning Rate: 0.022\n",
            "Iteration 10800: Current success : 85 \t All success : 162 \t mean(l0) : 13.623457 \t Current Learning Rate: 0.021\n",
            "Iteration 10900: Current success : 86 \t All success : 162 \t mean(l0) : 13.623457 \t Current Learning Rate: 0.021\n",
            "Iteration 11000: Current success : 87 \t All success : 162 \t mean(l0) : 13.623457 \t Current Learning Rate: 0.020\n",
            "Iteration 11100: Current success : 84 \t All success : 162 \t mean(l0) : 13.623457 \t Current Learning Rate: 0.020\n",
            "Iteration 11200: Current success : 86 \t All success : 162 \t mean(l0) : 13.623457 \t Current Learning Rate: 0.019\n",
            "Iteration 11300: Current success : 87 \t All success : 162 \t mean(l0) : 13.623457 \t Current Learning Rate: 0.019\n",
            "Iteration 11400: Current success : 85 \t All success : 162 \t mean(l0) : 13.623457 \t Current Learning Rate: 0.018\n",
            "Iteration 11500: Current success : 85 \t All success : 162 \t mean(l0) : 13.623457 \t Current Learning Rate: 0.018\n",
            "Iteration 11600: Current success : 86 \t All success : 162 \t mean(l0) : 13.623457 \t Current Learning Rate: 0.017\n",
            "Iteration 11700: Current success : 86 \t All success : 162 \t mean(l0) : 13.623457 \t Current Learning Rate: 0.017\n",
            "Iteration 11800: Current success : 86 \t All success : 162 \t mean(l0) : 13.623457 \t Current Learning Rate: 0.016\n",
            "Iteration 11900: Current success : 86 \t All success : 162 \t mean(l0) : 13.623457 \t Current Learning Rate: 0.016\n",
            "Iteration 12000: Current success : 87 \t All success : 162 \t mean(l0) : 13.623457 \t Current Learning Rate: 0.015\n",
            "Iteration 12100: Current success : 85 \t All success : 162 \t mean(l0) : 13.623457 \t Current Learning Rate: 0.015\n",
            "Iteration 12200: Current success : 86 \t All success : 162 \t mean(l0) : 13.623457 \t Current Learning Rate: 0.014\n",
            "Iteration 12300: Current success : 87 \t All success : 162 \t mean(l0) : 13.623457 \t Current Learning Rate: 0.014\n",
            "Iteration 12400: Current success : 86 \t All success : 162 \t mean(l0) : 13.623457 \t Current Learning Rate: 0.013\n",
            "Iteration 12500: Current success : 85 \t All success : 162 \t mean(l0) : 13.623457 \t Current Learning Rate: 0.013\n",
            "Iteration 12600: Current success : 87 \t All success : 162 \t mean(l0) : 13.623457 \t Current Learning Rate: 0.012\n",
            "Iteration 12700: Current success : 87 \t All success : 162 \t mean(l0) : 13.623457 \t Current Learning Rate: 0.012\n",
            "Iteration 12800: Current success : 86 \t All success : 162 \t mean(l0) : 13.623457 \t Current Learning Rate: 0.011\n",
            "Iteration 12900: Current success : 86 \t All success : 162 \t mean(l0) : 13.623457 \t Current Learning Rate: 0.011\n",
            "Iteration 13000: Current success : 87 \t All success : 162 \t mean(l0) : 13.623457 \t Current Learning Rate: 0.010\n",
            "Iteration 13100: Current success : 86 \t All success : 162 \t mean(l0) : 13.623457 \t Current Learning Rate: 0.010\n",
            "Iteration 13200: Current success : 86 \t All success : 162 \t mean(l0) : 13.623457 \t Current Learning Rate: 0.009\n",
            "Iteration 13300: Current success : 87 \t All success : 162 \t mean(l0) : 13.623457 \t Current Learning Rate: 0.009\n",
            "Iteration 13400: Current success : 87 \t All success : 162 \t mean(l0) : 13.623457 \t Current Learning Rate: 0.008\n",
            "Iteration 13500: Current success : 86 \t All success : 162 \t mean(l0) : 13.623457 \t Current Learning Rate: 0.008\n",
            "Iteration 13600: Current success : 87 \t All success : 162 \t mean(l0) : 13.623457 \t Current Learning Rate: 0.007\n",
            "Iteration 13700: Current success : 87 \t All success : 162 \t mean(l0) : 13.623457 \t Current Learning Rate: 0.007\n",
            "Iteration 13800: Current success : 86 \t All success : 162 \t mean(l0) : 13.623457 \t Current Learning Rate: 0.006\n",
            "Iteration 13900: Current success : 87 \t All success : 162 \t mean(l0) : 13.623457 \t Current Learning Rate: 0.006\n",
            "Iteration 14000: Current success : 87 \t All success : 162 \t mean(l0) : 13.623457 \t Current Learning Rate: 0.005\n",
            "Iteration 14100: Current success : 87 \t All success : 162 \t mean(l0) : 13.623457 \t Current Learning Rate: 0.005\n",
            "Iteration 14200: Current success : 86 \t All success : 162 \t mean(l0) : 13.623457 \t Current Learning Rate: 0.004\n",
            "Iteration 14300: Current success : 87 \t All success : 162 \t mean(l0) : 13.623457 \t Current Learning Rate: 0.004\n",
            "Iteration 14400: Current success : 87 \t All success : 162 \t mean(l0) : 13.623457 \t Current Learning Rate: 0.003\n",
            "Iteration 14500: Current success : 87 \t All success : 162 \t mean(l0) : 13.623457 \t Current Learning Rate: 0.003\n",
            "Iteration 14600: Current success : 87 \t All success : 162 \t mean(l0) : 13.623457 \t Current Learning Rate: 0.002\n",
            "Iteration 14700: Current success : 87 \t All success : 162 \t mean(l0) : 13.623457 \t Current Learning Rate: 0.002\n",
            "Iteration 14800: Current success : 87 \t All success : 162 \t mean(l0) : 13.623457 \t Current Learning Rate: 0.001\n",
            "Iteration 14900: Current success : 87 \t All success : 162 \t mean(l0) : 13.623457 \t Current Learning Rate: 0.001\n",
            "Iteration 15000: Current success : 87 \t All success : 162 \t mean(l0) : 13.623457 \t Current Learning Rate: 0.000\n",
            "Iteration 15100: Current success : 86 \t All success : 163 \t mean(l0) : 13.926380 \t Current Learning Rate: 0.012\n",
            "Iteration 15200: Current success : 82 \t All success : 166 \t mean(l0) : 14.740963 \t Current Learning Rate: 0.012\n",
            "Iteration 15300: Current success : 88 \t All success : 166 \t mean(l0) : 14.734940 \t Current Learning Rate: 0.012\n",
            "Iteration 15400: Current success : 94 \t All success : 166 \t mean(l0) : 14.734940 \t Current Learning Rate: 0.012\n",
            "Iteration 15500: Current success : 92 \t All success : 166 \t mean(l0) : 14.734940 \t Current Learning Rate: 0.011\n",
            "Iteration 15600: Current success : 91 \t All success : 166 \t mean(l0) : 14.734940 \t Current Learning Rate: 0.011\n",
            "Iteration 15700: Current success : 92 \t All success : 166 \t mean(l0) : 14.722891 \t Current Learning Rate: 0.011\n",
            "Iteration 15800: Current success : 92 \t All success : 166 \t mean(l0) : 14.716866 \t Current Learning Rate: 0.011\n",
            "Iteration 15900: Current success : 91 \t All success : 166 \t mean(l0) : 14.716866 \t Current Learning Rate: 0.010\n",
            "Iteration 16000: Current success : 91 \t All success : 166 \t mean(l0) : 14.716866 \t Current Learning Rate: 0.010\n",
            "Iteration 16100: Current success : 92 \t All success : 166 \t mean(l0) : 14.716866 \t Current Learning Rate: 0.010\n",
            "Iteration 16200: Current success : 93 \t All success : 166 \t mean(l0) : 14.716866 \t Current Learning Rate: 0.010\n",
            "Iteration 16300: Current success : 93 \t All success : 166 \t mean(l0) : 14.716866 \t Current Learning Rate: 0.009\n",
            "Iteration 16400: Current success : 92 \t All success : 166 \t mean(l0) : 14.716866 \t Current Learning Rate: 0.009\n",
            "Iteration 16500: Current success : 92 \t All success : 166 \t mean(l0) : 14.716866 \t Current Learning Rate: 0.009\n",
            "Iteration 16600: Current success : 93 \t All success : 166 \t mean(l0) : 14.716866 \t Current Learning Rate: 0.009\n",
            "Iteration 16700: Current success : 92 \t All success : 166 \t mean(l0) : 14.716866 \t Current Learning Rate: 0.008\n",
            "Iteration 16800: Current success : 90 \t All success : 166 \t mean(l0) : 14.716866 \t Current Learning Rate: 0.008\n",
            "Iteration 16900: Current success : 91 \t All success : 166 \t mean(l0) : 14.716866 \t Current Learning Rate: 0.008\n",
            "Iteration 17000: Current success : 91 \t All success : 166 \t mean(l0) : 14.716866 \t Current Learning Rate: 0.008\n",
            "Iteration 17100: Current success : 92 \t All success : 166 \t mean(l0) : 14.716866 \t Current Learning Rate: 0.007\n",
            "Iteration 17200: Current success : 92 \t All success : 166 \t mean(l0) : 14.716866 \t Current Learning Rate: 0.007\n",
            "Iteration 17300: Current success : 92 \t All success : 166 \t mean(l0) : 14.716866 \t Current Learning Rate: 0.007\n",
            "Iteration 17400: Current success : 91 \t All success : 166 \t mean(l0) : 14.716866 \t Current Learning Rate: 0.007\n",
            "Iteration 17500: Current success : 92 \t All success : 166 \t mean(l0) : 14.716866 \t Current Learning Rate: 0.006\n",
            "Iteration 17600: Current success : 92 \t All success : 166 \t mean(l0) : 14.716866 \t Current Learning Rate: 0.006\n",
            "Iteration 17700: Current success : 92 \t All success : 166 \t mean(l0) : 14.716866 \t Current Learning Rate: 0.006\n",
            "Iteration 17800: Current success : 91 \t All success : 166 \t mean(l0) : 14.716866 \t Current Learning Rate: 0.006\n",
            "Iteration 17900: Current success : 92 \t All success : 166 \t mean(l0) : 14.716866 \t Current Learning Rate: 0.005\n",
            "Iteration 18000: Current success : 92 \t All success : 166 \t mean(l0) : 14.716866 \t Current Learning Rate: 0.005\n",
            "Iteration 18100: Current success : 91 \t All success : 166 \t mean(l0) : 14.716866 \t Current Learning Rate: 0.005\n",
            "Iteration 18200: Current success : 91 \t All success : 166 \t mean(l0) : 14.716866 \t Current Learning Rate: 0.005\n",
            "Iteration 18300: Current success : 91 \t All success : 166 \t mean(l0) : 14.716866 \t Current Learning Rate: 0.004\n",
            "Iteration 18400: Current success : 92 \t All success : 166 \t mean(l0) : 14.716866 \t Current Learning Rate: 0.004\n",
            "Iteration 18500: Current success : 92 \t All success : 166 \t mean(l0) : 14.716866 \t Current Learning Rate: 0.004\n",
            "Iteration 18600: Current success : 92 \t All success : 166 \t mean(l0) : 14.716866 \t Current Learning Rate: 0.004\n",
            "Iteration 18700: Current success : 92 \t All success : 166 \t mean(l0) : 14.716866 \t Current Learning Rate: 0.003\n",
            "Iteration 18800: Current success : 92 \t All success : 166 \t mean(l0) : 14.716866 \t Current Learning Rate: 0.003\n",
            "Iteration 18900: Current success : 92 \t All success : 166 \t mean(l0) : 14.716866 \t Current Learning Rate: 0.003\n",
            "Iteration 19000: Current success : 92 \t All success : 166 \t mean(l0) : 14.716866 \t Current Learning Rate: 0.003\n",
            "Iteration 19100: Current success : 92 \t All success : 166 \t mean(l0) : 14.716866 \t Current Learning Rate: 0.002\n",
            "Iteration 19200: Current success : 92 \t All success : 166 \t mean(l0) : 14.716866 \t Current Learning Rate: 0.002\n",
            "Iteration 19300: Current success : 92 \t All success : 166 \t mean(l0) : 14.716866 \t Current Learning Rate: 0.002\n",
            "Iteration 19400: Current success : 92 \t All success : 166 \t mean(l0) : 14.716866 \t Current Learning Rate: 0.002\n",
            "Iteration 19500: Current success : 92 \t All success : 166 \t mean(l0) : 14.716866 \t Current Learning Rate: 0.001\n",
            "Iteration 19600: Current success : 92 \t All success : 166 \t mean(l0) : 14.716866 \t Current Learning Rate: 0.001\n",
            "Iteration 19700: Current success : 92 \t All success : 166 \t mean(l0) : 14.716866 \t Current Learning Rate: 0.001\n",
            "Iteration 19800: Current success : 92 \t All success : 166 \t mean(l0) : 14.716866 \t Current Learning Rate: 0.001\n",
            "Iteration 19900: Current success : 92 \t All success : 166 \t mean(l0) : 14.716866 \t Current Learning Rate: 0.000\n",
            "Iteration 20000: Current success : 92 \t All success : 166 \t mean(l0) : 14.716866 \t Current Learning Rate: 0.000\n",
            "outer_step 5: all success : 252 \t mean(l0)(success) : 16.1349\n",
            "-------------------------------------------------------------\n",
            "tensor([4.6875e-01, 1.0000e+05, 1.0000e+05,  ..., 1.0000e+05, 1.0000e+05,\n",
            "        1.0000e+05], device='cuda:0')\n",
            "Iteration 100: Current success : 77 \t All success : 112 \t mean(l0) : 6.714286 \t Current Learning Rate: 0.098\n",
            "Iteration 200: Current success : 77 \t All success : 120 \t mean(l0) : 7.075000 \t Current Learning Rate: 0.096\n",
            "Iteration 300: Current success : 84 \t All success : 133 \t mean(l0) : 9.000000 \t Current Learning Rate: 0.094\n",
            "Iteration 400: Current success : 58 \t All success : 142 \t mean(l0) : 10.119719 \t Current Learning Rate: 0.092\n",
            "Iteration 500: Current success : 57 \t All success : 148 \t mean(l0) : 10.756757 \t Current Learning Rate: 0.090\n",
            "Iteration 600: Current success : 84 \t All success : 149 \t mean(l0) : 10.939597 \t Current Learning Rate: 0.088\n",
            "Iteration 700: Current success : 98 \t All success : 159 \t mean(l0) : 10.968554 \t Current Learning Rate: 0.086\n",
            "Iteration 800: Current success : 90 \t All success : 160 \t mean(l0) : 11.118751 \t Current Learning Rate: 0.084\n",
            "Iteration 900: Current success : 88 \t All success : 161 \t mean(l0) : 11.465838 \t Current Learning Rate: 0.082\n",
            "Iteration 1000: Current success : 98 \t All success : 161 \t mean(l0) : 11.447206 \t Current Learning Rate: 0.080\n",
            "Iteration 1100: Current success : 97 \t All success : 161 \t mean(l0) : 11.447206 \t Current Learning Rate: 0.078\n",
            "Iteration 1200: Current success : 100 \t All success : 161 \t mean(l0) : 11.447206 \t Current Learning Rate: 0.076\n",
            "Iteration 1300: Current success : 103 \t All success : 162 \t mean(l0) : 11.691359 \t Current Learning Rate: 0.074\n",
            "Iteration 1400: Current success : 99 \t All success : 164 \t mean(l0) : 12.219512 \t Current Learning Rate: 0.072\n",
            "Iteration 1500: Current success : 103 \t All success : 164 \t mean(l0) : 12.219512 \t Current Learning Rate: 0.070\n",
            "Iteration 1600: Current success : 108 \t All success : 164 \t mean(l0) : 12.219512 \t Current Learning Rate: 0.068\n",
            "Iteration 1700: Current success : 108 \t All success : 164 \t mean(l0) : 12.219512 \t Current Learning Rate: 0.066\n",
            "Iteration 1800: Current success : 109 \t All success : 166 \t mean(l0) : 12.301205 \t Current Learning Rate: 0.064\n",
            "Iteration 1900: Current success : 109 \t All success : 166 \t mean(l0) : 12.301205 \t Current Learning Rate: 0.062\n",
            "Iteration 2000: Current success : 109 \t All success : 167 \t mean(l0) : 12.371258 \t Current Learning Rate: 0.060\n",
            "Iteration 2100: Current success : 108 \t All success : 167 \t mean(l0) : 12.371258 \t Current Learning Rate: 0.058\n",
            "Iteration 2200: Current success : 114 \t All success : 167 \t mean(l0) : 12.335330 \t Current Learning Rate: 0.056\n",
            "Iteration 2300: Current success : 110 \t All success : 167 \t mean(l0) : 12.299401 \t Current Learning Rate: 0.054\n",
            "Iteration 2400: Current success : 110 \t All success : 167 \t mean(l0) : 12.299401 \t Current Learning Rate: 0.052\n",
            "Iteration 2500: Current success : 108 \t All success : 167 \t mean(l0) : 12.281438 \t Current Learning Rate: 0.050\n",
            "Iteration 2600: Current success : 112 \t All success : 167 \t mean(l0) : 12.245509 \t Current Learning Rate: 0.048\n",
            "Iteration 2700: Current success : 111 \t All success : 167 \t mean(l0) : 12.233534 \t Current Learning Rate: 0.046\n",
            "Iteration 2800: Current success : 109 \t All success : 167 \t mean(l0) : 12.227546 \t Current Learning Rate: 0.044\n",
            "Iteration 2900: Current success : 111 \t All success : 167 \t mean(l0) : 12.227546 \t Current Learning Rate: 0.042\n",
            "Iteration 3000: Current success : 109 \t All success : 167 \t mean(l0) : 12.227546 \t Current Learning Rate: 0.040\n",
            "Iteration 3100: Current success : 113 \t All success : 168 \t mean(l0) : 12.666667 \t Current Learning Rate: 0.038\n",
            "Iteration 3200: Current success : 112 \t All success : 168 \t mean(l0) : 12.660714 \t Current Learning Rate: 0.036\n",
            "Iteration 3300: Current success : 112 \t All success : 168 \t mean(l0) : 12.660714 \t Current Learning Rate: 0.034\n",
            "Iteration 3400: Current success : 112 \t All success : 168 \t mean(l0) : 12.660714 \t Current Learning Rate: 0.032\n",
            "Iteration 3500: Current success : 112 \t All success : 168 \t mean(l0) : 12.660714 \t Current Learning Rate: 0.030\n",
            "Iteration 3600: Current success : 112 \t All success : 168 \t mean(l0) : 12.660714 \t Current Learning Rate: 0.028\n",
            "Iteration 3700: Current success : 111 \t All success : 168 \t mean(l0) : 12.660714 \t Current Learning Rate: 0.026\n",
            "Iteration 3800: Current success : 109 \t All success : 168 \t mean(l0) : 12.660714 \t Current Learning Rate: 0.024\n",
            "Iteration 3900: Current success : 113 \t All success : 168 \t mean(l0) : 12.660714 \t Current Learning Rate: 0.022\n",
            "Iteration 4000: Current success : 112 \t All success : 168 \t mean(l0) : 12.660714 \t Current Learning Rate: 0.020\n",
            "Iteration 4100: Current success : 113 \t All success : 168 \t mean(l0) : 12.660714 \t Current Learning Rate: 0.018\n",
            "Iteration 4200: Current success : 113 \t All success : 168 \t mean(l0) : 12.660714 \t Current Learning Rate: 0.016\n",
            "Iteration 4300: Current success : 113 \t All success : 168 \t mean(l0) : 12.660714 \t Current Learning Rate: 0.014\n",
            "Iteration 4400: Current success : 112 \t All success : 168 \t mean(l0) : 12.660714 \t Current Learning Rate: 0.012\n",
            "Iteration 4500: Current success : 113 \t All success : 168 \t mean(l0) : 12.660714 \t Current Learning Rate: 0.010\n",
            "Iteration 4600: Current success : 113 \t All success : 168 \t mean(l0) : 12.660714 \t Current Learning Rate: 0.008\n",
            "Iteration 4700: Current success : 113 \t All success : 168 \t mean(l0) : 12.660714 \t Current Learning Rate: 0.006\n",
            "Iteration 4800: Current success : 113 \t All success : 168 \t mean(l0) : 12.660714 \t Current Learning Rate: 0.004\n",
            "Iteration 4900: Current success : 113 \t All success : 168 \t mean(l0) : 12.660714 \t Current Learning Rate: 0.002\n",
            "Iteration 5000: Current success : 113 \t All success : 168 \t mean(l0) : 12.660714 \t Current Learning Rate: 0.000\n",
            "Iteration 5100: Current success : 76 \t All success : 174 \t mean(l0) : 12.885057 \t Current Learning Rate: 0.049\n",
            "Iteration 5200: Current success : 72 \t All success : 174 \t mean(l0) : 12.885057 \t Current Learning Rate: 0.048\n",
            "Iteration 5300: Current success : 78 \t All success : 175 \t mean(l0) : 13.125714 \t Current Learning Rate: 0.047\n",
            "Iteration 5400: Current success : 80 \t All success : 175 \t mean(l0) : 13.102857 \t Current Learning Rate: 0.046\n",
            "Iteration 5500: Current success : 79 \t All success : 175 \t mean(l0) : 13.091429 \t Current Learning Rate: 0.045\n",
            "Iteration 5600: Current success : 79 \t All success : 175 \t mean(l0) : 13.062857 \t Current Learning Rate: 0.044\n",
            "Iteration 5700: Current success : 81 \t All success : 176 \t mean(l0) : 13.073864 \t Current Learning Rate: 0.043\n",
            "Iteration 5800: Current success : 80 \t All success : 176 \t mean(l0) : 13.073864 \t Current Learning Rate: 0.042\n",
            "Iteration 5900: Current success : 78 \t All success : 176 \t mean(l0) : 13.073864 \t Current Learning Rate: 0.041\n",
            "Iteration 6000: Current success : 78 \t All success : 176 \t mean(l0) : 13.039773 \t Current Learning Rate: 0.040\n",
            "Iteration 6100: Current success : 79 \t All success : 176 \t mean(l0) : 13.039773 \t Current Learning Rate: 0.039\n",
            "Iteration 6200: Current success : 79 \t All success : 176 \t mean(l0) : 13.039773 \t Current Learning Rate: 0.038\n",
            "Iteration 6300: Current success : 79 \t All success : 176 \t mean(l0) : 13.039773 \t Current Learning Rate: 0.037\n",
            "Iteration 6400: Current success : 80 \t All success : 176 \t mean(l0) : 13.034091 \t Current Learning Rate: 0.036\n",
            "Iteration 6500: Current success : 78 \t All success : 176 \t mean(l0) : 13.028409 \t Current Learning Rate: 0.035\n",
            "Iteration 6600: Current success : 79 \t All success : 176 \t mean(l0) : 13.028409 \t Current Learning Rate: 0.034\n",
            "Iteration 6700: Current success : 78 \t All success : 176 \t mean(l0) : 13.028409 \t Current Learning Rate: 0.033\n",
            "Iteration 6800: Current success : 81 \t All success : 176 \t mean(l0) : 13.028409 \t Current Learning Rate: 0.032\n",
            "Iteration 6900: Current success : 78 \t All success : 176 \t mean(l0) : 13.028409 \t Current Learning Rate: 0.031\n",
            "Iteration 7000: Current success : 79 \t All success : 176 \t mean(l0) : 13.028409 \t Current Learning Rate: 0.030\n",
            "Iteration 7100: Current success : 81 \t All success : 178 \t mean(l0) : 13.028090 \t Current Learning Rate: 0.029\n",
            "Iteration 7200: Current success : 79 \t All success : 178 \t mean(l0) : 13.028090 \t Current Learning Rate: 0.028\n",
            "Iteration 7300: Current success : 79 \t All success : 178 \t mean(l0) : 13.028090 \t Current Learning Rate: 0.027\n",
            "Iteration 7400: Current success : 80 \t All success : 178 \t mean(l0) : 13.028090 \t Current Learning Rate: 0.026\n",
            "Iteration 7500: Current success : 80 \t All success : 178 \t mean(l0) : 13.028090 \t Current Learning Rate: 0.025\n",
            "Iteration 7600: Current success : 80 \t All success : 178 \t mean(l0) : 13.028090 \t Current Learning Rate: 0.024\n",
            "Iteration 7700: Current success : 80 \t All success : 178 \t mean(l0) : 13.028090 \t Current Learning Rate: 0.023\n",
            "Iteration 7800: Current success : 79 \t All success : 178 \t mean(l0) : 13.028090 \t Current Learning Rate: 0.022\n",
            "Iteration 7900: Current success : 80 \t All success : 178 \t mean(l0) : 13.028090 \t Current Learning Rate: 0.021\n",
            "Iteration 8000: Current success : 78 \t All success : 178 \t mean(l0) : 13.028090 \t Current Learning Rate: 0.020\n",
            "Iteration 8100: Current success : 80 \t All success : 178 \t mean(l0) : 13.028090 \t Current Learning Rate: 0.019\n",
            "Iteration 8200: Current success : 80 \t All success : 178 \t mean(l0) : 13.028090 \t Current Learning Rate: 0.018\n",
            "Iteration 8300: Current success : 80 \t All success : 178 \t mean(l0) : 13.028090 \t Current Learning Rate: 0.017\n",
            "Iteration 8400: Current success : 80 \t All success : 178 \t mean(l0) : 13.028090 \t Current Learning Rate: 0.016\n",
            "Iteration 8500: Current success : 80 \t All success : 178 \t mean(l0) : 13.028090 \t Current Learning Rate: 0.015\n",
            "Iteration 8600: Current success : 80 \t All success : 178 \t mean(l0) : 13.028090 \t Current Learning Rate: 0.014\n",
            "Iteration 8700: Current success : 80 \t All success : 178 \t mean(l0) : 13.028090 \t Current Learning Rate: 0.013\n",
            "Iteration 8800: Current success : 80 \t All success : 178 \t mean(l0) : 13.028090 \t Current Learning Rate: 0.012\n",
            "Iteration 8900: Current success : 80 \t All success : 178 \t mean(l0) : 13.028090 \t Current Learning Rate: 0.011\n",
            "Iteration 9000: Current success : 80 \t All success : 178 \t mean(l0) : 13.028090 \t Current Learning Rate: 0.010\n",
            "Iteration 9100: Current success : 80 \t All success : 178 \t mean(l0) : 13.028090 \t Current Learning Rate: 0.009\n",
            "Iteration 9200: Current success : 80 \t All success : 178 \t mean(l0) : 13.028090 \t Current Learning Rate: 0.008\n",
            "Iteration 9300: Current success : 80 \t All success : 178 \t mean(l0) : 13.028090 \t Current Learning Rate: 0.007\n",
            "Iteration 9400: Current success : 80 \t All success : 178 \t mean(l0) : 13.028090 \t Current Learning Rate: 0.006\n",
            "Iteration 9500: Current success : 80 \t All success : 178 \t mean(l0) : 13.028090 \t Current Learning Rate: 0.005\n",
            "Iteration 9600: Current success : 80 \t All success : 178 \t mean(l0) : 13.028090 \t Current Learning Rate: 0.004\n",
            "Iteration 9700: Current success : 80 \t All success : 178 \t mean(l0) : 13.028090 \t Current Learning Rate: 0.003\n",
            "Iteration 9800: Current success : 80 \t All success : 178 \t mean(l0) : 13.028090 \t Current Learning Rate: 0.002\n",
            "Iteration 9900: Current success : 80 \t All success : 178 \t mean(l0) : 13.028090 \t Current Learning Rate: 0.001\n",
            "Iteration 10000: Current success : 80 \t All success : 178 \t mean(l0) : 13.028090 \t Current Learning Rate: 0.000\n",
            "Iteration 10100: Current success : 76 \t All success : 181 \t mean(l0) : 13.088398 \t Current Learning Rate: 0.025\n",
            "Iteration 10200: Current success : 75 \t All success : 181 \t mean(l0) : 13.071824 \t Current Learning Rate: 0.024\n",
            "Iteration 10300: Current success : 73 \t All success : 181 \t mean(l0) : 13.027625 \t Current Learning Rate: 0.024\n",
            "Iteration 10400: Current success : 77 \t All success : 182 \t mean(l0) : 12.928572 \t Current Learning Rate: 0.023\n",
            "Iteration 10500: Current success : 77 \t All success : 182 \t mean(l0) : 12.895605 \t Current Learning Rate: 0.023\n",
            "Iteration 10600: Current success : 77 \t All success : 182 \t mean(l0) : 12.895605 \t Current Learning Rate: 0.022\n",
            "Iteration 10700: Current success : 77 \t All success : 182 \t mean(l0) : 12.895605 \t Current Learning Rate: 0.022\n",
            "Iteration 10800: Current success : 76 \t All success : 182 \t mean(l0) : 12.895605 \t Current Learning Rate: 0.021\n",
            "Iteration 10900: Current success : 76 \t All success : 182 \t mean(l0) : 12.895605 \t Current Learning Rate: 0.021\n",
            "Iteration 11000: Current success : 76 \t All success : 182 \t mean(l0) : 12.895605 \t Current Learning Rate: 0.020\n",
            "Iteration 11100: Current success : 77 \t All success : 182 \t mean(l0) : 12.895605 \t Current Learning Rate: 0.020\n",
            "Iteration 11200: Current success : 77 \t All success : 182 \t mean(l0) : 12.895605 \t Current Learning Rate: 0.019\n",
            "Iteration 11300: Current success : 78 \t All success : 182 \t mean(l0) : 12.895605 \t Current Learning Rate: 0.019\n",
            "Iteration 11400: Current success : 77 \t All success : 182 \t mean(l0) : 12.895605 \t Current Learning Rate: 0.018\n",
            "Iteration 11500: Current success : 77 \t All success : 182 \t mean(l0) : 12.895605 \t Current Learning Rate: 0.018\n",
            "Iteration 11600: Current success : 77 \t All success : 182 \t mean(l0) : 12.895605 \t Current Learning Rate: 0.017\n",
            "Iteration 11700: Current success : 75 \t All success : 182 \t mean(l0) : 12.895605 \t Current Learning Rate: 0.017\n",
            "Iteration 11800: Current success : 77 \t All success : 182 \t mean(l0) : 12.895605 \t Current Learning Rate: 0.016\n",
            "Iteration 11900: Current success : 77 \t All success : 182 \t mean(l0) : 12.895605 \t Current Learning Rate: 0.016\n",
            "Iteration 12000: Current success : 76 \t All success : 182 \t mean(l0) : 12.895605 \t Current Learning Rate: 0.015\n",
            "Iteration 12100: Current success : 77 \t All success : 182 \t mean(l0) : 12.895605 \t Current Learning Rate: 0.015\n",
            "Iteration 12200: Current success : 77 \t All success : 182 \t mean(l0) : 12.895605 \t Current Learning Rate: 0.014\n",
            "Iteration 12300: Current success : 77 \t All success : 182 \t mean(l0) : 12.895605 \t Current Learning Rate: 0.014\n",
            "Iteration 12400: Current success : 77 \t All success : 182 \t mean(l0) : 12.895605 \t Current Learning Rate: 0.013\n",
            "Iteration 12500: Current success : 77 \t All success : 182 \t mean(l0) : 12.895605 \t Current Learning Rate: 0.013\n",
            "Iteration 12600: Current success : 77 \t All success : 182 \t mean(l0) : 12.895605 \t Current Learning Rate: 0.012\n",
            "Iteration 12700: Current success : 77 \t All success : 182 \t mean(l0) : 12.895605 \t Current Learning Rate: 0.012\n",
            "Iteration 12800: Current success : 77 \t All success : 182 \t mean(l0) : 12.895605 \t Current Learning Rate: 0.011\n",
            "Iteration 12900: Current success : 77 \t All success : 182 \t mean(l0) : 12.895605 \t Current Learning Rate: 0.011\n",
            "Iteration 13000: Current success : 77 \t All success : 182 \t mean(l0) : 12.895605 \t Current Learning Rate: 0.010\n",
            "Iteration 13100: Current success : 77 \t All success : 182 \t mean(l0) : 12.895605 \t Current Learning Rate: 0.010\n",
            "Iteration 13200: Current success : 77 \t All success : 182 \t mean(l0) : 12.895605 \t Current Learning Rate: 0.009\n",
            "Iteration 13300: Current success : 77 \t All success : 182 \t mean(l0) : 12.895605 \t Current Learning Rate: 0.009\n",
            "Iteration 13400: Current success : 77 \t All success : 182 \t mean(l0) : 12.895605 \t Current Learning Rate: 0.008\n",
            "Iteration 13500: Current success : 77 \t All success : 182 \t mean(l0) : 12.895605 \t Current Learning Rate: 0.008\n",
            "Iteration 13600: Current success : 77 \t All success : 182 \t mean(l0) : 12.895605 \t Current Learning Rate: 0.007\n",
            "Iteration 13700: Current success : 77 \t All success : 182 \t mean(l0) : 12.895605 \t Current Learning Rate: 0.007\n",
            "Iteration 13800: Current success : 77 \t All success : 182 \t mean(l0) : 12.895605 \t Current Learning Rate: 0.006\n",
            "Iteration 13900: Current success : 77 \t All success : 182 \t mean(l0) : 12.895605 \t Current Learning Rate: 0.006\n",
            "Iteration 14000: Current success : 77 \t All success : 182 \t mean(l0) : 12.895605 \t Current Learning Rate: 0.005\n",
            "Iteration 14100: Current success : 77 \t All success : 182 \t mean(l0) : 12.895605 \t Current Learning Rate: 0.005\n",
            "Iteration 14200: Current success : 77 \t All success : 182 \t mean(l0) : 12.895605 \t Current Learning Rate: 0.004\n",
            "Iteration 14300: Current success : 77 \t All success : 182 \t mean(l0) : 12.895605 \t Current Learning Rate: 0.004\n",
            "Iteration 14400: Current success : 77 \t All success : 182 \t mean(l0) : 12.895605 \t Current Learning Rate: 0.003\n",
            "Iteration 14500: Current success : 77 \t All success : 182 \t mean(l0) : 12.895605 \t Current Learning Rate: 0.003\n",
            "Iteration 14600: Current success : 77 \t All success : 182 \t mean(l0) : 12.895605 \t Current Learning Rate: 0.002\n",
            "Iteration 14700: Current success : 77 \t All success : 182 \t mean(l0) : 12.895605 \t Current Learning Rate: 0.002\n",
            "Iteration 14800: Current success : 77 \t All success : 182 \t mean(l0) : 12.895605 \t Current Learning Rate: 0.001\n",
            "Iteration 14900: Current success : 77 \t All success : 182 \t mean(l0) : 12.895605 \t Current Learning Rate: 0.001\n",
            "Iteration 15000: Current success : 77 \t All success : 182 \t mean(l0) : 12.895605 \t Current Learning Rate: 0.000\n",
            "Iteration 15100: Current success : 77 \t All success : 182 \t mean(l0) : 12.895605 \t Current Learning Rate: 0.012\n",
            "Iteration 15200: Current success : 76 \t All success : 182 \t mean(l0) : 12.895605 \t Current Learning Rate: 0.012\n",
            "Iteration 15300: Current success : 79 \t All success : 182 \t mean(l0) : 12.895605 \t Current Learning Rate: 0.012\n",
            "Iteration 15400: Current success : 76 \t All success : 182 \t mean(l0) : 12.884616 \t Current Learning Rate: 0.012\n",
            "Iteration 15500: Current success : 78 \t All success : 182 \t mean(l0) : 12.884616 \t Current Learning Rate: 0.011\n",
            "Iteration 15600: Current success : 74 \t All success : 182 \t mean(l0) : 12.884616 \t Current Learning Rate: 0.011\n",
            "Iteration 15700: Current success : 75 \t All success : 182 \t mean(l0) : 12.884616 \t Current Learning Rate: 0.011\n",
            "Iteration 15800: Current success : 76 \t All success : 182 \t mean(l0) : 12.884616 \t Current Learning Rate: 0.011\n",
            "Iteration 15900: Current success : 76 \t All success : 182 \t mean(l0) : 12.884616 \t Current Learning Rate: 0.010\n",
            "Iteration 16000: Current success : 76 \t All success : 182 \t mean(l0) : 12.884616 \t Current Learning Rate: 0.010\n",
            "Iteration 16100: Current success : 76 \t All success : 182 \t mean(l0) : 12.884616 \t Current Learning Rate: 0.010\n",
            "Iteration 16200: Current success : 76 \t All success : 182 \t mean(l0) : 12.884616 \t Current Learning Rate: 0.010\n",
            "Iteration 16300: Current success : 76 \t All success : 182 \t mean(l0) : 12.884616 \t Current Learning Rate: 0.009\n",
            "Iteration 16400: Current success : 76 \t All success : 182 \t mean(l0) : 12.884616 \t Current Learning Rate: 0.009\n",
            "Iteration 16500: Current success : 76 \t All success : 182 \t mean(l0) : 12.884616 \t Current Learning Rate: 0.009\n",
            "Iteration 16600: Current success : 76 \t All success : 182 \t mean(l0) : 12.884616 \t Current Learning Rate: 0.009\n",
            "Iteration 16700: Current success : 76 \t All success : 182 \t mean(l0) : 12.884616 \t Current Learning Rate: 0.008\n",
            "Iteration 16800: Current success : 76 \t All success : 182 \t mean(l0) : 12.884616 \t Current Learning Rate: 0.008\n",
            "Iteration 16900: Current success : 76 \t All success : 182 \t mean(l0) : 12.884616 \t Current Learning Rate: 0.008\n",
            "Iteration 17000: Current success : 76 \t All success : 182 \t mean(l0) : 12.884616 \t Current Learning Rate: 0.008\n",
            "Iteration 17100: Current success : 76 \t All success : 182 \t mean(l0) : 12.884616 \t Current Learning Rate: 0.007\n",
            "Iteration 17200: Current success : 76 \t All success : 182 \t mean(l0) : 12.884616 \t Current Learning Rate: 0.007\n",
            "Iteration 17300: Current success : 76 \t All success : 182 \t mean(l0) : 12.884616 \t Current Learning Rate: 0.007\n",
            "Iteration 17400: Current success : 76 \t All success : 182 \t mean(l0) : 12.884616 \t Current Learning Rate: 0.007\n",
            "Iteration 17500: Current success : 76 \t All success : 182 \t mean(l0) : 12.884616 \t Current Learning Rate: 0.006\n",
            "Iteration 17600: Current success : 76 \t All success : 182 \t mean(l0) : 12.884616 \t Current Learning Rate: 0.006\n",
            "Iteration 17700: Current success : 76 \t All success : 182 \t mean(l0) : 12.884616 \t Current Learning Rate: 0.006\n",
            "Iteration 17800: Current success : 76 \t All success : 182 \t mean(l0) : 12.884616 \t Current Learning Rate: 0.006\n",
            "Iteration 17900: Current success : 76 \t All success : 182 \t mean(l0) : 12.884616 \t Current Learning Rate: 0.005\n",
            "Iteration 18000: Current success : 76 \t All success : 182 \t mean(l0) : 12.884616 \t Current Learning Rate: 0.005\n",
            "Iteration 18100: Current success : 76 \t All success : 182 \t mean(l0) : 12.884616 \t Current Learning Rate: 0.005\n",
            "Iteration 18200: Current success : 76 \t All success : 182 \t mean(l0) : 12.884616 \t Current Learning Rate: 0.005\n",
            "Iteration 18300: Current success : 76 \t All success : 182 \t mean(l0) : 12.884616 \t Current Learning Rate: 0.004\n",
            "Iteration 18400: Current success : 76 \t All success : 182 \t mean(l0) : 12.884616 \t Current Learning Rate: 0.004\n",
            "Iteration 18500: Current success : 76 \t All success : 182 \t mean(l0) : 12.884616 \t Current Learning Rate: 0.004\n",
            "Iteration 18600: Current success : 76 \t All success : 182 \t mean(l0) : 12.884616 \t Current Learning Rate: 0.004\n",
            "Iteration 18700: Current success : 76 \t All success : 182 \t mean(l0) : 12.884616 \t Current Learning Rate: 0.003\n",
            "Iteration 18800: Current success : 76 \t All success : 182 \t mean(l0) : 12.884616 \t Current Learning Rate: 0.003\n",
            "Iteration 18900: Current success : 76 \t All success : 182 \t mean(l0) : 12.884616 \t Current Learning Rate: 0.003\n",
            "Iteration 19000: Current success : 76 \t All success : 182 \t mean(l0) : 12.884616 \t Current Learning Rate: 0.003\n",
            "Iteration 19100: Current success : 76 \t All success : 182 \t mean(l0) : 12.884616 \t Current Learning Rate: 0.002\n",
            "Iteration 19200: Current success : 76 \t All success : 182 \t mean(l0) : 12.884616 \t Current Learning Rate: 0.002\n",
            "Iteration 19300: Current success : 76 \t All success : 182 \t mean(l0) : 12.884616 \t Current Learning Rate: 0.002\n",
            "Iteration 19400: Current success : 76 \t All success : 182 \t mean(l0) : 12.884616 \t Current Learning Rate: 0.002\n",
            "Iteration 19500: Current success : 76 \t All success : 182 \t mean(l0) : 12.884616 \t Current Learning Rate: 0.001\n",
            "Iteration 19600: Current success : 76 \t All success : 182 \t mean(l0) : 12.884616 \t Current Learning Rate: 0.001\n",
            "Iteration 19700: Current success : 76 \t All success : 182 \t mean(l0) : 12.884616 \t Current Learning Rate: 0.001\n",
            "Iteration 19800: Current success : 76 \t All success : 182 \t mean(l0) : 12.884616 \t Current Learning Rate: 0.001\n",
            "Iteration 19900: Current success : 76 \t All success : 182 \t mean(l0) : 12.884616 \t Current Learning Rate: 0.000\n",
            "Iteration 20000: Current success : 76 \t All success : 182 \t mean(l0) : 12.884616 \t Current Learning Rate: 0.000\n",
            "outer_step 6: all success : 257 \t mean(l0)(success) : 15.4786\n",
            "-------------------------------------------------------------\n",
            "tensor([4.8438e-01, 1.0000e+06, 1.0000e+06,  ..., 1.0000e+06, 1.0000e+06,\n",
            "        1.0000e+06], device='cuda:0')\n",
            "Iteration 100: Current success : 119 \t All success : 121 \t mean(l0) : 8.504132 \t Current Learning Rate: 0.098\n",
            "Iteration 200: Current success : 116 \t All success : 132 \t mean(l0) : 9.916667 \t Current Learning Rate: 0.096\n",
            "Iteration 300: Current success : 126 \t All success : 180 \t mean(l0) : 10.916667 \t Current Learning Rate: 0.094\n",
            "Iteration 400: Current success : 161 \t All success : 185 \t mean(l0) : 10.967567 \t Current Learning Rate: 0.092\n",
            "Iteration 500: Current success : 164 \t All success : 187 \t mean(l0) : 11.219252 \t Current Learning Rate: 0.090\n",
            "Iteration 600: Current success : 165 \t All success : 188 \t mean(l0) : 11.234042 \t Current Learning Rate: 0.088\n",
            "Iteration 700: Current success : 169 \t All success : 192 \t mean(l0) : 11.557292 \t Current Learning Rate: 0.086\n",
            "Iteration 800: Current success : 170 \t All success : 192 \t mean(l0) : 11.557292 \t Current Learning Rate: 0.084\n",
            "Iteration 900: Current success : 166 \t All success : 192 \t mean(l0) : 11.557292 \t Current Learning Rate: 0.082\n",
            "Iteration 1000: Current success : 166 \t All success : 192 \t mean(l0) : 11.557292 \t Current Learning Rate: 0.080\n",
            "Iteration 1100: Current success : 169 \t All success : 192 \t mean(l0) : 11.557292 \t Current Learning Rate: 0.078\n",
            "Iteration 1200: Current success : 169 \t All success : 192 \t mean(l0) : 11.557292 \t Current Learning Rate: 0.076\n",
            "Iteration 1300: Current success : 168 \t All success : 192 \t mean(l0) : 11.557292 \t Current Learning Rate: 0.074\n",
            "Iteration 1400: Current success : 169 \t All success : 193 \t mean(l0) : 11.761658 \t Current Learning Rate: 0.072\n",
            "Iteration 1500: Current success : 170 \t All success : 193 \t mean(l0) : 11.761658 \t Current Learning Rate: 0.070\n",
            "Iteration 1600: Current success : 170 \t All success : 193 \t mean(l0) : 11.761658 \t Current Learning Rate: 0.068\n",
            "Iteration 1700: Current success : 170 \t All success : 193 \t mean(l0) : 11.761658 \t Current Learning Rate: 0.066\n",
            "Iteration 1800: Current success : 172 \t All success : 195 \t mean(l0) : 12.205129 \t Current Learning Rate: 0.064\n",
            "Iteration 1900: Current success : 172 \t All success : 195 \t mean(l0) : 12.205129 \t Current Learning Rate: 0.062\n",
            "Iteration 2000: Current success : 172 \t All success : 195 \t mean(l0) : 12.205129 \t Current Learning Rate: 0.060\n",
            "Iteration 2100: Current success : 172 \t All success : 195 \t mean(l0) : 12.205129 \t Current Learning Rate: 0.058\n",
            "Iteration 2200: Current success : 172 \t All success : 195 \t mean(l0) : 12.205129 \t Current Learning Rate: 0.056\n",
            "Iteration 2300: Current success : 172 \t All success : 195 \t mean(l0) : 12.205129 \t Current Learning Rate: 0.054\n",
            "Iteration 2400: Current success : 174 \t All success : 197 \t mean(l0) : 12.335025 \t Current Learning Rate: 0.052\n",
            "Iteration 2500: Current success : 174 \t All success : 197 \t mean(l0) : 12.335025 \t Current Learning Rate: 0.050\n",
            "Iteration 2600: Current success : 174 \t All success : 197 \t mean(l0) : 12.335025 \t Current Learning Rate: 0.048\n",
            "Iteration 2700: Current success : 174 \t All success : 197 \t mean(l0) : 12.335025 \t Current Learning Rate: 0.046\n",
            "Iteration 2800: Current success : 174 \t All success : 197 \t mean(l0) : 12.335025 \t Current Learning Rate: 0.044\n",
            "Iteration 2900: Current success : 174 \t All success : 197 \t mean(l0) : 12.335025 \t Current Learning Rate: 0.042\n",
            "Iteration 3000: Current success : 174 \t All success : 197 \t mean(l0) : 12.335025 \t Current Learning Rate: 0.040\n",
            "Iteration 3100: Current success : 174 \t All success : 197 \t mean(l0) : 12.335025 \t Current Learning Rate: 0.038\n",
            "Iteration 3200: Current success : 175 \t All success : 198 \t mean(l0) : 12.434343 \t Current Learning Rate: 0.036\n",
            "Iteration 3300: Current success : 175 \t All success : 198 \t mean(l0) : 12.434343 \t Current Learning Rate: 0.034\n",
            "Iteration 3400: Current success : 174 \t All success : 198 \t mean(l0) : 12.434343 \t Current Learning Rate: 0.032\n",
            "Iteration 3500: Current success : 175 \t All success : 198 \t mean(l0) : 12.434343 \t Current Learning Rate: 0.030\n",
            "Iteration 3600: Current success : 175 \t All success : 198 \t mean(l0) : 12.434343 \t Current Learning Rate: 0.028\n",
            "Iteration 3700: Current success : 175 \t All success : 198 \t mean(l0) : 12.434343 \t Current Learning Rate: 0.026\n",
            "Iteration 3800: Current success : 175 \t All success : 198 \t mean(l0) : 12.434343 \t Current Learning Rate: 0.024\n",
            "Iteration 3900: Current success : 175 \t All success : 198 \t mean(l0) : 12.434343 \t Current Learning Rate: 0.022\n",
            "Iteration 4000: Current success : 175 \t All success : 198 \t mean(l0) : 12.434343 \t Current Learning Rate: 0.020\n",
            "Iteration 4100: Current success : 175 \t All success : 198 \t mean(l0) : 12.434343 \t Current Learning Rate: 0.018\n",
            "Iteration 4200: Current success : 175 \t All success : 198 \t mean(l0) : 12.434343 \t Current Learning Rate: 0.016\n",
            "Iteration 4300: Current success : 175 \t All success : 198 \t mean(l0) : 12.434343 \t Current Learning Rate: 0.014\n",
            "Iteration 4400: Current success : 175 \t All success : 198 \t mean(l0) : 12.434343 \t Current Learning Rate: 0.012\n",
            "Iteration 4500: Current success : 175 \t All success : 198 \t mean(l0) : 12.434343 \t Current Learning Rate: 0.010\n",
            "Iteration 4600: Current success : 175 \t All success : 198 \t mean(l0) : 12.434343 \t Current Learning Rate: 0.008\n",
            "Iteration 4700: Current success : 175 \t All success : 198 \t mean(l0) : 12.434343 \t Current Learning Rate: 0.006\n",
            "Iteration 4800: Current success : 175 \t All success : 198 \t mean(l0) : 12.434343 \t Current Learning Rate: 0.004\n",
            "Iteration 4900: Current success : 175 \t All success : 198 \t mean(l0) : 12.434343 \t Current Learning Rate: 0.002\n",
            "Iteration 5000: Current success : 175 \t All success : 198 \t mean(l0) : 12.434343 \t Current Learning Rate: 0.000\n",
            "Iteration 5100: Current success : 198 \t All success : 213 \t mean(l0) : 13.018780 \t Current Learning Rate: 0.049\n",
            "Iteration 5200: Current success : 198 \t All success : 219 \t mean(l0) : 13.155251 \t Current Learning Rate: 0.048\n",
            "Iteration 5300: Current success : 198 \t All success : 219 \t mean(l0) : 13.155251 \t Current Learning Rate: 0.047\n",
            "Iteration 5400: Current success : 201 \t All success : 220 \t mean(l0) : 13.422727 \t Current Learning Rate: 0.046\n",
            "Iteration 5500: Current success : 201 \t All success : 221 \t mean(l0) : 13.398190 \t Current Learning Rate: 0.045\n",
            "Iteration 5600: Current success : 200 \t All success : 221 \t mean(l0) : 13.398190 \t Current Learning Rate: 0.044\n",
            "Iteration 5700: Current success : 200 \t All success : 221 \t mean(l0) : 13.398190 \t Current Learning Rate: 0.043\n",
            "Iteration 5800: Current success : 201 \t All success : 221 \t mean(l0) : 13.398190 \t Current Learning Rate: 0.042\n",
            "Iteration 5900: Current success : 201 \t All success : 221 \t mean(l0) : 13.398190 \t Current Learning Rate: 0.041\n",
            "Iteration 6000: Current success : 201 \t All success : 221 \t mean(l0) : 13.398190 \t Current Learning Rate: 0.040\n",
            "Iteration 6100: Current success : 201 \t All success : 221 \t mean(l0) : 13.398190 \t Current Learning Rate: 0.039\n",
            "Iteration 6200: Current success : 201 \t All success : 221 \t mean(l0) : 13.398190 \t Current Learning Rate: 0.038\n",
            "Iteration 6300: Current success : 201 \t All success : 221 \t mean(l0) : 13.398190 \t Current Learning Rate: 0.037\n",
            "Iteration 6400: Current success : 201 \t All success : 221 \t mean(l0) : 13.398190 \t Current Learning Rate: 0.036\n",
            "Iteration 6500: Current success : 201 \t All success : 221 \t mean(l0) : 13.398190 \t Current Learning Rate: 0.035\n",
            "Iteration 6600: Current success : 201 \t All success : 221 \t mean(l0) : 13.398190 \t Current Learning Rate: 0.034\n",
            "Iteration 6700: Current success : 201 \t All success : 221 \t mean(l0) : 13.398190 \t Current Learning Rate: 0.033\n",
            "Iteration 6800: Current success : 201 \t All success : 221 \t mean(l0) : 13.398190 \t Current Learning Rate: 0.032\n",
            "Iteration 6900: Current success : 201 \t All success : 221 \t mean(l0) : 13.398190 \t Current Learning Rate: 0.031\n",
            "Iteration 7000: Current success : 201 \t All success : 221 \t mean(l0) : 13.398190 \t Current Learning Rate: 0.030\n",
            "Iteration 7100: Current success : 201 \t All success : 221 \t mean(l0) : 13.398190 \t Current Learning Rate: 0.029\n",
            "Iteration 7200: Current success : 201 \t All success : 221 \t mean(l0) : 13.398190 \t Current Learning Rate: 0.028\n",
            "Iteration 7300: Current success : 201 \t All success : 221 \t mean(l0) : 13.398190 \t Current Learning Rate: 0.027\n",
            "Iteration 7400: Current success : 201 \t All success : 221 \t mean(l0) : 13.398190 \t Current Learning Rate: 0.026\n",
            "Iteration 7500: Current success : 201 \t All success : 221 \t mean(l0) : 13.398190 \t Current Learning Rate: 0.025\n",
            "Iteration 7600: Current success : 201 \t All success : 221 \t mean(l0) : 13.398190 \t Current Learning Rate: 0.024\n",
            "Iteration 7700: Current success : 201 \t All success : 221 \t mean(l0) : 13.398190 \t Current Learning Rate: 0.023\n",
            "Iteration 7800: Current success : 201 \t All success : 221 \t mean(l0) : 13.398190 \t Current Learning Rate: 0.022\n",
            "Iteration 7900: Current success : 201 \t All success : 221 \t mean(l0) : 13.398190 \t Current Learning Rate: 0.021\n",
            "Iteration 8000: Current success : 201 \t All success : 221 \t mean(l0) : 13.398190 \t Current Learning Rate: 0.020\n",
            "Iteration 8100: Current success : 201 \t All success : 221 \t mean(l0) : 13.398190 \t Current Learning Rate: 0.019\n",
            "Iteration 8200: Current success : 201 \t All success : 221 \t mean(l0) : 13.398190 \t Current Learning Rate: 0.018\n",
            "Iteration 8300: Current success : 201 \t All success : 221 \t mean(l0) : 13.398190 \t Current Learning Rate: 0.017\n",
            "Iteration 8400: Current success : 201 \t All success : 221 \t mean(l0) : 13.398190 \t Current Learning Rate: 0.016\n",
            "Iteration 8500: Current success : 201 \t All success : 221 \t mean(l0) : 13.398190 \t Current Learning Rate: 0.015\n",
            "Iteration 8600: Current success : 201 \t All success : 221 \t mean(l0) : 13.398190 \t Current Learning Rate: 0.014\n",
            "Iteration 8700: Current success : 201 \t All success : 221 \t mean(l0) : 13.398190 \t Current Learning Rate: 0.013\n",
            "Iteration 8800: Current success : 201 \t All success : 221 \t mean(l0) : 13.398190 \t Current Learning Rate: 0.012\n",
            "Iteration 8900: Current success : 201 \t All success : 221 \t mean(l0) : 13.398190 \t Current Learning Rate: 0.011\n",
            "Iteration 9000: Current success : 201 \t All success : 221 \t mean(l0) : 13.398190 \t Current Learning Rate: 0.010\n",
            "Iteration 9100: Current success : 201 \t All success : 221 \t mean(l0) : 13.398190 \t Current Learning Rate: 0.009\n",
            "Iteration 9200: Current success : 201 \t All success : 221 \t mean(l0) : 13.398190 \t Current Learning Rate: 0.008\n",
            "Iteration 9300: Current success : 201 \t All success : 221 \t mean(l0) : 13.398190 \t Current Learning Rate: 0.007\n",
            "Iteration 9400: Current success : 201 \t All success : 221 \t mean(l0) : 13.398190 \t Current Learning Rate: 0.006\n",
            "Iteration 9500: Current success : 201 \t All success : 221 \t mean(l0) : 13.398190 \t Current Learning Rate: 0.005\n",
            "Iteration 9600: Current success : 201 \t All success : 221 \t mean(l0) : 13.398190 \t Current Learning Rate: 0.004\n",
            "Iteration 9700: Current success : 201 \t All success : 221 \t mean(l0) : 13.398190 \t Current Learning Rate: 0.003\n",
            "Iteration 9800: Current success : 201 \t All success : 221 \t mean(l0) : 13.398190 \t Current Learning Rate: 0.002\n",
            "Iteration 9900: Current success : 201 \t All success : 221 \t mean(l0) : 13.398190 \t Current Learning Rate: 0.001\n",
            "Iteration 10000: Current success : 201 \t All success : 221 \t mean(l0) : 13.398190 \t Current Learning Rate: 0.000\n",
            "Iteration 10100: Current success : 197 \t All success : 221 \t mean(l0) : 13.398190 \t Current Learning Rate: 0.025\n",
            "Iteration 10200: Current success : 198 \t All success : 221 \t mean(l0) : 13.398190 \t Current Learning Rate: 0.024\n",
            "Iteration 10300: Current success : 199 \t All success : 221 \t mean(l0) : 13.398190 \t Current Learning Rate: 0.024\n",
            "Iteration 10400: Current success : 199 \t All success : 221 \t mean(l0) : 13.398190 \t Current Learning Rate: 0.023\n",
            "Iteration 10500: Current success : 199 \t All success : 221 \t mean(l0) : 13.398190 \t Current Learning Rate: 0.023\n",
            "Iteration 10600: Current success : 199 \t All success : 221 \t mean(l0) : 13.398190 \t Current Learning Rate: 0.022\n",
            "Iteration 10700: Current success : 200 \t All success : 222 \t mean(l0) : 13.360361 \t Current Learning Rate: 0.022\n",
            "Iteration 10800: Current success : 200 \t All success : 222 \t mean(l0) : 13.360361 \t Current Learning Rate: 0.021\n",
            "Iteration 10900: Current success : 200 \t All success : 222 \t mean(l0) : 13.360361 \t Current Learning Rate: 0.021\n",
            "Iteration 11000: Current success : 200 \t All success : 222 \t mean(l0) : 13.360361 \t Current Learning Rate: 0.020\n",
            "Iteration 11100: Current success : 200 \t All success : 222 \t mean(l0) : 13.360361 \t Current Learning Rate: 0.020\n",
            "Iteration 11200: Current success : 200 \t All success : 222 \t mean(l0) : 13.360361 \t Current Learning Rate: 0.019\n",
            "Iteration 11300: Current success : 200 \t All success : 222 \t mean(l0) : 13.360361 \t Current Learning Rate: 0.019\n",
            "Iteration 11400: Current success : 200 \t All success : 223 \t mean(l0) : 13.331840 \t Current Learning Rate: 0.018\n",
            "Iteration 11500: Current success : 200 \t All success : 223 \t mean(l0) : 13.331840 \t Current Learning Rate: 0.018\n",
            "Iteration 11600: Current success : 200 \t All success : 223 \t mean(l0) : 13.331840 \t Current Learning Rate: 0.017\n",
            "Iteration 11700: Current success : 200 \t All success : 223 \t mean(l0) : 13.331840 \t Current Learning Rate: 0.017\n",
            "Iteration 11800: Current success : 200 \t All success : 223 \t mean(l0) : 13.331840 \t Current Learning Rate: 0.016\n",
            "Iteration 11900: Current success : 200 \t All success : 223 \t mean(l0) : 13.331840 \t Current Learning Rate: 0.016\n",
            "Iteration 12000: Current success : 200 \t All success : 223 \t mean(l0) : 13.331840 \t Current Learning Rate: 0.015\n",
            "Iteration 12100: Current success : 200 \t All success : 223 \t mean(l0) : 13.331840 \t Current Learning Rate: 0.015\n",
            "Iteration 12200: Current success : 201 \t All success : 223 \t mean(l0) : 13.331840 \t Current Learning Rate: 0.014\n",
            "Iteration 12300: Current success : 201 \t All success : 223 \t mean(l0) : 13.331840 \t Current Learning Rate: 0.014\n",
            "Iteration 12400: Current success : 200 \t All success : 223 \t mean(l0) : 13.331840 \t Current Learning Rate: 0.013\n",
            "Iteration 12500: Current success : 201 \t All success : 223 \t mean(l0) : 13.331840 \t Current Learning Rate: 0.013\n",
            "Iteration 12600: Current success : 201 \t All success : 223 \t mean(l0) : 13.331840 \t Current Learning Rate: 0.012\n",
            "Iteration 12700: Current success : 201 \t All success : 223 \t mean(l0) : 13.331840 \t Current Learning Rate: 0.012\n",
            "Iteration 12800: Current success : 201 \t All success : 223 \t mean(l0) : 13.331840 \t Current Learning Rate: 0.011\n",
            "Iteration 12900: Current success : 201 \t All success : 223 \t mean(l0) : 13.331840 \t Current Learning Rate: 0.011\n",
            "Iteration 13000: Current success : 200 \t All success : 223 \t mean(l0) : 13.331840 \t Current Learning Rate: 0.010\n",
            "Iteration 13100: Current success : 200 \t All success : 223 \t mean(l0) : 13.331840 \t Current Learning Rate: 0.010\n",
            "Iteration 13200: Current success : 200 \t All success : 223 \t mean(l0) : 13.331840 \t Current Learning Rate: 0.009\n",
            "Iteration 13300: Current success : 200 \t All success : 223 \t mean(l0) : 13.331840 \t Current Learning Rate: 0.009\n",
            "Iteration 13400: Current success : 200 \t All success : 223 \t mean(l0) : 13.331840 \t Current Learning Rate: 0.008\n",
            "Iteration 13500: Current success : 200 \t All success : 223 \t mean(l0) : 13.331840 \t Current Learning Rate: 0.008\n",
            "Iteration 13600: Current success : 200 \t All success : 223 \t mean(l0) : 13.331840 \t Current Learning Rate: 0.007\n",
            "Iteration 13700: Current success : 200 \t All success : 223 \t mean(l0) : 13.331840 \t Current Learning Rate: 0.007\n",
            "Iteration 13800: Current success : 200 \t All success : 223 \t mean(l0) : 13.331840 \t Current Learning Rate: 0.006\n",
            "Iteration 13900: Current success : 200 \t All success : 223 \t mean(l0) : 13.331840 \t Current Learning Rate: 0.006\n",
            "Iteration 14000: Current success : 200 \t All success : 223 \t mean(l0) : 13.331840 \t Current Learning Rate: 0.005\n",
            "Iteration 14100: Current success : 200 \t All success : 223 \t mean(l0) : 13.331840 \t Current Learning Rate: 0.005\n",
            "Iteration 14200: Current success : 200 \t All success : 223 \t mean(l0) : 13.331840 \t Current Learning Rate: 0.004\n",
            "Iteration 14300: Current success : 200 \t All success : 223 \t mean(l0) : 13.331840 \t Current Learning Rate: 0.004\n",
            "Iteration 14400: Current success : 200 \t All success : 223 \t mean(l0) : 13.331840 \t Current Learning Rate: 0.003\n",
            "Iteration 14500: Current success : 200 \t All success : 223 \t mean(l0) : 13.331840 \t Current Learning Rate: 0.003\n",
            "Iteration 14600: Current success : 200 \t All success : 223 \t mean(l0) : 13.331840 \t Current Learning Rate: 0.002\n",
            "Iteration 14700: Current success : 200 \t All success : 223 \t mean(l0) : 13.331840 \t Current Learning Rate: 0.002\n",
            "Iteration 14800: Current success : 200 \t All success : 223 \t mean(l0) : 13.331840 \t Current Learning Rate: 0.001\n",
            "Iteration 14900: Current success : 200 \t All success : 223 \t mean(l0) : 13.331840 \t Current Learning Rate: 0.001\n",
            "Iteration 15000: Current success : 200 \t All success : 223 \t mean(l0) : 13.331840 \t Current Learning Rate: 0.000\n",
            "Iteration 15100: Current success : 201 \t All success : 223 \t mean(l0) : 13.331840 \t Current Learning Rate: 0.012\n",
            "Iteration 15200: Current success : 202 \t All success : 223 \t mean(l0) : 13.331840 \t Current Learning Rate: 0.012\n",
            "Iteration 15300: Current success : 201 \t All success : 223 \t mean(l0) : 13.331840 \t Current Learning Rate: 0.012\n",
            "Iteration 15400: Current success : 201 \t All success : 224 \t mean(l0) : 13.549108 \t Current Learning Rate: 0.012\n",
            "Iteration 15500: Current success : 201 \t All success : 224 \t mean(l0) : 13.549108 \t Current Learning Rate: 0.011\n",
            "Iteration 15600: Current success : 201 \t All success : 224 \t mean(l0) : 13.549108 \t Current Learning Rate: 0.011\n",
            "Iteration 15700: Current success : 201 \t All success : 224 \t mean(l0) : 13.549108 \t Current Learning Rate: 0.011\n",
            "Iteration 15800: Current success : 202 \t All success : 224 \t mean(l0) : 13.549108 \t Current Learning Rate: 0.011\n",
            "Iteration 15900: Current success : 202 \t All success : 224 \t mean(l0) : 13.549108 \t Current Learning Rate: 0.010\n",
            "Iteration 16000: Current success : 203 \t All success : 224 \t mean(l0) : 13.549108 \t Current Learning Rate: 0.010\n",
            "Iteration 16100: Current success : 203 \t All success : 224 \t mean(l0) : 13.549108 \t Current Learning Rate: 0.010\n",
            "Iteration 16200: Current success : 202 \t All success : 224 \t mean(l0) : 13.549108 \t Current Learning Rate: 0.010\n",
            "Iteration 16300: Current success : 203 \t All success : 224 \t mean(l0) : 13.549108 \t Current Learning Rate: 0.009\n",
            "Iteration 16400: Current success : 203 \t All success : 224 \t mean(l0) : 13.549108 \t Current Learning Rate: 0.009\n",
            "Iteration 16500: Current success : 203 \t All success : 224 \t mean(l0) : 13.549108 \t Current Learning Rate: 0.009\n",
            "Iteration 16600: Current success : 203 \t All success : 224 \t mean(l0) : 13.549108 \t Current Learning Rate: 0.009\n",
            "Iteration 16700: Current success : 203 \t All success : 224 \t mean(l0) : 13.549108 \t Current Learning Rate: 0.008\n",
            "Iteration 16800: Current success : 203 \t All success : 224 \t mean(l0) : 13.549108 \t Current Learning Rate: 0.008\n",
            "Iteration 16900: Current success : 203 \t All success : 224 \t mean(l0) : 13.549108 \t Current Learning Rate: 0.008\n",
            "Iteration 17000: Current success : 203 \t All success : 224 \t mean(l0) : 13.549108 \t Current Learning Rate: 0.008\n",
            "Iteration 17100: Current success : 203 \t All success : 224 \t mean(l0) : 13.549108 \t Current Learning Rate: 0.007\n",
            "Iteration 17200: Current success : 203 \t All success : 224 \t mean(l0) : 13.549108 \t Current Learning Rate: 0.007\n",
            "Iteration 17300: Current success : 202 \t All success : 224 \t mean(l0) : 13.549108 \t Current Learning Rate: 0.007\n",
            "Iteration 17400: Current success : 202 \t All success : 224 \t mean(l0) : 13.549108 \t Current Learning Rate: 0.007\n",
            "Iteration 17500: Current success : 202 \t All success : 224 \t mean(l0) : 13.549108 \t Current Learning Rate: 0.006\n",
            "Iteration 17600: Current success : 202 \t All success : 224 \t mean(l0) : 13.549108 \t Current Learning Rate: 0.006\n",
            "Iteration 17700: Current success : 202 \t All success : 224 \t mean(l0) : 13.549108 \t Current Learning Rate: 0.006\n",
            "Iteration 17800: Current success : 202 \t All success : 224 \t mean(l0) : 13.549108 \t Current Learning Rate: 0.006\n",
            "Iteration 17900: Current success : 202 \t All success : 224 \t mean(l0) : 13.549108 \t Current Learning Rate: 0.005\n",
            "Iteration 18000: Current success : 202 \t All success : 224 \t mean(l0) : 13.549108 \t Current Learning Rate: 0.005\n",
            "Iteration 18100: Current success : 202 \t All success : 224 \t mean(l0) : 13.549108 \t Current Learning Rate: 0.005\n",
            "Iteration 18200: Current success : 203 \t All success : 224 \t mean(l0) : 13.549108 \t Current Learning Rate: 0.005\n",
            "Iteration 18300: Current success : 203 \t All success : 224 \t mean(l0) : 13.549108 \t Current Learning Rate: 0.004\n",
            "Iteration 18400: Current success : 203 \t All success : 224 \t mean(l0) : 13.549108 \t Current Learning Rate: 0.004\n",
            "Iteration 18500: Current success : 202 \t All success : 224 \t mean(l0) : 13.549108 \t Current Learning Rate: 0.004\n",
            "Iteration 18600: Current success : 202 \t All success : 224 \t mean(l0) : 13.549108 \t Current Learning Rate: 0.004\n",
            "Iteration 18700: Current success : 203 \t All success : 224 \t mean(l0) : 13.549108 \t Current Learning Rate: 0.003\n",
            "Iteration 18800: Current success : 202 \t All success : 224 \t mean(l0) : 13.549108 \t Current Learning Rate: 0.003\n",
            "Iteration 18900: Current success : 202 \t All success : 224 \t mean(l0) : 13.549108 \t Current Learning Rate: 0.003\n",
            "Iteration 19000: Current success : 202 \t All success : 224 \t mean(l0) : 13.549108 \t Current Learning Rate: 0.003\n",
            "Iteration 19100: Current success : 201 \t All success : 224 \t mean(l0) : 13.549108 \t Current Learning Rate: 0.002\n",
            "Iteration 19200: Current success : 201 \t All success : 224 \t mean(l0) : 13.549108 \t Current Learning Rate: 0.002\n",
            "Iteration 19300: Current success : 201 \t All success : 224 \t mean(l0) : 13.549108 \t Current Learning Rate: 0.002\n",
            "Iteration 19400: Current success : 201 \t All success : 224 \t mean(l0) : 13.549108 \t Current Learning Rate: 0.002\n",
            "Iteration 19500: Current success : 201 \t All success : 224 \t mean(l0) : 13.549108 \t Current Learning Rate: 0.001\n",
            "Iteration 19600: Current success : 201 \t All success : 224 \t mean(l0) : 13.549108 \t Current Learning Rate: 0.001\n",
            "Iteration 19700: Current success : 201 \t All success : 224 \t mean(l0) : 13.549108 \t Current Learning Rate: 0.001\n",
            "Iteration 19800: Current success : 201 \t All success : 224 \t mean(l0) : 13.549108 \t Current Learning Rate: 0.001\n",
            "Iteration 19900: Current success : 201 \t All success : 224 \t mean(l0) : 13.549108 \t Current Learning Rate: 0.000\n",
            "Iteration 20000: Current success : 201 \t All success : 224 \t mean(l0) : 13.549108 \t Current Learning Rate: 0.000\n",
            "1130\n",
            "260\n",
            "Accuracy of just malwares (without attack): 98.85% | Under attack: 76.99%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xaOs5ZT9R0xx"
      },
      "outputs": [],
      "source": [
        "for x_test, y_test in test_loader:\n",
        "    x_test, y_test = x_test.to(torch.float32).to(device), y_test.to(device)\n",
        "    mal_x_batch, mal_y_batch = x_test[y_test.squeeze() == 1], y_test[y_test.squeeze() == 1]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation on rounded adversarial examples\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "combined_model.eval()\n",
        "\n",
        "# Apply the final rounding solution to adversarial examples\n",
        "adv_rounded = final_rounding(mal_x_batch, adv, combined_model,removal_array)\n",
        "\n",
        "# Get the model's predictions on the rounded adversarial examples\n",
        "outputs = combined_model(adv_rounded)\n",
        "y_pred = outputs.argmax(dim=1)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "num_malwares = len(mal_y_batch)\n",
        "num_successful_attacks = (y_pred == 0).sum().item()\n",
        "total_difference = (torch.abs(adv_rounded[y_pred == 0] - mal_x_batch[y_pred == 0])).sum().item()\n",
        "accuracy = (y_pred == 1).sum().item() / num_malwares * 100\n",
        "\n",
        "# Display the results with clear formatting\n",
        "print(f\"Evaluation Results on Rounded Adversarial Examples\")\n",
        "print(f\"-----------------------------------------------\")\n",
        "print(f\"Number of malware samples: {num_malwares}\")\n",
        "print(f\"Number of successful attacks: {num_successful_attacks}\")\n",
        "print(f\"Total L1 norm difference (successful adversarial examples vs. original): {total_difference}\")\n",
        "print(f\"Accuracy of the model on malware under attack: {accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae32fd2c-7aaa-4f96-b315-10c3a6398051",
        "id": "Nwcu7RUCR0xy"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Results on Rounded Adversarial Examples\n",
            "-----------------------------------------------\n",
            "Number of malware samples: 1130\n",
            "Number of successful attacks: 260\n",
            "Total L1 norm difference (successful adversarial examples vs. original): 4148.0\n",
            "Accuracy of the model on malware under attack: 76.99%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure that fixed elements are not changed\n",
        "\n",
        "# Ensure `removal_array` is expanded to match the batch size\n",
        "expanded_removal_array = removal_array.expand(mal_x_batch.shape[0], -1)\n",
        "\n",
        "# Create a mask where non-fixed features are set to 1\n",
        "non_fixed_features_mask = torch.bitwise_or(expanded_removal_array, 1 - mal_x_batch.to(torch.uint8))\n",
        "\n",
        "# Calculate indices where the mask is 0 (fixed) or 1 (non-fixed)\n",
        "mask_fixed = non_fixed_features_mask == 0\n",
        "mask_non_fixed = non_fixed_features_mask == 1\n",
        "\n",
        "# Calculate and print the differences\n",
        "diff_adv_fixed = (torch.abs(adv - mal_x_batch) * mask_fixed).sum().item()\n",
        "diff_adv_non_fixed = (torch.abs(adv - mal_x_batch) * mask_non_fixed).sum().item()\n",
        "diff_adv_rounded_fixed = (torch.abs(adv_rounded - mal_x_batch) * mask_fixed).sum().item()\n",
        "diff_adv_rounded_non_fixed = (torch.abs(adv_rounded - mal_x_batch) * mask_non_fixed).sum().item()\n",
        "\n",
        "# Display results with clear formatting\n",
        "print(f\"Differences when mask is 0 (fixed features):\")\n",
        "print(f\"  Adv vs. Original: {diff_adv_fixed}\")\n",
        "print(f\"  Rounded Adv vs. Original: {diff_adv_rounded_fixed}\")\n",
        "print(f\"Differences when mask is 1 (non-fixed features):\")\n",
        "print(f\"  Adv vs. Original: {diff_adv_non_fixed}\")\n",
        "print(f\"  Rounded Adv vs. Original: {diff_adv_rounded_non_fixed}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fa832ea-ea1b-4f43-a328-31a7c65c571e",
        "id": "Z9T2QVASR0xy"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Differences when mask is 0 (fixed features):\n",
            "  Adv vs. Original: 0.0\n",
            "  Rounded Adv vs. Original: 0.0\n",
            "Differences when mask is 1 (non-fixed features):\n",
            "  Adv vs. Original: 59920.03515625\n",
            "  Rounded Adv vs. Original: 74596.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming differences is already computed as in your code\n",
        "differences = (torch.abs(adv_rounded[y_pred == 0] - mal_x_batch[y_pred == 0])).sum(1)\n",
        "\n",
        "# Convert differences to a numpy array for plotting\n",
        "differences_np = differences.cpu().numpy()\n",
        "\n",
        "# Set the style of the plot\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# Create a histogram with Seaborn\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(differences_np, bins=16, kde=False, color='skyblue')\n",
        "#sns.histplot(differences_np, bins=30, kde=True, color='skyblue')\n",
        "\n",
        "# Set the title and labels\n",
        "plt.title('Abundance of Differences', fontsize=16)\n",
        "plt.xlabel('Sum of Absolute Differences', fontsize=14)\n",
        "plt.ylabel('Frequency', fontsize=14)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 576
        },
        "outputId": "05d10337-e433-43b1-b732-c25532a3b870",
        "id": "jerFyvWuR0xz"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0lIVkqcHR4fE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adv[y_pred==0][0][:200]- mal_x_batch[y_pred==0][0][:200]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a9df1b0-2979-4f19-e6e9-b4dc29f0953c",
        "id": "5iaqhCsuR4on"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00, -2.3711e-01,  0.0000e+00,\n",
              "         0.0000e+00, -5.6602e-03,  0.0000e+00,  1.6560e-01,  0.0000e+00,\n",
              "         4.0701e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         3.8184e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  1.6689e-06,  0.0000e+00,  0.0000e+00,\n",
              "         1.2139e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  4.9442e-05,  0.0000e+00,  0.0000e+00,  6.1479e-02,\n",
              "         4.5905e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  6.5267e-05,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  3.0577e-04,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  7.1108e-02,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  1.5214e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  1.1340e-01,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  2.9802e-08,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  9.2387e-07,  0.0000e+00,\n",
              "         4.0913e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  1.2005e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         5.8609e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  1.0163e-05,  0.0000e+00,  7.8633e-02,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  2.0216e-03,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  1.9357e-04,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  6.8039e-05,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "adv[y_pred==0][1][:200] - mal_x_batch[y_pred==0][1][:200]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "932ec16f-32e0-4837-d2b7-0a2bd555bfa2",
        "id": "qXJI8m7kR4on"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.0000e+00,  1.0000e+00,  0.0000e+00,  0.0000e+00,  4.7386e-06,\n",
              "         0.0000e+00,  6.6370e-05,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         4.0435e-03,  1.0000e+00,  7.3716e-04,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  9.9986e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  5.3458e-02,  9.9902e-01,  1.0000e+00,  1.0133e-06,\n",
              "         0.0000e+00,  1.5241e-04,  0.0000e+00,  0.0000e+00,  9.9999e-01,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  9.9263e-01, -1.1683e-04,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  2.9802e-08,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  6.6340e-05,\n",
              "         0.0000e+00,  0.0000e+00, -4.6888e-01,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  3.7909e-05,\n",
              "        -9.9988e-01,  3.2783e-07,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,  1.1027e-05,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00, -5.1332e-04,  0.0000e+00,\n",
              "         0.0000e+00, -1.0000e+00,  2.3550e-03, -6.5565e-07, -2.6703e-01,\n",
              "         4.9865e-04,  3.7094e-01,  3.2783e-07,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  2.6107e-05,  5.8413e-06,  9.1636e-01,\n",
              "         0.0000e+00,  1.6502e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  3.2187e-06,  1.0000e+00,\n",
              "         0.0000e+00,  9.9901e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         5.9605e-08,  0.0000e+00,  5.9605e-08,  1.0000e+00,  0.0000e+00,\n",
              "         2.9802e-08,  1.2904e-05,  9.2387e-07,  0.0000e+00,  0.0000e+00,\n",
              "         5.5114e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  2.7418e-06,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  9.9979e-01,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  2.9802e-08,  1.9372e-06,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.1921e-07,\n",
              "         0.0000e+00,  1.6361e-05,  0.0000e+00,  0.0000e+00,  1.0000e+00,\n",
              "         4.5505e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fDKLsgTQjK3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#perturbation"
      ],
      "metadata": {
        "id": "16V4LDrIsqiI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "from torch.optim.lr_scheduler import CyclicLR\n",
        "\n",
        "def compare(output, target, confidence):\n",
        "    if not isinstance(output, (float, int, torch.LongTensor, torch.cuda.LongTensor)):\n",
        "        output = torch.clone(output).detach()\n",
        "        output[target] -= confidence  # Slightly reduce the confidence for targeted attacks\n",
        "        output = torch.argmax(output, dim=1)\n",
        "    return output == target\n",
        "\n",
        "def compare_round(mal_x_batch,newimg, model,removal_array):\n",
        "\n",
        "    adv_rounded = final_rounding(mal_x_batch, newimg, model,removal_array)\n",
        "    #l0dist = torch.sum((mal_x_batch - newimg)!=0, dim=1)\n",
        "    l0dist = torch.sum(torch.abs(adv_rounded - mal_x_batch), dim=1)\n",
        "    logits = model(adv_rounded)\n",
        "    output = torch.argmax(logits, dim=1)\n",
        "\n",
        "    return output == 0, l0dist\n",
        "\n",
        "def sin_func_torch(input_tensor):\n",
        "    \"\"\"\n",
        "    A sine-based transformation applied to the input tensor.\n",
        "\n",
        "    The function applies a sine transformation and rescales the output to [0, 1].\n",
        "    \"\"\"\n",
        "    return (torch.sin((input_tensor - 0.5) * torch.pi) + 1) / 2.0\n",
        "\n",
        "def CW_l1(imgs, y, model, removal_array, max_iterations=1000, max_learning_rate=0.1,\n",
        "       binary_search_steps=3, confidence=0.01, initial_const=0.1,\n",
        "       cons_increase_factor=2, device='cuda'):\n",
        "    \"\"\"\n",
        "    Implements the Carlini & Wagner (CW) L1 attack.\n",
        "\n",
        "    Parameters:\n",
        "    - imgs: The original images to be attacked.\n",
        "    - y: The true labels of the images.\n",
        "    - model: The target model to attack.\n",
        "    - removal_array: A mask array specifying which features are fixed.\n",
        "    - max_iterations: The maximum number of iterations for the attack.\n",
        "    - learning_rate: The learning rate for the optimizer.\n",
        "    - binary_search_steps: The number of steps for the binary search over the constant.\n",
        "    - confidence: Confidence value for the attack.\n",
        "    - initial_const: The initial constant for the attack loss.\n",
        "    - cons_increase_factor: The factor by which the constant is increased during binary search.\n",
        "    - device: The device on which to perform computations.\n",
        "\n",
        "    Returns:\n",
        "    - final_output: The perturbed images after the attack.\n",
        "    \"\"\"\n",
        "\n",
        "    labs = torch.zeros_like(y.squeeze(), dtype=torch.int64)\n",
        "    batch_size = imgs.shape[0]\n",
        "\n",
        "    # Expand removal array to match the batch size\n",
        "    expanded_removal_array = removal_array.expand(imgs.shape[0], -1)\n",
        "    non_fixed_features_mask = torch.bitwise_or(expanded_removal_array, 1 - imgs.to(torch.uint8))\n",
        "\n",
        "    mask_0 = non_fixed_features_mask == 0\n",
        "    mask_1 = non_fixed_features_mask == 1\n",
        "\n",
        "    imgs_tensor = imgs.clone().detach().to(dtype=torch.float32, device=device)\n",
        "    labs_tensor = labs.clone().detach().to(dtype=torch.int64, device=device)\n",
        "\n",
        "    lower_bound = torch.zeros(batch_size, dtype=torch.float32, device=device)\n",
        "    CONST = torch.full((batch_size,), initial_const, dtype=torch.float32, device=device)\n",
        "    upper_bound = torch.full((batch_size,), 1e10, dtype=torch.float32, device=device)\n",
        "\n",
        "    o_bestl0 = torch.full((batch_size,), 1e10, device=device)\n",
        "    o_bestattack = imgs.clone().detach()\n",
        "\n",
        "    o_best_loss1 = torch.full((batch_size,), float('inf'), device=device)\n",
        "    o_best_unsuccessfull_attack = imgs.clone().detach()\n",
        "    o_success_attack = torch.zeros(batch_size, dtype=torch.bool, device=device)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    for outer_step in range(binary_search_steps):\n",
        "        print(CONST)\n",
        "\n",
        "        modifier = torch.zeros_like(imgs_tensor, requires_grad=True).to(device)\n",
        "        optimizer = torch.optim.Adam([modifier], lr=max_learning_rate)\n",
        "\n",
        "        def grad_hook(grad):\n",
        "            return grad * non_fixed_features_mask\n",
        "        modifier.register_hook(grad_hook)\n",
        "\n",
        "        scheduler = CyclicLR(optimizer, base_lr=0.0001, max_lr=max_learning_rate, step_size_up=5,\n",
        "                             step_size_down=4995, mode='triangular2')\n",
        "\n",
        "        best_loss = torch.full((batch_size,), float('inf'), device=device)\n",
        "        no_improvement_count = torch.zeros(batch_size, dtype=torch.int, device=device)\n",
        "        active_mask = torch.ones(batch_size, dtype=torch.bool, device=device)\n",
        "\n",
        "        bestl0 = torch.full((batch_size,), 1e10, device=device)\n",
        "        bestscore = torch.full((batch_size,), -1, device=device)\n",
        "        success_mask = torch.zeros(batch_size, dtype=torch.bool, device=device)\n",
        "\n",
        "        for iteration in range(max_iterations):\n",
        "\n",
        "            active_indices = torch.where(active_mask)[0]\n",
        "            active_modifier = modifier[active_mask]\n",
        "            active_imgs_tensor = imgs_tensor[active_mask]\n",
        "            active_labs_tensor = labs_tensor[active_mask]\n",
        "            active_CONST = CONST[active_mask]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            modified_input = active_imgs_tensor + active_modifier\n",
        "            newimg = sin_func_torch(modified_input)\n",
        "            output = model(newimg)\n",
        "\n",
        "            #eps = 1e-2\n",
        "            perturbation = torch.where((active_imgs_tensor==1.), (active_imgs_tensor - newimg), (newimg - active_imgs_tensor))\n",
        "\n",
        "            #l2dist = torch.sum((1 - (perturbation - 1)**2 + eps)**0.5 - eps**0.5 , dim=1)\n",
        "            #l2dist = torch.sum((newimg - active_imgs_tensor) ** 2, dim=1)\n",
        "            l2dist = torch.sum(perturbation, dim=1)\n",
        "\n",
        "            real = output[torch.arange(active_indices.size(0)), active_labs_tensor]\n",
        "            other = output[torch.arange(active_indices.size(0)), active_labs_tensor + 1]\n",
        "\n",
        "            loss1 = torch.clamp((other - real + confidence), min=0.0)\n",
        "            #loss2 = l2dist\n",
        "\n",
        "            if outer_step != (binary_search_steps - 1):\n",
        "                loss = (active_CONST * loss1) + l2dist\n",
        "            else:\n",
        "                loss = loss1\n",
        "\n",
        "            total_loss = torch.sum(loss)\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            if (iteration + 1) % 10 == 0:\n",
        "                with torch.no_grad():\n",
        "                    success_mask_inloop, l0dist = compare_round(active_imgs_tensor,newimg, model,removal_array)\n",
        "                    success_mask  = torch.logical_or(success_mask_inloop, success_mask)\n",
        "                    current_lr = scheduler.get_last_lr()[0]\n",
        "\n",
        "                    improved_mask2 = (loss1 < o_best_loss1[active_mask])\n",
        "                    o_best_loss1[active_mask] = torch.where(improved_mask2, loss1, o_best_loss1[active_mask])\n",
        "                    o_best_unsuccessfull_attack[active_mask] = torch.where(improved_mask2.unsqueeze(-1), newimg, o_best_unsuccessfull_attack[active_mask])\n",
        "\n",
        "                    update_mask = (l0dist < bestl0[active_mask]) & success_mask_inloop\n",
        "                    bestl0[active_mask] = torch.where(update_mask, l0dist, bestl0[active_mask])\n",
        "                    bestscore[active_mask] = torch.where(update_mask, torch.argmax(output, dim=1), bestscore[active_mask])\n",
        "\n",
        "                    update_mask2 = (l0dist < o_bestl0[active_mask]) & success_mask_inloop\n",
        "                    o_bestl0[active_mask] = torch.where(update_mask2, l0dist, o_bestl0[active_mask])\n",
        "                    o_bestattack[active_mask] = torch.where(update_mask2.unsqueeze(-1), newimg, o_bestattack[active_mask])\n",
        "\n",
        "                    # Skip the rest of the loop for samples that haven't improved in 50 iterations\n",
        "                    #active_mask[active_mask.clone()] = no_improvement_count[active_mask.clone()] < 50\n",
        "\n",
        "                    if (iteration + 1) % 100 == 0:\n",
        "                        print(f\"Iteration {iteration + 1}: Current success : {success_mask_inloop.sum()} \\t All success : {success_mask.sum()} \\t mean(l0) : {o_bestl0[success_mask].mean():.6f} \\t Current Learning Rate: {current_lr:.3f}\")\n",
        "\n",
        "        if outer_step != (binary_search_steps - 1):\n",
        "            #o_success_attack  = torch.logical_or(o_success_attack, success_mask)\n",
        "            o_success_attack, o_l0dist = compare_round(imgs,o_bestattack, model,removal_array)\n",
        "            print(f\"outer_step {outer_step + 1}: all success : {o_success_attack.sum()} \\t mean(l0)(success) : {o_l0dist[o_success_attack].mean():.4f}\")\n",
        "            #print('o_success_attack ', o_success_attack.sum())\n",
        "            print('-------------------------------------------------------------')\n",
        "            failure_mask = ~success_mask\n",
        "            upper_bound[success_mask] = torch.minimum(upper_bound[success_mask], CONST[success_mask])\n",
        "            lower_bound[failure_mask] = torch.maximum(lower_bound[failure_mask], CONST[failure_mask])\n",
        "\n",
        "            upper_limit_mask = upper_bound < 1e10\n",
        "            CONST[upper_limit_mask] = (lower_bound[upper_limit_mask] + upper_bound[upper_limit_mask]) / 2\n",
        "            CONST[failure_mask & ~upper_limit_mask] *= cons_increase_factor\n",
        "\n",
        "    success_attack_mask = (o_bestl0 != 1e10)\n",
        "    final_output = torch.where(success_attack_mask.unsqueeze(-1), o_bestattack, o_best_unsuccessfull_attack)\n",
        "\n",
        "    return final_output\n"
      ],
      "metadata": {
        "id": "nCCzVAeojLYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#l2dist = torch.sum(perturbation, dim=1)\n",
        "attack_params =  {'device':device, 'confidence':0.5, 'removal_array':removal_array,'max_iterations': 20000, 'max_learning_rate': 0.1, 'initial_const': 1 , 'binary_search_steps' : 7, 'cons_increase_factor' : 10}\n",
        "adv = adv_predict(test_loader, combined_model, CW_l1, **attack_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "498c319d-92ef-4a5a-96c4-b7ea45bb4856",
        "id": "_HGeLEYmuQvX"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0')\n",
            "Iteration 100: Current success : 40 \t All success : 64 \t mean(l0) : 3.984375 \t Current Learning Rate: 0.098\n",
            "Iteration 200: Current success : 53 \t All success : 66 \t mean(l0) : 4.045455 \t Current Learning Rate: 0.096\n",
            "Iteration 300: Current success : 44 \t All success : 66 \t mean(l0) : 4.000000 \t Current Learning Rate: 0.094\n",
            "Iteration 400: Current success : 42 \t All success : 67 \t mean(l0) : 4.014925 \t Current Learning Rate: 0.092\n",
            "Iteration 500: Current success : 42 \t All success : 72 \t mean(l0) : 4.180556 \t Current Learning Rate: 0.090\n",
            "Iteration 600: Current success : 42 \t All success : 72 \t mean(l0) : 4.180556 \t Current Learning Rate: 0.088\n",
            "Iteration 700: Current success : 43 \t All success : 72 \t mean(l0) : 4.138889 \t Current Learning Rate: 0.086\n",
            "Iteration 800: Current success : 42 \t All success : 72 \t mean(l0) : 4.138889 \t Current Learning Rate: 0.084\n",
            "Iteration 900: Current success : 42 \t All success : 72 \t mean(l0) : 4.138889 \t Current Learning Rate: 0.082\n",
            "Iteration 1000: Current success : 42 \t All success : 72 \t mean(l0) : 4.138889 \t Current Learning Rate: 0.080\n",
            "Iteration 1100: Current success : 43 \t All success : 73 \t mean(l0) : 4.191781 \t Current Learning Rate: 0.078\n",
            "Iteration 1200: Current success : 43 \t All success : 73 \t mean(l0) : 4.191781 \t Current Learning Rate: 0.076\n",
            "Iteration 1300: Current success : 44 \t All success : 73 \t mean(l0) : 4.178082 \t Current Learning Rate: 0.074\n",
            "Iteration 1400: Current success : 43 \t All success : 73 \t mean(l0) : 4.178082 \t Current Learning Rate: 0.072\n",
            "Iteration 1500: Current success : 43 \t All success : 73 \t mean(l0) : 4.178082 \t Current Learning Rate: 0.070\n",
            "Iteration 1600: Current success : 43 \t All success : 73 \t mean(l0) : 4.178082 \t Current Learning Rate: 0.068\n",
            "Iteration 1700: Current success : 43 \t All success : 73 \t mean(l0) : 4.178082 \t Current Learning Rate: 0.066\n",
            "Iteration 1800: Current success : 43 \t All success : 73 \t mean(l0) : 4.178082 \t Current Learning Rate: 0.064\n",
            "Iteration 1900: Current success : 43 \t All success : 73 \t mean(l0) : 4.178082 \t Current Learning Rate: 0.062\n",
            "Iteration 2000: Current success : 43 \t All success : 73 \t mean(l0) : 4.178082 \t Current Learning Rate: 0.060\n",
            "Iteration 2100: Current success : 44 \t All success : 73 \t mean(l0) : 4.178082 \t Current Learning Rate: 0.058\n",
            "Iteration 2200: Current success : 43 \t All success : 73 \t mean(l0) : 4.178082 \t Current Learning Rate: 0.056\n",
            "Iteration 2300: Current success : 44 \t All success : 73 \t mean(l0) : 4.178082 \t Current Learning Rate: 0.054\n",
            "Iteration 2400: Current success : 43 \t All success : 73 \t mean(l0) : 4.178082 \t Current Learning Rate: 0.052\n",
            "Iteration 2500: Current success : 43 \t All success : 73 \t mean(l0) : 4.178082 \t Current Learning Rate: 0.050\n",
            "Iteration 2600: Current success : 43 \t All success : 73 \t mean(l0) : 4.136986 \t Current Learning Rate: 0.048\n",
            "Iteration 2700: Current success : 49 \t All success : 73 \t mean(l0) : 4.136986 \t Current Learning Rate: 0.046\n",
            "Iteration 2800: Current success : 50 \t All success : 73 \t mean(l0) : 4.136986 \t Current Learning Rate: 0.044\n",
            "Iteration 2900: Current success : 50 \t All success : 73 \t mean(l0) : 4.136986 \t Current Learning Rate: 0.042\n",
            "Iteration 3000: Current success : 52 \t All success : 73 \t mean(l0) : 4.136986 \t Current Learning Rate: 0.040\n",
            "Iteration 3100: Current success : 52 \t All success : 73 \t mean(l0) : 4.136986 \t Current Learning Rate: 0.038\n",
            "Iteration 3200: Current success : 52 \t All success : 73 \t mean(l0) : 4.136986 \t Current Learning Rate: 0.036\n",
            "Iteration 3300: Current success : 53 \t All success : 73 \t mean(l0) : 4.136986 \t Current Learning Rate: 0.034\n",
            "Iteration 3400: Current success : 52 \t All success : 73 \t mean(l0) : 4.136986 \t Current Learning Rate: 0.032\n",
            "Iteration 3500: Current success : 53 \t All success : 73 \t mean(l0) : 4.136986 \t Current Learning Rate: 0.030\n",
            "Iteration 3600: Current success : 53 \t All success : 73 \t mean(l0) : 4.136986 \t Current Learning Rate: 0.028\n",
            "Iteration 3700: Current success : 53 \t All success : 73 \t mean(l0) : 4.136986 \t Current Learning Rate: 0.026\n",
            "Iteration 3800: Current success : 52 \t All success : 73 \t mean(l0) : 4.136986 \t Current Learning Rate: 0.024\n",
            "Iteration 3900: Current success : 52 \t All success : 73 \t mean(l0) : 4.136986 \t Current Learning Rate: 0.022\n",
            "Iteration 4000: Current success : 53 \t All success : 73 \t mean(l0) : 4.136986 \t Current Learning Rate: 0.020\n",
            "Iteration 4100: Current success : 53 \t All success : 73 \t mean(l0) : 4.136986 \t Current Learning Rate: 0.018\n",
            "Iteration 4200: Current success : 53 \t All success : 73 \t mean(l0) : 4.136986 \t Current Learning Rate: 0.016\n",
            "Iteration 4300: Current success : 52 \t All success : 73 \t mean(l0) : 4.136986 \t Current Learning Rate: 0.014\n",
            "Iteration 4400: Current success : 52 \t All success : 73 \t mean(l0) : 4.136986 \t Current Learning Rate: 0.012\n",
            "Iteration 4500: Current success : 52 \t All success : 73 \t mean(l0) : 4.136986 \t Current Learning Rate: 0.010\n",
            "Iteration 4600: Current success : 52 \t All success : 73 \t mean(l0) : 4.136986 \t Current Learning Rate: 0.008\n",
            "Iteration 4700: Current success : 52 \t All success : 73 \t mean(l0) : 4.136986 \t Current Learning Rate: 0.006\n",
            "Iteration 4800: Current success : 52 \t All success : 73 \t mean(l0) : 4.136986 \t Current Learning Rate: 0.004\n",
            "Iteration 4900: Current success : 52 \t All success : 73 \t mean(l0) : 4.136986 \t Current Learning Rate: 0.002\n",
            "Iteration 5000: Current success : 52 \t All success : 73 \t mean(l0) : 4.136986 \t Current Learning Rate: 0.000\n",
            "Iteration 5100: Current success : 52 \t All success : 80 \t mean(l0) : 4.387500 \t Current Learning Rate: 0.049\n",
            "Iteration 5200: Current success : 63 \t All success : 88 \t mean(l0) : 4.159091 \t Current Learning Rate: 0.048\n",
            "Iteration 5300: Current success : 61 \t All success : 88 \t mean(l0) : 4.159091 \t Current Learning Rate: 0.047\n",
            "Iteration 5400: Current success : 61 \t All success : 88 \t mean(l0) : 4.159091 \t Current Learning Rate: 0.046\n",
            "Iteration 5500: Current success : 61 \t All success : 88 \t mean(l0) : 4.159091 \t Current Learning Rate: 0.045\n",
            "Iteration 5600: Current success : 61 \t All success : 88 \t mean(l0) : 4.159091 \t Current Learning Rate: 0.044\n",
            "Iteration 5700: Current success : 61 \t All success : 88 \t mean(l0) : 4.159091 \t Current Learning Rate: 0.043\n",
            "Iteration 5800: Current success : 60 \t All success : 88 \t mean(l0) : 4.159091 \t Current Learning Rate: 0.042\n",
            "Iteration 5900: Current success : 61 \t All success : 88 \t mean(l0) : 4.159091 \t Current Learning Rate: 0.041\n",
            "Iteration 6000: Current success : 60 \t All success : 88 \t mean(l0) : 4.159091 \t Current Learning Rate: 0.040\n",
            "Iteration 6100: Current success : 61 \t All success : 88 \t mean(l0) : 4.159091 \t Current Learning Rate: 0.039\n",
            "Iteration 6200: Current success : 59 \t All success : 88 \t mean(l0) : 4.159091 \t Current Learning Rate: 0.038\n",
            "Iteration 6300: Current success : 59 \t All success : 88 \t mean(l0) : 4.159091 \t Current Learning Rate: 0.037\n",
            "Iteration 6400: Current success : 60 \t All success : 88 \t mean(l0) : 4.159091 \t Current Learning Rate: 0.036\n",
            "Iteration 6500: Current success : 58 \t All success : 88 \t mean(l0) : 4.159091 \t Current Learning Rate: 0.035\n",
            "Iteration 6600: Current success : 59 \t All success : 88 \t mean(l0) : 4.159091 \t Current Learning Rate: 0.034\n",
            "Iteration 6700: Current success : 59 \t All success : 88 \t mean(l0) : 4.159091 \t Current Learning Rate: 0.033\n",
            "Iteration 6800: Current success : 58 \t All success : 88 \t mean(l0) : 4.159091 \t Current Learning Rate: 0.032\n",
            "Iteration 6900: Current success : 59 \t All success : 88 \t mean(l0) : 4.159091 \t Current Learning Rate: 0.031\n",
            "Iteration 7000: Current success : 59 \t All success : 88 \t mean(l0) : 4.159091 \t Current Learning Rate: 0.030\n",
            "Iteration 7100: Current success : 59 \t All success : 88 \t mean(l0) : 4.159091 \t Current Learning Rate: 0.029\n",
            "Iteration 7200: Current success : 59 \t All success : 88 \t mean(l0) : 4.159091 \t Current Learning Rate: 0.028\n",
            "Iteration 7300: Current success : 58 \t All success : 88 \t mean(l0) : 4.159091 \t Current Learning Rate: 0.027\n",
            "Iteration 7400: Current success : 59 \t All success : 88 \t mean(l0) : 4.159091 \t Current Learning Rate: 0.026\n",
            "Iteration 7500: Current success : 59 \t All success : 88 \t mean(l0) : 4.159091 \t Current Learning Rate: 0.025\n",
            "Iteration 7600: Current success : 60 \t All success : 88 \t mean(l0) : 4.159091 \t Current Learning Rate: 0.024\n",
            "Iteration 7700: Current success : 60 \t All success : 88 \t mean(l0) : 4.159091 \t Current Learning Rate: 0.023\n",
            "Iteration 7800: Current success : 60 \t All success : 88 \t mean(l0) : 4.159091 \t Current Learning Rate: 0.022\n",
            "Iteration 7900: Current success : 61 \t All success : 88 \t mean(l0) : 4.159091 \t Current Learning Rate: 0.021\n",
            "Iteration 8000: Current success : 61 \t All success : 88 \t mean(l0) : 4.159091 \t Current Learning Rate: 0.020\n",
            "Iteration 8100: Current success : 61 \t All success : 88 \t mean(l0) : 4.159091 \t Current Learning Rate: 0.019\n",
            "Iteration 8200: Current success : 61 \t All success : 88 \t mean(l0) : 4.159091 \t Current Learning Rate: 0.018\n",
            "Iteration 8300: Current success : 61 \t All success : 88 \t mean(l0) : 4.159091 \t Current Learning Rate: 0.017\n",
            "Iteration 8400: Current success : 61 \t All success : 88 \t mean(l0) : 4.159091 \t Current Learning Rate: 0.016\n",
            "Iteration 8500: Current success : 61 \t All success : 88 \t mean(l0) : 4.159091 \t Current Learning Rate: 0.015\n",
            "Iteration 8600: Current success : 61 \t All success : 88 \t mean(l0) : 4.159091 \t Current Learning Rate: 0.014\n",
            "Iteration 8700: Current success : 61 \t All success : 88 \t mean(l0) : 4.159091 \t Current Learning Rate: 0.013\n",
            "Iteration 8800: Current success : 61 \t All success : 88 \t mean(l0) : 4.159091 \t Current Learning Rate: 0.012\n",
            "Iteration 8900: Current success : 61 \t All success : 88 \t mean(l0) : 4.159091 \t Current Learning Rate: 0.011\n",
            "Iteration 9000: Current success : 61 \t All success : 88 \t mean(l0) : 4.159091 \t Current Learning Rate: 0.010\n",
            "Iteration 9100: Current success : 61 \t All success : 88 \t mean(l0) : 4.159091 \t Current Learning Rate: 0.009\n",
            "Iteration 9200: Current success : 61 \t All success : 88 \t mean(l0) : 4.159091 \t Current Learning Rate: 0.008\n",
            "Iteration 9300: Current success : 61 \t All success : 88 \t mean(l0) : 4.159091 \t Current Learning Rate: 0.007\n",
            "Iteration 9400: Current success : 61 \t All success : 88 \t mean(l0) : 4.159091 \t Current Learning Rate: 0.006\n",
            "Iteration 9500: Current success : 61 \t All success : 88 \t mean(l0) : 4.159091 \t Current Learning Rate: 0.005\n",
            "Iteration 9600: Current success : 61 \t All success : 88 \t mean(l0) : 4.159091 \t Current Learning Rate: 0.004\n",
            "Iteration 9700: Current success : 61 \t All success : 88 \t mean(l0) : 4.159091 \t Current Learning Rate: 0.003\n",
            "Iteration 9800: Current success : 61 \t All success : 88 \t mean(l0) : 4.159091 \t Current Learning Rate: 0.002\n",
            "Iteration 9900: Current success : 61 \t All success : 88 \t mean(l0) : 4.159091 \t Current Learning Rate: 0.001\n",
            "Iteration 10000: Current success : 61 \t All success : 88 \t mean(l0) : 4.159091 \t Current Learning Rate: 0.000\n",
            "Iteration 10100: Current success : 51 \t All success : 89 \t mean(l0) : 4.078652 \t Current Learning Rate: 0.025\n",
            "Iteration 10200: Current success : 57 \t All success : 90 \t mean(l0) : 4.144444 \t Current Learning Rate: 0.024\n",
            "Iteration 10300: Current success : 59 \t All success : 90 \t mean(l0) : 4.133333 \t Current Learning Rate: 0.024\n",
            "Iteration 10400: Current success : 57 \t All success : 90 \t mean(l0) : 4.133333 \t Current Learning Rate: 0.023\n",
            "Iteration 10500: Current success : 56 \t All success : 90 \t mean(l0) : 4.133333 \t Current Learning Rate: 0.023\n",
            "Iteration 10600: Current success : 56 \t All success : 90 \t mean(l0) : 4.133333 \t Current Learning Rate: 0.022\n",
            "Iteration 10700: Current success : 56 \t All success : 90 \t mean(l0) : 4.133333 \t Current Learning Rate: 0.022\n",
            "Iteration 10800: Current success : 56 \t All success : 90 \t mean(l0) : 4.133333 \t Current Learning Rate: 0.021\n",
            "Iteration 10900: Current success : 55 \t All success : 90 \t mean(l0) : 4.133333 \t Current Learning Rate: 0.021\n",
            "Iteration 11000: Current success : 56 \t All success : 90 \t mean(l0) : 4.133333 \t Current Learning Rate: 0.020\n",
            "Iteration 11100: Current success : 55 \t All success : 90 \t mean(l0) : 4.133333 \t Current Learning Rate: 0.020\n",
            "Iteration 11200: Current success : 56 \t All success : 90 \t mean(l0) : 4.133333 \t Current Learning Rate: 0.019\n",
            "Iteration 11300: Current success : 56 \t All success : 90 \t mean(l0) : 4.133333 \t Current Learning Rate: 0.019\n",
            "Iteration 11400: Current success : 56 \t All success : 90 \t mean(l0) : 4.133333 \t Current Learning Rate: 0.018\n",
            "Iteration 11500: Current success : 55 \t All success : 90 \t mean(l0) : 4.133333 \t Current Learning Rate: 0.018\n",
            "Iteration 11600: Current success : 55 \t All success : 90 \t mean(l0) : 4.133333 \t Current Learning Rate: 0.017\n",
            "Iteration 11700: Current success : 55 \t All success : 90 \t mean(l0) : 4.133333 \t Current Learning Rate: 0.017\n",
            "Iteration 11800: Current success : 56 \t All success : 90 \t mean(l0) : 4.133333 \t Current Learning Rate: 0.016\n",
            "Iteration 11900: Current success : 56 \t All success : 90 \t mean(l0) : 4.133333 \t Current Learning Rate: 0.016\n",
            "Iteration 12000: Current success : 55 \t All success : 90 \t mean(l0) : 4.133333 \t Current Learning Rate: 0.015\n",
            "Iteration 12100: Current success : 57 \t All success : 90 \t mean(l0) : 4.133333 \t Current Learning Rate: 0.015\n",
            "Iteration 12200: Current success : 56 \t All success : 90 \t mean(l0) : 4.133333 \t Current Learning Rate: 0.014\n",
            "Iteration 12300: Current success : 56 \t All success : 90 \t mean(l0) : 4.133333 \t Current Learning Rate: 0.014\n",
            "Iteration 12400: Current success : 55 \t All success : 90 \t mean(l0) : 4.133333 \t Current Learning Rate: 0.013\n",
            "Iteration 12500: Current success : 56 \t All success : 90 \t mean(l0) : 4.133333 \t Current Learning Rate: 0.013\n",
            "Iteration 12600: Current success : 56 \t All success : 90 \t mean(l0) : 4.133333 \t Current Learning Rate: 0.012\n",
            "Iteration 12700: Current success : 55 \t All success : 90 \t mean(l0) : 4.133333 \t Current Learning Rate: 0.012\n",
            "Iteration 12800: Current success : 55 \t All success : 90 \t mean(l0) : 4.133333 \t Current Learning Rate: 0.011\n",
            "Iteration 12900: Current success : 56 \t All success : 90 \t mean(l0) : 4.133333 \t Current Learning Rate: 0.011\n",
            "Iteration 13000: Current success : 61 \t All success : 90 \t mean(l0) : 4.133333 \t Current Learning Rate: 0.010\n",
            "Iteration 13100: Current success : 63 \t All success : 90 \t mean(l0) : 4.133333 \t Current Learning Rate: 0.010\n",
            "Iteration 13200: Current success : 61 \t All success : 90 \t mean(l0) : 4.133333 \t Current Learning Rate: 0.009\n",
            "Iteration 13300: Current success : 61 \t All success : 90 \t mean(l0) : 4.133333 \t Current Learning Rate: 0.009\n",
            "Iteration 13400: Current success : 63 \t All success : 90 \t mean(l0) : 4.133333 \t Current Learning Rate: 0.008\n",
            "Iteration 13500: Current success : 63 \t All success : 90 \t mean(l0) : 4.133333 \t Current Learning Rate: 0.008\n",
            "Iteration 13600: Current success : 63 \t All success : 90 \t mean(l0) : 4.133333 \t Current Learning Rate: 0.007\n",
            "Iteration 13700: Current success : 63 \t All success : 90 \t mean(l0) : 4.133333 \t Current Learning Rate: 0.007\n",
            "Iteration 13800: Current success : 63 \t All success : 90 \t mean(l0) : 4.133333 \t Current Learning Rate: 0.006\n",
            "Iteration 13900: Current success : 63 \t All success : 90 \t mean(l0) : 4.133333 \t Current Learning Rate: 0.006\n",
            "Iteration 14000: Current success : 63 \t All success : 90 \t mean(l0) : 4.133333 \t Current Learning Rate: 0.005\n",
            "Iteration 14100: Current success : 63 \t All success : 90 \t mean(l0) : 4.133333 \t Current Learning Rate: 0.005\n",
            "Iteration 14200: Current success : 63 \t All success : 90 \t mean(l0) : 4.133333 \t Current Learning Rate: 0.004\n",
            "Iteration 14300: Current success : 63 \t All success : 90 \t mean(l0) : 4.133333 \t Current Learning Rate: 0.004\n",
            "Iteration 14400: Current success : 63 \t All success : 90 \t mean(l0) : 4.133333 \t Current Learning Rate: 0.003\n",
            "Iteration 14500: Current success : 63 \t All success : 90 \t mean(l0) : 4.133333 \t Current Learning Rate: 0.003\n",
            "Iteration 14600: Current success : 63 \t All success : 90 \t mean(l0) : 4.133333 \t Current Learning Rate: 0.002\n",
            "Iteration 14700: Current success : 63 \t All success : 90 \t mean(l0) : 4.133333 \t Current Learning Rate: 0.002\n",
            "Iteration 14800: Current success : 63 \t All success : 90 \t mean(l0) : 4.133333 \t Current Learning Rate: 0.001\n",
            "Iteration 14900: Current success : 63 \t All success : 90 \t mean(l0) : 4.133333 \t Current Learning Rate: 0.001\n",
            "Iteration 15000: Current success : 63 \t All success : 90 \t mean(l0) : 4.133333 \t Current Learning Rate: 0.000\n",
            "Iteration 15100: Current success : 62 \t All success : 90 \t mean(l0) : 4.133333 \t Current Learning Rate: 0.012\n",
            "Iteration 15200: Current success : 62 \t All success : 90 \t mean(l0) : 4.133333 \t Current Learning Rate: 0.012\n",
            "Iteration 15300: Current success : 61 \t All success : 90 \t mean(l0) : 4.133333 \t Current Learning Rate: 0.012\n",
            "Iteration 15400: Current success : 61 \t All success : 90 \t mean(l0) : 4.133333 \t Current Learning Rate: 0.012\n",
            "Iteration 15500: Current success : 60 \t All success : 90 \t mean(l0) : 4.133333 \t Current Learning Rate: 0.011\n",
            "Iteration 15600: Current success : 59 \t All success : 90 \t mean(l0) : 4.133333 \t Current Learning Rate: 0.011\n",
            "Iteration 15700: Current success : 61 \t All success : 90 \t mean(l0) : 4.133333 \t Current Learning Rate: 0.011\n",
            "Iteration 15800: Current success : 60 \t All success : 90 \t mean(l0) : 4.133333 \t Current Learning Rate: 0.011\n",
            "Iteration 15900: Current success : 60 \t All success : 90 \t mean(l0) : 4.133333 \t Current Learning Rate: 0.010\n",
            "Iteration 16000: Current success : 60 \t All success : 90 \t mean(l0) : 4.133333 \t Current Learning Rate: 0.010\n",
            "Iteration 16100: Current success : 59 \t All success : 90 \t mean(l0) : 4.133333 \t Current Learning Rate: 0.010\n",
            "Iteration 16200: Current success : 59 \t All success : 90 \t mean(l0) : 4.133333 \t Current Learning Rate: 0.010\n",
            "Iteration 16300: Current success : 59 \t All success : 90 \t mean(l0) : 4.133333 \t Current Learning Rate: 0.009\n",
            "Iteration 16400: Current success : 59 \t All success : 90 \t mean(l0) : 4.133333 \t Current Learning Rate: 0.009\n",
            "Iteration 16500: Current success : 60 \t All success : 90 \t mean(l0) : 4.133333 \t Current Learning Rate: 0.009\n",
            "Iteration 16600: Current success : 59 \t All success : 90 \t mean(l0) : 4.133333 \t Current Learning Rate: 0.009\n",
            "Iteration 16700: Current success : 58 \t All success : 90 \t mean(l0) : 4.133333 \t Current Learning Rate: 0.008\n",
            "Iteration 16800: Current success : 60 \t All success : 90 \t mean(l0) : 4.133333 \t Current Learning Rate: 0.008\n",
            "Iteration 16900: Current success : 60 \t All success : 90 \t mean(l0) : 4.133333 \t Current Learning Rate: 0.008\n",
            "Iteration 17000: Current success : 59 \t All success : 90 \t mean(l0) : 4.133333 \t Current Learning Rate: 0.008\n",
            "Iteration 17100: Current success : 59 \t All success : 90 \t mean(l0) : 4.133333 \t Current Learning Rate: 0.007\n",
            "Iteration 17200: Current success : 60 \t All success : 90 \t mean(l0) : 4.133333 \t Current Learning Rate: 0.007\n",
            "Iteration 17300: Current success : 61 \t All success : 90 \t mean(l0) : 4.133333 \t Current Learning Rate: 0.007\n",
            "Iteration 17400: Current success : 58 \t All success : 90 \t mean(l0) : 4.133333 \t Current Learning Rate: 0.007\n",
            "Iteration 17500: Current success : 60 \t All success : 90 \t mean(l0) : 4.133333 \t Current Learning Rate: 0.006\n",
            "Iteration 17600: Current success : 59 \t All success : 90 \t mean(l0) : 4.133333 \t Current Learning Rate: 0.006\n",
            "Iteration 17700: Current success : 60 \t All success : 90 \t mean(l0) : 4.133333 \t Current Learning Rate: 0.006\n",
            "Iteration 17800: Current success : 59 \t All success : 90 \t mean(l0) : 4.133333 \t Current Learning Rate: 0.006\n",
            "Iteration 17900: Current success : 60 \t All success : 90 \t mean(l0) : 4.133333 \t Current Learning Rate: 0.005\n",
            "Iteration 18000: Current success : 62 \t All success : 90 \t mean(l0) : 4.133333 \t Current Learning Rate: 0.005\n",
            "Iteration 18100: Current success : 62 \t All success : 90 \t mean(l0) : 4.133333 \t Current Learning Rate: 0.005\n",
            "Iteration 18200: Current success : 61 \t All success : 90 \t mean(l0) : 4.133333 \t Current Learning Rate: 0.005\n",
            "Iteration 18300: Current success : 62 \t All success : 90 \t mean(l0) : 4.133333 \t Current Learning Rate: 0.004\n",
            "Iteration 18400: Current success : 62 \t All success : 90 \t mean(l0) : 4.133333 \t Current Learning Rate: 0.004\n",
            "Iteration 18500: Current success : 63 \t All success : 90 \t mean(l0) : 4.133333 \t Current Learning Rate: 0.004\n",
            "Iteration 18600: Current success : 64 \t All success : 90 \t mean(l0) : 4.133333 \t Current Learning Rate: 0.004\n",
            "Iteration 18700: Current success : 63 \t All success : 90 \t mean(l0) : 4.133333 \t Current Learning Rate: 0.003\n",
            "Iteration 18800: Current success : 63 \t All success : 90 \t mean(l0) : 4.133333 \t Current Learning Rate: 0.003\n",
            "Iteration 18900: Current success : 62 \t All success : 90 \t mean(l0) : 4.133333 \t Current Learning Rate: 0.003\n",
            "Iteration 19000: Current success : 63 \t All success : 90 \t mean(l0) : 4.133333 \t Current Learning Rate: 0.003\n",
            "Iteration 19100: Current success : 63 \t All success : 90 \t mean(l0) : 4.133333 \t Current Learning Rate: 0.002\n",
            "Iteration 19200: Current success : 63 \t All success : 90 \t mean(l0) : 4.133333 \t Current Learning Rate: 0.002\n",
            "Iteration 19300: Current success : 63 \t All success : 90 \t mean(l0) : 4.133333 \t Current Learning Rate: 0.002\n",
            "Iteration 19400: Current success : 63 \t All success : 90 \t mean(l0) : 4.133333 \t Current Learning Rate: 0.002\n",
            "Iteration 19500: Current success : 63 \t All success : 90 \t mean(l0) : 4.133333 \t Current Learning Rate: 0.001\n",
            "Iteration 19600: Current success : 63 \t All success : 90 \t mean(l0) : 4.133333 \t Current Learning Rate: 0.001\n",
            "Iteration 19700: Current success : 63 \t All success : 90 \t mean(l0) : 4.133333 \t Current Learning Rate: 0.001\n",
            "Iteration 19800: Current success : 63 \t All success : 90 \t mean(l0) : 4.133333 \t Current Learning Rate: 0.001\n",
            "Iteration 19900: Current success : 63 \t All success : 90 \t mean(l0) : 4.133333 \t Current Learning Rate: 0.000\n",
            "Iteration 20000: Current success : 63 \t All success : 90 \t mean(l0) : 4.133333 \t Current Learning Rate: 0.000\n",
            "outer_step 1: all success : 91 \t mean(l0)(success) : 4.1538\n",
            "-------------------------------------------------------------\n",
            "tensor([10., 10., 10.,  ..., 10., 10., 10.], device='cuda:0')\n",
            "Iteration 100: Current success : 54 \t All success : 79 \t mean(l0) : 7.810127 \t Current Learning Rate: 0.098\n",
            "Iteration 200: Current success : 57 \t All success : 83 \t mean(l0) : 8.409638 \t Current Learning Rate: 0.096\n",
            "Iteration 300: Current success : 60 \t All success : 91 \t mean(l0) : 10.857143 \t Current Learning Rate: 0.094\n",
            "Iteration 400: Current success : 71 \t All success : 103 \t mean(l0) : 13.941748 \t Current Learning Rate: 0.092\n",
            "Iteration 500: Current success : 93 \t All success : 136 \t mean(l0) : 15.088235 \t Current Learning Rate: 0.090\n",
            "Iteration 600: Current success : 95 \t All success : 138 \t mean(l0) : 15.420290 \t Current Learning Rate: 0.088\n",
            "Iteration 700: Current success : 99 \t All success : 138 \t mean(l0) : 15.289855 \t Current Learning Rate: 0.086\n",
            "Iteration 800: Current success : 98 \t All success : 139 \t mean(l0) : 15.539569 \t Current Learning Rate: 0.084\n",
            "Iteration 900: Current success : 98 \t All success : 139 \t mean(l0) : 15.532374 \t Current Learning Rate: 0.082\n",
            "Iteration 1000: Current success : 101 \t All success : 140 \t mean(l0) : 15.642858 \t Current Learning Rate: 0.080\n",
            "Iteration 1100: Current success : 99 \t All success : 140 \t mean(l0) : 15.642858 \t Current Learning Rate: 0.078\n",
            "Iteration 1200: Current success : 102 \t All success : 143 \t mean(l0) : 15.398602 \t Current Learning Rate: 0.076\n",
            "Iteration 1300: Current success : 102 \t All success : 144 \t mean(l0) : 15.687500 \t Current Learning Rate: 0.074\n",
            "Iteration 1400: Current success : 102 \t All success : 144 \t mean(l0) : 15.687500 \t Current Learning Rate: 0.072\n",
            "Iteration 1500: Current success : 102 \t All success : 144 \t mean(l0) : 15.687500 \t Current Learning Rate: 0.070\n",
            "Iteration 1600: Current success : 102 \t All success : 144 \t mean(l0) : 15.687500 \t Current Learning Rate: 0.068\n",
            "Iteration 1700: Current success : 102 \t All success : 147 \t mean(l0) : 15.489796 \t Current Learning Rate: 0.066\n",
            "Iteration 1800: Current success : 102 \t All success : 147 \t mean(l0) : 15.489796 \t Current Learning Rate: 0.064\n",
            "Iteration 1900: Current success : 102 \t All success : 147 \t mean(l0) : 15.489796 \t Current Learning Rate: 0.062\n",
            "Iteration 2000: Current success : 102 \t All success : 147 \t mean(l0) : 15.489796 \t Current Learning Rate: 0.060\n",
            "Iteration 2100: Current success : 102 \t All success : 147 \t mean(l0) : 15.489796 \t Current Learning Rate: 0.058\n",
            "Iteration 2200: Current success : 103 \t All success : 148 \t mean(l0) : 15.587838 \t Current Learning Rate: 0.056\n",
            "Iteration 2300: Current success : 104 \t All success : 148 \t mean(l0) : 15.587838 \t Current Learning Rate: 0.054\n",
            "Iteration 2400: Current success : 103 \t All success : 148 \t mean(l0) : 15.587838 \t Current Learning Rate: 0.052\n",
            "Iteration 2500: Current success : 106 \t All success : 151 \t mean(l0) : 15.874172 \t Current Learning Rate: 0.050\n",
            "Iteration 2600: Current success : 106 \t All success : 151 \t mean(l0) : 15.569536 \t Current Learning Rate: 0.048\n",
            "Iteration 2700: Current success : 110 \t All success : 151 \t mean(l0) : 15.569536 \t Current Learning Rate: 0.046\n",
            "Iteration 2800: Current success : 114 \t All success : 151 \t mean(l0) : 15.543046 \t Current Learning Rate: 0.044\n",
            "Iteration 2900: Current success : 112 \t All success : 151 \t mean(l0) : 15.543046 \t Current Learning Rate: 0.042\n",
            "Iteration 3000: Current success : 114 \t All success : 151 \t mean(l0) : 15.543046 \t Current Learning Rate: 0.040\n",
            "Iteration 3100: Current success : 113 \t All success : 151 \t mean(l0) : 15.543046 \t Current Learning Rate: 0.038\n",
            "Iteration 3200: Current success : 114 \t All success : 151 \t mean(l0) : 15.543046 \t Current Learning Rate: 0.036\n",
            "Iteration 3300: Current success : 116 \t All success : 151 \t mean(l0) : 15.543046 \t Current Learning Rate: 0.034\n",
            "Iteration 3400: Current success : 116 \t All success : 151 \t mean(l0) : 15.543046 \t Current Learning Rate: 0.032\n",
            "Iteration 3500: Current success : 116 \t All success : 151 \t mean(l0) : 15.543046 \t Current Learning Rate: 0.030\n",
            "Iteration 3600: Current success : 116 \t All success : 151 \t mean(l0) : 15.543046 \t Current Learning Rate: 0.028\n",
            "Iteration 3700: Current success : 116 \t All success : 151 \t mean(l0) : 15.543046 \t Current Learning Rate: 0.026\n",
            "Iteration 3800: Current success : 116 \t All success : 151 \t mean(l0) : 15.543046 \t Current Learning Rate: 0.024\n",
            "Iteration 3900: Current success : 116 \t All success : 151 \t mean(l0) : 15.543046 \t Current Learning Rate: 0.022\n",
            "Iteration 4000: Current success : 116 \t All success : 151 \t mean(l0) : 15.543046 \t Current Learning Rate: 0.020\n",
            "Iteration 4100: Current success : 116 \t All success : 151 \t mean(l0) : 15.543046 \t Current Learning Rate: 0.018\n",
            "Iteration 4200: Current success : 116 \t All success : 151 \t mean(l0) : 15.543046 \t Current Learning Rate: 0.016\n",
            "Iteration 4300: Current success : 116 \t All success : 151 \t mean(l0) : 15.543046 \t Current Learning Rate: 0.014\n",
            "Iteration 4400: Current success : 116 \t All success : 151 \t mean(l0) : 15.543046 \t Current Learning Rate: 0.012\n",
            "Iteration 4500: Current success : 116 \t All success : 151 \t mean(l0) : 15.543046 \t Current Learning Rate: 0.010\n",
            "Iteration 4600: Current success : 116 \t All success : 151 \t mean(l0) : 15.543046 \t Current Learning Rate: 0.008\n",
            "Iteration 4700: Current success : 116 \t All success : 151 \t mean(l0) : 15.543046 \t Current Learning Rate: 0.006\n",
            "Iteration 4800: Current success : 116 \t All success : 151 \t mean(l0) : 15.543046 \t Current Learning Rate: 0.004\n",
            "Iteration 4900: Current success : 116 \t All success : 151 \t mean(l0) : 15.543046 \t Current Learning Rate: 0.002\n",
            "Iteration 5000: Current success : 116 \t All success : 151 \t mean(l0) : 15.543046 \t Current Learning Rate: 0.000\n",
            "Iteration 5100: Current success : 96 \t All success : 164 \t mean(l0) : 16.121950 \t Current Learning Rate: 0.049\n",
            "Iteration 5200: Current success : 95 \t All success : 169 \t mean(l0) : 16.615385 \t Current Learning Rate: 0.048\n",
            "Iteration 5300: Current success : 92 \t All success : 171 \t mean(l0) : 16.567251 \t Current Learning Rate: 0.047\n",
            "Iteration 5400: Current success : 92 \t All success : 171 \t mean(l0) : 16.526316 \t Current Learning Rate: 0.046\n",
            "Iteration 5500: Current success : 91 \t All success : 171 \t mean(l0) : 16.520468 \t Current Learning Rate: 0.045\n",
            "Iteration 5600: Current success : 93 \t All success : 171 \t mean(l0) : 16.473684 \t Current Learning Rate: 0.044\n",
            "Iteration 5700: Current success : 92 \t All success : 171 \t mean(l0) : 16.473684 \t Current Learning Rate: 0.043\n",
            "Iteration 5800: Current success : 93 \t All success : 171 \t mean(l0) : 16.473684 \t Current Learning Rate: 0.042\n",
            "Iteration 5900: Current success : 92 \t All success : 171 \t mean(l0) : 16.473684 \t Current Learning Rate: 0.041\n",
            "Iteration 6000: Current success : 90 \t All success : 171 \t mean(l0) : 16.456141 \t Current Learning Rate: 0.040\n",
            "Iteration 6100: Current success : 92 \t All success : 171 \t mean(l0) : 16.456141 \t Current Learning Rate: 0.039\n",
            "Iteration 6200: Current success : 91 \t All success : 171 \t mean(l0) : 16.438597 \t Current Learning Rate: 0.038\n",
            "Iteration 6300: Current success : 92 \t All success : 171 \t mean(l0) : 16.426901 \t Current Learning Rate: 0.037\n",
            "Iteration 6400: Current success : 93 \t All success : 171 \t mean(l0) : 16.426901 \t Current Learning Rate: 0.036\n",
            "Iteration 6500: Current success : 90 \t All success : 171 \t mean(l0) : 16.426901 \t Current Learning Rate: 0.035\n",
            "Iteration 6600: Current success : 88 \t All success : 171 \t mean(l0) : 16.426901 \t Current Learning Rate: 0.034\n",
            "Iteration 6700: Current success : 91 \t All success : 171 \t mean(l0) : 16.426901 \t Current Learning Rate: 0.033\n",
            "Iteration 6800: Current success : 89 \t All success : 171 \t mean(l0) : 16.426901 \t Current Learning Rate: 0.032\n",
            "Iteration 6900: Current success : 87 \t All success : 171 \t mean(l0) : 16.426901 \t Current Learning Rate: 0.031\n",
            "Iteration 7000: Current success : 88 \t All success : 171 \t mean(l0) : 16.426901 \t Current Learning Rate: 0.030\n",
            "Iteration 7100: Current success : 89 \t All success : 171 \t mean(l0) : 16.426901 \t Current Learning Rate: 0.029\n",
            "Iteration 7200: Current success : 87 \t All success : 171 \t mean(l0) : 16.426901 \t Current Learning Rate: 0.028\n",
            "Iteration 7300: Current success : 89 \t All success : 171 \t mean(l0) : 16.426901 \t Current Learning Rate: 0.027\n",
            "Iteration 7400: Current success : 88 \t All success : 171 \t mean(l0) : 16.426901 \t Current Learning Rate: 0.026\n",
            "Iteration 7500: Current success : 87 \t All success : 171 \t mean(l0) : 16.426901 \t Current Learning Rate: 0.025\n",
            "Iteration 7600: Current success : 90 \t All success : 171 \t mean(l0) : 16.426901 \t Current Learning Rate: 0.024\n",
            "Iteration 7700: Current success : 92 \t All success : 171 \t mean(l0) : 16.421053 \t Current Learning Rate: 0.023\n",
            "Iteration 7800: Current success : 91 \t All success : 171 \t mean(l0) : 16.421053 \t Current Learning Rate: 0.022\n",
            "Iteration 7900: Current success : 92 \t All success : 171 \t mean(l0) : 16.421053 \t Current Learning Rate: 0.021\n",
            "Iteration 8000: Current success : 92 \t All success : 171 \t mean(l0) : 16.421053 \t Current Learning Rate: 0.020\n",
            "Iteration 8100: Current success : 93 \t All success : 171 \t mean(l0) : 16.421053 \t Current Learning Rate: 0.019\n",
            "Iteration 8200: Current success : 94 \t All success : 171 \t mean(l0) : 16.421053 \t Current Learning Rate: 0.018\n",
            "Iteration 8300: Current success : 94 \t All success : 171 \t mean(l0) : 16.421053 \t Current Learning Rate: 0.017\n",
            "Iteration 8400: Current success : 94 \t All success : 171 \t mean(l0) : 16.421053 \t Current Learning Rate: 0.016\n",
            "Iteration 8500: Current success : 95 \t All success : 171 \t mean(l0) : 16.421053 \t Current Learning Rate: 0.015\n",
            "Iteration 8600: Current success : 95 \t All success : 171 \t mean(l0) : 16.421053 \t Current Learning Rate: 0.014\n",
            "Iteration 8700: Current success : 95 \t All success : 171 \t mean(l0) : 16.421053 \t Current Learning Rate: 0.013\n",
            "Iteration 8800: Current success : 95 \t All success : 171 \t mean(l0) : 16.421053 \t Current Learning Rate: 0.012\n",
            "Iteration 8900: Current success : 95 \t All success : 171 \t mean(l0) : 16.421053 \t Current Learning Rate: 0.011\n",
            "Iteration 9000: Current success : 95 \t All success : 171 \t mean(l0) : 16.421053 \t Current Learning Rate: 0.010\n",
            "Iteration 9100: Current success : 95 \t All success : 171 \t mean(l0) : 16.421053 \t Current Learning Rate: 0.009\n",
            "Iteration 9200: Current success : 95 \t All success : 171 \t mean(l0) : 16.421053 \t Current Learning Rate: 0.008\n",
            "Iteration 9300: Current success : 95 \t All success : 171 \t mean(l0) : 16.421053 \t Current Learning Rate: 0.007\n",
            "Iteration 9400: Current success : 95 \t All success : 171 \t mean(l0) : 16.421053 \t Current Learning Rate: 0.006\n",
            "Iteration 9500: Current success : 95 \t All success : 171 \t mean(l0) : 16.421053 \t Current Learning Rate: 0.005\n",
            "Iteration 9600: Current success : 95 \t All success : 171 \t mean(l0) : 16.421053 \t Current Learning Rate: 0.004\n",
            "Iteration 9700: Current success : 95 \t All success : 171 \t mean(l0) : 16.421053 \t Current Learning Rate: 0.003\n",
            "Iteration 9800: Current success : 95 \t All success : 171 \t mean(l0) : 16.421053 \t Current Learning Rate: 0.002\n",
            "Iteration 9900: Current success : 95 \t All success : 171 \t mean(l0) : 16.421053 \t Current Learning Rate: 0.001\n",
            "Iteration 10000: Current success : 95 \t All success : 171 \t mean(l0) : 16.421053 \t Current Learning Rate: 0.000\n",
            "Iteration 10100: Current success : 85 \t All success : 171 \t mean(l0) : 16.251463 \t Current Learning Rate: 0.025\n",
            "Iteration 10200: Current success : 81 \t All success : 171 \t mean(l0) : 16.233919 \t Current Learning Rate: 0.024\n",
            "Iteration 10300: Current success : 81 \t All success : 171 \t mean(l0) : 16.222223 \t Current Learning Rate: 0.024\n",
            "Iteration 10400: Current success : 82 \t All success : 171 \t mean(l0) : 16.198830 \t Current Learning Rate: 0.023\n",
            "Iteration 10500: Current success : 83 \t All success : 171 \t mean(l0) : 16.175438 \t Current Learning Rate: 0.023\n",
            "Iteration 10600: Current success : 82 \t All success : 171 \t mean(l0) : 16.169590 \t Current Learning Rate: 0.022\n",
            "Iteration 10700: Current success : 82 \t All success : 171 \t mean(l0) : 16.169590 \t Current Learning Rate: 0.022\n",
            "Iteration 10800: Current success : 82 \t All success : 171 \t mean(l0) : 16.157894 \t Current Learning Rate: 0.021\n",
            "Iteration 10900: Current success : 82 \t All success : 171 \t mean(l0) : 16.111111 \t Current Learning Rate: 0.021\n",
            "Iteration 11000: Current success : 82 \t All success : 171 \t mean(l0) : 16.076023 \t Current Learning Rate: 0.020\n",
            "Iteration 11100: Current success : 81 \t All success : 171 \t mean(l0) : 16.076023 \t Current Learning Rate: 0.020\n",
            "Iteration 11200: Current success : 81 \t All success : 171 \t mean(l0) : 16.076023 \t Current Learning Rate: 0.019\n",
            "Iteration 11300: Current success : 81 \t All success : 171 \t mean(l0) : 16.076023 \t Current Learning Rate: 0.019\n",
            "Iteration 11400: Current success : 81 \t All success : 171 \t mean(l0) : 16.076023 \t Current Learning Rate: 0.018\n",
            "Iteration 11500: Current success : 81 \t All success : 171 \t mean(l0) : 16.076023 \t Current Learning Rate: 0.018\n",
            "Iteration 11600: Current success : 81 \t All success : 171 \t mean(l0) : 16.076023 \t Current Learning Rate: 0.017\n",
            "Iteration 11700: Current success : 81 \t All success : 171 \t mean(l0) : 16.076023 \t Current Learning Rate: 0.017\n",
            "Iteration 11800: Current success : 81 \t All success : 171 \t mean(l0) : 16.058479 \t Current Learning Rate: 0.016\n",
            "Iteration 11900: Current success : 81 \t All success : 171 \t mean(l0) : 16.058479 \t Current Learning Rate: 0.016\n",
            "Iteration 12000: Current success : 81 \t All success : 171 \t mean(l0) : 16.058479 \t Current Learning Rate: 0.015\n",
            "Iteration 12100: Current success : 82 \t All success : 171 \t mean(l0) : 16.058479 \t Current Learning Rate: 0.015\n",
            "Iteration 12200: Current success : 82 \t All success : 171 \t mean(l0) : 16.058479 \t Current Learning Rate: 0.014\n",
            "Iteration 12300: Current success : 82 \t All success : 171 \t mean(l0) : 16.040936 \t Current Learning Rate: 0.014\n",
            "Iteration 12400: Current success : 82 \t All success : 171 \t mean(l0) : 16.040936 \t Current Learning Rate: 0.013\n",
            "Iteration 12500: Current success : 82 \t All success : 171 \t mean(l0) : 16.040936 \t Current Learning Rate: 0.013\n",
            "Iteration 12600: Current success : 82 \t All success : 171 \t mean(l0) : 16.040936 \t Current Learning Rate: 0.012\n",
            "Iteration 12700: Current success : 82 \t All success : 171 \t mean(l0) : 16.040936 \t Current Learning Rate: 0.012\n",
            "Iteration 12800: Current success : 82 \t All success : 171 \t mean(l0) : 16.040936 \t Current Learning Rate: 0.011\n",
            "Iteration 12900: Current success : 82 \t All success : 171 \t mean(l0) : 16.040936 \t Current Learning Rate: 0.011\n",
            "Iteration 13000: Current success : 88 \t All success : 171 \t mean(l0) : 16.040936 \t Current Learning Rate: 0.010\n",
            "Iteration 13100: Current success : 90 \t All success : 171 \t mean(l0) : 16.040936 \t Current Learning Rate: 0.010\n",
            "Iteration 13200: Current success : 91 \t All success : 171 \t mean(l0) : 16.040936 \t Current Learning Rate: 0.009\n",
            "Iteration 13300: Current success : 91 \t All success : 171 \t mean(l0) : 16.040936 \t Current Learning Rate: 0.009\n",
            "Iteration 13400: Current success : 93 \t All success : 171 \t mean(l0) : 16.040936 \t Current Learning Rate: 0.008\n",
            "Iteration 13500: Current success : 93 \t All success : 171 \t mean(l0) : 16.040936 \t Current Learning Rate: 0.008\n",
            "Iteration 13600: Current success : 93 \t All success : 171 \t mean(l0) : 16.040936 \t Current Learning Rate: 0.007\n",
            "Iteration 13700: Current success : 93 \t All success : 171 \t mean(l0) : 16.040936 \t Current Learning Rate: 0.007\n",
            "Iteration 13800: Current success : 93 \t All success : 171 \t mean(l0) : 16.040936 \t Current Learning Rate: 0.006\n",
            "Iteration 13900: Current success : 93 \t All success : 171 \t mean(l0) : 16.040936 \t Current Learning Rate: 0.006\n",
            "Iteration 14000: Current success : 93 \t All success : 171 \t mean(l0) : 16.040936 \t Current Learning Rate: 0.005\n",
            "Iteration 14100: Current success : 93 \t All success : 171 \t mean(l0) : 16.040936 \t Current Learning Rate: 0.005\n",
            "Iteration 14200: Current success : 93 \t All success : 171 \t mean(l0) : 16.040936 \t Current Learning Rate: 0.004\n",
            "Iteration 14300: Current success : 93 \t All success : 171 \t mean(l0) : 16.040936 \t Current Learning Rate: 0.004\n",
            "Iteration 14400: Current success : 93 \t All success : 171 \t mean(l0) : 16.040936 \t Current Learning Rate: 0.003\n",
            "Iteration 14500: Current success : 93 \t All success : 171 \t mean(l0) : 16.040936 \t Current Learning Rate: 0.003\n",
            "Iteration 14600: Current success : 93 \t All success : 171 \t mean(l0) : 16.040936 \t Current Learning Rate: 0.002\n",
            "Iteration 14700: Current success : 93 \t All success : 171 \t mean(l0) : 16.040936 \t Current Learning Rate: 0.002\n",
            "Iteration 14800: Current success : 93 \t All success : 171 \t mean(l0) : 16.040936 \t Current Learning Rate: 0.001\n",
            "Iteration 14900: Current success : 93 \t All success : 171 \t mean(l0) : 16.040936 \t Current Learning Rate: 0.001\n",
            "Iteration 15000: Current success : 93 \t All success : 171 \t mean(l0) : 16.040936 \t Current Learning Rate: 0.000\n",
            "Iteration 15100: Current success : 96 \t All success : 171 \t mean(l0) : 15.982456 \t Current Learning Rate: 0.012\n",
            "Iteration 15200: Current success : 89 \t All success : 171 \t mean(l0) : 15.964912 \t Current Learning Rate: 0.012\n",
            "Iteration 15300: Current success : 90 \t All success : 171 \t mean(l0) : 15.964912 \t Current Learning Rate: 0.012\n",
            "Iteration 15400: Current success : 90 \t All success : 171 \t mean(l0) : 15.929825 \t Current Learning Rate: 0.012\n",
            "Iteration 15500: Current success : 88 \t All success : 171 \t mean(l0) : 15.929825 \t Current Learning Rate: 0.011\n",
            "Iteration 15600: Current success : 88 \t All success : 171 \t mean(l0) : 15.923977 \t Current Learning Rate: 0.011\n",
            "Iteration 15700: Current success : 88 \t All success : 171 \t mean(l0) : 15.923977 \t Current Learning Rate: 0.011\n",
            "Iteration 15800: Current success : 85 \t All success : 171 \t mean(l0) : 15.918129 \t Current Learning Rate: 0.011\n",
            "Iteration 15900: Current success : 85 \t All success : 171 \t mean(l0) : 15.918129 \t Current Learning Rate: 0.010\n",
            "Iteration 16000: Current success : 87 \t All success : 171 \t mean(l0) : 15.918129 \t Current Learning Rate: 0.010\n",
            "Iteration 16100: Current success : 84 \t All success : 171 \t mean(l0) : 15.918129 \t Current Learning Rate: 0.010\n",
            "Iteration 16200: Current success : 84 \t All success : 171 \t mean(l0) : 15.912281 \t Current Learning Rate: 0.010\n",
            "Iteration 16300: Current success : 87 \t All success : 171 \t mean(l0) : 15.912281 \t Current Learning Rate: 0.009\n",
            "Iteration 16400: Current success : 86 \t All success : 171 \t mean(l0) : 15.912281 \t Current Learning Rate: 0.009\n",
            "Iteration 16500: Current success : 85 \t All success : 171 \t mean(l0) : 15.912281 \t Current Learning Rate: 0.009\n",
            "Iteration 16600: Current success : 85 \t All success : 171 \t mean(l0) : 15.912281 \t Current Learning Rate: 0.009\n",
            "Iteration 16700: Current success : 86 \t All success : 171 \t mean(l0) : 15.912281 \t Current Learning Rate: 0.008\n",
            "Iteration 16800: Current success : 83 \t All success : 171 \t mean(l0) : 15.912281 \t Current Learning Rate: 0.008\n",
            "Iteration 16900: Current success : 86 \t All success : 171 \t mean(l0) : 15.906433 \t Current Learning Rate: 0.008\n",
            "Iteration 17000: Current success : 85 \t All success : 171 \t mean(l0) : 15.906433 \t Current Learning Rate: 0.008\n",
            "Iteration 17100: Current success : 86 \t All success : 171 \t mean(l0) : 15.900585 \t Current Learning Rate: 0.007\n",
            "Iteration 17200: Current success : 86 \t All success : 171 \t mean(l0) : 15.900585 \t Current Learning Rate: 0.007\n",
            "Iteration 17300: Current success : 84 \t All success : 171 \t mean(l0) : 15.900585 \t Current Learning Rate: 0.007\n",
            "Iteration 17400: Current success : 88 \t All success : 171 \t mean(l0) : 15.900585 \t Current Learning Rate: 0.007\n",
            "Iteration 17500: Current success : 84 \t All success : 171 \t mean(l0) : 15.900585 \t Current Learning Rate: 0.006\n",
            "Iteration 17600: Current success : 85 \t All success : 171 \t mean(l0) : 15.894737 \t Current Learning Rate: 0.006\n",
            "Iteration 17700: Current success : 87 \t All success : 171 \t mean(l0) : 15.894737 \t Current Learning Rate: 0.006\n",
            "Iteration 17800: Current success : 86 \t All success : 171 \t mean(l0) : 15.894737 \t Current Learning Rate: 0.006\n",
            "Iteration 17900: Current success : 88 \t All success : 171 \t mean(l0) : 15.894737 \t Current Learning Rate: 0.005\n",
            "Iteration 18000: Current success : 89 \t All success : 171 \t mean(l0) : 15.894737 \t Current Learning Rate: 0.005\n",
            "Iteration 18100: Current success : 89 \t All success : 171 \t mean(l0) : 15.894737 \t Current Learning Rate: 0.005\n",
            "Iteration 18200: Current success : 91 \t All success : 171 \t mean(l0) : 15.894737 \t Current Learning Rate: 0.005\n",
            "Iteration 18300: Current success : 91 \t All success : 171 \t mean(l0) : 15.894737 \t Current Learning Rate: 0.004\n",
            "Iteration 18400: Current success : 91 \t All success : 171 \t mean(l0) : 15.894737 \t Current Learning Rate: 0.004\n",
            "Iteration 18500: Current success : 92 \t All success : 171 \t mean(l0) : 15.894737 \t Current Learning Rate: 0.004\n",
            "Iteration 18600: Current success : 93 \t All success : 171 \t mean(l0) : 15.894737 \t Current Learning Rate: 0.004\n",
            "Iteration 18700: Current success : 93 \t All success : 171 \t mean(l0) : 15.894737 \t Current Learning Rate: 0.003\n",
            "Iteration 18800: Current success : 93 \t All success : 171 \t mean(l0) : 15.894737 \t Current Learning Rate: 0.003\n",
            "Iteration 18900: Current success : 93 \t All success : 171 \t mean(l0) : 15.894737 \t Current Learning Rate: 0.003\n",
            "Iteration 19000: Current success : 93 \t All success : 171 \t mean(l0) : 15.894737 \t Current Learning Rate: 0.003\n",
            "Iteration 19100: Current success : 93 \t All success : 171 \t mean(l0) : 15.894737 \t Current Learning Rate: 0.002\n",
            "Iteration 19200: Current success : 93 \t All success : 171 \t mean(l0) : 15.894737 \t Current Learning Rate: 0.002\n",
            "Iteration 19300: Current success : 93 \t All success : 171 \t mean(l0) : 15.894737 \t Current Learning Rate: 0.002\n",
            "Iteration 19400: Current success : 93 \t All success : 171 \t mean(l0) : 15.894737 \t Current Learning Rate: 0.002\n",
            "Iteration 19500: Current success : 93 \t All success : 171 \t mean(l0) : 15.894737 \t Current Learning Rate: 0.001\n",
            "Iteration 19600: Current success : 93 \t All success : 171 \t mean(l0) : 15.894737 \t Current Learning Rate: 0.001\n",
            "Iteration 19700: Current success : 93 \t All success : 171 \t mean(l0) : 15.894737 \t Current Learning Rate: 0.001\n",
            "Iteration 19800: Current success : 93 \t All success : 171 \t mean(l0) : 15.894737 \t Current Learning Rate: 0.001\n",
            "Iteration 19900: Current success : 93 \t All success : 171 \t mean(l0) : 15.894737 \t Current Learning Rate: 0.000\n",
            "Iteration 20000: Current success : 93 \t All success : 171 \t mean(l0) : 15.894737 \t Current Learning Rate: 0.000\n",
            "outer_step 2: all success : 193 \t mean(l0)(success) : 13.1554\n",
            "-------------------------------------------------------------\n",
            "tensor([  5.5000, 100.0000, 100.0000,  ..., 100.0000, 100.0000, 100.0000],\n",
            "       device='cuda:0')\n",
            "Iteration 100: Current success : 54 \t All success : 79 \t mean(l0) : 7.962026 \t Current Learning Rate: 0.098\n",
            "Iteration 200: Current success : 53 \t All success : 88 \t mean(l0) : 8.579546 \t Current Learning Rate: 0.096\n",
            "Iteration 300: Current success : 60 \t All success : 94 \t mean(l0) : 10.563829 \t Current Learning Rate: 0.094\n",
            "Iteration 400: Current success : 59 \t All success : 97 \t mean(l0) : 11.721649 \t Current Learning Rate: 0.092\n",
            "Iteration 500: Current success : 60 \t All success : 99 \t mean(l0) : 11.808081 \t Current Learning Rate: 0.090\n",
            "Iteration 600: Current success : 61 \t All success : 100 \t mean(l0) : 11.969999 \t Current Learning Rate: 0.088\n",
            "Iteration 700: Current success : 61 \t All success : 101 \t mean(l0) : 12.079207 \t Current Learning Rate: 0.086\n",
            "Iteration 800: Current success : 61 \t All success : 101 \t mean(l0) : 12.069306 \t Current Learning Rate: 0.084\n",
            "Iteration 900: Current success : 62 \t All success : 103 \t mean(l0) : 13.087379 \t Current Learning Rate: 0.082\n",
            "Iteration 1000: Current success : 61 \t All success : 104 \t mean(l0) : 13.144231 \t Current Learning Rate: 0.080\n",
            "Iteration 1100: Current success : 65 \t All success : 105 \t mean(l0) : 13.514286 \t Current Learning Rate: 0.078\n",
            "Iteration 1200: Current success : 66 \t All success : 105 \t mean(l0) : 13.476191 \t Current Learning Rate: 0.076\n",
            "Iteration 1300: Current success : 66 \t All success : 105 \t mean(l0) : 13.380953 \t Current Learning Rate: 0.074\n",
            "Iteration 1400: Current success : 68 \t All success : 105 \t mean(l0) : 13.380953 \t Current Learning Rate: 0.072\n",
            "Iteration 1500: Current success : 67 \t All success : 105 \t mean(l0) : 13.380953 \t Current Learning Rate: 0.070\n",
            "Iteration 1600: Current success : 67 \t All success : 105 \t mean(l0) : 13.380953 \t Current Learning Rate: 0.068\n",
            "Iteration 1700: Current success : 66 \t All success : 105 \t mean(l0) : 13.380953 \t Current Learning Rate: 0.066\n",
            "Iteration 1800: Current success : 67 \t All success : 105 \t mean(l0) : 13.380953 \t Current Learning Rate: 0.064\n",
            "Iteration 1900: Current success : 67 \t All success : 105 \t mean(l0) : 13.380953 \t Current Learning Rate: 0.062\n",
            "Iteration 2000: Current success : 67 \t All success : 105 \t mean(l0) : 13.380953 \t Current Learning Rate: 0.060\n",
            "Iteration 2100: Current success : 67 \t All success : 105 \t mean(l0) : 13.380953 \t Current Learning Rate: 0.058\n",
            "Iteration 2200: Current success : 67 \t All success : 105 \t mean(l0) : 13.380953 \t Current Learning Rate: 0.056\n",
            "Iteration 2300: Current success : 67 \t All success : 105 \t mean(l0) : 13.380953 \t Current Learning Rate: 0.054\n",
            "Iteration 2400: Current success : 67 \t All success : 105 \t mean(l0) : 13.380953 \t Current Learning Rate: 0.052\n",
            "Iteration 2500: Current success : 69 \t All success : 105 \t mean(l0) : 13.380953 \t Current Learning Rate: 0.050\n",
            "Iteration 2600: Current success : 68 \t All success : 105 \t mean(l0) : 13.380953 \t Current Learning Rate: 0.048\n",
            "Iteration 2700: Current success : 74 \t All success : 107 \t mean(l0) : 13.205607 \t Current Learning Rate: 0.046\n",
            "Iteration 2800: Current success : 74 \t All success : 107 \t mean(l0) : 13.205607 \t Current Learning Rate: 0.044\n",
            "Iteration 2900: Current success : 76 \t All success : 108 \t mean(l0) : 13.120371 \t Current Learning Rate: 0.042\n",
            "Iteration 3000: Current success : 76 \t All success : 108 \t mean(l0) : 13.120371 \t Current Learning Rate: 0.040\n",
            "Iteration 3100: Current success : 77 \t All success : 108 \t mean(l0) : 13.120371 \t Current Learning Rate: 0.038\n",
            "Iteration 3200: Current success : 76 \t All success : 108 \t mean(l0) : 13.120371 \t Current Learning Rate: 0.036\n",
            "Iteration 3300: Current success : 77 \t All success : 108 \t mean(l0) : 13.120371 \t Current Learning Rate: 0.034\n",
            "Iteration 3400: Current success : 78 \t All success : 108 \t mean(l0) : 13.120371 \t Current Learning Rate: 0.032\n",
            "Iteration 3500: Current success : 78 \t All success : 108 \t mean(l0) : 13.120371 \t Current Learning Rate: 0.030\n",
            "Iteration 3600: Current success : 78 \t All success : 108 \t mean(l0) : 13.120371 \t Current Learning Rate: 0.028\n",
            "Iteration 3700: Current success : 78 \t All success : 108 \t mean(l0) : 13.120371 \t Current Learning Rate: 0.026\n",
            "Iteration 3800: Current success : 79 \t All success : 108 \t mean(l0) : 13.120371 \t Current Learning Rate: 0.024\n",
            "Iteration 3900: Current success : 79 \t All success : 108 \t mean(l0) : 13.120371 \t Current Learning Rate: 0.022\n",
            "Iteration 4000: Current success : 79 \t All success : 108 \t mean(l0) : 13.120371 \t Current Learning Rate: 0.020\n",
            "Iteration 4100: Current success : 78 \t All success : 108 \t mean(l0) : 13.120371 \t Current Learning Rate: 0.018\n",
            "Iteration 4200: Current success : 78 \t All success : 108 \t mean(l0) : 13.120371 \t Current Learning Rate: 0.016\n",
            "Iteration 4300: Current success : 79 \t All success : 108 \t mean(l0) : 13.120371 \t Current Learning Rate: 0.014\n",
            "Iteration 4400: Current success : 78 \t All success : 108 \t mean(l0) : 13.120371 \t Current Learning Rate: 0.012\n",
            "Iteration 4500: Current success : 79 \t All success : 108 \t mean(l0) : 13.120371 \t Current Learning Rate: 0.010\n",
            "Iteration 4600: Current success : 79 \t All success : 108 \t mean(l0) : 13.120371 \t Current Learning Rate: 0.008\n",
            "Iteration 4700: Current success : 79 \t All success : 108 \t mean(l0) : 13.120371 \t Current Learning Rate: 0.006\n",
            "Iteration 4800: Current success : 79 \t All success : 108 \t mean(l0) : 13.120371 \t Current Learning Rate: 0.004\n",
            "Iteration 4900: Current success : 79 \t All success : 108 \t mean(l0) : 13.120371 \t Current Learning Rate: 0.002\n",
            "Iteration 5000: Current success : 79 \t All success : 108 \t mean(l0) : 13.120371 \t Current Learning Rate: 0.000\n",
            "Iteration 5100: Current success : 70 \t All success : 119 \t mean(l0) : 14.756303 \t Current Learning Rate: 0.049\n",
            "Iteration 5200: Current success : 77 \t All success : 126 \t mean(l0) : 14.912700 \t Current Learning Rate: 0.048\n",
            "Iteration 5300: Current success : 79 \t All success : 126 \t mean(l0) : 14.523810 \t Current Learning Rate: 0.047\n",
            "Iteration 5400: Current success : 83 \t All success : 127 \t mean(l0) : 14.874016 \t Current Learning Rate: 0.046\n",
            "Iteration 5500: Current success : 81 \t All success : 127 \t mean(l0) : 14.787401 \t Current Learning Rate: 0.045\n",
            "Iteration 5600: Current success : 79 \t All success : 127 \t mean(l0) : 14.787401 \t Current Learning Rate: 0.044\n",
            "Iteration 5700: Current success : 80 \t All success : 127 \t mean(l0) : 14.787401 \t Current Learning Rate: 0.043\n",
            "Iteration 5800: Current success : 81 \t All success : 127 \t mean(l0) : 14.787401 \t Current Learning Rate: 0.042\n",
            "Iteration 5900: Current success : 80 \t All success : 127 \t mean(l0) : 14.787401 \t Current Learning Rate: 0.041\n",
            "Iteration 6000: Current success : 80 \t All success : 128 \t mean(l0) : 15.406250 \t Current Learning Rate: 0.040\n",
            "Iteration 6100: Current success : 81 \t All success : 128 \t mean(l0) : 15.406250 \t Current Learning Rate: 0.039\n",
            "Iteration 6200: Current success : 80 \t All success : 128 \t mean(l0) : 15.406250 \t Current Learning Rate: 0.038\n",
            "Iteration 6300: Current success : 79 \t All success : 128 \t mean(l0) : 15.406250 \t Current Learning Rate: 0.037\n",
            "Iteration 6400: Current success : 79 \t All success : 128 \t mean(l0) : 15.406250 \t Current Learning Rate: 0.036\n",
            "Iteration 6500: Current success : 77 \t All success : 128 \t mean(l0) : 15.406250 \t Current Learning Rate: 0.035\n",
            "Iteration 6600: Current success : 77 \t All success : 128 \t mean(l0) : 15.406250 \t Current Learning Rate: 0.034\n",
            "Iteration 6700: Current success : 77 \t All success : 128 \t mean(l0) : 15.406250 \t Current Learning Rate: 0.033\n",
            "Iteration 6800: Current success : 77 \t All success : 128 \t mean(l0) : 15.406250 \t Current Learning Rate: 0.032\n",
            "Iteration 6900: Current success : 78 \t All success : 128 \t mean(l0) : 15.406250 \t Current Learning Rate: 0.031\n",
            "Iteration 7000: Current success : 77 \t All success : 128 \t mean(l0) : 15.406250 \t Current Learning Rate: 0.030\n",
            "Iteration 7100: Current success : 71 \t All success : 128 \t mean(l0) : 15.398438 \t Current Learning Rate: 0.029\n",
            "Iteration 7200: Current success : 71 \t All success : 128 \t mean(l0) : 15.398438 \t Current Learning Rate: 0.028\n",
            "Iteration 7300: Current success : 72 \t All success : 128 \t mean(l0) : 15.398438 \t Current Learning Rate: 0.027\n",
            "Iteration 7400: Current success : 72 \t All success : 128 \t mean(l0) : 15.398438 \t Current Learning Rate: 0.026\n",
            "Iteration 7500: Current success : 71 \t All success : 128 \t mean(l0) : 15.398438 \t Current Learning Rate: 0.025\n",
            "Iteration 7600: Current success : 72 \t All success : 128 \t mean(l0) : 15.398438 \t Current Learning Rate: 0.024\n",
            "Iteration 7700: Current success : 72 \t All success : 128 \t mean(l0) : 15.398438 \t Current Learning Rate: 0.023\n",
            "Iteration 7800: Current success : 72 \t All success : 128 \t mean(l0) : 15.398438 \t Current Learning Rate: 0.022\n",
            "Iteration 7900: Current success : 74 \t All success : 128 \t mean(l0) : 15.398438 \t Current Learning Rate: 0.021\n",
            "Iteration 8000: Current success : 74 \t All success : 128 \t mean(l0) : 15.398438 \t Current Learning Rate: 0.020\n",
            "Iteration 8100: Current success : 74 \t All success : 128 \t mean(l0) : 15.398438 \t Current Learning Rate: 0.019\n",
            "Iteration 8200: Current success : 75 \t All success : 128 \t mean(l0) : 15.398438 \t Current Learning Rate: 0.018\n",
            "Iteration 8300: Current success : 74 \t All success : 128 \t mean(l0) : 15.398438 \t Current Learning Rate: 0.017\n",
            "Iteration 8400: Current success : 75 \t All success : 128 \t mean(l0) : 15.398438 \t Current Learning Rate: 0.016\n",
            "Iteration 8500: Current success : 75 \t All success : 128 \t mean(l0) : 15.398438 \t Current Learning Rate: 0.015\n",
            "Iteration 8600: Current success : 75 \t All success : 128 \t mean(l0) : 15.398438 \t Current Learning Rate: 0.014\n",
            "Iteration 8700: Current success : 75 \t All success : 128 \t mean(l0) : 15.398438 \t Current Learning Rate: 0.013\n",
            "Iteration 8800: Current success : 75 \t All success : 128 \t mean(l0) : 15.398438 \t Current Learning Rate: 0.012\n",
            "Iteration 8900: Current success : 75 \t All success : 128 \t mean(l0) : 15.398438 \t Current Learning Rate: 0.011\n",
            "Iteration 9000: Current success : 75 \t All success : 128 \t mean(l0) : 15.398438 \t Current Learning Rate: 0.010\n",
            "Iteration 9100: Current success : 75 \t All success : 128 \t mean(l0) : 15.398438 \t Current Learning Rate: 0.009\n",
            "Iteration 9200: Current success : 76 \t All success : 128 \t mean(l0) : 15.398438 \t Current Learning Rate: 0.008\n",
            "Iteration 9300: Current success : 76 \t All success : 128 \t mean(l0) : 15.398438 \t Current Learning Rate: 0.007\n",
            "Iteration 9400: Current success : 76 \t All success : 128 \t mean(l0) : 15.398438 \t Current Learning Rate: 0.006\n",
            "Iteration 9500: Current success : 76 \t All success : 128 \t mean(l0) : 15.398438 \t Current Learning Rate: 0.005\n",
            "Iteration 9600: Current success : 76 \t All success : 128 \t mean(l0) : 15.398438 \t Current Learning Rate: 0.004\n",
            "Iteration 9700: Current success : 76 \t All success : 128 \t mean(l0) : 15.398438 \t Current Learning Rate: 0.003\n",
            "Iteration 9800: Current success : 76 \t All success : 128 \t mean(l0) : 15.398438 \t Current Learning Rate: 0.002\n",
            "Iteration 9900: Current success : 76 \t All success : 128 \t mean(l0) : 15.398438 \t Current Learning Rate: 0.001\n",
            "Iteration 10000: Current success : 76 \t All success : 128 \t mean(l0) : 15.398438 \t Current Learning Rate: 0.000\n",
            "Iteration 10100: Current success : 63 \t All success : 131 \t mean(l0) : 15.870229 \t Current Learning Rate: 0.025\n",
            "Iteration 10200: Current success : 62 \t All success : 132 \t mean(l0) : 15.643940 \t Current Learning Rate: 0.024\n",
            "Iteration 10300: Current success : 64 \t All success : 137 \t mean(l0) : 15.175182 \t Current Learning Rate: 0.024\n",
            "Iteration 10400: Current success : 63 \t All success : 137 \t mean(l0) : 15.167883 \t Current Learning Rate: 0.023\n",
            "Iteration 10500: Current success : 64 \t All success : 137 \t mean(l0) : 15.167883 \t Current Learning Rate: 0.023\n",
            "Iteration 10600: Current success : 65 \t All success : 137 \t mean(l0) : 15.167883 \t Current Learning Rate: 0.022\n",
            "Iteration 10700: Current success : 63 \t All success : 137 \t mean(l0) : 15.160583 \t Current Learning Rate: 0.022\n",
            "Iteration 10800: Current success : 63 \t All success : 137 \t mean(l0) : 15.153285 \t Current Learning Rate: 0.021\n",
            "Iteration 10900: Current success : 62 \t All success : 137 \t mean(l0) : 15.153285 \t Current Learning Rate: 0.021\n",
            "Iteration 11000: Current success : 64 \t All success : 137 \t mean(l0) : 15.153285 \t Current Learning Rate: 0.020\n",
            "Iteration 11100: Current success : 63 \t All success : 137 \t mean(l0) : 15.153285 \t Current Learning Rate: 0.020\n",
            "Iteration 11200: Current success : 61 \t All success : 137 \t mean(l0) : 15.145986 \t Current Learning Rate: 0.019\n",
            "Iteration 11300: Current success : 61 \t All success : 137 \t mean(l0) : 15.145986 \t Current Learning Rate: 0.019\n",
            "Iteration 11400: Current success : 61 \t All success : 137 \t mean(l0) : 15.138686 \t Current Learning Rate: 0.018\n",
            "Iteration 11500: Current success : 62 \t All success : 137 \t mean(l0) : 15.138686 \t Current Learning Rate: 0.018\n",
            "Iteration 11600: Current success : 62 \t All success : 137 \t mean(l0) : 15.124087 \t Current Learning Rate: 0.017\n",
            "Iteration 11700: Current success : 62 \t All success : 137 \t mean(l0) : 15.124087 \t Current Learning Rate: 0.017\n",
            "Iteration 11800: Current success : 63 \t All success : 137 \t mean(l0) : 15.124087 \t Current Learning Rate: 0.016\n",
            "Iteration 11900: Current success : 63 \t All success : 137 \t mean(l0) : 15.124087 \t Current Learning Rate: 0.016\n",
            "Iteration 12000: Current success : 63 \t All success : 137 \t mean(l0) : 15.124087 \t Current Learning Rate: 0.015\n",
            "Iteration 12100: Current success : 62 \t All success : 137 \t mean(l0) : 15.124087 \t Current Learning Rate: 0.015\n",
            "Iteration 12200: Current success : 64 \t All success : 137 \t mean(l0) : 15.124087 \t Current Learning Rate: 0.014\n",
            "Iteration 12300: Current success : 62 \t All success : 137 \t mean(l0) : 15.124087 \t Current Learning Rate: 0.014\n",
            "Iteration 12400: Current success : 62 \t All success : 137 \t mean(l0) : 15.124087 \t Current Learning Rate: 0.013\n",
            "Iteration 12500: Current success : 62 \t All success : 137 \t mean(l0) : 15.124087 \t Current Learning Rate: 0.013\n",
            "Iteration 12600: Current success : 62 \t All success : 137 \t mean(l0) : 15.116788 \t Current Learning Rate: 0.012\n",
            "Iteration 12700: Current success : 63 \t All success : 137 \t mean(l0) : 15.116788 \t Current Learning Rate: 0.012\n",
            "Iteration 12800: Current success : 62 \t All success : 137 \t mean(l0) : 15.116788 \t Current Learning Rate: 0.011\n",
            "Iteration 12900: Current success : 63 \t All success : 137 \t mean(l0) : 15.116788 \t Current Learning Rate: 0.011\n",
            "Iteration 13000: Current success : 71 \t All success : 137 \t mean(l0) : 15.116788 \t Current Learning Rate: 0.010\n",
            "Iteration 13100: Current success : 72 \t All success : 137 \t mean(l0) : 15.116788 \t Current Learning Rate: 0.010\n",
            "Iteration 13200: Current success : 73 \t All success : 137 \t mean(l0) : 15.116788 \t Current Learning Rate: 0.009\n",
            "Iteration 13300: Current success : 72 \t All success : 137 \t mean(l0) : 15.116788 \t Current Learning Rate: 0.009\n",
            "Iteration 13400: Current success : 73 \t All success : 137 \t mean(l0) : 15.116788 \t Current Learning Rate: 0.008\n",
            "Iteration 13500: Current success : 73 \t All success : 137 \t mean(l0) : 15.116788 \t Current Learning Rate: 0.008\n",
            "Iteration 13600: Current success : 72 \t All success : 137 \t mean(l0) : 15.116788 \t Current Learning Rate: 0.007\n",
            "Iteration 13700: Current success : 73 \t All success : 137 \t mean(l0) : 15.116788 \t Current Learning Rate: 0.007\n",
            "Iteration 13800: Current success : 73 \t All success : 137 \t mean(l0) : 15.116788 \t Current Learning Rate: 0.006\n",
            "Iteration 13900: Current success : 73 \t All success : 137 \t mean(l0) : 15.116788 \t Current Learning Rate: 0.006\n",
            "Iteration 14000: Current success : 73 \t All success : 137 \t mean(l0) : 15.116788 \t Current Learning Rate: 0.005\n",
            "Iteration 14100: Current success : 74 \t All success : 137 \t mean(l0) : 15.116788 \t Current Learning Rate: 0.005\n",
            "Iteration 14200: Current success : 74 \t All success : 137 \t mean(l0) : 15.116788 \t Current Learning Rate: 0.004\n",
            "Iteration 14300: Current success : 74 \t All success : 137 \t mean(l0) : 15.116788 \t Current Learning Rate: 0.004\n",
            "Iteration 14400: Current success : 74 \t All success : 137 \t mean(l0) : 15.116788 \t Current Learning Rate: 0.003\n",
            "Iteration 14500: Current success : 74 \t All success : 137 \t mean(l0) : 15.116788 \t Current Learning Rate: 0.003\n",
            "Iteration 14600: Current success : 74 \t All success : 137 \t mean(l0) : 15.116788 \t Current Learning Rate: 0.002\n",
            "Iteration 14700: Current success : 74 \t All success : 137 \t mean(l0) : 15.116788 \t Current Learning Rate: 0.002\n",
            "Iteration 14800: Current success : 74 \t All success : 137 \t mean(l0) : 15.116788 \t Current Learning Rate: 0.001\n",
            "Iteration 14900: Current success : 74 \t All success : 137 \t mean(l0) : 15.116788 \t Current Learning Rate: 0.001\n",
            "Iteration 15000: Current success : 74 \t All success : 137 \t mean(l0) : 15.116788 \t Current Learning Rate: 0.000\n",
            "Iteration 15100: Current success : 70 \t All success : 137 \t mean(l0) : 15.014599 \t Current Learning Rate: 0.012\n",
            "Iteration 15200: Current success : 71 \t All success : 137 \t mean(l0) : 14.948905 \t Current Learning Rate: 0.012\n",
            "Iteration 15300: Current success : 71 \t All success : 137 \t mean(l0) : 14.897810 \t Current Learning Rate: 0.012\n",
            "Iteration 15400: Current success : 71 \t All success : 137 \t mean(l0) : 14.868613 \t Current Learning Rate: 0.012\n",
            "Iteration 15500: Current success : 69 \t All success : 137 \t mean(l0) : 14.854014 \t Current Learning Rate: 0.011\n",
            "Iteration 15600: Current success : 69 \t All success : 137 \t mean(l0) : 14.839416 \t Current Learning Rate: 0.011\n",
            "Iteration 15700: Current success : 69 \t All success : 137 \t mean(l0) : 14.832117 \t Current Learning Rate: 0.011\n",
            "Iteration 15800: Current success : 69 \t All success : 137 \t mean(l0) : 14.832117 \t Current Learning Rate: 0.011\n",
            "Iteration 15900: Current success : 68 \t All success : 137 \t mean(l0) : 14.832117 \t Current Learning Rate: 0.010\n",
            "Iteration 16000: Current success : 68 \t All success : 137 \t mean(l0) : 14.832117 \t Current Learning Rate: 0.010\n",
            "Iteration 16100: Current success : 68 \t All success : 138 \t mean(l0) : 14.739131 \t Current Learning Rate: 0.010\n",
            "Iteration 16200: Current success : 70 \t All success : 138 \t mean(l0) : 14.731884 \t Current Learning Rate: 0.010\n",
            "Iteration 16300: Current success : 67 \t All success : 138 \t mean(l0) : 14.731884 \t Current Learning Rate: 0.009\n",
            "Iteration 16400: Current success : 69 \t All success : 138 \t mean(l0) : 14.710145 \t Current Learning Rate: 0.009\n",
            "Iteration 16500: Current success : 68 \t All success : 138 \t mean(l0) : 14.710145 \t Current Learning Rate: 0.009\n",
            "Iteration 16600: Current success : 70 \t All success : 138 \t mean(l0) : 14.702899 \t Current Learning Rate: 0.009\n",
            "Iteration 16700: Current success : 68 \t All success : 138 \t mean(l0) : 14.702899 \t Current Learning Rate: 0.008\n",
            "Iteration 16800: Current success : 67 \t All success : 138 \t mean(l0) : 14.702899 \t Current Learning Rate: 0.008\n",
            "Iteration 16900: Current success : 67 \t All success : 138 \t mean(l0) : 14.702899 \t Current Learning Rate: 0.008\n",
            "Iteration 17000: Current success : 67 \t All success : 138 \t mean(l0) : 14.702899 \t Current Learning Rate: 0.008\n",
            "Iteration 17100: Current success : 70 \t All success : 138 \t mean(l0) : 14.702899 \t Current Learning Rate: 0.007\n",
            "Iteration 17200: Current success : 67 \t All success : 138 \t mean(l0) : 14.695652 \t Current Learning Rate: 0.007\n",
            "Iteration 17300: Current success : 67 \t All success : 138 \t mean(l0) : 14.681160 \t Current Learning Rate: 0.007\n",
            "Iteration 17400: Current success : 69 \t All success : 138 \t mean(l0) : 14.681160 \t Current Learning Rate: 0.007\n",
            "Iteration 17500: Current success : 67 \t All success : 138 \t mean(l0) : 14.681160 \t Current Learning Rate: 0.006\n",
            "Iteration 17600: Current success : 69 \t All success : 138 \t mean(l0) : 14.681160 \t Current Learning Rate: 0.006\n",
            "Iteration 17700: Current success : 70 \t All success : 138 \t mean(l0) : 14.681160 \t Current Learning Rate: 0.006\n",
            "Iteration 17800: Current success : 69 \t All success : 138 \t mean(l0) : 14.681160 \t Current Learning Rate: 0.006\n",
            "Iteration 17900: Current success : 70 \t All success : 138 \t mean(l0) : 14.681160 \t Current Learning Rate: 0.005\n",
            "Iteration 18000: Current success : 73 \t All success : 138 \t mean(l0) : 14.681160 \t Current Learning Rate: 0.005\n",
            "Iteration 18100: Current success : 75 \t All success : 138 \t mean(l0) : 14.681160 \t Current Learning Rate: 0.005\n",
            "Iteration 18200: Current success : 74 \t All success : 138 \t mean(l0) : 14.681160 \t Current Learning Rate: 0.005\n",
            "Iteration 18300: Current success : 74 \t All success : 138 \t mean(l0) : 14.681160 \t Current Learning Rate: 0.004\n",
            "Iteration 18400: Current success : 75 \t All success : 138 \t mean(l0) : 14.681160 \t Current Learning Rate: 0.004\n",
            "Iteration 18500: Current success : 74 \t All success : 138 \t mean(l0) : 14.681160 \t Current Learning Rate: 0.004\n",
            "Iteration 18600: Current success : 74 \t All success : 138 \t mean(l0) : 14.681160 \t Current Learning Rate: 0.004\n",
            "Iteration 18700: Current success : 74 \t All success : 138 \t mean(l0) : 14.681160 \t Current Learning Rate: 0.003\n",
            "Iteration 18800: Current success : 74 \t All success : 138 \t mean(l0) : 14.681160 \t Current Learning Rate: 0.003\n",
            "Iteration 18900: Current success : 74 \t All success : 138 \t mean(l0) : 14.681160 \t Current Learning Rate: 0.003\n",
            "Iteration 19000: Current success : 75 \t All success : 138 \t mean(l0) : 14.681160 \t Current Learning Rate: 0.003\n",
            "Iteration 19100: Current success : 75 \t All success : 138 \t mean(l0) : 14.681160 \t Current Learning Rate: 0.002\n",
            "Iteration 19200: Current success : 75 \t All success : 138 \t mean(l0) : 14.681160 \t Current Learning Rate: 0.002\n",
            "Iteration 19300: Current success : 75 \t All success : 138 \t mean(l0) : 14.681160 \t Current Learning Rate: 0.002\n",
            "Iteration 19400: Current success : 75 \t All success : 138 \t mean(l0) : 14.681160 \t Current Learning Rate: 0.002\n",
            "Iteration 19500: Current success : 75 \t All success : 138 \t mean(l0) : 14.681160 \t Current Learning Rate: 0.001\n",
            "Iteration 19600: Current success : 75 \t All success : 138 \t mean(l0) : 14.681160 \t Current Learning Rate: 0.001\n",
            "Iteration 19700: Current success : 75 \t All success : 138 \t mean(l0) : 14.681160 \t Current Learning Rate: 0.001\n",
            "Iteration 19800: Current success : 75 \t All success : 138 \t mean(l0) : 14.681160 \t Current Learning Rate: 0.001\n",
            "Iteration 19900: Current success : 75 \t All success : 138 \t mean(l0) : 14.681160 \t Current Learning Rate: 0.000\n",
            "Iteration 20000: Current success : 75 \t All success : 138 \t mean(l0) : 14.681160 \t Current Learning Rate: 0.000\n",
            "outer_step 3: all success : 231 \t mean(l0)(success) : 16.9654\n",
            "-------------------------------------------------------------\n",
            "tensor([   7.7500, 1000.0000, 1000.0000,  ..., 1000.0000, 1000.0000,\n",
            "        1000.0000], device='cuda:0')\n",
            "Iteration 100: Current success : 43 \t All success : 73 \t mean(l0) : 6.972603 \t Current Learning Rate: 0.098\n",
            "Iteration 200: Current success : 60 \t All success : 89 \t mean(l0) : 11.471910 \t Current Learning Rate: 0.096\n",
            "Iteration 300: Current success : 63 \t All success : 100 \t mean(l0) : 14.780000 \t Current Learning Rate: 0.094\n",
            "Iteration 400: Current success : 65 \t All success : 102 \t mean(l0) : 14.990196 \t Current Learning Rate: 0.092\n",
            "Iteration 500: Current success : 69 \t All success : 107 \t mean(l0) : 16.364485 \t Current Learning Rate: 0.090\n",
            "Iteration 600: Current success : 66 \t All success : 108 \t mean(l0) : 16.370371 \t Current Learning Rate: 0.088\n",
            "Iteration 700: Current success : 68 \t All success : 108 \t mean(l0) : 16.277779 \t Current Learning Rate: 0.086\n",
            "Iteration 800: Current success : 73 \t All success : 111 \t mean(l0) : 17.495497 \t Current Learning Rate: 0.084\n",
            "Iteration 900: Current success : 74 \t All success : 115 \t mean(l0) : 17.634781 \t Current Learning Rate: 0.082\n",
            "Iteration 1000: Current success : 76 \t All success : 118 \t mean(l0) : 18.050848 \t Current Learning Rate: 0.080\n",
            "Iteration 1100: Current success : 75 \t All success : 118 \t mean(l0) : 18.016949 \t Current Learning Rate: 0.078\n",
            "Iteration 1200: Current success : 76 \t All success : 118 \t mean(l0) : 18.016949 \t Current Learning Rate: 0.076\n",
            "Iteration 1300: Current success : 76 \t All success : 120 \t mean(l0) : 18.650002 \t Current Learning Rate: 0.074\n",
            "Iteration 1400: Current success : 75 \t All success : 120 \t mean(l0) : 18.600000 \t Current Learning Rate: 0.072\n",
            "Iteration 1500: Current success : 73 \t All success : 121 \t mean(l0) : 18.867767 \t Current Learning Rate: 0.070\n",
            "Iteration 1600: Current success : 75 \t All success : 121 \t mean(l0) : 18.851238 \t Current Learning Rate: 0.068\n",
            "Iteration 1700: Current success : 74 \t All success : 121 \t mean(l0) : 18.851238 \t Current Learning Rate: 0.066\n",
            "Iteration 1800: Current success : 74 \t All success : 121 \t mean(l0) : 18.851238 \t Current Learning Rate: 0.064\n",
            "Iteration 1900: Current success : 74 \t All success : 121 \t mean(l0) : 18.851238 \t Current Learning Rate: 0.062\n",
            "Iteration 2000: Current success : 75 \t All success : 122 \t mean(l0) : 18.745901 \t Current Learning Rate: 0.060\n",
            "Iteration 2100: Current success : 76 \t All success : 123 \t mean(l0) : 19.154470 \t Current Learning Rate: 0.058\n",
            "Iteration 2200: Current success : 77 \t All success : 123 \t mean(l0) : 19.138210 \t Current Learning Rate: 0.056\n",
            "Iteration 2300: Current success : 78 \t All success : 123 \t mean(l0) : 19.138210 \t Current Learning Rate: 0.054\n",
            "Iteration 2400: Current success : 78 \t All success : 123 \t mean(l0) : 19.138210 \t Current Learning Rate: 0.052\n",
            "Iteration 2500: Current success : 78 \t All success : 124 \t mean(l0) : 19.000000 \t Current Learning Rate: 0.050\n",
            "Iteration 2600: Current success : 77 \t All success : 124 \t mean(l0) : 19.000000 \t Current Learning Rate: 0.048\n",
            "Iteration 2700: Current success : 85 \t All success : 125 \t mean(l0) : 18.872002 \t Current Learning Rate: 0.046\n",
            "Iteration 2800: Current success : 85 \t All success : 126 \t mean(l0) : 18.761906 \t Current Learning Rate: 0.044\n",
            "Iteration 2900: Current success : 85 \t All success : 128 \t mean(l0) : 18.500000 \t Current Learning Rate: 0.042\n",
            "Iteration 3000: Current success : 87 \t All success : 128 \t mean(l0) : 18.500000 \t Current Learning Rate: 0.040\n",
            "Iteration 3100: Current success : 90 \t All success : 128 \t mean(l0) : 18.500000 \t Current Learning Rate: 0.038\n",
            "Iteration 3200: Current success : 91 \t All success : 129 \t mean(l0) : 18.775194 \t Current Learning Rate: 0.036\n",
            "Iteration 3300: Current success : 90 \t All success : 129 \t mean(l0) : 18.759689 \t Current Learning Rate: 0.034\n",
            "Iteration 3400: Current success : 90 \t All success : 129 \t mean(l0) : 18.759689 \t Current Learning Rate: 0.032\n",
            "Iteration 3500: Current success : 91 \t All success : 129 \t mean(l0) : 18.759689 \t Current Learning Rate: 0.030\n",
            "Iteration 3600: Current success : 91 \t All success : 129 \t mean(l0) : 18.759689 \t Current Learning Rate: 0.028\n",
            "Iteration 3700: Current success : 91 \t All success : 129 \t mean(l0) : 18.759689 \t Current Learning Rate: 0.026\n",
            "Iteration 3800: Current success : 91 \t All success : 129 \t mean(l0) : 18.759689 \t Current Learning Rate: 0.024\n",
            "Iteration 3900: Current success : 91 \t All success : 129 \t mean(l0) : 18.759689 \t Current Learning Rate: 0.022\n",
            "Iteration 4000: Current success : 91 \t All success : 129 \t mean(l0) : 18.759689 \t Current Learning Rate: 0.020\n",
            "Iteration 4100: Current success : 91 \t All success : 129 \t mean(l0) : 18.759689 \t Current Learning Rate: 0.018\n",
            "Iteration 4200: Current success : 91 \t All success : 129 \t mean(l0) : 18.759689 \t Current Learning Rate: 0.016\n",
            "Iteration 4300: Current success : 91 \t All success : 129 \t mean(l0) : 18.759689 \t Current Learning Rate: 0.014\n",
            "Iteration 4400: Current success : 91 \t All success : 129 \t mean(l0) : 18.759689 \t Current Learning Rate: 0.012\n",
            "Iteration 4500: Current success : 91 \t All success : 129 \t mean(l0) : 18.759689 \t Current Learning Rate: 0.010\n",
            "Iteration 4600: Current success : 91 \t All success : 129 \t mean(l0) : 18.759689 \t Current Learning Rate: 0.008\n",
            "Iteration 4700: Current success : 91 \t All success : 129 \t mean(l0) : 18.759689 \t Current Learning Rate: 0.006\n",
            "Iteration 4800: Current success : 91 \t All success : 129 \t mean(l0) : 18.759689 \t Current Learning Rate: 0.004\n",
            "Iteration 4900: Current success : 91 \t All success : 129 \t mean(l0) : 18.759689 \t Current Learning Rate: 0.002\n",
            "Iteration 5000: Current success : 91 \t All success : 129 \t mean(l0) : 18.759689 \t Current Learning Rate: 0.000\n",
            "Iteration 5100: Current success : 74 \t All success : 136 \t mean(l0) : 18.852942 \t Current Learning Rate: 0.049\n",
            "Iteration 5200: Current success : 77 \t All success : 168 \t mean(l0) : 17.654762 \t Current Learning Rate: 0.048\n",
            "Iteration 5300: Current success : 82 \t All success : 169 \t mean(l0) : 17.982248 \t Current Learning Rate: 0.047\n",
            "Iteration 5400: Current success : 79 \t All success : 169 \t mean(l0) : 17.982248 \t Current Learning Rate: 0.046\n",
            "Iteration 5500: Current success : 77 \t All success : 169 \t mean(l0) : 17.982248 \t Current Learning Rate: 0.045\n",
            "Iteration 5600: Current success : 78 \t All success : 169 \t mean(l0) : 17.976332 \t Current Learning Rate: 0.044\n",
            "Iteration 5700: Current success : 80 \t All success : 169 \t mean(l0) : 17.964497 \t Current Learning Rate: 0.043\n",
            "Iteration 5800: Current success : 75 \t All success : 169 \t mean(l0) : 17.952663 \t Current Learning Rate: 0.042\n",
            "Iteration 5900: Current success : 77 \t All success : 169 \t mean(l0) : 17.952663 \t Current Learning Rate: 0.041\n",
            "Iteration 6000: Current success : 76 \t All success : 169 \t mean(l0) : 17.952663 \t Current Learning Rate: 0.040\n",
            "Iteration 6100: Current success : 75 \t All success : 169 \t mean(l0) : 17.952663 \t Current Learning Rate: 0.039\n",
            "Iteration 6200: Current success : 75 \t All success : 169 \t mean(l0) : 17.952663 \t Current Learning Rate: 0.038\n",
            "Iteration 6300: Current success : 77 \t All success : 169 \t mean(l0) : 17.952663 \t Current Learning Rate: 0.037\n",
            "Iteration 6400: Current success : 75 \t All success : 169 \t mean(l0) : 17.952663 \t Current Learning Rate: 0.036\n",
            "Iteration 6500: Current success : 73 \t All success : 169 \t mean(l0) : 17.952663 \t Current Learning Rate: 0.035\n",
            "Iteration 6600: Current success : 72 \t All success : 169 \t mean(l0) : 17.952663 \t Current Learning Rate: 0.034\n",
            "Iteration 6700: Current success : 75 \t All success : 169 \t mean(l0) : 17.952663 \t Current Learning Rate: 0.033\n",
            "Iteration 6800: Current success : 76 \t All success : 169 \t mean(l0) : 17.952663 \t Current Learning Rate: 0.032\n",
            "Iteration 6900: Current success : 73 \t All success : 169 \t mean(l0) : 17.952663 \t Current Learning Rate: 0.031\n",
            "Iteration 7000: Current success : 74 \t All success : 169 \t mean(l0) : 17.952663 \t Current Learning Rate: 0.030\n",
            "Iteration 7100: Current success : 76 \t All success : 169 \t mean(l0) : 17.952663 \t Current Learning Rate: 0.029\n",
            "Iteration 7200: Current success : 74 \t All success : 169 \t mean(l0) : 17.952663 \t Current Learning Rate: 0.028\n",
            "Iteration 7300: Current success : 73 \t All success : 169 \t mean(l0) : 17.952663 \t Current Learning Rate: 0.027\n",
            "Iteration 7400: Current success : 73 \t All success : 169 \t mean(l0) : 17.952663 \t Current Learning Rate: 0.026\n",
            "Iteration 7500: Current success : 75 \t All success : 169 \t mean(l0) : 17.952663 \t Current Learning Rate: 0.025\n",
            "Iteration 7600: Current success : 76 \t All success : 169 \t mean(l0) : 17.952663 \t Current Learning Rate: 0.024\n",
            "Iteration 7700: Current success : 77 \t All success : 169 \t mean(l0) : 17.952663 \t Current Learning Rate: 0.023\n",
            "Iteration 7800: Current success : 83 \t All success : 170 \t mean(l0) : 17.876471 \t Current Learning Rate: 0.022\n",
            "Iteration 7900: Current success : 82 \t All success : 170 \t mean(l0) : 17.876471 \t Current Learning Rate: 0.021\n",
            "Iteration 8000: Current success : 83 \t All success : 170 \t mean(l0) : 17.876471 \t Current Learning Rate: 0.020\n",
            "Iteration 8100: Current success : 82 \t All success : 170 \t mean(l0) : 17.876471 \t Current Learning Rate: 0.019\n",
            "Iteration 8200: Current success : 84 \t All success : 170 \t mean(l0) : 17.876471 \t Current Learning Rate: 0.018\n",
            "Iteration 8300: Current success : 83 \t All success : 170 \t mean(l0) : 17.876471 \t Current Learning Rate: 0.017\n",
            "Iteration 8400: Current success : 82 \t All success : 170 \t mean(l0) : 17.876471 \t Current Learning Rate: 0.016\n",
            "Iteration 8500: Current success : 82 \t All success : 170 \t mean(l0) : 17.876471 \t Current Learning Rate: 0.015\n",
            "Iteration 8600: Current success : 82 \t All success : 170 \t mean(l0) : 17.876471 \t Current Learning Rate: 0.014\n",
            "Iteration 8700: Current success : 82 \t All success : 170 \t mean(l0) : 17.876471 \t Current Learning Rate: 0.013\n",
            "Iteration 8800: Current success : 82 \t All success : 170 \t mean(l0) : 17.876471 \t Current Learning Rate: 0.012\n",
            "Iteration 8900: Current success : 82 \t All success : 170 \t mean(l0) : 17.876471 \t Current Learning Rate: 0.011\n",
            "Iteration 9000: Current success : 82 \t All success : 170 \t mean(l0) : 17.876471 \t Current Learning Rate: 0.010\n",
            "Iteration 9100: Current success : 82 \t All success : 170 \t mean(l0) : 17.876471 \t Current Learning Rate: 0.009\n",
            "Iteration 9200: Current success : 83 \t All success : 170 \t mean(l0) : 17.876471 \t Current Learning Rate: 0.008\n",
            "Iteration 9300: Current success : 83 \t All success : 170 \t mean(l0) : 17.876471 \t Current Learning Rate: 0.007\n",
            "Iteration 9400: Current success : 83 \t All success : 170 \t mean(l0) : 17.876471 \t Current Learning Rate: 0.006\n",
            "Iteration 9500: Current success : 83 \t All success : 170 \t mean(l0) : 17.876471 \t Current Learning Rate: 0.005\n",
            "Iteration 9600: Current success : 83 \t All success : 170 \t mean(l0) : 17.876471 \t Current Learning Rate: 0.004\n",
            "Iteration 9700: Current success : 83 \t All success : 170 \t mean(l0) : 17.876471 \t Current Learning Rate: 0.003\n",
            "Iteration 9800: Current success : 83 \t All success : 170 \t mean(l0) : 17.876471 \t Current Learning Rate: 0.002\n",
            "Iteration 9900: Current success : 83 \t All success : 170 \t mean(l0) : 17.876471 \t Current Learning Rate: 0.001\n",
            "Iteration 10000: Current success : 83 \t All success : 170 \t mean(l0) : 17.876471 \t Current Learning Rate: 0.000\n",
            "Iteration 10100: Current success : 73 \t All success : 172 \t mean(l0) : 17.965117 \t Current Learning Rate: 0.025\n",
            "Iteration 10200: Current success : 71 \t All success : 177 \t mean(l0) : 18.655367 \t Current Learning Rate: 0.024\n",
            "Iteration 10300: Current success : 68 \t All success : 177 \t mean(l0) : 18.598869 \t Current Learning Rate: 0.024\n",
            "Iteration 10400: Current success : 68 \t All success : 177 \t mean(l0) : 18.598869 \t Current Learning Rate: 0.023\n",
            "Iteration 10500: Current success : 68 \t All success : 177 \t mean(l0) : 18.593220 \t Current Learning Rate: 0.023\n",
            "Iteration 10600: Current success : 67 \t All success : 177 \t mean(l0) : 18.593220 \t Current Learning Rate: 0.022\n",
            "Iteration 10700: Current success : 67 \t All success : 177 \t mean(l0) : 18.593220 \t Current Learning Rate: 0.022\n",
            "Iteration 10800: Current success : 68 \t All success : 177 \t mean(l0) : 18.593220 \t Current Learning Rate: 0.021\n",
            "Iteration 10900: Current success : 68 \t All success : 177 \t mean(l0) : 18.593220 \t Current Learning Rate: 0.021\n",
            "Iteration 11000: Current success : 68 \t All success : 177 \t mean(l0) : 18.587570 \t Current Learning Rate: 0.020\n",
            "Iteration 11100: Current success : 68 \t All success : 177 \t mean(l0) : 18.587570 \t Current Learning Rate: 0.020\n",
            "Iteration 11200: Current success : 67 \t All success : 177 \t mean(l0) : 18.587570 \t Current Learning Rate: 0.019\n",
            "Iteration 11300: Current success : 68 \t All success : 177 \t mean(l0) : 18.587570 \t Current Learning Rate: 0.019\n",
            "Iteration 11400: Current success : 67 \t All success : 177 \t mean(l0) : 18.587570 \t Current Learning Rate: 0.018\n",
            "Iteration 11500: Current success : 68 \t All success : 177 \t mean(l0) : 18.587570 \t Current Learning Rate: 0.018\n",
            "Iteration 11600: Current success : 69 \t All success : 177 \t mean(l0) : 18.587570 \t Current Learning Rate: 0.017\n",
            "Iteration 11700: Current success : 69 \t All success : 178 \t mean(l0) : 18.488764 \t Current Learning Rate: 0.017\n",
            "Iteration 11800: Current success : 69 \t All success : 178 \t mean(l0) : 18.488764 \t Current Learning Rate: 0.016\n",
            "Iteration 11900: Current success : 67 \t All success : 178 \t mean(l0) : 18.488764 \t Current Learning Rate: 0.016\n",
            "Iteration 12000: Current success : 68 \t All success : 178 \t mean(l0) : 18.488764 \t Current Learning Rate: 0.015\n",
            "Iteration 12100: Current success : 69 \t All success : 178 \t mean(l0) : 18.488764 \t Current Learning Rate: 0.015\n",
            "Iteration 12200: Current success : 68 \t All success : 178 \t mean(l0) : 18.488764 \t Current Learning Rate: 0.014\n",
            "Iteration 12300: Current success : 68 \t All success : 178 \t mean(l0) : 18.488764 \t Current Learning Rate: 0.014\n",
            "Iteration 12400: Current success : 67 \t All success : 178 \t mean(l0) : 18.488764 \t Current Learning Rate: 0.013\n",
            "Iteration 12500: Current success : 68 \t All success : 178 \t mean(l0) : 18.488764 \t Current Learning Rate: 0.013\n",
            "Iteration 12600: Current success : 69 \t All success : 178 \t mean(l0) : 18.477528 \t Current Learning Rate: 0.012\n",
            "Iteration 12700: Current success : 68 \t All success : 178 \t mean(l0) : 18.477528 \t Current Learning Rate: 0.012\n",
            "Iteration 12800: Current success : 68 \t All success : 178 \t mean(l0) : 18.477528 \t Current Learning Rate: 0.011\n",
            "Iteration 12900: Current success : 69 \t All success : 178 \t mean(l0) : 18.471910 \t Current Learning Rate: 0.011\n",
            "Iteration 13000: Current success : 78 \t All success : 178 \t mean(l0) : 18.471910 \t Current Learning Rate: 0.010\n",
            "Iteration 13100: Current success : 80 \t All success : 178 \t mean(l0) : 18.471910 \t Current Learning Rate: 0.010\n",
            "Iteration 13200: Current success : 80 \t All success : 178 \t mean(l0) : 18.471910 \t Current Learning Rate: 0.009\n",
            "Iteration 13300: Current success : 80 \t All success : 178 \t mean(l0) : 18.471910 \t Current Learning Rate: 0.009\n",
            "Iteration 13400: Current success : 81 \t All success : 178 \t mean(l0) : 18.471910 \t Current Learning Rate: 0.008\n",
            "Iteration 13500: Current success : 80 \t All success : 178 \t mean(l0) : 18.471910 \t Current Learning Rate: 0.008\n",
            "Iteration 13600: Current success : 79 \t All success : 178 \t mean(l0) : 18.471910 \t Current Learning Rate: 0.007\n",
            "Iteration 13700: Current success : 79 \t All success : 178 \t mean(l0) : 18.471910 \t Current Learning Rate: 0.007\n",
            "Iteration 13800: Current success : 79 \t All success : 178 \t mean(l0) : 18.471910 \t Current Learning Rate: 0.006\n",
            "Iteration 13900: Current success : 80 \t All success : 178 \t mean(l0) : 18.471910 \t Current Learning Rate: 0.006\n",
            "Iteration 14000: Current success : 79 \t All success : 178 \t mean(l0) : 18.471910 \t Current Learning Rate: 0.005\n",
            "Iteration 14100: Current success : 79 \t All success : 178 \t mean(l0) : 18.471910 \t Current Learning Rate: 0.005\n",
            "Iteration 14200: Current success : 80 \t All success : 178 \t mean(l0) : 18.471910 \t Current Learning Rate: 0.004\n",
            "Iteration 14300: Current success : 80 \t All success : 178 \t mean(l0) : 18.471910 \t Current Learning Rate: 0.004\n",
            "Iteration 14400: Current success : 80 \t All success : 178 \t mean(l0) : 18.471910 \t Current Learning Rate: 0.003\n",
            "Iteration 14500: Current success : 80 \t All success : 178 \t mean(l0) : 18.471910 \t Current Learning Rate: 0.003\n",
            "Iteration 14600: Current success : 80 \t All success : 178 \t mean(l0) : 18.471910 \t Current Learning Rate: 0.002\n",
            "Iteration 14700: Current success : 80 \t All success : 178 \t mean(l0) : 18.471910 \t Current Learning Rate: 0.002\n",
            "Iteration 14800: Current success : 80 \t All success : 178 \t mean(l0) : 18.471910 \t Current Learning Rate: 0.001\n",
            "Iteration 14900: Current success : 80 \t All success : 178 \t mean(l0) : 18.471910 \t Current Learning Rate: 0.001\n",
            "Iteration 15000: Current success : 80 \t All success : 178 \t mean(l0) : 18.471910 \t Current Learning Rate: 0.000\n",
            "Iteration 15100: Current success : 109 \t All success : 187 \t mean(l0) : 18.566845 \t Current Learning Rate: 0.012\n",
            "Iteration 15200: Current success : 111 \t All success : 189 \t mean(l0) : 18.391533 \t Current Learning Rate: 0.012\n",
            "Iteration 15300: Current success : 111 \t All success : 189 \t mean(l0) : 18.375660 \t Current Learning Rate: 0.012\n",
            "Iteration 15400: Current success : 110 \t All success : 190 \t mean(l0) : 18.289474 \t Current Learning Rate: 0.012\n",
            "Iteration 15500: Current success : 110 \t All success : 190 \t mean(l0) : 18.273685 \t Current Learning Rate: 0.011\n",
            "Iteration 15600: Current success : 82 \t All success : 190 \t mean(l0) : 18.273685 \t Current Learning Rate: 0.011\n",
            "Iteration 15700: Current success : 108 \t All success : 190 \t mean(l0) : 18.257896 \t Current Learning Rate: 0.011\n",
            "Iteration 15800: Current success : 111 \t All success : 190 \t mean(l0) : 18.257896 \t Current Learning Rate: 0.011\n",
            "Iteration 15900: Current success : 107 \t All success : 190 \t mean(l0) : 18.257896 \t Current Learning Rate: 0.010\n",
            "Iteration 16000: Current success : 109 \t All success : 190 \t mean(l0) : 18.257896 \t Current Learning Rate: 0.010\n",
            "Iteration 16100: Current success : 107 \t All success : 190 \t mean(l0) : 18.257896 \t Current Learning Rate: 0.010\n",
            "Iteration 16200: Current success : 108 \t All success : 190 \t mean(l0) : 18.247370 \t Current Learning Rate: 0.010\n",
            "Iteration 16300: Current success : 106 \t All success : 190 \t mean(l0) : 18.247370 \t Current Learning Rate: 0.009\n",
            "Iteration 16400: Current success : 109 \t All success : 190 \t mean(l0) : 18.247370 \t Current Learning Rate: 0.009\n",
            "Iteration 16500: Current success : 110 \t All success : 190 \t mean(l0) : 18.247370 \t Current Learning Rate: 0.009\n",
            "Iteration 16600: Current success : 107 \t All success : 190 \t mean(l0) : 18.247370 \t Current Learning Rate: 0.009\n",
            "Iteration 16700: Current success : 105 \t All success : 190 \t mean(l0) : 18.242105 \t Current Learning Rate: 0.008\n",
            "Iteration 16800: Current success : 108 \t All success : 190 \t mean(l0) : 18.242105 \t Current Learning Rate: 0.008\n",
            "Iteration 16900: Current success : 105 \t All success : 190 \t mean(l0) : 18.242105 \t Current Learning Rate: 0.008\n",
            "Iteration 17000: Current success : 107 \t All success : 190 \t mean(l0) : 18.242105 \t Current Learning Rate: 0.008\n",
            "Iteration 17100: Current success : 107 \t All success : 190 \t mean(l0) : 18.242105 \t Current Learning Rate: 0.007\n",
            "Iteration 17200: Current success : 106 \t All success : 190 \t mean(l0) : 18.242105 \t Current Learning Rate: 0.007\n",
            "Iteration 17300: Current success : 105 \t All success : 190 \t mean(l0) : 18.242105 \t Current Learning Rate: 0.007\n",
            "Iteration 17400: Current success : 106 \t All success : 190 \t mean(l0) : 18.242105 \t Current Learning Rate: 0.007\n",
            "Iteration 17500: Current success : 110 \t All success : 190 \t mean(l0) : 18.242105 \t Current Learning Rate: 0.006\n",
            "Iteration 17600: Current success : 109 \t All success : 190 \t mean(l0) : 18.242105 \t Current Learning Rate: 0.006\n",
            "Iteration 17700: Current success : 107 \t All success : 190 \t mean(l0) : 18.242105 \t Current Learning Rate: 0.006\n",
            "Iteration 17800: Current success : 108 \t All success : 190 \t mean(l0) : 18.242105 \t Current Learning Rate: 0.006\n",
            "Iteration 17900: Current success : 109 \t All success : 190 \t mean(l0) : 18.242105 \t Current Learning Rate: 0.005\n",
            "Iteration 18000: Current success : 110 \t All success : 190 \t mean(l0) : 18.242105 \t Current Learning Rate: 0.005\n",
            "Iteration 18100: Current success : 113 \t All success : 190 \t mean(l0) : 18.242105 \t Current Learning Rate: 0.005\n",
            "Iteration 18200: Current success : 112 \t All success : 190 \t mean(l0) : 18.242105 \t Current Learning Rate: 0.005\n",
            "Iteration 18300: Current success : 111 \t All success : 190 \t mean(l0) : 18.242105 \t Current Learning Rate: 0.004\n",
            "Iteration 18400: Current success : 110 \t All success : 190 \t mean(l0) : 18.242105 \t Current Learning Rate: 0.004\n",
            "Iteration 18500: Current success : 111 \t All success : 190 \t mean(l0) : 18.242105 \t Current Learning Rate: 0.004\n",
            "Iteration 18600: Current success : 112 \t All success : 190 \t mean(l0) : 18.242105 \t Current Learning Rate: 0.004\n",
            "Iteration 18700: Current success : 112 \t All success : 190 \t mean(l0) : 18.242105 \t Current Learning Rate: 0.003\n",
            "Iteration 18800: Current success : 112 \t All success : 190 \t mean(l0) : 18.242105 \t Current Learning Rate: 0.003\n",
            "Iteration 18900: Current success : 112 \t All success : 190 \t mean(l0) : 18.242105 \t Current Learning Rate: 0.003\n",
            "Iteration 19000: Current success : 112 \t All success : 190 \t mean(l0) : 18.242105 \t Current Learning Rate: 0.003\n",
            "Iteration 19100: Current success : 112 \t All success : 190 \t mean(l0) : 18.242105 \t Current Learning Rate: 0.002\n",
            "Iteration 19200: Current success : 112 \t All success : 190 \t mean(l0) : 18.242105 \t Current Learning Rate: 0.002\n",
            "Iteration 19300: Current success : 112 \t All success : 190 \t mean(l0) : 18.242105 \t Current Learning Rate: 0.002\n",
            "Iteration 19400: Current success : 112 \t All success : 190 \t mean(l0) : 18.242105 \t Current Learning Rate: 0.002\n",
            "Iteration 19500: Current success : 112 \t All success : 190 \t mean(l0) : 18.242105 \t Current Learning Rate: 0.001\n",
            "Iteration 19600: Current success : 112 \t All success : 190 \t mean(l0) : 18.242105 \t Current Learning Rate: 0.001\n",
            "Iteration 19700: Current success : 112 \t All success : 190 \t mean(l0) : 18.242105 \t Current Learning Rate: 0.001\n",
            "Iteration 19800: Current success : 112 \t All success : 190 \t mean(l0) : 18.242105 \t Current Learning Rate: 0.001\n",
            "Iteration 19900: Current success : 112 \t All success : 190 \t mean(l0) : 18.242105 \t Current Learning Rate: 0.000\n",
            "Iteration 20000: Current success : 112 \t All success : 190 \t mean(l0) : 18.242105 \t Current Learning Rate: 0.000\n",
            "outer_step 4: all success : 260 \t mean(l0)(success) : 19.4923\n",
            "-------------------------------------------------------------\n",
            "tensor([8.8750e+00, 1.0000e+04, 1.0000e+04,  ..., 1.0000e+04, 1.0000e+04,\n",
            "        1.0000e+04], device='cuda:0')\n",
            "Iteration 100: Current success : 26 \t All success : 46 \t mean(l0) : 9.739131 \t Current Learning Rate: 0.098\n",
            "Iteration 200: Current success : 34 \t All success : 63 \t mean(l0) : 12.158731 \t Current Learning Rate: 0.096\n",
            "Iteration 300: Current success : 36 \t All success : 80 \t mean(l0) : 13.425000 \t Current Learning Rate: 0.094\n",
            "Iteration 400: Current success : 42 \t All success : 86 \t mean(l0) : 14.662790 \t Current Learning Rate: 0.092\n",
            "Iteration 500: Current success : 51 \t All success : 94 \t mean(l0) : 16.287233 \t Current Learning Rate: 0.090\n",
            "Iteration 600: Current success : 55 \t All success : 98 \t mean(l0) : 17.132652 \t Current Learning Rate: 0.088\n",
            "Iteration 700: Current success : 57 \t All success : 101 \t mean(l0) : 16.722773 \t Current Learning Rate: 0.086\n",
            "Iteration 800: Current success : 56 \t All success : 104 \t mean(l0) : 16.913462 \t Current Learning Rate: 0.084\n",
            "Iteration 900: Current success : 58 \t All success : 104 \t mean(l0) : 16.913462 \t Current Learning Rate: 0.082\n",
            "Iteration 1000: Current success : 54 \t All success : 104 \t mean(l0) : 16.913462 \t Current Learning Rate: 0.080\n",
            "Iteration 1100: Current success : 55 \t All success : 104 \t mean(l0) : 16.875000 \t Current Learning Rate: 0.078\n",
            "Iteration 1200: Current success : 55 \t All success : 104 \t mean(l0) : 16.875000 \t Current Learning Rate: 0.076\n",
            "Iteration 1300: Current success : 59 \t All success : 108 \t mean(l0) : 16.638889 \t Current Learning Rate: 0.074\n",
            "Iteration 1400: Current success : 58 \t All success : 109 \t mean(l0) : 16.596329 \t Current Learning Rate: 0.072\n",
            "Iteration 1500: Current success : 58 \t All success : 109 \t mean(l0) : 16.596329 \t Current Learning Rate: 0.070\n",
            "Iteration 1600: Current success : 59 \t All success : 109 \t mean(l0) : 16.596329 \t Current Learning Rate: 0.068\n",
            "Iteration 1700: Current success : 58 \t All success : 109 \t mean(l0) : 16.596329 \t Current Learning Rate: 0.066\n",
            "Iteration 1800: Current success : 59 \t All success : 110 \t mean(l0) : 16.509090 \t Current Learning Rate: 0.064\n",
            "Iteration 1900: Current success : 60 \t All success : 110 \t mean(l0) : 16.509090 \t Current Learning Rate: 0.062\n",
            "Iteration 2000: Current success : 60 \t All success : 111 \t mean(l0) : 16.423424 \t Current Learning Rate: 0.060\n",
            "Iteration 2100: Current success : 58 \t All success : 111 \t mean(l0) : 16.414415 \t Current Learning Rate: 0.058\n",
            "Iteration 2200: Current success : 58 \t All success : 111 \t mean(l0) : 16.414415 \t Current Learning Rate: 0.056\n",
            "Iteration 2300: Current success : 58 \t All success : 111 \t mean(l0) : 16.414415 \t Current Learning Rate: 0.054\n",
            "Iteration 2400: Current success : 58 \t All success : 111 \t mean(l0) : 16.414415 \t Current Learning Rate: 0.052\n",
            "Iteration 2500: Current success : 60 \t All success : 111 \t mean(l0) : 16.396397 \t Current Learning Rate: 0.050\n",
            "Iteration 2600: Current success : 60 \t All success : 111 \t mean(l0) : 16.396397 \t Current Learning Rate: 0.048\n",
            "Iteration 2700: Current success : 67 \t All success : 113 \t mean(l0) : 16.176991 \t Current Learning Rate: 0.046\n",
            "Iteration 2800: Current success : 67 \t All success : 113 \t mean(l0) : 16.176991 \t Current Learning Rate: 0.044\n",
            "Iteration 2900: Current success : 67 \t All success : 113 \t mean(l0) : 16.176991 \t Current Learning Rate: 0.042\n",
            "Iteration 3000: Current success : 64 \t All success : 113 \t mean(l0) : 16.168142 \t Current Learning Rate: 0.040\n",
            "Iteration 3100: Current success : 64 \t All success : 113 \t mean(l0) : 16.159292 \t Current Learning Rate: 0.038\n",
            "Iteration 3200: Current success : 64 \t All success : 113 \t mean(l0) : 16.159292 \t Current Learning Rate: 0.036\n",
            "Iteration 3300: Current success : 65 \t All success : 114 \t mean(l0) : 16.412281 \t Current Learning Rate: 0.034\n",
            "Iteration 3400: Current success : 65 \t All success : 114 \t mean(l0) : 16.412281 \t Current Learning Rate: 0.032\n",
            "Iteration 3500: Current success : 66 \t All success : 114 \t mean(l0) : 16.412281 \t Current Learning Rate: 0.030\n",
            "Iteration 3600: Current success : 66 \t All success : 114 \t mean(l0) : 16.412281 \t Current Learning Rate: 0.028\n",
            "Iteration 3700: Current success : 66 \t All success : 114 \t mean(l0) : 16.412281 \t Current Learning Rate: 0.026\n",
            "Iteration 3800: Current success : 66 \t All success : 114 \t mean(l0) : 16.412281 \t Current Learning Rate: 0.024\n",
            "Iteration 3900: Current success : 66 \t All success : 114 \t mean(l0) : 16.412281 \t Current Learning Rate: 0.022\n",
            "Iteration 4000: Current success : 66 \t All success : 114 \t mean(l0) : 16.412281 \t Current Learning Rate: 0.020\n",
            "Iteration 4100: Current success : 67 \t All success : 114 \t mean(l0) : 16.412281 \t Current Learning Rate: 0.018\n",
            "Iteration 4200: Current success : 67 \t All success : 114 \t mean(l0) : 16.412281 \t Current Learning Rate: 0.016\n",
            "Iteration 4300: Current success : 67 \t All success : 114 \t mean(l0) : 16.412281 \t Current Learning Rate: 0.014\n",
            "Iteration 4400: Current success : 67 \t All success : 114 \t mean(l0) : 16.412281 \t Current Learning Rate: 0.012\n",
            "Iteration 4500: Current success : 67 \t All success : 114 \t mean(l0) : 16.412281 \t Current Learning Rate: 0.010\n",
            "Iteration 4600: Current success : 67 \t All success : 114 \t mean(l0) : 16.412281 \t Current Learning Rate: 0.008\n",
            "Iteration 4700: Current success : 67 \t All success : 114 \t mean(l0) : 16.412281 \t Current Learning Rate: 0.006\n",
            "Iteration 4800: Current success : 67 \t All success : 114 \t mean(l0) : 16.412281 \t Current Learning Rate: 0.004\n",
            "Iteration 4900: Current success : 67 \t All success : 114 \t mean(l0) : 16.412281 \t Current Learning Rate: 0.002\n",
            "Iteration 5000: Current success : 67 \t All success : 114 \t mean(l0) : 16.412281 \t Current Learning Rate: 0.000\n",
            "Iteration 5100: Current success : 84 \t All success : 151 \t mean(l0) : 16.417219 \t Current Learning Rate: 0.049\n",
            "Iteration 5200: Current success : 63 \t All success : 160 \t mean(l0) : 16.687500 \t Current Learning Rate: 0.048\n",
            "Iteration 5300: Current success : 64 \t All success : 161 \t mean(l0) : 16.819876 \t Current Learning Rate: 0.047\n",
            "Iteration 5400: Current success : 65 \t All success : 162 \t mean(l0) : 17.055555 \t Current Learning Rate: 0.046\n",
            "Iteration 5500: Current success : 63 \t All success : 162 \t mean(l0) : 17.055555 \t Current Learning Rate: 0.045\n",
            "Iteration 5600: Current success : 61 \t All success : 162 \t mean(l0) : 17.055555 \t Current Learning Rate: 0.044\n",
            "Iteration 5700: Current success : 64 \t All success : 162 \t mean(l0) : 16.993828 \t Current Learning Rate: 0.043\n",
            "Iteration 5800: Current success : 64 \t All success : 162 \t mean(l0) : 16.987654 \t Current Learning Rate: 0.042\n",
            "Iteration 5900: Current success : 62 \t All success : 162 \t mean(l0) : 16.987654 \t Current Learning Rate: 0.041\n",
            "Iteration 6000: Current success : 64 \t All success : 162 \t mean(l0) : 16.987654 \t Current Learning Rate: 0.040\n",
            "Iteration 6100: Current success : 62 \t All success : 162 \t mean(l0) : 16.987654 \t Current Learning Rate: 0.039\n",
            "Iteration 6200: Current success : 62 \t All success : 162 \t mean(l0) : 16.987654 \t Current Learning Rate: 0.038\n",
            "Iteration 6300: Current success : 62 \t All success : 162 \t mean(l0) : 16.987654 \t Current Learning Rate: 0.037\n",
            "Iteration 6400: Current success : 62 \t All success : 162 \t mean(l0) : 16.987654 \t Current Learning Rate: 0.036\n",
            "Iteration 6500: Current success : 61 \t All success : 162 \t mean(l0) : 16.981482 \t Current Learning Rate: 0.035\n",
            "Iteration 6600: Current success : 61 \t All success : 162 \t mean(l0) : 16.981482 \t Current Learning Rate: 0.034\n",
            "Iteration 6700: Current success : 63 \t All success : 162 \t mean(l0) : 16.981482 \t Current Learning Rate: 0.033\n",
            "Iteration 6800: Current success : 62 \t All success : 162 \t mean(l0) : 16.981482 \t Current Learning Rate: 0.032\n",
            "Iteration 6900: Current success : 60 \t All success : 162 \t mean(l0) : 16.981482 \t Current Learning Rate: 0.031\n",
            "Iteration 7000: Current success : 60 \t All success : 162 \t mean(l0) : 16.981482 \t Current Learning Rate: 0.030\n",
            "Iteration 7100: Current success : 63 \t All success : 162 \t mean(l0) : 16.981482 \t Current Learning Rate: 0.029\n",
            "Iteration 7200: Current success : 62 \t All success : 162 \t mean(l0) : 16.981482 \t Current Learning Rate: 0.028\n",
            "Iteration 7300: Current success : 61 \t All success : 162 \t mean(l0) : 16.981482 \t Current Learning Rate: 0.027\n",
            "Iteration 7400: Current success : 60 \t All success : 162 \t mean(l0) : 16.981482 \t Current Learning Rate: 0.026\n",
            "Iteration 7500: Current success : 61 \t All success : 162 \t mean(l0) : 16.969135 \t Current Learning Rate: 0.025\n",
            "Iteration 7600: Current success : 62 \t All success : 162 \t mean(l0) : 16.956791 \t Current Learning Rate: 0.024\n",
            "Iteration 7700: Current success : 62 \t All success : 162 \t mean(l0) : 16.956791 \t Current Learning Rate: 0.023\n",
            "Iteration 7800: Current success : 64 \t All success : 162 \t mean(l0) : 16.956791 \t Current Learning Rate: 0.022\n",
            "Iteration 7900: Current success : 64 \t All success : 162 \t mean(l0) : 16.956791 \t Current Learning Rate: 0.021\n",
            "Iteration 8000: Current success : 64 \t All success : 162 \t mean(l0) : 16.956791 \t Current Learning Rate: 0.020\n",
            "Iteration 8100: Current success : 64 \t All success : 162 \t mean(l0) : 16.956791 \t Current Learning Rate: 0.019\n",
            "Iteration 8200: Current success : 64 \t All success : 162 \t mean(l0) : 16.956791 \t Current Learning Rate: 0.018\n",
            "Iteration 8300: Current success : 64 \t All success : 162 \t mean(l0) : 16.956791 \t Current Learning Rate: 0.017\n",
            "Iteration 8400: Current success : 63 \t All success : 162 \t mean(l0) : 16.956791 \t Current Learning Rate: 0.016\n",
            "Iteration 8500: Current success : 65 \t All success : 162 \t mean(l0) : 16.956791 \t Current Learning Rate: 0.015\n",
            "Iteration 8600: Current success : 64 \t All success : 162 \t mean(l0) : 16.956791 \t Current Learning Rate: 0.014\n",
            "Iteration 8700: Current success : 64 \t All success : 162 \t mean(l0) : 16.956791 \t Current Learning Rate: 0.013\n",
            "Iteration 8800: Current success : 65 \t All success : 162 \t mean(l0) : 16.956791 \t Current Learning Rate: 0.012\n",
            "Iteration 8900: Current success : 64 \t All success : 162 \t mean(l0) : 16.956791 \t Current Learning Rate: 0.011\n",
            "Iteration 9000: Current success : 64 \t All success : 162 \t mean(l0) : 16.956791 \t Current Learning Rate: 0.010\n",
            "Iteration 9100: Current success : 64 \t All success : 162 \t mean(l0) : 16.956791 \t Current Learning Rate: 0.009\n",
            "Iteration 9200: Current success : 64 \t All success : 162 \t mean(l0) : 16.956791 \t Current Learning Rate: 0.008\n",
            "Iteration 9300: Current success : 65 \t All success : 162 \t mean(l0) : 16.956791 \t Current Learning Rate: 0.007\n",
            "Iteration 9400: Current success : 65 \t All success : 162 \t mean(l0) : 16.956791 \t Current Learning Rate: 0.006\n",
            "Iteration 9500: Current success : 65 \t All success : 162 \t mean(l0) : 16.956791 \t Current Learning Rate: 0.005\n",
            "Iteration 9600: Current success : 65 \t All success : 162 \t mean(l0) : 16.956791 \t Current Learning Rate: 0.004\n",
            "Iteration 9700: Current success : 65 \t All success : 162 \t mean(l0) : 16.956791 \t Current Learning Rate: 0.003\n",
            "Iteration 9800: Current success : 65 \t All success : 162 \t mean(l0) : 16.956791 \t Current Learning Rate: 0.002\n",
            "Iteration 9900: Current success : 65 \t All success : 162 \t mean(l0) : 16.956791 \t Current Learning Rate: 0.001\n",
            "Iteration 10000: Current success : 65 \t All success : 162 \t mean(l0) : 16.956791 \t Current Learning Rate: 0.000\n",
            "Iteration 10100: Current success : 59 \t All success : 167 \t mean(l0) : 17.227545 \t Current Learning Rate: 0.025\n",
            "Iteration 10200: Current success : 56 \t All success : 168 \t mean(l0) : 17.434525 \t Current Learning Rate: 0.024\n",
            "Iteration 10300: Current success : 55 \t All success : 168 \t mean(l0) : 17.422619 \t Current Learning Rate: 0.024\n",
            "Iteration 10400: Current success : 55 \t All success : 168 \t mean(l0) : 17.416668 \t Current Learning Rate: 0.023\n",
            "Iteration 10500: Current success : 55 \t All success : 168 \t mean(l0) : 17.416668 \t Current Learning Rate: 0.023\n",
            "Iteration 10600: Current success : 85 \t All success : 168 \t mean(l0) : 17.416668 \t Current Learning Rate: 0.022\n",
            "Iteration 10700: Current success : 85 \t All success : 168 \t mean(l0) : 17.416668 \t Current Learning Rate: 0.022\n",
            "Iteration 10800: Current success : 85 \t All success : 168 \t mean(l0) : 17.410715 \t Current Learning Rate: 0.021\n",
            "Iteration 10900: Current success : 85 \t All success : 168 \t mean(l0) : 17.375000 \t Current Learning Rate: 0.021\n",
            "Iteration 11000: Current success : 85 \t All success : 168 \t mean(l0) : 17.369047 \t Current Learning Rate: 0.020\n",
            "Iteration 11100: Current success : 85 \t All success : 168 \t mean(l0) : 17.351191 \t Current Learning Rate: 0.020\n",
            "Iteration 11200: Current success : 55 \t All success : 168 \t mean(l0) : 17.351191 \t Current Learning Rate: 0.019\n",
            "Iteration 11300: Current success : 56 \t All success : 168 \t mean(l0) : 17.351191 \t Current Learning Rate: 0.019\n",
            "Iteration 11400: Current success : 56 \t All success : 168 \t mean(l0) : 17.351191 \t Current Learning Rate: 0.018\n",
            "Iteration 11500: Current success : 55 \t All success : 168 \t mean(l0) : 17.351191 \t Current Learning Rate: 0.018\n",
            "Iteration 11600: Current success : 55 \t All success : 168 \t mean(l0) : 17.351191 \t Current Learning Rate: 0.017\n",
            "Iteration 11700: Current success : 55 \t All success : 168 \t mean(l0) : 17.351191 \t Current Learning Rate: 0.017\n",
            "Iteration 11800: Current success : 55 \t All success : 168 \t mean(l0) : 17.351191 \t Current Learning Rate: 0.016\n",
            "Iteration 11900: Current success : 55 \t All success : 168 \t mean(l0) : 17.351191 \t Current Learning Rate: 0.016\n",
            "Iteration 12000: Current success : 55 \t All success : 168 \t mean(l0) : 17.351191 \t Current Learning Rate: 0.015\n",
            "Iteration 12100: Current success : 55 \t All success : 168 \t mean(l0) : 17.351191 \t Current Learning Rate: 0.015\n",
            "Iteration 12200: Current success : 84 \t All success : 168 \t mean(l0) : 17.351191 \t Current Learning Rate: 0.014\n",
            "Iteration 12300: Current success : 56 \t All success : 168 \t mean(l0) : 17.351191 \t Current Learning Rate: 0.014\n",
            "Iteration 12400: Current success : 56 \t All success : 168 \t mean(l0) : 17.351191 \t Current Learning Rate: 0.013\n",
            "Iteration 12500: Current success : 57 \t All success : 168 \t mean(l0) : 17.345238 \t Current Learning Rate: 0.013\n",
            "Iteration 12600: Current success : 56 \t All success : 168 \t mean(l0) : 17.345238 \t Current Learning Rate: 0.012\n",
            "Iteration 12700: Current success : 56 \t All success : 168 \t mean(l0) : 17.345238 \t Current Learning Rate: 0.012\n",
            "Iteration 12800: Current success : 56 \t All success : 168 \t mean(l0) : 17.345238 \t Current Learning Rate: 0.011\n",
            "Iteration 12900: Current success : 56 \t All success : 168 \t mean(l0) : 17.339287 \t Current Learning Rate: 0.011\n",
            "Iteration 13000: Current success : 67 \t All success : 168 \t mean(l0) : 17.339287 \t Current Learning Rate: 0.010\n",
            "Iteration 13100: Current success : 96 \t All success : 168 \t mean(l0) : 17.339287 \t Current Learning Rate: 0.010\n",
            "Iteration 13200: Current success : 95 \t All success : 168 \t mean(l0) : 17.339287 \t Current Learning Rate: 0.009\n",
            "Iteration 13300: Current success : 97 \t All success : 168 \t mean(l0) : 17.339287 \t Current Learning Rate: 0.009\n",
            "Iteration 13400: Current success : 96 \t All success : 168 \t mean(l0) : 17.339287 \t Current Learning Rate: 0.008\n",
            "Iteration 13500: Current success : 97 \t All success : 168 \t mean(l0) : 17.339287 \t Current Learning Rate: 0.008\n",
            "Iteration 13600: Current success : 96 \t All success : 168 \t mean(l0) : 17.339287 \t Current Learning Rate: 0.007\n",
            "Iteration 13700: Current success : 67 \t All success : 168 \t mean(l0) : 17.339287 \t Current Learning Rate: 0.007\n",
            "Iteration 13800: Current success : 68 \t All success : 168 \t mean(l0) : 17.339287 \t Current Learning Rate: 0.006\n",
            "Iteration 13900: Current success : 68 \t All success : 168 \t mean(l0) : 17.339287 \t Current Learning Rate: 0.006\n",
            "Iteration 14000: Current success : 68 \t All success : 168 \t mean(l0) : 17.339287 \t Current Learning Rate: 0.005\n",
            "Iteration 14100: Current success : 68 \t All success : 168 \t mean(l0) : 17.339287 \t Current Learning Rate: 0.005\n",
            "Iteration 14200: Current success : 68 \t All success : 168 \t mean(l0) : 17.339287 \t Current Learning Rate: 0.004\n",
            "Iteration 14300: Current success : 69 \t All success : 168 \t mean(l0) : 17.339287 \t Current Learning Rate: 0.004\n",
            "Iteration 14400: Current success : 69 \t All success : 168 \t mean(l0) : 17.339287 \t Current Learning Rate: 0.003\n",
            "Iteration 14500: Current success : 69 \t All success : 168 \t mean(l0) : 17.339287 \t Current Learning Rate: 0.003\n",
            "Iteration 14600: Current success : 69 \t All success : 168 \t mean(l0) : 17.339287 \t Current Learning Rate: 0.002\n",
            "Iteration 14700: Current success : 69 \t All success : 168 \t mean(l0) : 17.339287 \t Current Learning Rate: 0.002\n",
            "Iteration 14800: Current success : 69 \t All success : 168 \t mean(l0) : 17.339287 \t Current Learning Rate: 0.001\n",
            "Iteration 14900: Current success : 69 \t All success : 168 \t mean(l0) : 17.339287 \t Current Learning Rate: 0.001\n",
            "Iteration 15000: Current success : 69 \t All success : 168 \t mean(l0) : 17.339287 \t Current Learning Rate: 0.000\n",
            "Iteration 15100: Current success : 87 \t All success : 169 \t mean(l0) : 17.325443 \t Current Learning Rate: 0.012\n",
            "Iteration 15200: Current success : 95 \t All success : 170 \t mean(l0) : 17.282352 \t Current Learning Rate: 0.012\n",
            "Iteration 15300: Current success : 95 \t All success : 170 \t mean(l0) : 17.282352 \t Current Learning Rate: 0.012\n",
            "Iteration 15400: Current success : 95 \t All success : 170 \t mean(l0) : 17.270588 \t Current Learning Rate: 0.012\n",
            "Iteration 15500: Current success : 94 \t All success : 170 \t mean(l0) : 17.270588 \t Current Learning Rate: 0.011\n",
            "Iteration 15600: Current success : 94 \t All success : 170 \t mean(l0) : 17.270588 \t Current Learning Rate: 0.011\n",
            "Iteration 15700: Current success : 95 \t All success : 170 \t mean(l0) : 17.247059 \t Current Learning Rate: 0.011\n",
            "Iteration 15800: Current success : 65 \t All success : 172 \t mean(l0) : 17.069767 \t Current Learning Rate: 0.011\n",
            "Iteration 15900: Current success : 94 \t All success : 172 \t mean(l0) : 17.063953 \t Current Learning Rate: 0.010\n",
            "Iteration 16000: Current success : 93 \t All success : 172 \t mean(l0) : 17.058140 \t Current Learning Rate: 0.010\n",
            "Iteration 16100: Current success : 66 \t All success : 172 \t mean(l0) : 17.058140 \t Current Learning Rate: 0.010\n",
            "Iteration 16200: Current success : 67 \t All success : 172 \t mean(l0) : 17.058140 \t Current Learning Rate: 0.010\n",
            "Iteration 16300: Current success : 67 \t All success : 172 \t mean(l0) : 17.058140 \t Current Learning Rate: 0.009\n",
            "Iteration 16400: Current success : 65 \t All success : 172 \t mean(l0) : 17.058140 \t Current Learning Rate: 0.009\n",
            "Iteration 16500: Current success : 65 \t All success : 172 \t mean(l0) : 17.058140 \t Current Learning Rate: 0.009\n",
            "Iteration 16600: Current success : 64 \t All success : 172 \t mean(l0) : 17.058140 \t Current Learning Rate: 0.009\n",
            "Iteration 16700: Current success : 63 \t All success : 172 \t mean(l0) : 17.058140 \t Current Learning Rate: 0.008\n",
            "Iteration 16800: Current success : 64 \t All success : 172 \t mean(l0) : 17.058140 \t Current Learning Rate: 0.008\n",
            "Iteration 16900: Current success : 64 \t All success : 172 \t mean(l0) : 17.058140 \t Current Learning Rate: 0.008\n",
            "Iteration 17000: Current success : 64 \t All success : 172 \t mean(l0) : 17.058140 \t Current Learning Rate: 0.008\n",
            "Iteration 17100: Current success : 65 \t All success : 172 \t mean(l0) : 17.058140 \t Current Learning Rate: 0.007\n",
            "Iteration 17200: Current success : 66 \t All success : 172 \t mean(l0) : 17.058140 \t Current Learning Rate: 0.007\n",
            "Iteration 17300: Current success : 63 \t All success : 172 \t mean(l0) : 17.058140 \t Current Learning Rate: 0.007\n",
            "Iteration 17400: Current success : 65 \t All success : 172 \t mean(l0) : 17.058140 \t Current Learning Rate: 0.007\n",
            "Iteration 17500: Current success : 65 \t All success : 172 \t mean(l0) : 17.058140 \t Current Learning Rate: 0.006\n",
            "Iteration 17600: Current success : 65 \t All success : 172 \t mean(l0) : 17.058140 \t Current Learning Rate: 0.006\n",
            "Iteration 17700: Current success : 65 \t All success : 172 \t mean(l0) : 17.058140 \t Current Learning Rate: 0.006\n",
            "Iteration 17800: Current success : 66 \t All success : 172 \t mean(l0) : 17.058140 \t Current Learning Rate: 0.006\n",
            "Iteration 17900: Current success : 66 \t All success : 172 \t mean(l0) : 17.058140 \t Current Learning Rate: 0.005\n",
            "Iteration 18000: Current success : 66 \t All success : 172 \t mean(l0) : 17.058140 \t Current Learning Rate: 0.005\n",
            "Iteration 18100: Current success : 66 \t All success : 172 \t mean(l0) : 17.058140 \t Current Learning Rate: 0.005\n",
            "Iteration 18200: Current success : 66 \t All success : 172 \t mean(l0) : 17.058140 \t Current Learning Rate: 0.005\n",
            "Iteration 18300: Current success : 66 \t All success : 172 \t mean(l0) : 17.058140 \t Current Learning Rate: 0.004\n",
            "Iteration 18400: Current success : 66 \t All success : 172 \t mean(l0) : 17.058140 \t Current Learning Rate: 0.004\n",
            "Iteration 18500: Current success : 66 \t All success : 172 \t mean(l0) : 17.058140 \t Current Learning Rate: 0.004\n",
            "Iteration 18600: Current success : 67 \t All success : 172 \t mean(l0) : 17.058140 \t Current Learning Rate: 0.004\n",
            "Iteration 18700: Current success : 66 \t All success : 172 \t mean(l0) : 17.058140 \t Current Learning Rate: 0.003\n",
            "Iteration 18800: Current success : 66 \t All success : 172 \t mean(l0) : 17.058140 \t Current Learning Rate: 0.003\n",
            "Iteration 18900: Current success : 67 \t All success : 172 \t mean(l0) : 17.058140 \t Current Learning Rate: 0.003\n",
            "Iteration 19000: Current success : 67 \t All success : 172 \t mean(l0) : 17.058140 \t Current Learning Rate: 0.003\n",
            "Iteration 19100: Current success : 67 \t All success : 172 \t mean(l0) : 17.058140 \t Current Learning Rate: 0.002\n",
            "Iteration 19200: Current success : 67 \t All success : 172 \t mean(l0) : 17.058140 \t Current Learning Rate: 0.002\n",
            "Iteration 19300: Current success : 67 \t All success : 172 \t mean(l0) : 17.058140 \t Current Learning Rate: 0.002\n",
            "Iteration 19400: Current success : 67 \t All success : 172 \t mean(l0) : 17.058140 \t Current Learning Rate: 0.002\n",
            "Iteration 19500: Current success : 66 \t All success : 172 \t mean(l0) : 17.058140 \t Current Learning Rate: 0.001\n",
            "Iteration 19600: Current success : 66 \t All success : 172 \t mean(l0) : 17.058140 \t Current Learning Rate: 0.001\n",
            "Iteration 19700: Current success : 66 \t All success : 172 \t mean(l0) : 17.058140 \t Current Learning Rate: 0.001\n",
            "Iteration 19800: Current success : 67 \t All success : 172 \t mean(l0) : 17.058140 \t Current Learning Rate: 0.001\n",
            "Iteration 19900: Current success : 67 \t All success : 172 \t mean(l0) : 17.058140 \t Current Learning Rate: 0.000\n",
            "Iteration 20000: Current success : 67 \t All success : 172 \t mean(l0) : 17.058140 \t Current Learning Rate: 0.000\n",
            "outer_step 5: all success : 266 \t mean(l0)(success) : 19.1090\n",
            "-------------------------------------------------------------\n",
            "tensor([9.4375e+00, 1.0000e+05, 1.0000e+05,  ..., 1.0000e+05, 1.0000e+05,\n",
            "        1.0000e+05], device='cuda:0')\n",
            "Iteration 100: Current success : 19 \t All success : 37 \t mean(l0) : 6.297297 \t Current Learning Rate: 0.098\n",
            "Iteration 200: Current success : 34 \t All success : 53 \t mean(l0) : 12.452830 \t Current Learning Rate: 0.096\n",
            "Iteration 300: Current success : 34 \t All success : 63 \t mean(l0) : 14.634921 \t Current Learning Rate: 0.094\n",
            "Iteration 400: Current success : 41 \t All success : 70 \t mean(l0) : 15.514286 \t Current Learning Rate: 0.092\n",
            "Iteration 500: Current success : 45 \t All success : 76 \t mean(l0) : 15.947369 \t Current Learning Rate: 0.090\n",
            "Iteration 600: Current success : 48 \t All success : 82 \t mean(l0) : 16.756098 \t Current Learning Rate: 0.088\n",
            "Iteration 700: Current success : 50 \t All success : 83 \t mean(l0) : 17.084337 \t Current Learning Rate: 0.086\n",
            "Iteration 800: Current success : 53 \t All success : 88 \t mean(l0) : 17.829546 \t Current Learning Rate: 0.084\n",
            "Iteration 900: Current success : 50 \t All success : 88 \t mean(l0) : 17.829546 \t Current Learning Rate: 0.082\n",
            "Iteration 1000: Current success : 49 \t All success : 89 \t mean(l0) : 17.640450 \t Current Learning Rate: 0.080\n",
            "Iteration 1100: Current success : 53 \t All success : 90 \t mean(l0) : 17.544445 \t Current Learning Rate: 0.078\n",
            "Iteration 1200: Current success : 53 \t All success : 90 \t mean(l0) : 17.544445 \t Current Learning Rate: 0.076\n",
            "Iteration 1300: Current success : 53 \t All success : 91 \t mean(l0) : 17.417583 \t Current Learning Rate: 0.074\n",
            "Iteration 1400: Current success : 53 \t All success : 92 \t mean(l0) : 17.239130 \t Current Learning Rate: 0.072\n",
            "Iteration 1500: Current success : 53 \t All success : 92 \t mean(l0) : 17.239130 \t Current Learning Rate: 0.070\n",
            "Iteration 1600: Current success : 54 \t All success : 93 \t mean(l0) : 17.096775 \t Current Learning Rate: 0.068\n",
            "Iteration 1700: Current success : 52 \t All success : 94 \t mean(l0) : 17.319149 \t Current Learning Rate: 0.066\n",
            "Iteration 1800: Current success : 52 \t All success : 94 \t mean(l0) : 17.319149 \t Current Learning Rate: 0.064\n",
            "Iteration 1900: Current success : 52 \t All success : 94 \t mean(l0) : 17.319149 \t Current Learning Rate: 0.062\n",
            "Iteration 2000: Current success : 53 \t All success : 94 \t mean(l0) : 17.319149 \t Current Learning Rate: 0.060\n",
            "Iteration 2100: Current success : 53 \t All success : 94 \t mean(l0) : 17.319149 \t Current Learning Rate: 0.058\n",
            "Iteration 2200: Current success : 53 \t All success : 94 \t mean(l0) : 17.319149 \t Current Learning Rate: 0.056\n",
            "Iteration 2300: Current success : 53 \t All success : 94 \t mean(l0) : 17.319149 \t Current Learning Rate: 0.054\n",
            "Iteration 2400: Current success : 53 \t All success : 94 \t mean(l0) : 17.319149 \t Current Learning Rate: 0.052\n",
            "Iteration 2500: Current success : 53 \t All success : 94 \t mean(l0) : 17.319149 \t Current Learning Rate: 0.050\n",
            "Iteration 2600: Current success : 53 \t All success : 94 \t mean(l0) : 17.319149 \t Current Learning Rate: 0.048\n",
            "Iteration 2700: Current success : 58 \t All success : 95 \t mean(l0) : 17.178947 \t Current Learning Rate: 0.046\n",
            "Iteration 2800: Current success : 60 \t All success : 98 \t mean(l0) : 16.775511 \t Current Learning Rate: 0.044\n",
            "Iteration 2900: Current success : 60 \t All success : 98 \t mean(l0) : 16.775511 \t Current Learning Rate: 0.042\n",
            "Iteration 3000: Current success : 60 \t All success : 98 \t mean(l0) : 16.775511 \t Current Learning Rate: 0.040\n",
            "Iteration 3100: Current success : 61 \t All success : 99 \t mean(l0) : 17.040403 \t Current Learning Rate: 0.038\n",
            "Iteration 3200: Current success : 60 \t All success : 99 \t mean(l0) : 17.040403 \t Current Learning Rate: 0.036\n",
            "Iteration 3300: Current success : 62 \t All success : 99 \t mean(l0) : 17.040403 \t Current Learning Rate: 0.034\n",
            "Iteration 3400: Current success : 62 \t All success : 99 \t mean(l0) : 17.040403 \t Current Learning Rate: 0.032\n",
            "Iteration 3500: Current success : 63 \t All success : 99 \t mean(l0) : 17.040403 \t Current Learning Rate: 0.030\n",
            "Iteration 3600: Current success : 63 \t All success : 99 \t mean(l0) : 17.040403 \t Current Learning Rate: 0.028\n",
            "Iteration 3700: Current success : 63 \t All success : 99 \t mean(l0) : 17.040403 \t Current Learning Rate: 0.026\n",
            "Iteration 3800: Current success : 62 \t All success : 99 \t mean(l0) : 17.040403 \t Current Learning Rate: 0.024\n",
            "Iteration 3900: Current success : 62 \t All success : 99 \t mean(l0) : 17.040403 \t Current Learning Rate: 0.022\n",
            "Iteration 4000: Current success : 63 \t All success : 99 \t mean(l0) : 17.040403 \t Current Learning Rate: 0.020\n",
            "Iteration 4100: Current success : 62 \t All success : 99 \t mean(l0) : 17.040403 \t Current Learning Rate: 0.018\n",
            "Iteration 4200: Current success : 63 \t All success : 99 \t mean(l0) : 17.040403 \t Current Learning Rate: 0.016\n",
            "Iteration 4300: Current success : 62 \t All success : 99 \t mean(l0) : 17.040403 \t Current Learning Rate: 0.014\n",
            "Iteration 4400: Current success : 63 \t All success : 99 \t mean(l0) : 17.040403 \t Current Learning Rate: 0.012\n",
            "Iteration 4500: Current success : 61 \t All success : 99 \t mean(l0) : 17.040403 \t Current Learning Rate: 0.010\n",
            "Iteration 4600: Current success : 61 \t All success : 99 \t mean(l0) : 17.040403 \t Current Learning Rate: 0.008\n",
            "Iteration 4700: Current success : 61 \t All success : 99 \t mean(l0) : 17.040403 \t Current Learning Rate: 0.006\n",
            "Iteration 4800: Current success : 61 \t All success : 99 \t mean(l0) : 17.040403 \t Current Learning Rate: 0.004\n",
            "Iteration 4900: Current success : 61 \t All success : 99 \t mean(l0) : 17.040403 \t Current Learning Rate: 0.002\n",
            "Iteration 5000: Current success : 61 \t All success : 99 \t mean(l0) : 17.040403 \t Current Learning Rate: 0.000\n",
            "Iteration 5100: Current success : 58 \t All success : 112 \t mean(l0) : 17.660715 \t Current Learning Rate: 0.049\n",
            "Iteration 5200: Current success : 57 \t All success : 124 \t mean(l0) : 17.806452 \t Current Learning Rate: 0.048\n",
            "Iteration 5300: Current success : 57 \t All success : 124 \t mean(l0) : 17.806452 \t Current Learning Rate: 0.047\n",
            "Iteration 5400: Current success : 58 \t All success : 124 \t mean(l0) : 17.806452 \t Current Learning Rate: 0.046\n",
            "Iteration 5500: Current success : 58 \t All success : 124 \t mean(l0) : 17.798386 \t Current Learning Rate: 0.045\n",
            "Iteration 5600: Current success : 59 \t All success : 124 \t mean(l0) : 17.782257 \t Current Learning Rate: 0.044\n",
            "Iteration 5700: Current success : 58 \t All success : 124 \t mean(l0) : 17.782257 \t Current Learning Rate: 0.043\n",
            "Iteration 5800: Current success : 58 \t All success : 124 \t mean(l0) : 17.782257 \t Current Learning Rate: 0.042\n",
            "Iteration 5900: Current success : 56 \t All success : 124 \t mean(l0) : 17.766129 \t Current Learning Rate: 0.041\n",
            "Iteration 6000: Current success : 55 \t All success : 124 \t mean(l0) : 17.741936 \t Current Learning Rate: 0.040\n",
            "Iteration 6100: Current success : 58 \t All success : 124 \t mean(l0) : 17.741936 \t Current Learning Rate: 0.039\n",
            "Iteration 6200: Current success : 57 \t All success : 124 \t mean(l0) : 17.741936 \t Current Learning Rate: 0.038\n",
            "Iteration 6300: Current success : 60 \t All success : 127 \t mean(l0) : 18.362206 \t Current Learning Rate: 0.037\n",
            "Iteration 6400: Current success : 60 \t All success : 127 \t mean(l0) : 18.362206 \t Current Learning Rate: 0.036\n",
            "Iteration 6500: Current success : 60 \t All success : 127 \t mean(l0) : 18.362206 \t Current Learning Rate: 0.035\n",
            "Iteration 6600: Current success : 60 \t All success : 127 \t mean(l0) : 18.314960 \t Current Learning Rate: 0.034\n",
            "Iteration 6700: Current success : 60 \t All success : 127 \t mean(l0) : 18.259842 \t Current Learning Rate: 0.033\n",
            "Iteration 6800: Current success : 60 \t All success : 127 \t mean(l0) : 18.251968 \t Current Learning Rate: 0.032\n",
            "Iteration 6900: Current success : 60 \t All success : 127 \t mean(l0) : 18.220472 \t Current Learning Rate: 0.031\n",
            "Iteration 7000: Current success : 60 \t All success : 127 \t mean(l0) : 18.212599 \t Current Learning Rate: 0.030\n",
            "Iteration 7100: Current success : 60 \t All success : 127 \t mean(l0) : 18.204725 \t Current Learning Rate: 0.029\n",
            "Iteration 7200: Current success : 60 \t All success : 127 \t mean(l0) : 18.204725 \t Current Learning Rate: 0.028\n",
            "Iteration 7300: Current success : 61 \t All success : 127 \t mean(l0) : 18.204725 \t Current Learning Rate: 0.027\n",
            "Iteration 7400: Current success : 60 \t All success : 127 \t mean(l0) : 18.204725 \t Current Learning Rate: 0.026\n",
            "Iteration 7500: Current success : 61 \t All success : 127 \t mean(l0) : 18.196850 \t Current Learning Rate: 0.025\n",
            "Iteration 7600: Current success : 60 \t All success : 127 \t mean(l0) : 18.196850 \t Current Learning Rate: 0.024\n",
            "Iteration 7700: Current success : 61 \t All success : 127 \t mean(l0) : 18.196850 \t Current Learning Rate: 0.023\n",
            "Iteration 7800: Current success : 61 \t All success : 127 \t mean(l0) : 18.196850 \t Current Learning Rate: 0.022\n",
            "Iteration 7900: Current success : 61 \t All success : 127 \t mean(l0) : 18.196850 \t Current Learning Rate: 0.021\n",
            "Iteration 8000: Current success : 61 \t All success : 127 \t mean(l0) : 18.196850 \t Current Learning Rate: 0.020\n",
            "Iteration 8100: Current success : 61 \t All success : 127 \t mean(l0) : 18.196850 \t Current Learning Rate: 0.019\n",
            "Iteration 8200: Current success : 61 \t All success : 127 \t mean(l0) : 18.196850 \t Current Learning Rate: 0.018\n",
            "Iteration 8300: Current success : 61 \t All success : 127 \t mean(l0) : 18.196850 \t Current Learning Rate: 0.017\n",
            "Iteration 8400: Current success : 61 \t All success : 127 \t mean(l0) : 18.196850 \t Current Learning Rate: 0.016\n",
            "Iteration 8500: Current success : 61 \t All success : 127 \t mean(l0) : 18.196850 \t Current Learning Rate: 0.015\n",
            "Iteration 8600: Current success : 61 \t All success : 127 \t mean(l0) : 18.173229 \t Current Learning Rate: 0.014\n",
            "Iteration 8700: Current success : 61 \t All success : 127 \t mean(l0) : 18.173229 \t Current Learning Rate: 0.013\n",
            "Iteration 8800: Current success : 61 \t All success : 127 \t mean(l0) : 18.173229 \t Current Learning Rate: 0.012\n",
            "Iteration 8900: Current success : 61 \t All success : 127 \t mean(l0) : 18.173229 \t Current Learning Rate: 0.011\n",
            "Iteration 9000: Current success : 61 \t All success : 127 \t mean(l0) : 18.173229 \t Current Learning Rate: 0.010\n",
            "Iteration 9100: Current success : 61 \t All success : 127 \t mean(l0) : 18.173229 \t Current Learning Rate: 0.009\n",
            "Iteration 9200: Current success : 61 \t All success : 127 \t mean(l0) : 18.173229 \t Current Learning Rate: 0.008\n",
            "Iteration 9300: Current success : 61 \t All success : 127 \t mean(l0) : 18.173229 \t Current Learning Rate: 0.007\n",
            "Iteration 9400: Current success : 61 \t All success : 127 \t mean(l0) : 18.173229 \t Current Learning Rate: 0.006\n",
            "Iteration 9500: Current success : 61 \t All success : 127 \t mean(l0) : 18.173229 \t Current Learning Rate: 0.005\n",
            "Iteration 9600: Current success : 61 \t All success : 127 \t mean(l0) : 18.173229 \t Current Learning Rate: 0.004\n",
            "Iteration 9700: Current success : 61 \t All success : 127 \t mean(l0) : 18.173229 \t Current Learning Rate: 0.003\n",
            "Iteration 9800: Current success : 61 \t All success : 127 \t mean(l0) : 18.173229 \t Current Learning Rate: 0.002\n",
            "Iteration 9900: Current success : 61 \t All success : 127 \t mean(l0) : 18.173229 \t Current Learning Rate: 0.001\n",
            "Iteration 10000: Current success : 61 \t All success : 127 \t mean(l0) : 18.173229 \t Current Learning Rate: 0.000\n",
            "Iteration 10100: Current success : 59 \t All success : 137 \t mean(l0) : 18.978102 \t Current Learning Rate: 0.025\n",
            "Iteration 10200: Current success : 57 \t All success : 137 \t mean(l0) : 18.978102 \t Current Learning Rate: 0.024\n",
            "Iteration 10300: Current success : 56 \t All success : 137 \t mean(l0) : 18.978102 \t Current Learning Rate: 0.024\n",
            "Iteration 10400: Current success : 57 \t All success : 138 \t mean(l0) : 19.355072 \t Current Learning Rate: 0.023\n",
            "Iteration 10500: Current success : 57 \t All success : 138 \t mean(l0) : 19.355072 \t Current Learning Rate: 0.023\n",
            "Iteration 10600: Current success : 57 \t All success : 138 \t mean(l0) : 19.355072 \t Current Learning Rate: 0.022\n",
            "Iteration 10700: Current success : 58 \t All success : 138 \t mean(l0) : 19.355072 \t Current Learning Rate: 0.022\n",
            "Iteration 10800: Current success : 61 \t All success : 138 \t mean(l0) : 19.355072 \t Current Learning Rate: 0.021\n",
            "Iteration 10900: Current success : 61 \t All success : 138 \t mean(l0) : 19.355072 \t Current Learning Rate: 0.021\n",
            "Iteration 11000: Current success : 60 \t All success : 138 \t mean(l0) : 19.355072 \t Current Learning Rate: 0.020\n",
            "Iteration 11100: Current success : 60 \t All success : 138 \t mean(l0) : 19.355072 \t Current Learning Rate: 0.020\n",
            "Iteration 11200: Current success : 60 \t All success : 138 \t mean(l0) : 19.355072 \t Current Learning Rate: 0.019\n",
            "Iteration 11300: Current success : 61 \t All success : 138 \t mean(l0) : 19.347826 \t Current Learning Rate: 0.019\n",
            "Iteration 11400: Current success : 60 \t All success : 138 \t mean(l0) : 19.347826 \t Current Learning Rate: 0.018\n",
            "Iteration 11500: Current success : 60 \t All success : 138 \t mean(l0) : 19.347826 \t Current Learning Rate: 0.018\n",
            "Iteration 11600: Current success : 60 \t All success : 138 \t mean(l0) : 19.347826 \t Current Learning Rate: 0.017\n",
            "Iteration 11700: Current success : 60 \t All success : 138 \t mean(l0) : 19.347826 \t Current Learning Rate: 0.017\n",
            "Iteration 11800: Current success : 60 \t All success : 138 \t mean(l0) : 19.347826 \t Current Learning Rate: 0.016\n",
            "Iteration 11900: Current success : 60 \t All success : 138 \t mean(l0) : 19.347826 \t Current Learning Rate: 0.016\n",
            "Iteration 12000: Current success : 60 \t All success : 138 \t mean(l0) : 19.347826 \t Current Learning Rate: 0.015\n",
            "Iteration 12100: Current success : 61 \t All success : 138 \t mean(l0) : 19.347826 \t Current Learning Rate: 0.015\n",
            "Iteration 12200: Current success : 60 \t All success : 138 \t mean(l0) : 19.347826 \t Current Learning Rate: 0.014\n",
            "Iteration 12300: Current success : 61 \t All success : 138 \t mean(l0) : 19.347826 \t Current Learning Rate: 0.014\n",
            "Iteration 12400: Current success : 60 \t All success : 138 \t mean(l0) : 19.347826 \t Current Learning Rate: 0.013\n",
            "Iteration 12500: Current success : 60 \t All success : 138 \t mean(l0) : 19.347826 \t Current Learning Rate: 0.013\n",
            "Iteration 12600: Current success : 60 \t All success : 138 \t mean(l0) : 19.347826 \t Current Learning Rate: 0.012\n",
            "Iteration 12700: Current success : 60 \t All success : 138 \t mean(l0) : 19.347826 \t Current Learning Rate: 0.012\n",
            "Iteration 12800: Current success : 60 \t All success : 138 \t mean(l0) : 19.347826 \t Current Learning Rate: 0.011\n",
            "Iteration 12900: Current success : 62 \t All success : 138 \t mean(l0) : 19.347826 \t Current Learning Rate: 0.011\n",
            "Iteration 13000: Current success : 68 \t All success : 138 \t mean(l0) : 19.347826 \t Current Learning Rate: 0.010\n",
            "Iteration 13100: Current success : 68 \t All success : 138 \t mean(l0) : 19.347826 \t Current Learning Rate: 0.010\n",
            "Iteration 13200: Current success : 68 \t All success : 138 \t mean(l0) : 19.347826 \t Current Learning Rate: 0.009\n",
            "Iteration 13300: Current success : 68 \t All success : 138 \t mean(l0) : 19.347826 \t Current Learning Rate: 0.009\n",
            "Iteration 13400: Current success : 68 \t All success : 138 \t mean(l0) : 19.347826 \t Current Learning Rate: 0.008\n",
            "Iteration 13500: Current success : 68 \t All success : 138 \t mean(l0) : 19.347826 \t Current Learning Rate: 0.008\n",
            "Iteration 13600: Current success : 68 \t All success : 138 \t mean(l0) : 19.347826 \t Current Learning Rate: 0.007\n",
            "Iteration 13700: Current success : 68 \t All success : 138 \t mean(l0) : 19.347826 \t Current Learning Rate: 0.007\n",
            "Iteration 13800: Current success : 68 \t All success : 138 \t mean(l0) : 19.347826 \t Current Learning Rate: 0.006\n",
            "Iteration 13900: Current success : 68 \t All success : 138 \t mean(l0) : 19.347826 \t Current Learning Rate: 0.006\n",
            "Iteration 14000: Current success : 68 \t All success : 138 \t mean(l0) : 19.347826 \t Current Learning Rate: 0.005\n",
            "Iteration 14100: Current success : 68 \t All success : 138 \t mean(l0) : 19.347826 \t Current Learning Rate: 0.005\n",
            "Iteration 14200: Current success : 68 \t All success : 138 \t mean(l0) : 19.347826 \t Current Learning Rate: 0.004\n",
            "Iteration 14300: Current success : 68 \t All success : 138 \t mean(l0) : 19.347826 \t Current Learning Rate: 0.004\n",
            "Iteration 14400: Current success : 68 \t All success : 138 \t mean(l0) : 19.347826 \t Current Learning Rate: 0.003\n",
            "Iteration 14500: Current success : 68 \t All success : 138 \t mean(l0) : 19.347826 \t Current Learning Rate: 0.003\n",
            "Iteration 14600: Current success : 68 \t All success : 138 \t mean(l0) : 19.347826 \t Current Learning Rate: 0.002\n",
            "Iteration 14700: Current success : 68 \t All success : 138 \t mean(l0) : 19.347826 \t Current Learning Rate: 0.002\n",
            "Iteration 14800: Current success : 68 \t All success : 138 \t mean(l0) : 19.347826 \t Current Learning Rate: 0.001\n",
            "Iteration 14900: Current success : 68 \t All success : 138 \t mean(l0) : 19.347826 \t Current Learning Rate: 0.001\n",
            "Iteration 15000: Current success : 68 \t All success : 138 \t mean(l0) : 19.347826 \t Current Learning Rate: 0.000\n",
            "Iteration 15100: Current success : 65 \t All success : 144 \t mean(l0) : 19.201389 \t Current Learning Rate: 0.012\n",
            "Iteration 15200: Current success : 74 \t All success : 150 \t mean(l0) : 18.526667 \t Current Learning Rate: 0.012\n",
            "Iteration 15300: Current success : 68 \t All success : 151 \t mean(l0) : 18.655628 \t Current Learning Rate: 0.012\n",
            "Iteration 15400: Current success : 70 \t All success : 151 \t mean(l0) : 18.642384 \t Current Learning Rate: 0.012\n",
            "Iteration 15500: Current success : 70 \t All success : 151 \t mean(l0) : 18.622517 \t Current Learning Rate: 0.011\n",
            "Iteration 15600: Current success : 71 \t All success : 151 \t mean(l0) : 18.622517 \t Current Learning Rate: 0.011\n",
            "Iteration 15700: Current success : 70 \t All success : 151 \t mean(l0) : 18.622517 \t Current Learning Rate: 0.011\n",
            "Iteration 15800: Current success : 70 \t All success : 151 \t mean(l0) : 18.602650 \t Current Learning Rate: 0.011\n",
            "Iteration 15900: Current success : 70 \t All success : 151 \t mean(l0) : 18.602650 \t Current Learning Rate: 0.010\n",
            "Iteration 16000: Current success : 70 \t All success : 151 \t mean(l0) : 18.602650 \t Current Learning Rate: 0.010\n",
            "Iteration 16100: Current success : 70 \t All success : 151 \t mean(l0) : 18.596027 \t Current Learning Rate: 0.010\n",
            "Iteration 16200: Current success : 70 \t All success : 151 \t mean(l0) : 18.596027 \t Current Learning Rate: 0.010\n",
            "Iteration 16300: Current success : 71 \t All success : 151 \t mean(l0) : 18.596027 \t Current Learning Rate: 0.009\n",
            "Iteration 16400: Current success : 70 \t All success : 151 \t mean(l0) : 18.596027 \t Current Learning Rate: 0.009\n",
            "Iteration 16500: Current success : 70 \t All success : 151 \t mean(l0) : 18.596027 \t Current Learning Rate: 0.009\n",
            "Iteration 16600: Current success : 70 \t All success : 151 \t mean(l0) : 18.596027 \t Current Learning Rate: 0.009\n",
            "Iteration 16700: Current success : 71 \t All success : 151 \t mean(l0) : 18.596027 \t Current Learning Rate: 0.008\n",
            "Iteration 16800: Current success : 71 \t All success : 151 \t mean(l0) : 18.596027 \t Current Learning Rate: 0.008\n",
            "Iteration 16900: Current success : 71 \t All success : 151 \t mean(l0) : 18.596027 \t Current Learning Rate: 0.008\n",
            "Iteration 17000: Current success : 71 \t All success : 151 \t mean(l0) : 18.569536 \t Current Learning Rate: 0.008\n",
            "Iteration 17100: Current success : 71 \t All success : 151 \t mean(l0) : 18.569536 \t Current Learning Rate: 0.007\n",
            "Iteration 17200: Current success : 71 \t All success : 151 \t mean(l0) : 18.569536 \t Current Learning Rate: 0.007\n",
            "Iteration 17300: Current success : 71 \t All success : 151 \t mean(l0) : 18.569536 \t Current Learning Rate: 0.007\n",
            "Iteration 17400: Current success : 71 \t All success : 151 \t mean(l0) : 18.569536 \t Current Learning Rate: 0.007\n",
            "Iteration 17500: Current success : 71 \t All success : 151 \t mean(l0) : 18.569536 \t Current Learning Rate: 0.006\n",
            "Iteration 17600: Current success : 71 \t All success : 151 \t mean(l0) : 18.569536 \t Current Learning Rate: 0.006\n",
            "Iteration 17700: Current success : 71 \t All success : 151 \t mean(l0) : 18.569536 \t Current Learning Rate: 0.006\n",
            "Iteration 17800: Current success : 71 \t All success : 151 \t mean(l0) : 18.569536 \t Current Learning Rate: 0.006\n",
            "Iteration 17900: Current success : 71 \t All success : 151 \t mean(l0) : 18.569536 \t Current Learning Rate: 0.005\n",
            "Iteration 18000: Current success : 72 \t All success : 151 \t mean(l0) : 18.569536 \t Current Learning Rate: 0.005\n",
            "Iteration 18100: Current success : 71 \t All success : 151 \t mean(l0) : 18.569536 \t Current Learning Rate: 0.005\n",
            "Iteration 18200: Current success : 71 \t All success : 151 \t mean(l0) : 18.569536 \t Current Learning Rate: 0.005\n",
            "Iteration 18300: Current success : 72 \t All success : 151 \t mean(l0) : 18.569536 \t Current Learning Rate: 0.004\n",
            "Iteration 18400: Current success : 72 \t All success : 151 \t mean(l0) : 18.569536 \t Current Learning Rate: 0.004\n",
            "Iteration 18500: Current success : 72 \t All success : 151 \t mean(l0) : 18.569536 \t Current Learning Rate: 0.004\n",
            "Iteration 18600: Current success : 72 \t All success : 151 \t mean(l0) : 18.569536 \t Current Learning Rate: 0.004\n",
            "Iteration 18700: Current success : 71 \t All success : 151 \t mean(l0) : 18.569536 \t Current Learning Rate: 0.003\n",
            "Iteration 18800: Current success : 72 \t All success : 151 \t mean(l0) : 18.569536 \t Current Learning Rate: 0.003\n",
            "Iteration 18900: Current success : 72 \t All success : 151 \t mean(l0) : 18.569536 \t Current Learning Rate: 0.003\n",
            "Iteration 19000: Current success : 72 \t All success : 151 \t mean(l0) : 18.569536 \t Current Learning Rate: 0.003\n",
            "Iteration 19100: Current success : 72 \t All success : 151 \t mean(l0) : 18.569536 \t Current Learning Rate: 0.002\n",
            "Iteration 19200: Current success : 72 \t All success : 151 \t mean(l0) : 18.569536 \t Current Learning Rate: 0.002\n",
            "Iteration 19300: Current success : 72 \t All success : 151 \t mean(l0) : 18.569536 \t Current Learning Rate: 0.002\n",
            "Iteration 19400: Current success : 72 \t All success : 151 \t mean(l0) : 18.569536 \t Current Learning Rate: 0.002\n",
            "Iteration 19500: Current success : 72 \t All success : 151 \t mean(l0) : 18.569536 \t Current Learning Rate: 0.001\n",
            "Iteration 19600: Current success : 72 \t All success : 151 \t mean(l0) : 18.569536 \t Current Learning Rate: 0.001\n",
            "Iteration 19700: Current success : 72 \t All success : 151 \t mean(l0) : 18.569536 \t Current Learning Rate: 0.001\n",
            "Iteration 19800: Current success : 72 \t All success : 151 \t mean(l0) : 18.569536 \t Current Learning Rate: 0.001\n",
            "Iteration 19900: Current success : 72 \t All success : 151 \t mean(l0) : 18.569536 \t Current Learning Rate: 0.000\n",
            "Iteration 20000: Current success : 72 \t All success : 151 \t mean(l0) : 18.569536 \t Current Learning Rate: 0.000\n",
            "outer_step 6: all success : 268 \t mean(l0)(success) : 19.1007\n",
            "-------------------------------------------------------------\n",
            "tensor([9.1562e+00, 1.0000e+06, 1.0000e+06,  ..., 1.0000e+06, 1.0000e+06,\n",
            "        1.0000e+06], device='cuda:0')\n",
            "Iteration 100: Current success : 119 \t All success : 121 \t mean(l0) : 7.917355 \t Current Learning Rate: 0.098\n",
            "Iteration 200: Current success : 116 \t All success : 132 \t mean(l0) : 9.545455 \t Current Learning Rate: 0.096\n",
            "Iteration 300: Current success : 126 \t All success : 180 \t mean(l0) : 12.266667 \t Current Learning Rate: 0.094\n",
            "Iteration 400: Current success : 161 \t All success : 185 \t mean(l0) : 12.459459 \t Current Learning Rate: 0.092\n",
            "Iteration 500: Current success : 164 \t All success : 187 \t mean(l0) : 12.641711 \t Current Learning Rate: 0.090\n",
            "Iteration 600: Current success : 165 \t All success : 188 \t mean(l0) : 12.654255 \t Current Learning Rate: 0.088\n",
            "Iteration 700: Current success : 169 \t All success : 192 \t mean(l0) : 13.005209 \t Current Learning Rate: 0.086\n",
            "Iteration 800: Current success : 170 \t All success : 192 \t mean(l0) : 13.005209 \t Current Learning Rate: 0.084\n",
            "Iteration 900: Current success : 166 \t All success : 192 \t mean(l0) : 13.000000 \t Current Learning Rate: 0.082\n",
            "Iteration 1000: Current success : 166 \t All success : 192 \t mean(l0) : 13.000000 \t Current Learning Rate: 0.080\n",
            "Iteration 1100: Current success : 169 \t All success : 192 \t mean(l0) : 13.000000 \t Current Learning Rate: 0.078\n",
            "Iteration 1200: Current success : 169 \t All success : 192 \t mean(l0) : 13.000000 \t Current Learning Rate: 0.076\n",
            "Iteration 1300: Current success : 168 \t All success : 192 \t mean(l0) : 13.000000 \t Current Learning Rate: 0.074\n",
            "Iteration 1400: Current success : 169 \t All success : 193 \t mean(l0) : 13.202072 \t Current Learning Rate: 0.072\n",
            "Iteration 1500: Current success : 170 \t All success : 193 \t mean(l0) : 13.202072 \t Current Learning Rate: 0.070\n",
            "Iteration 1600: Current success : 170 \t All success : 193 \t mean(l0) : 13.202072 \t Current Learning Rate: 0.068\n",
            "Iteration 1700: Current success : 170 \t All success : 193 \t mean(l0) : 13.202072 \t Current Learning Rate: 0.066\n",
            "Iteration 1800: Current success : 172 \t All success : 195 \t mean(l0) : 13.610257 \t Current Learning Rate: 0.064\n",
            "Iteration 1900: Current success : 172 \t All success : 195 \t mean(l0) : 13.610257 \t Current Learning Rate: 0.062\n",
            "Iteration 2000: Current success : 172 \t All success : 195 \t mean(l0) : 13.610257 \t Current Learning Rate: 0.060\n",
            "Iteration 2100: Current success : 172 \t All success : 195 \t mean(l0) : 13.610257 \t Current Learning Rate: 0.058\n",
            "Iteration 2200: Current success : 172 \t All success : 195 \t mean(l0) : 13.610257 \t Current Learning Rate: 0.056\n",
            "Iteration 2300: Current success : 172 \t All success : 195 \t mean(l0) : 13.610257 \t Current Learning Rate: 0.054\n",
            "Iteration 2400: Current success : 174 \t All success : 197 \t mean(l0) : 13.781726 \t Current Learning Rate: 0.052\n",
            "Iteration 2500: Current success : 174 \t All success : 197 \t mean(l0) : 13.781726 \t Current Learning Rate: 0.050\n",
            "Iteration 2600: Current success : 174 \t All success : 197 \t mean(l0) : 13.781726 \t Current Learning Rate: 0.048\n",
            "Iteration 2700: Current success : 174 \t All success : 197 \t mean(l0) : 13.781726 \t Current Learning Rate: 0.046\n",
            "Iteration 2800: Current success : 174 \t All success : 197 \t mean(l0) : 13.781726 \t Current Learning Rate: 0.044\n",
            "Iteration 2900: Current success : 174 \t All success : 197 \t mean(l0) : 13.781726 \t Current Learning Rate: 0.042\n",
            "Iteration 3000: Current success : 174 \t All success : 197 \t mean(l0) : 13.781726 \t Current Learning Rate: 0.040\n",
            "Iteration 3100: Current success : 174 \t All success : 197 \t mean(l0) : 13.781726 \t Current Learning Rate: 0.038\n",
            "Iteration 3200: Current success : 175 \t All success : 198 \t mean(l0) : 13.838384 \t Current Learning Rate: 0.036\n",
            "Iteration 3300: Current success : 175 \t All success : 198 \t mean(l0) : 13.838384 \t Current Learning Rate: 0.034\n",
            "Iteration 3400: Current success : 174 \t All success : 198 \t mean(l0) : 13.838384 \t Current Learning Rate: 0.032\n",
            "Iteration 3500: Current success : 175 \t All success : 198 \t mean(l0) : 13.838384 \t Current Learning Rate: 0.030\n",
            "Iteration 3600: Current success : 175 \t All success : 198 \t mean(l0) : 13.838384 \t Current Learning Rate: 0.028\n",
            "Iteration 3700: Current success : 175 \t All success : 198 \t mean(l0) : 13.838384 \t Current Learning Rate: 0.026\n",
            "Iteration 3800: Current success : 175 \t All success : 198 \t mean(l0) : 13.838384 \t Current Learning Rate: 0.024\n",
            "Iteration 3900: Current success : 175 \t All success : 198 \t mean(l0) : 13.838384 \t Current Learning Rate: 0.022\n",
            "Iteration 4000: Current success : 175 \t All success : 198 \t mean(l0) : 13.838384 \t Current Learning Rate: 0.020\n",
            "Iteration 4100: Current success : 175 \t All success : 198 \t mean(l0) : 13.838384 \t Current Learning Rate: 0.018\n",
            "Iteration 4200: Current success : 175 \t All success : 198 \t mean(l0) : 13.838384 \t Current Learning Rate: 0.016\n",
            "Iteration 4300: Current success : 175 \t All success : 198 \t mean(l0) : 13.838384 \t Current Learning Rate: 0.014\n",
            "Iteration 4400: Current success : 175 \t All success : 198 \t mean(l0) : 13.838384 \t Current Learning Rate: 0.012\n",
            "Iteration 4500: Current success : 175 \t All success : 198 \t mean(l0) : 13.838384 \t Current Learning Rate: 0.010\n",
            "Iteration 4600: Current success : 175 \t All success : 198 \t mean(l0) : 13.838384 \t Current Learning Rate: 0.008\n",
            "Iteration 4700: Current success : 175 \t All success : 198 \t mean(l0) : 13.838384 \t Current Learning Rate: 0.006\n",
            "Iteration 4800: Current success : 175 \t All success : 198 \t mean(l0) : 13.838384 \t Current Learning Rate: 0.004\n",
            "Iteration 4900: Current success : 175 \t All success : 198 \t mean(l0) : 13.838384 \t Current Learning Rate: 0.002\n",
            "Iteration 5000: Current success : 175 \t All success : 198 \t mean(l0) : 13.838384 \t Current Learning Rate: 0.000\n",
            "Iteration 5100: Current success : 198 \t All success : 213 \t mean(l0) : 15.004695 \t Current Learning Rate: 0.049\n",
            "Iteration 5200: Current success : 198 \t All success : 219 \t mean(l0) : 15.442922 \t Current Learning Rate: 0.048\n",
            "Iteration 5300: Current success : 198 \t All success : 219 \t mean(l0) : 15.442922 \t Current Learning Rate: 0.047\n",
            "Iteration 5400: Current success : 201 \t All success : 220 \t mean(l0) : 15.695454 \t Current Learning Rate: 0.046\n",
            "Iteration 5500: Current success : 201 \t All success : 221 \t mean(l0) : 15.678734 \t Current Learning Rate: 0.045\n",
            "Iteration 5600: Current success : 200 \t All success : 221 \t mean(l0) : 15.674209 \t Current Learning Rate: 0.044\n",
            "Iteration 5700: Current success : 200 \t All success : 221 \t mean(l0) : 15.674209 \t Current Learning Rate: 0.043\n",
            "Iteration 5800: Current success : 201 \t All success : 221 \t mean(l0) : 15.674209 \t Current Learning Rate: 0.042\n",
            "Iteration 5900: Current success : 201 \t All success : 221 \t mean(l0) : 15.674209 \t Current Learning Rate: 0.041\n",
            "Iteration 6000: Current success : 201 \t All success : 221 \t mean(l0) : 15.674209 \t Current Learning Rate: 0.040\n",
            "Iteration 6100: Current success : 201 \t All success : 221 \t mean(l0) : 15.674209 \t Current Learning Rate: 0.039\n",
            "Iteration 6200: Current success : 201 \t All success : 221 \t mean(l0) : 15.674209 \t Current Learning Rate: 0.038\n",
            "Iteration 6300: Current success : 201 \t All success : 221 \t mean(l0) : 15.674209 \t Current Learning Rate: 0.037\n",
            "Iteration 6400: Current success : 201 \t All success : 221 \t mean(l0) : 15.674209 \t Current Learning Rate: 0.036\n",
            "Iteration 6500: Current success : 201 \t All success : 221 \t mean(l0) : 15.674209 \t Current Learning Rate: 0.035\n",
            "Iteration 6600: Current success : 201 \t All success : 221 \t mean(l0) : 15.674209 \t Current Learning Rate: 0.034\n",
            "Iteration 6700: Current success : 201 \t All success : 221 \t mean(l0) : 15.674209 \t Current Learning Rate: 0.033\n",
            "Iteration 6800: Current success : 201 \t All success : 221 \t mean(l0) : 15.674209 \t Current Learning Rate: 0.032\n",
            "Iteration 6900: Current success : 201 \t All success : 221 \t mean(l0) : 15.674209 \t Current Learning Rate: 0.031\n",
            "Iteration 7000: Current success : 201 \t All success : 221 \t mean(l0) : 15.674209 \t Current Learning Rate: 0.030\n",
            "Iteration 7100: Current success : 201 \t All success : 221 \t mean(l0) : 15.674209 \t Current Learning Rate: 0.029\n",
            "Iteration 7200: Current success : 201 \t All success : 221 \t mean(l0) : 15.674209 \t Current Learning Rate: 0.028\n",
            "Iteration 7300: Current success : 201 \t All success : 221 \t mean(l0) : 15.674209 \t Current Learning Rate: 0.027\n",
            "Iteration 7400: Current success : 201 \t All success : 221 \t mean(l0) : 15.674209 \t Current Learning Rate: 0.026\n",
            "Iteration 7500: Current success : 201 \t All success : 221 \t mean(l0) : 15.674209 \t Current Learning Rate: 0.025\n",
            "Iteration 7600: Current success : 201 \t All success : 221 \t mean(l0) : 15.674209 \t Current Learning Rate: 0.024\n",
            "Iteration 7700: Current success : 201 \t All success : 221 \t mean(l0) : 15.674209 \t Current Learning Rate: 0.023\n",
            "Iteration 7800: Current success : 201 \t All success : 221 \t mean(l0) : 15.674209 \t Current Learning Rate: 0.022\n",
            "Iteration 7900: Current success : 201 \t All success : 221 \t mean(l0) : 15.674209 \t Current Learning Rate: 0.021\n",
            "Iteration 8000: Current success : 201 \t All success : 221 \t mean(l0) : 15.674209 \t Current Learning Rate: 0.020\n",
            "Iteration 8100: Current success : 201 \t All success : 221 \t mean(l0) : 15.674209 \t Current Learning Rate: 0.019\n",
            "Iteration 8200: Current success : 201 \t All success : 221 \t mean(l0) : 15.674209 \t Current Learning Rate: 0.018\n",
            "Iteration 8300: Current success : 201 \t All success : 221 \t mean(l0) : 15.674209 \t Current Learning Rate: 0.017\n",
            "Iteration 8400: Current success : 201 \t All success : 221 \t mean(l0) : 15.674209 \t Current Learning Rate: 0.016\n",
            "Iteration 8500: Current success : 201 \t All success : 221 \t mean(l0) : 15.674209 \t Current Learning Rate: 0.015\n",
            "Iteration 8600: Current success : 201 \t All success : 221 \t mean(l0) : 15.674209 \t Current Learning Rate: 0.014\n",
            "Iteration 8700: Current success : 201 \t All success : 221 \t mean(l0) : 15.674209 \t Current Learning Rate: 0.013\n",
            "Iteration 8800: Current success : 201 \t All success : 221 \t mean(l0) : 15.674209 \t Current Learning Rate: 0.012\n",
            "Iteration 8900: Current success : 201 \t All success : 221 \t mean(l0) : 15.674209 \t Current Learning Rate: 0.011\n",
            "Iteration 9000: Current success : 201 \t All success : 221 \t mean(l0) : 15.674209 \t Current Learning Rate: 0.010\n",
            "Iteration 9100: Current success : 201 \t All success : 221 \t mean(l0) : 15.674209 \t Current Learning Rate: 0.009\n",
            "Iteration 9200: Current success : 201 \t All success : 221 \t mean(l0) : 15.674209 \t Current Learning Rate: 0.008\n",
            "Iteration 9300: Current success : 201 \t All success : 221 \t mean(l0) : 15.674209 \t Current Learning Rate: 0.007\n",
            "Iteration 9400: Current success : 201 \t All success : 221 \t mean(l0) : 15.674209 \t Current Learning Rate: 0.006\n",
            "Iteration 9500: Current success : 201 \t All success : 221 \t mean(l0) : 15.674209 \t Current Learning Rate: 0.005\n",
            "Iteration 9600: Current success : 201 \t All success : 221 \t mean(l0) : 15.674209 \t Current Learning Rate: 0.004\n",
            "Iteration 9700: Current success : 201 \t All success : 221 \t mean(l0) : 15.674209 \t Current Learning Rate: 0.003\n",
            "Iteration 9800: Current success : 201 \t All success : 221 \t mean(l0) : 15.674209 \t Current Learning Rate: 0.002\n",
            "Iteration 9900: Current success : 201 \t All success : 221 \t mean(l0) : 15.674209 \t Current Learning Rate: 0.001\n",
            "Iteration 10000: Current success : 201 \t All success : 221 \t mean(l0) : 15.674209 \t Current Learning Rate: 0.000\n",
            "Iteration 10100: Current success : 197 \t All success : 221 \t mean(l0) : 15.674209 \t Current Learning Rate: 0.025\n",
            "Iteration 10200: Current success : 198 \t All success : 221 \t mean(l0) : 15.674209 \t Current Learning Rate: 0.024\n",
            "Iteration 10300: Current success : 199 \t All success : 221 \t mean(l0) : 15.674209 \t Current Learning Rate: 0.024\n",
            "Iteration 10400: Current success : 199 \t All success : 221 \t mean(l0) : 15.674209 \t Current Learning Rate: 0.023\n",
            "Iteration 10500: Current success : 199 \t All success : 221 \t mean(l0) : 15.674209 \t Current Learning Rate: 0.023\n",
            "Iteration 10600: Current success : 199 \t All success : 221 \t mean(l0) : 15.674209 \t Current Learning Rate: 0.022\n",
            "Iteration 10700: Current success : 200 \t All success : 222 \t mean(l0) : 15.806307 \t Current Learning Rate: 0.022\n",
            "Iteration 10800: Current success : 200 \t All success : 222 \t mean(l0) : 15.792793 \t Current Learning Rate: 0.021\n",
            "Iteration 10900: Current success : 200 \t All success : 222 \t mean(l0) : 15.792793 \t Current Learning Rate: 0.021\n",
            "Iteration 11000: Current success : 200 \t All success : 222 \t mean(l0) : 15.792793 \t Current Learning Rate: 0.020\n",
            "Iteration 11100: Current success : 200 \t All success : 222 \t mean(l0) : 15.792793 \t Current Learning Rate: 0.020\n",
            "Iteration 11200: Current success : 200 \t All success : 222 \t mean(l0) : 15.788289 \t Current Learning Rate: 0.019\n",
            "Iteration 11300: Current success : 200 \t All success : 222 \t mean(l0) : 15.788289 \t Current Learning Rate: 0.019\n",
            "Iteration 11400: Current success : 200 \t All success : 223 \t mean(l0) : 15.753364 \t Current Learning Rate: 0.018\n",
            "Iteration 11500: Current success : 200 \t All success : 223 \t mean(l0) : 15.753364 \t Current Learning Rate: 0.018\n",
            "Iteration 11600: Current success : 200 \t All success : 223 \t mean(l0) : 15.753364 \t Current Learning Rate: 0.017\n",
            "Iteration 11700: Current success : 200 \t All success : 223 \t mean(l0) : 15.753364 \t Current Learning Rate: 0.017\n",
            "Iteration 11800: Current success : 200 \t All success : 223 \t mean(l0) : 15.753364 \t Current Learning Rate: 0.016\n",
            "Iteration 11900: Current success : 200 \t All success : 223 \t mean(l0) : 15.753364 \t Current Learning Rate: 0.016\n",
            "Iteration 12000: Current success : 200 \t All success : 223 \t mean(l0) : 15.753364 \t Current Learning Rate: 0.015\n",
            "Iteration 12100: Current success : 200 \t All success : 223 \t mean(l0) : 15.753364 \t Current Learning Rate: 0.015\n",
            "Iteration 12200: Current success : 201 \t All success : 223 \t mean(l0) : 15.753364 \t Current Learning Rate: 0.014\n",
            "Iteration 12300: Current success : 201 \t All success : 223 \t mean(l0) : 15.753364 \t Current Learning Rate: 0.014\n",
            "Iteration 12400: Current success : 200 \t All success : 223 \t mean(l0) : 15.753364 \t Current Learning Rate: 0.013\n",
            "Iteration 12500: Current success : 201 \t All success : 223 \t mean(l0) : 15.753364 \t Current Learning Rate: 0.013\n",
            "Iteration 12600: Current success : 201 \t All success : 223 \t mean(l0) : 15.753364 \t Current Learning Rate: 0.012\n",
            "Iteration 12700: Current success : 201 \t All success : 223 \t mean(l0) : 15.753364 \t Current Learning Rate: 0.012\n",
            "Iteration 12800: Current success : 201 \t All success : 223 \t mean(l0) : 15.753364 \t Current Learning Rate: 0.011\n",
            "Iteration 12900: Current success : 201 \t All success : 223 \t mean(l0) : 15.753364 \t Current Learning Rate: 0.011\n",
            "Iteration 13000: Current success : 200 \t All success : 223 \t mean(l0) : 15.753364 \t Current Learning Rate: 0.010\n",
            "Iteration 13100: Current success : 200 \t All success : 223 \t mean(l0) : 15.753364 \t Current Learning Rate: 0.010\n",
            "Iteration 13200: Current success : 200 \t All success : 223 \t mean(l0) : 15.753364 \t Current Learning Rate: 0.009\n",
            "Iteration 13300: Current success : 200 \t All success : 223 \t mean(l0) : 15.753364 \t Current Learning Rate: 0.009\n",
            "Iteration 13400: Current success : 200 \t All success : 223 \t mean(l0) : 15.753364 \t Current Learning Rate: 0.008\n",
            "Iteration 13500: Current success : 200 \t All success : 223 \t mean(l0) : 15.753364 \t Current Learning Rate: 0.008\n",
            "Iteration 13600: Current success : 200 \t All success : 223 \t mean(l0) : 15.753364 \t Current Learning Rate: 0.007\n",
            "Iteration 13700: Current success : 200 \t All success : 223 \t mean(l0) : 15.753364 \t Current Learning Rate: 0.007\n",
            "Iteration 13800: Current success : 200 \t All success : 223 \t mean(l0) : 15.753364 \t Current Learning Rate: 0.006\n",
            "Iteration 13900: Current success : 200 \t All success : 223 \t mean(l0) : 15.753364 \t Current Learning Rate: 0.006\n",
            "Iteration 14000: Current success : 200 \t All success : 223 \t mean(l0) : 15.753364 \t Current Learning Rate: 0.005\n",
            "Iteration 14100: Current success : 200 \t All success : 223 \t mean(l0) : 15.753364 \t Current Learning Rate: 0.005\n",
            "Iteration 14200: Current success : 200 \t All success : 223 \t mean(l0) : 15.753364 \t Current Learning Rate: 0.004\n",
            "Iteration 14300: Current success : 200 \t All success : 223 \t mean(l0) : 15.753364 \t Current Learning Rate: 0.004\n",
            "Iteration 14400: Current success : 200 \t All success : 223 \t mean(l0) : 15.753364 \t Current Learning Rate: 0.003\n",
            "Iteration 14500: Current success : 200 \t All success : 223 \t mean(l0) : 15.753364 \t Current Learning Rate: 0.003\n",
            "Iteration 14600: Current success : 200 \t All success : 223 \t mean(l0) : 15.753364 \t Current Learning Rate: 0.002\n",
            "Iteration 14700: Current success : 200 \t All success : 223 \t mean(l0) : 15.753364 \t Current Learning Rate: 0.002\n",
            "Iteration 14800: Current success : 200 \t All success : 223 \t mean(l0) : 15.753364 \t Current Learning Rate: 0.001\n",
            "Iteration 14900: Current success : 200 \t All success : 223 \t mean(l0) : 15.753364 \t Current Learning Rate: 0.001\n",
            "Iteration 15000: Current success : 200 \t All success : 223 \t mean(l0) : 15.753364 \t Current Learning Rate: 0.000\n",
            "Iteration 15100: Current success : 201 \t All success : 223 \t mean(l0) : 15.753364 \t Current Learning Rate: 0.012\n",
            "Iteration 15200: Current success : 202 \t All success : 223 \t mean(l0) : 15.753364 \t Current Learning Rate: 0.012\n",
            "Iteration 15300: Current success : 201 \t All success : 223 \t mean(l0) : 15.748879 \t Current Learning Rate: 0.012\n",
            "Iteration 15400: Current success : 201 \t All success : 224 \t mean(l0) : 15.879465 \t Current Learning Rate: 0.012\n",
            "Iteration 15500: Current success : 201 \t All success : 224 \t mean(l0) : 15.879465 \t Current Learning Rate: 0.011\n",
            "Iteration 15600: Current success : 201 \t All success : 224 \t mean(l0) : 15.879465 \t Current Learning Rate: 0.011\n",
            "Iteration 15700: Current success : 201 \t All success : 224 \t mean(l0) : 15.879465 \t Current Learning Rate: 0.011\n",
            "Iteration 15800: Current success : 202 \t All success : 224 \t mean(l0) : 15.879465 \t Current Learning Rate: 0.011\n",
            "Iteration 15900: Current success : 202 \t All success : 224 \t mean(l0) : 15.879465 \t Current Learning Rate: 0.010\n",
            "Iteration 16000: Current success : 203 \t All success : 224 \t mean(l0) : 15.879465 \t Current Learning Rate: 0.010\n",
            "Iteration 16100: Current success : 203 \t All success : 224 \t mean(l0) : 15.879465 \t Current Learning Rate: 0.010\n",
            "Iteration 16200: Current success : 202 \t All success : 224 \t mean(l0) : 15.879465 \t Current Learning Rate: 0.010\n",
            "Iteration 16300: Current success : 203 \t All success : 224 \t mean(l0) : 15.879465 \t Current Learning Rate: 0.009\n",
            "Iteration 16400: Current success : 203 \t All success : 224 \t mean(l0) : 15.879465 \t Current Learning Rate: 0.009\n",
            "Iteration 16500: Current success : 203 \t All success : 224 \t mean(l0) : 15.879465 \t Current Learning Rate: 0.009\n",
            "Iteration 16600: Current success : 203 \t All success : 224 \t mean(l0) : 15.879465 \t Current Learning Rate: 0.009\n",
            "Iteration 16700: Current success : 203 \t All success : 224 \t mean(l0) : 15.879465 \t Current Learning Rate: 0.008\n",
            "Iteration 16800: Current success : 203 \t All success : 224 \t mean(l0) : 15.879465 \t Current Learning Rate: 0.008\n",
            "Iteration 16900: Current success : 203 \t All success : 224 \t mean(l0) : 15.879465 \t Current Learning Rate: 0.008\n",
            "Iteration 17000: Current success : 203 \t All success : 224 \t mean(l0) : 15.879465 \t Current Learning Rate: 0.008\n",
            "Iteration 17100: Current success : 203 \t All success : 224 \t mean(l0) : 15.879465 \t Current Learning Rate: 0.007\n",
            "Iteration 17200: Current success : 203 \t All success : 224 \t mean(l0) : 15.879465 \t Current Learning Rate: 0.007\n",
            "Iteration 17300: Current success : 202 \t All success : 224 \t mean(l0) : 15.879465 \t Current Learning Rate: 0.007\n",
            "Iteration 17400: Current success : 202 \t All success : 224 \t mean(l0) : 15.879465 \t Current Learning Rate: 0.007\n",
            "Iteration 17500: Current success : 202 \t All success : 224 \t mean(l0) : 15.879465 \t Current Learning Rate: 0.006\n",
            "Iteration 17600: Current success : 202 \t All success : 224 \t mean(l0) : 15.879465 \t Current Learning Rate: 0.006\n",
            "Iteration 17700: Current success : 202 \t All success : 224 \t mean(l0) : 15.879465 \t Current Learning Rate: 0.006\n",
            "Iteration 17800: Current success : 202 \t All success : 224 \t mean(l0) : 15.879465 \t Current Learning Rate: 0.006\n",
            "Iteration 17900: Current success : 202 \t All success : 224 \t mean(l0) : 15.879465 \t Current Learning Rate: 0.005\n",
            "Iteration 18000: Current success : 202 \t All success : 224 \t mean(l0) : 15.879465 \t Current Learning Rate: 0.005\n",
            "Iteration 18100: Current success : 202 \t All success : 224 \t mean(l0) : 15.879465 \t Current Learning Rate: 0.005\n",
            "Iteration 18200: Current success : 203 \t All success : 224 \t mean(l0) : 15.879465 \t Current Learning Rate: 0.005\n",
            "Iteration 18300: Current success : 203 \t All success : 224 \t mean(l0) : 15.879465 \t Current Learning Rate: 0.004\n",
            "Iteration 18400: Current success : 203 \t All success : 224 \t mean(l0) : 15.879465 \t Current Learning Rate: 0.004\n",
            "Iteration 18500: Current success : 202 \t All success : 224 \t mean(l0) : 15.879465 \t Current Learning Rate: 0.004\n",
            "Iteration 18600: Current success : 202 \t All success : 224 \t mean(l0) : 15.879465 \t Current Learning Rate: 0.004\n",
            "Iteration 18700: Current success : 203 \t All success : 224 \t mean(l0) : 15.879465 \t Current Learning Rate: 0.003\n",
            "Iteration 18800: Current success : 202 \t All success : 224 \t mean(l0) : 15.879465 \t Current Learning Rate: 0.003\n",
            "Iteration 18900: Current success : 202 \t All success : 224 \t mean(l0) : 15.879465 \t Current Learning Rate: 0.003\n",
            "Iteration 19000: Current success : 202 \t All success : 224 \t mean(l0) : 15.879465 \t Current Learning Rate: 0.003\n",
            "Iteration 19100: Current success : 201 \t All success : 224 \t mean(l0) : 15.879465 \t Current Learning Rate: 0.002\n",
            "Iteration 19200: Current success : 201 \t All success : 224 \t mean(l0) : 15.879465 \t Current Learning Rate: 0.002\n",
            "Iteration 19300: Current success : 201 \t All success : 224 \t mean(l0) : 15.879465 \t Current Learning Rate: 0.002\n",
            "Iteration 19400: Current success : 201 \t All success : 224 \t mean(l0) : 15.879465 \t Current Learning Rate: 0.002\n",
            "Iteration 19500: Current success : 201 \t All success : 224 \t mean(l0) : 15.879465 \t Current Learning Rate: 0.001\n",
            "Iteration 19600: Current success : 201 \t All success : 224 \t mean(l0) : 15.879465 \t Current Learning Rate: 0.001\n",
            "Iteration 19700: Current success : 201 \t All success : 224 \t mean(l0) : 15.879465 \t Current Learning Rate: 0.001\n",
            "Iteration 19800: Current success : 201 \t All success : 224 \t mean(l0) : 15.879465 \t Current Learning Rate: 0.001\n",
            "Iteration 19900: Current success : 201 \t All success : 224 \t mean(l0) : 15.879465 \t Current Learning Rate: 0.000\n",
            "Iteration 20000: Current success : 201 \t All success : 224 \t mean(l0) : 15.879465 \t Current Learning Rate: 0.000\n",
            "1130\n",
            "272\n",
            "Accuracy of just malwares (without attack): 98.85% | Under attack: 75.93%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "71_LC4r1soo2"
      },
      "outputs": [],
      "source": [
        "for x_test, y_test in test_loader:\n",
        "    x_test, y_test = x_test.to(torch.float32).to(device), y_test.to(device)\n",
        "    mal_x_batch, mal_y_batch = x_test[y_test.squeeze() == 1], y_test[y_test.squeeze() == 1]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation on rounded adversarial examples\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "combined_model.eval()\n",
        "\n",
        "# Apply the final rounding solution to adversarial examples\n",
        "adv_rounded = final_rounding(mal_x_batch, adv, combined_model,removal_array)\n",
        "\n",
        "# Get the model's predictions on the rounded adversarial examples\n",
        "outputs = combined_model(adv_rounded)\n",
        "y_pred = outputs.argmax(dim=1)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "num_malwares = len(mal_y_batch)\n",
        "num_successful_attacks = (y_pred == 0).sum().item()\n",
        "total_difference = (torch.abs(adv_rounded[y_pred == 0] - mal_x_batch[y_pred == 0])).sum().item()\n",
        "accuracy = (y_pred == 1).sum().item() / num_malwares * 100\n",
        "\n",
        "# Display the results with clear formatting\n",
        "print(f\"Evaluation Results on Rounded Adversarial Examples\")\n",
        "print(f\"-----------------------------------------------\")\n",
        "print(f\"Number of malware samples: {num_malwares}\")\n",
        "print(f\"Number of successful attacks: {num_successful_attacks}\")\n",
        "print(f\"Total L1 norm difference (successful adversarial examples vs. original): {total_difference}\")\n",
        "print(f\"Accuracy of the model on malware under attack: {accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6487b93c-e50a-4bbf-c304-9b6595ccf157",
        "id": "EFyxxfjpsoo2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Results on Rounded Adversarial Examples\n",
            "-----------------------------------------------\n",
            "Number of malware samples: 1130\n",
            "Number of successful attacks: 272\n",
            "Total L1 norm difference (successful adversarial examples vs. original): 5260.0\n",
            "Accuracy of the model on malware under attack: 75.93%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure that fixed elements are not changed\n",
        "\n",
        "# Ensure `removal_array` is expanded to match the batch size\n",
        "expanded_removal_array = removal_array.expand(mal_x_batch.shape[0], -1)\n",
        "\n",
        "# Create a mask where non-fixed features are set to 1\n",
        "non_fixed_features_mask = torch.bitwise_or(expanded_removal_array, 1 - mal_x_batch.to(torch.uint8))\n",
        "\n",
        "# Calculate indices where the mask is 0 (fixed) or 1 (non-fixed)\n",
        "mask_fixed = non_fixed_features_mask == 0\n",
        "mask_non_fixed = non_fixed_features_mask == 1\n",
        "\n",
        "# Calculate and print the differences\n",
        "diff_adv_fixed = (torch.abs(adv - mal_x_batch) * mask_fixed).sum().item()\n",
        "diff_adv_non_fixed = (torch.abs(adv - mal_x_batch) * mask_non_fixed).sum().item()\n",
        "diff_adv_rounded_fixed = (torch.abs(adv_rounded - mal_x_batch) * mask_fixed).sum().item()\n",
        "diff_adv_rounded_non_fixed = (torch.abs(adv_rounded - mal_x_batch) * mask_non_fixed).sum().item()\n",
        "\n",
        "# Display results with clear formatting\n",
        "print(f\"Differences when mask is 0 (fixed features):\")\n",
        "print(f\"  Adv vs. Original: {diff_adv_fixed}\")\n",
        "print(f\"  Rounded Adv vs. Original: {diff_adv_rounded_fixed}\")\n",
        "print(f\"Differences when mask is 1 (non-fixed features):\")\n",
        "print(f\"  Adv vs. Original: {diff_adv_non_fixed}\")\n",
        "print(f\"  Rounded Adv vs. Original: {diff_adv_rounded_non_fixed}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff1edf89-9858-44fb-c29a-ae931c4ed9a1",
        "id": "E7GeqQiLsoo3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Differences when mask is 0 (fixed features):\n",
            "  Adv vs. Original: 0.0\n",
            "  Rounded Adv vs. Original: 0.0\n",
            "Differences when mask is 1 (non-fixed features):\n",
            "  Adv vs. Original: 59716.0859375\n",
            "  Rounded Adv vs. Original: 82390.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming differences is already computed as in your code\n",
        "differences = (torch.abs(adv_rounded[y_pred == 0] - mal_x_batch[y_pred == 0])).sum(1)\n",
        "\n",
        "# Convert differences to a numpy array for plotting\n",
        "differences_np = differences.cpu().numpy()\n",
        "\n",
        "# Set the style of the plot\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# Create a histogram with Seaborn\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(differences_np, bins=16, kde=False, color='skyblue')\n",
        "#sns.histplot(differences_np, bins=30, kde=True, color='skyblue')\n",
        "\n",
        "# Set the title and labels\n",
        "plt.title('Abundance of Differences', fontsize=16)\n",
        "plt.xlabel('Sum of Absolute Differences', fontsize=14)\n",
        "plt.ylabel('Frequency', fontsize=14)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 576
        },
        "outputId": "0c9a51ec-956c-4f2a-ecb8-adcc2a16c51b",
        "id": "ST0elASbsoo3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1QAAAIvCAYAAABz85rrAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABmY0lEQVR4nO3df3zN9f//8fvZ7HCGmR+L/MpMZ4xphE1EjBgSvSnlR+/oh0rFO72teqt30Tv1fuuHn32sUqJ6R/SDUfmRhPyqCJMfs4wiYb9snP14ff/wPufrtGF7nc05s9v1cnGZ83o+X6/X45w9befu9Xw9j8UwDEMAAAAAgBLz83YBAAAAAFBeEagAAAAAwCQCFQAAAACYRKACAAAAAJMIVAAAAABgEoEKAAAAAEwiUAEAAACASQQqAAAAADCJQAUAAAAAJhGoAFQo/fv3V3h4uFq1aqVTp05dtO/w4cMVHh6uTZs2XabqysamTZsUHh6u4cOHe7uUcu3777/XyJEj1aFDBzVv3lzh4eFavHjxJfdzjqPz/0RFRalz584aMmSIJk2apI0bN8owjAseIz4+/oLny8jI0HPPPadu3bqpVatWhb7Xx44d0xNPPKHOnTsrIiJC4eHhio+PN/ciAAAKqeTtAgDgctmxY4d+/vlnSVJubq4+++wz3X333V6uCuXBsWPH9MADDygzM1PXX3+9GjRoID8/PzVu3LjYx2jevLlatGgh6dz4S0tL0549e/TDDz9o/vz5Cg8P15QpUxQREVGi2iZOnKgVK1aoQYMG6tmzpypXrqymTZtKkgzD0JgxY7Rjxw41a9ZM0dHRCggI0PXXX1+icwAALoxABaDCWLRokSSpbt26OnbsmBYtWkSgQrGsX79eGRkZ6tevn6ZOnWrqGD169NAjjzxSaPvWrVv10ksvaceOHbrrrrv03nvvKTIy0q3P3/72N91333266qqr3Lbn5uZq5cqVqly5sj777DNVq1bNrf3IkSPasWOH6tevr08//VSVKvFrHwBKG1P+AFQIOTk5WrZsmSTp5ZdfVmBgoPbu3asdO3Z4uTKUB7/++qskqUmTJqV+7Hbt2mnBggW6/vrrlZOTo/Hjxys/P9+tz1VXXaWwsDBVr17dbfvx48eVl5enOnXqFApTkvTbb79Jkho2bEiYAoAywk9XABXCihUrlJWVJbvdrpiYGPXp00eLFi3SokWL1Lp160vuv3nzZr3xxhvauXOnzp49q2uvvVbDhg3TgAEDCvUdPny4Nm/erHnz5ik6OrpQ+/Tp0zVjxgyNGTPG7YrF+duHDh2q6dOna/Xq1Tpx4oRq166tHj166LHHHlNQUFCRNX7yySd67733tH//flWuXFmRkZF68MEHL/q8vvzyS61du1bbt2/XsWPHdObMGYWEhCg6Olr33Xefa+rY+eLj47VkyRK9+OKLat++vaZNm6YNGzYoPT1d9erVU9++ffXwww/LarUWec6dO3fqvffe05YtW3T8+HHZbDbVq1dPnTp10rBhw9SgQQO3/seOHdPbb7+tb775Rr/++qv8/PzUtGlTDRw4UEOGDDEVFJYtW6aPPvpISUlJys7OVkhIiGJiYnT//fcrNDTU1W/x4sV68sknXY9nzJihGTNmSJIaNGig1atXl/jcRbFarXruuefUr18/paSkaOXKlerVq5er/fzX/LbbbpMkhYeHu9qPHDni9vjFF190q3vz5s1u7atWrVLDhg1dj1esWKGFCxdq165dysrKUs2aNRUdHa3Ro0erWbNmbrUePnxYsbGxatCggb766ivNmzdPn376qX755RdlZ2e7ptVK0sGDBzV37lxt2LBBx44dk9VqVfPmzXX77bfr1ltvLfQ6nP9vJygoSDNnztSWLVt0+vRpNW7cWIMGDdI999wji8VS5Ou4ceNGffDBB/rxxx918uRJVatWTQ0aNFDXrl01fPhw1axZ061/SevLzMzUm2++qdWrVys1NVV5eXkKDg5Ww4YN1bFjRz300EMKCAgosjYAVy4CFYAKwTnd7y9/+Yvr66JFi5SYmKinnnpKVapUueC+X331lRYsWKCmTZuqc+fO+v3337Vt2zZNmDBBe/bsKfUb/H/77TcNHDhQeXl5atu2rc6ePavvv/9e8+fP1/bt2/XBBx8UetM2efJkvffee/Lz89P111+vq666Sj///LOGDx+uYcOGXfBcY8eOldVqVVhYmGJiYpSXl6d9+/Zp8eLFWrFihd566y21bdu2yH2TkpL0wgsvqEaNGmrfvr3S09P1/fff64033tD+/fs1c+bMQvu8+eabmjp1qgoKCtSkSRPFxsbqzJkzOnTokN5++21de+21rsAgSVu2bNHDDz+s9PR0NWjQQDfccIMcDod++uknTZo0SWvWrNEbb7xR7DexhmEoPj5en3zyiSpVqqR27dqpdu3a2rVrlxYvXqzly5dr2rRp6tKliySpcePGGjhwoJKSkrRnzx63+6D+/ObcU9dee60iIiK0e/durV+/3i1QFWXgwIHKzs7WF198ocDAQLf+zrqPHz+ub7/9VnXq1NGNN97oag8MDJQk5eXlafz48Vq+fLmsVqtatmypunXrKiUlRZ9//rm++uorTZ8+3fV6nM95f9a6devUrl07hYWFad++fa725cuXa8KECTp79qyaNm2qrl27KjMzUzt27NDf//53fffdd3rxxReLfG7ffvut5s6dq8aNG6tTp046fvy4tm3bppdeekm//fabnn766UL7OP8NSFKLFi3Url07ZWZm6uDBg5o5c6aio6Pd/oOjpPXl5OTorrvu0t69e1WrVi3FxMQoMDBQx48f18GDBzVr1izdc889BCqgIjIA4AqXnJxs2O12o2XLlsaJEydc23v37m3Y7XZjyZIlRe43bNgww263G3a73XjjjTfc2jZt2mS0bt3asNvtxjfffFPkft99912Rx502bZpht9uNadOmFbndbrcb8fHxxtmzZ11tv/76q3HjjTcadrvd+Pzzz932W7NmjWG3242oqChjy5Ytbm1vvPGG65jDhg0rVMuyZcuM06dPu20rKCgw5s+fb9jtdqNv375GQUGBW/uECRNcx3zllVeMvLw8V9vPP/9sREVFGXa73fj+++/d9lu5cqVht9uNyMhIY9myZYVq2bdvn7F//37X499//93o0KGDER4ebixYsMDIz893tZ08edIYMWKEYbfbjenTpxc61oW8//77ht1uN6Kjo43du3e7PWfn69+uXTu3cWIYF/6eFYdzPBRn36efftqw2+3GnXfe6bbd+Zp//PHHbttTU1MNu91udOvWrcjjfffddxf83huGYbzyyiuG3W43Bg8ebBw6dMitbfny5UaLFi2M9u3bG+np6YXOabfbjS5duhjJycmFjrtnzx6jVatWRmRkpPHFF1+4tR0+fNjo169fkf/2zv8398EHH7i1bdiwwQgPDzdatGhh/Pbbb25t8+bNM+x2u9GhQwdj48aNherZvn278euvv3pU35IlSwy73W7ce++9hsPhcNsnPz/f2LRpk9u/WQAVB/dQAbjiffzxx5Kk7t27q1atWq7tzqtVzvYLiYiI0AMPPOC2rUOHDrrrrrskSXPnzi3NclWvXj0988wzblPmrr76ateVpg0bNrj1f/fddyVJQ4cOVbt27dzaHnjgAdcVlaL06dPHdbXCyWKxaOjQoWrTpo327dunAwcOFLlvy5YtNXbsWPn7+7u22e129e/fv8g6p0+fLkkaN26c+vTpU+h4zZo1U1hYmNvzSktL09ChQ3XXXXfJz+///8qqWbOmXn75ZQUEBGjBggUXXXL8fG+//bYk6eGHH3Z7XSwWi8aMGaPw8HBlZGToo48+KtbxSpvzqldaWlqZnystLU3vvPOOKleurOnTp6tRo0Zu7b1799Ydd9yh9PR0ffbZZ0UeY9y4cW5TJJ3eeOMNORwOjR07VjfffLNbW4MGDfTCCy9IkubNm1fkcW+++WYNGTLEbVvHjh3VuXNn5efn67vvvnNtz8vL06xZsyRJkyZNUkxMTKHjtW7dWldffbVH9f3xxx+SpE6dOhW6CuXn56cOHTpccJorgCsbgQrAFS0vL0+ffPKJpP8foJwGDBigSpUqacuWLTp06NAFj1HUvRTO/SVp27ZthRYR8ETHjh1ls9kKbXeGjWPHjrm25eXladu2bZLkCjIXqvNCfvnlF82fP18vvPCCnnrqKcXHxys+Pt71BvLgwYNF7tetW7ci72Upqs7jx48rKSlJfn5+GjRo0EXrcVq7dq0kKS4ursj2unXr6pprrtHJkyeVkpJyyeMdPXrU9X0eOHBgoXaLxeKabuitzx4rKChw1VLWNm3apDNnzqht27aqW7dukX06dOggSfrhhx+KbC9qWmJBQYG++eYbSSoyOEtSZGSkAgMDlZSUpLNnzxZq79atW5H7OcfW77//7tq2a9cunTx5UjVr1lTPnj2L3K806nOuvPjmm2/qk08+uSyhF0D5wD1UAK5oX3/9tY4fP666deuqc+fObm116tRRly5dtHr1an388ccaN25ckcc4/+b9orafOXNGaWlpql27dqnUfP7/pJ/PuYqbw+FwbUtLS3O94btUnX+Wn5+v559/Xv/9738veoUnKyvL4zqdq82FhIQUWqnuQlJTUyWdu/J2KSdPnizySsn5nAEvODi4yBXxJLk+V+r8MHg5OT9sukaNGmV+Lufru3HjRrcFK4py8uTJQttq165dZPBPS0tzjZmuXbteso60tLRCge5SY+v8EHbkyBFJUmhoaLGCqNn6nAu1vPXWW5owYYIsFouuueYatW3bVrGxserevbvbVVQAFQeBCsAVzbkYxdmzZ4tcnMH5xnnx4sV69NFH3aavlURxp5xJ//8qxIVcrjdl8+bN04cffqiQkBDFx8erTZs2qlOnjipXrixJevzxx7V06dILPreyrtP5OvXq1avQtMQ/Cw4OLtNaLpfdu3dLOjd1sqw5X19nKLiYolZ7vNBCLueP76KuBP5ZUYs4lOXY8qS+8ePHa8iQIVqzZo22bdum77//XosXL9bixYsVGRmpefPmXXKsArjyEKgAXLF+//1319SetLQ0ff/99xftu27dOt10002F2g4fPlzkPs7/Ga9cubLbG3rnG7DTp08XuZ/zM41KQ3BwsKxWqxwOh44cOaJrr722UJ8L1b98+XJJ0nPPPafY2NhC7cWZRldczisOx48fV2ZmZrGuUl199dVKSUnRfffdV+iDbs1wXgVxXqEo6iqV86rNhabAlaV9+/YpKSlJkgpdTS0Lzu9JaGiopkyZUmrHrVmzpqpUqaIzZ87o73//u9t9i2Whfv36ks6NV8MwLnmVytP6GjZsqOHDh2v48OGSpB07duiJJ57QTz/9pDfffFOPPvqouScCoNzi2jSAK9aSJUuUn5+v6667Tj///PMF/9x7772S/v/VrD+70A35znuzrr/+erfPQnK+GS9qMYecnJxSvT+nUqVKrqsLn3/+eZF9LlR/enq6JBX63Cfp3Jv7PXv2lFKV56b6NW/eXAUFBZdcBMTJucy3M/h5ql69eq4pfYsXLy7UbhiGlixZIklFfn5YWXI4HHr22Wclnbsa1L179zI/Z8eOHRUQEKDNmzfrxIkTpXZcf39/3XDDDZJK73t3Ma1atVLNmjV18uRJrVy58pL9S7u+1q1buxaocQZiABULgQrAFcv5xv1SizI427/++usi7xXZtWuXEhIS3LZt3bpV77//viTpr3/9q1tbx44dJUnvv/++27042dnZmjhxout+otJy9913S5Lee++9QlfhEhIStGvXriL3c07jWrBggds0qN9//10TJkxQXl5eqdY5ZswYSdKrr76qL774olD7/v373ULovffeq6CgIL3zzjt6++233e7JckpNTdWnn35a7BpGjhwpSZo1a5ZbYDQMQ7NmzVJSUpKCgoJ0++23F/uYntq2bZuGDh2qbdu2KTAwUP/5z38uy7TPOnXqaPjw4crOztbo0aPdPpDXyeFwaNWqVRdc6fFCxowZo4CAAP373//WkiVLipzmunfvXn355Zem63eqVKmSRo8eLUmaOHGitmzZUqjPjh07dPToUY/q++qrr7Rly5ZCfXNzc7Vu3TpJRf/nBIArH1P+AFyRNm/erF9++UVWq1V9+/a9aN9rr71WLVu21K5du/TJJ5+43nQ7DR8+XK+88oo+/fRThYeH6/fff9fWrVtVUFCgESNGFLqxPS4uTu+++6527typvn376vrrr1dBQYF27typgIAA/eUvfyn2VZri6N69u4YOHaoFCxa4lk53frDvgQMHNGLEiCKXpx49erTWrVunjz76SJs2bVJERISysrK0ZcsWNWrUSD179tRXX31VanX27NlT48aN02uvvaZHH31UTZs2VfPmzV0f7Lt//369+OKLrpXc6tWrp1mzZumRRx7RSy+9pDfffFPXXnutQkJClJWVpQMHDujQoUO67rrrLrgS458NGTJEP/zwgz799FP95S9/Ufv27V0f7Hvw4EFVqVJF//nPf8pkmtrKlStd00Rzc3OVnp6uPXv26Pjx45Kk5s2ba8qUKRdd5r60Pf744/r999+1dOlSDRgwQM2bN1ejRo3k7++vo0ePas+ePcrOzlZCQoLbkvaX0rJlS/373//Wk08+qfj4eL322mtq1qyZatasqfT0dO3du1dHjx5Vnz59Ci1bbsbdd9+tgwcP6sMPP9SwYcMUERGh0NBQZWVlKTk5WampqZo3b57q1atnur7Nmzdr3rx5qlmzpiIiIlSrVi2dPn1a27dv14kTJ1S3bl3X1W4AFQuBCsAVyTl9r1u3bsVaMe3WW2/Vrl27tGjRokKBqmfPnoqNjdX//d//ae3atcrNzVVERISGDRtW5E3tAQEBmjt3rl5//XWtXLlS69evV61atdSzZ0899thjritbpemZZ55Ry5YttWDBAm3fvl1Wq1WRkZGaOHGipKI/7+e6667Txx9/rNdee00//fSTVq9e7fq8qwcffFCTJ08u9TpHjx6tmJgYvffee9qyZYu++uorVa1aVfXq1dO9995b6DOE2rdvr2XLlmn+/Plau3atfvrpJzkcDtWuXVtXX321+vfvX6I35BaLRS+//LK6dOmi//73v9q1a5dycnJUp04d3XbbbbrvvvuKXIChNOzZs8d1VaxKlSqqXr26GjZsqF69eqlHjx6KiYm5LMuln69SpUqaOnWq+vfvr0WLFmn79u3at2+fbDabQkJC1K1bN3Xv3l3t27cv8bHj4uIUGRmp9957Txs2bND333+v/Px81alTR40bN9bQoUPVu3fvUnkeFovFdS/ghx9+6Hoeztd4wIABhVYyLGl9t912m6pUqaJt27Zp//79OnnypKpXr66rr75ad999t26//XbX54gBqFgsRkmWpgIAAAAAuHAPFQAAAACYRKACAAAAAJMIVAAAAABgEoEKAAAAAEwiUAEAAACASQQqAAAAADCJz6H6nx9++EGGYSggIMDbpQAAAADwotzcXFksFrVp0+aSfblC9T+GYchXPpLLMAw5HA6fqQdXBsYVygLjCmWBcYWywLhCSZQkG3CF6n+cV6YiIyO9XImUnZ2tpKQkNWvWTIGBgd4uB1cIxhXKAuMKZYFxhbLAuEJJ/PTTT8XuyxUqAAAAADCJQAUAAAAAJhGoAAAAAMAkAhUAAAAAmESgAgAAAACTCFQAAAAAYBKBCgAAAABMIlABAAAAgEkEKgAAAAAwyScD1apVqzR48GC1adNGnTt31mOPPabU1NRC/RYuXKhevXopMjJS/fv315o1a7xQLQAAAICKyucC1aZNmzRmzBg1a9ZMM2fO1FNPPaU9e/Zo5MiROnPmjKvfsmXLNHHiRMXFxSkhIUFRUVEaM2aMfvzxR+8VDwAAAKBCqeTtAv5s2bJlql+/vv71r3/JYrFIkmrVqqW7775bO3fuVLt27SRJ06ZNU9++fTV27FhJUkxMjPbu3auZM2cqISHBW+UDAAAAqEB87gpVXl6eqlat6gpTklS9enVJkmEYkqTU1FSlpKQoLi7Obd8+ffpo48aNcjgcl69gAAAAABWWzwWq2267TQcOHNCCBQuUmZmp1NRUvfLKK4qIiFDbtm0lScnJyZKk0NBQt33DwsKUm5tb5P1WAAAAAFDafG7KX7t27TRjxgw9/vjjev755yVJLVq00Jtvvil/f39JUnp6uiQpKCjIbV/nY2d7SRmGoezsbLOll5qcnBy3r0BpYFyhLDCuUBYYVygLjCuUhGEYbjPmLsbnAtX333+vv//977r99tt10003KS0tTbNmzdL999+v999/X1WqVCmzc+fm5iopKanMjl9SKSkp3i4BVyDGFcoC4wplgXGFssC4QnFZrdZi9fO5QDV58mTFxMQoPj7etS0qKko33XSTPv30U91xxx2qUaOGJCkzM1MhISGufhkZGZLkai+pgIAANWvWzIPqS0dOTo5SUlLUpEkT2Ww2b5eDKwTjCmWBcYWywLhCWWBcoST2799f7L4+F6gOHDig2NhYt2316tVTzZo1dejQIUlS06ZNJZ27l8r5d+fjgIAANWrUyNS5LRaLAgMDTVZe+mw2m0/VgysD4wplgXGFssC4QllgXKE4ijvdT/LBRSnq16+v3bt3u207cuSITp06pQYNGkiSGjVqpCZNmmjFihVu/RITE9WxY8diX54DAAAAAE/43BWqIUOG6F//+pcmT56s7t27Ky0tTbNnz1bt2rXdlkl/5JFHNH78eDVu3FjR0dFKTEzUjh07NH/+fC9WX3oCAgJKlIwBAAAAXH4+F6hGjBghq9WqDz74QB9//LGqVq2qqKgovfbaa6pZs6arX79+/ZSTk6OEhATNmTNHoaGhmjFjhtq0aePF6kuHxWJRy5at5O/vcxcQL6gkK6EAAAAAVwqfC1QWi0V33nmn7rzzzkv2HTx4sAYPHnwZqrr8/P399O2vWcrMM7xdyiXVsPrrhnrMRQYAAEDF43OBCv9f+tl8ped5uwoAAAAAF1J+5pQBAAAAgI8hUAEAAACASQQqAAAAADCJQAUAAAAAJhGoAAAAAMAkAhUAAAAAmESgAgAAAACTCFQAAAAAYBKBCgAAAABMIlABAAAAgEkEKgAAAAAwiUAFAAAAACYRqAAAAADAJAIVAAAAAJhEoAIAAAAAkwhUAAAAAGASgQoAAAAATCJQAQAAAIBJBCoAAAAAMIlABQAAAAAmEagAAAAAwCQCFQAAAACYRKACAAAAAJMIVAAAAABgEoEKAAAAAEwiUAEAAACASQQqAAAAADCJQAUAAAAAJhGoAAAAAMAkAhUAAAAAmESgAgAAAACTCFQAAAAAYBKBCgAAAABMIlABAAAAgEkEKgAAAAAwiUAFAAAAACYRqAAAAADAJAIVAAAAAJhEoAIAAAAAkwhUAAAAAGASgQoAAAAATPK5QDV8+HCFh4cX+WfZsmWufgsXLlSvXr0UGRmp/v37a82aNV6sGgAAAEBFVMnbBfzZs88+q6ysLLdt7777rr788kt17NhRkrRs2TJNnDhRo0ePVkxMjBITEzVmzBgtWLBAUVFRXqgaAAAAQEXkc4GqWbNmhbY9/vjj6tSpk2rVqiVJmjZtmvr27auxY8dKkmJiYrR3717NnDlTCQkJl7NcAAAAABWYz035+7Pvv/9ehw8f1i233CJJSk1NVUpKiuLi4tz69enTRxs3bpTD4fBGmQAAAAAqIJ8PVEuXLlVgYKBiY2MlScnJyZKk0NBQt35hYWHKzc1VamrqZa8RAAAAQMXkc1P+zpeXl6fly5ere/fuCgwMlCSlp6dLkoKCgtz6Oh87280wDEPZ2dmm9y8tDodDNptNBUaB8vMNb5dzSQX5577m5OTIMHy/3ooqJyfH7StQGhhXKAuMK5QFxhVKwjAMWSyWYvX16UC1fv16nTx5Uv369bss58vNzVVSUtJlOdfF2Gw2BQcHy3HWoeycXG+Xc0mBRoCkGjp48CA/pMqBlJQUb5eAKxDjCmWBcYWywLhCcVmt1mL18+lAtXTpUgUHB6tz586ubTVq1JAkZWZmKiQkxLU9IyPDrd2MgICAIhfFuNyc94FZK1sVaAnwcjWXVqWyv6Rz0zC5QuW7cnJylJKSoiZNmshms3m7HFwhGFcoC4wrlAXGFUpi//79xe7rs4HqzJkzWrlypfr376+AgP8fKpo2bSrp3L1Uzr87HwcEBKhRo0amz2mxWFxTC73JeXnRz+Inf38vF1MMfv7nbsXjh1P5YLPZfGKc48rCuEJZYFyhLDCuUBzFne4n+fCiFKtXr1Z2drZrdT+nRo0aqUmTJlqxYoXb9sTERHXs2LHYl+YAAAAAwFM+e4Xq888/V/369XX99dcXanvkkUc0fvx4NW7cWNHR0UpMTNSOHTs0f/58L1QKAAAAoKLyyUCVnp6udevW6e677y7yclu/fv2Uk5OjhIQEzZkzR6GhoZoxY4batGnjhWoBAAAAVFQ+Gahq1KihnTt3XrTP4MGDNXjw4MtUEQAAAAAU5rP3UAEAAACAryNQAQAAAIBJBCoAAAAAMIlABQAAAAAmEagAAAAAwCQCFQAAAACYRKACAAAAAJMIVAAAAABgEoEKAAAAAEwiUAEAAACASQQqAAAAADCJQAUAAAAAJhGoAAAAAMAkAhUAAAAAmESgAgAAAACTCFQAAAAAYBKBCgAAAABMIlABAAAAgEkEKgAAAAAwiUAFAAAAACYRqAAAAADAJAIVAAAAAJhEoAIAAAAAkwhUAAAAAGASgQoAAAAATCJQAQAAAIBJBCoAAAAAMIlABQAAAAAmEagAAAAAwCQCFQAAAACYRKACAAAAAJMIVAAAAABgEoEKAAAAAEwiUAEAAACASQQqAAAAADCJQAUAAAAAJhGoAAAAAMAkAhUAAAAAmESgAgAAAACTCFQAAAAAYBKBCgAAAABMIlABAAAAgEkEKgAAAAAwyWcD1ZIlSzRgwABFRkYqOjpa9957r86cOeNqX716tfr376/IyEj16tVLH3/8sRerBQAAAFARVfJ2AUWZPXu2EhISNHr0aEVFRenUqVPauHGj8vPzJUlbt27VmDFjNGjQID311FP67rvv9PTTT6tq1arq3bu3l6sHAAAAUFH4XKBKTk7WjBkzNGvWLHXt2tW1vVevXq6/z549W61bt9bzzz8vSYqJiVFqaqqmTZtGoAIAAABw2fjclL/FixerYcOGbmHqfA6HQ5s2bSoUnPr06aMDBw7o8OHDl6NMAAAAAPC9QLV9+3bZ7XbNmjVLHTt2VKtWrTRkyBBt375dknTo0CHl5uaqadOmbvuFhYVJOneFCwAAAAAuB5+b8nf8+HHt3LlTe/fu1bPPPiubzaY33nhDI0eO1Jdffqn09HRJUlBQkNt+zsfOdjMMw1B2drb54kuJw+GQzWZTgVGg/HzD2+VcUsG5W9uUk5Mjw/D9eiuqnJwct69AaWBcoSwwrlAWGFcoCcMwZLFYitXX5wKVM9S8/vrrat68uSTpuuuuU/fu3TV//nx17ty5zM6dm5urpKSkMjt+cdlsNgUHB8tx1qHsnFxvl3NJgUaApBo6ePAgP6TKgZSUFG+XgCsQ4wplgXGFssC4QnFZrdZi9fO5QBUUFKTg4GBXmJKk4OBgRUREaP/+/erbt68kKTMz022/jIwMSVKNGjVMnzsgIEDNmjUzvX9pcTgckiRrZasCLQFerubSqlT2lySFhoZyhcqH5eTkKCUlRU2aNJHNZvN2ObhCMK5QFhhXKAuMK5TE/v37i93X5wJVs2bNdOjQoSLbzp49q8aNGysgIEDJycm68cYbXW3Oe6f+fG9VSVgsFgUGBprev7Q4Ly/6Wfzk7+/lYorBz//crXj8cCofbDabT4xzXFkYVygLjCuUBcYViqO40/0kH1yUolu3bkpLS3Obenfq1Cnt2rVLLVu2lNVqVXR0tL744gu3/RITExUWFqaGDRte7pIBAAAAVFA+d4WqR48eioyM1KOPPqpx48apcuXKmjNnjqxWq+666y5J0oMPPqgRI0bon//8p+Li4rRp0yYtXbpUr776qperBwAAAFCR+NwVKj8/P82ZM0dRUVF65pln9Le//U3VqlXTggULFBISIklq166dpk+frm3btmnUqFFaunSpJk+erLi4OC9XDwAAAKAi8bkrVJJUq1Yt/fvf/75on9jYWMXGxl6migAAAACgMJ+7QgUAAAAA5QWBCgAAAABMIlABAAAAgEkEKgAAAAAwiUAFAAAAACYRqAAAAADAJAIVAAAAAJhEoAIAAAAAkwhUAAAAAGASgQoAAAAATCJQAQAAAIBJBCoAAAAAMIlABQAAAAAmEagAAAAAwCQCFQAAAACYRKACAAAAAJMIVAAAAABgEoEKAAAAAEwiUAEAAACASQQqAAAAADCJQAUAAAAAJhGoAAAAAMAkAhUAAAAAmESgAgAAAACTCFQAAAAAYBKBCgAAAABMIlABAAAAgEkEKgAAAAAwiUAFAAAAACYRqAAAAADAJAIVAAAAAJhEoAIAAAAAkwhUAAAAAGASgQoAAAAATCJQAQAAAIBJBCoAAAAAMIlABQAAAAAmEagAAAAAwCQCFQAAAACYRKACAAAAAJMIVAAAAABgEoEKAAAAAEwiUAEAAACAST4XqBYvXqzw8PBCf/7zn/+49Vu4cKF69eqlyMhI9e/fX2vWrPFSxQAAAAAqqkreLuBC3nzzTVWvXt31uG7duq6/L1u2TBMnTtTo0aMVExOjxMREjRkzRgsWLFBUVJQXqgUAAABQEflsoGrZsqVq1apVZNu0adPUt29fjR07VpIUExOjvXv3aubMmUpISLiMVQIAAACoyHxuyt+lpKamKiUlRXFxcW7b+/Tpo40bN8rhcHipMgAAAAAVjc8Gqn79+qlFixaKjY3V//3f/yk/P1+SlJycLEkKDQ116x8WFqbc3FylpqZe9loBAAAAVEw+N+UvJCREjzzyiK677jpZLBatXr1ar732mo4dO6ZnnnlG6enpkqSgoCC3/ZyPne1mGIah7Oxs88WXEofDIZvNpgKjQPn5hrfLuaSCc1lXOTk5Mgzfr7eiysnJcfsKlAbGFcoC4wplgXGFkjAMQxaLpVh9PQpUDodDVqvVk0MUcuONN+rGG290Pe7cubMqV66sd999V6NHjy7Vc/1Zbm6ukpKSyvQcxWGz2RQcHCzHWYeyc3K9Xc4lBRoBkmro4MGD/JAqB1JSUrxdAq5AjCuUBcYVygLjCsVV3JzjUaC68cYbdcstt2jw4MEKDw/35FAXFRcXp7fffltJSUmqUaOGJCkzM1MhISGuPhkZGZLkajcjICBAzZo186zYUuC8D8xa2apAS4CXq7m0KpX9JZ2bhskVKt+Vk5OjlJQUNWnSRDabzdvl4ArBuEJZYFyhLDCuUBL79+8vdl+PAlXVqlU1f/58LViwQK1bt9bgwYPVt2/fMh2kTZs2lXTuXirn352PAwIC1KhRI9PHtlgsCgwM9LhGTzkvL/pZ/OTv7+ViisHP/9ytePxwKh9sNptPjHNcWRhXKAuMK5QFxhWKo7jT/SQPF6VYtWqVEhIS1LNnT+3evVsTJ05U586d9cwzz+inn37y5NBuEhMT5e/vr4iICDVq1EhNmjTRihUrCvXp2LFjqU9BBAAAAIAL8egKlcVicd3zdPLkSX3yySdatGiRPvroIy1cuFDh4eG6/fbb1b9/f1WrVq1Yxxw1apSio6NdUwhXrVqljz76SCNGjHBN8XvkkUc0fvx4NW7cWNHR0UpMTNSOHTs0f/58T54OAAAAAJRIqa3yV6tWLY0cOVIjR47Utm3btGjRIq1YsUKTJk3Sv//9b/Xu3Vt33nmnWrdufdHjhIaG6uOPP9bRo0dVUFCgJk2a6KmnntLw4cNdffr166ecnBwlJCRozpw5Cg0N1YwZM9SmTZvSejoAAAAAcEllsmx61apVZbPZVKlSJRmGofz8fC1ZskSffPKJbrzxRr344ouqXbt2kfv+4x//KNY5Bg8erMGDB5dm2QAAAABQIqUWqE6fPq2lS5dq4cKF2rVrlwzDUGRkpIYMGaK+fftq3759euutt7RixQo988wzmjlzZmmdGgAAAAC8wuNA9eOPP+qjjz7SihUrlJ2drcDAQN1+++0aMmSIWrRo4eoXGRmp1157TU888YRWr17t6WkBAAAAwOs8ClS33HKL9u/fL8MwFBERoTvuuEP9+vVT1apVL7jPtddeq88//9yT0wIAAACAT/AoUKWmpuq2227THXfcccnFJpxuueUWRUVFeXJaAAAAAPAJHgWqb7/9ttjLoTtdffXVuvrqqz05LQAAAAD4BI8+2NdmsykrK0sFBQVFthcUFCgrK0v5+fmenAYAAAAAfJJHgWrGjBnq2LGj0tLSimxPS0vTDTfcoNmzZ3tyGgAAAADwSR4Fqq+//lodO3ZUrVq1imyvVauWbrjhBlb1AwAAAHBF8ihQpaamqmnTphftExoaqsOHD3tyGgAAAADwSR4Fqry8PFkslkv2O3v2rCenAQAAAACf5FGgaty4sTZt2nTRPps2bVLDhg09OQ0AAAAA+CSPAtXNN9+spKQkvf7664VW8svPz9drr72mpKQk9e7d26MiAQAAAMAXefQ5VPfcc4+WLVumN954Q4mJiYqOjtZVV12l33//XZs2bdKhQ4cUFhamkSNHlla9AAAAAOAzPApUVatW1YIFC/TPf/5TX331lX755RdXm5+fn3r16qVnn31WVatW9bhQAAAAAPA1HgUq6dzS6NOmTdMff/yhnTt3KjMzU0FBQWrVqpVq165dGjUCAAAAgE/yOFA51alTRzfddFNpHQ4AAAAAfJ5Hi1IAAAAAQEXm8RWq/fv3a/78+frpp5+UmZlZaLU/SbJYLFq5cqWnpwIAAAAAn+JRoNq8ebPuvfdeORwOVapUSbVr15a/v3+hfoZheHIaAAAAAPBJHgWqqVOnKj8/X5MnT9bAgQOLDFMAAAAAcKXyKFDt2bNHffr00aBBg0qrHgAAAAAoNzxalMJms7E0OgAAAIAKy6NA1bVrV23durW0agEAAACAcsWjQPX3v/9dmZmZmjx5snJyckqrJgAAAAAoFzy6h2rcuHEKDAzUggULtHjxYjVp0kTVqlUr1M9isejdd9/15FQAAAAA4HM8XjbdKTs7W7t37y6yn8Vi8eQ0AAAAAOCTPF7lDwAAAAAqKo/uoQIAAACAisyjK1TnO336tFJSUpSTk6N27dqV1mEBAAAAwGd5fIXq8OHDevDBB9WhQwcNGjRII0aMcLVt27ZNffr00aZNmzw9DQAAAAD4HI8C1a+//qo77rhD33zzjWJjYxUVFSXDMFzt1113nU6dOqVly5Z5XCgAAAAA+BqPAtX06dOVnp6u9957T9OmTVOnTp3c2itVqqR27drp+++/96hIAAAAAPBFHgWqdevWqWfPnmrbtu0F+9SvX1/Hjh3z5DQAAAAA4JM8ClTp6elq0KDBRfsYhiGHw+HJaQAAAADAJ3kUqOrUqaNffvnlon327t2rq6++2pPTAAAAAIBP8ihQ3XDDDVqzZs0FP+B369at+u6779S1a1dPTgMAAAAAPsmjz6F68MEH9cUXX2jYsGEaNWqU62rV2rVr9cMPP+idd95RzZo1NWrUqFIpFgAAAAB8iUeBqmHDhnrrrbc0btw4vf7667JYLDIMQ6NHj5ZhGKpfv75ef/11XXXVVaVVL3xQFf9z33eLxeLtUoqtvNULAAAA3+RRoJLOfdbUl19+qTVr1mj79u1KT09XtWrV1Lp1a8XGxspqtZZGnfBhVj+LLBaLNhzNVroj39vlXFINq79uqBfo7TIAAABwBfA4UEnnPm+qZ8+e6tmzZ2kcDuVUuiNfp84WeLsMAAAA4LLxaFEKAAAAAKjIPLpCNWPGjGL1s1gsevjhhz05FQAAAAD4nDINVM5FKghUAAAAAK5EHgWqefPmFbk9MzNTu3fv1nvvvaeOHTtq6NChpo5/+vRpxcXF6dixY1q0aJEiIyNdbQsXLtSbb76pX3/9VaGhoRo3bpy6detm6jyoWFiVEAAAAKXFo0DVoUOHC7bFxsbqlltu0cCBA9WrVy9Tx581a5by8wuvGrds2TJNnDhRo0ePVkxMjBITEzVmzBgtWLBAUVFRps6FioNVCQEAAFBaSmWVvwtp0qSJevbsqTlz5qhPnz4l2vfAgQN6//33NWHCBD377LNubdOmTVPfvn01duxYSVJMTIz27t2rmTNnKiEhobTKxxWOVQkBAADgqTJf5a927do6ePBgifebPHmyhgwZotDQULftqampSklJUVxcnNv2Pn36aOPGjXI4HB7VCwAAAADFVaZXqBwOh9atW6fq1auXaL8VK1Zo7969mj59unbt2uXWlpycLEmFglZYWJhyc3OVmpqqsLAwU/UahqHs7GxT+5Ymh8Mhm82mAqNA+fmGt8u5pIKCc7ncKCgocoqmryl39f6vxJycHBmG+fGQk5Pj9hUoDYwrlAXGFcoC4wolUZL71z0KVJ988kmR2/Py8nTs2DElJiYqOTlZw4cPL/Yxc3JyNGXKFI0bN07VqlUr1J6eni5JCgoKctvufOxsNyM3N1dJSUmm9y8tNptNwcHBcpx1KDsn19vlXJKjsk2SdOasQ9nZZ71czaWVt3oDjQBJNXTw4MFS+SWQkpLi8TGAP2NcoSwwrlAWGFcoLqvVWqx+HgWq+Pj4IpOb83/RLRaL+vbtq/Hjxxf7mLNnz1bt2rX1l7/8xZPSTAkICFCzZs0u+3n/zDlt0VrZqkBLgJeruTSr9VyNVSpbFWj4e7maSytv9VapfK7G0NBQj69QpaSkqEmTJrLZbKVVHio4xhXKAuMKZYFxhZLYv39/sft6FKhefPHFIrdbLBbVqFFDLVu21FVXXVXs4x05ckRvv/22Zs6cqczMTElyTcHLzs7W6dOnVaNGDUnnlmYPCQlx7ZuRkSFJrnYzLBaLAgO9v5qaM6T6Wfzk7/vv9+Xnd24KncWPesuCn/+5ekvrh7/NZvOJcY4rC+MKZYFxhbLAuEJxlOTjajwKVAMHDvRk90IOHz6s3Nxc3X///YXaRowYoeuuu05Tp06VdO5eqqZNm7rak5OTFRAQoEaNGpVqTQAAAABwIWW6KEVJtWjRotCHBSclJenFF1/Uc889p8jISDVq1EhNmjTRihUr1KNHD1e/xMREdezYsdhzHQEAAADAUx4Fqi1btpjet3379oW2BQUFKTo6usj+LVu2VMuWLSVJjzzyiMaPH6/GjRsrOjpaiYmJ2rFjh+bPn2+6HgAAAAAoKY8C1fDhw0s0v/B8nqym169fP+Xk5CghIUFz5sxRaGioZsyYoTZt2pg+JgAAAACUlEeB6uGHH9b27dv17bff6pprrlHbtm1Vp04d/fHHH/rhhx+UkpKizp07KyoqyvQ5oqOj9fPPPxfaPnjwYA0ePNiD6gEAAADAMx4Fqo4dO2rOnDmaNGmSBg0a5Ha1yjAMffTRR3rhhRc0evRotWvXzuNiAQAAAMCX+Hmy8+uvv66bbrpJgwcPLjT1z2Kx6I477lCXLl30+uuve1QkAAAAAPgijwLVzp073ZYuL0pYWJh27tzpyWkAAAAAwCd5FKisVuslF5fYvXs3S5kDAAAAuCJ5FKg6deqkdevWac6cOXI4HG5tDodD//d//6dvv/1WnTt39qhIAAAAAPBFHi1K8fe//11bt27Vq6++qnnz5qlVq1aqVauWTp48qZ07d+rEiRO66qqr9MQTT5RWvQAAAADgMzwKVPXq1dPHH3+sqVOnavny5fr6669dbZUrV9att96qxx9/XCEhIZ7WCQAAAAA+x6NAJUkhISGaMmWKJk2apIMHDyozM1PVq1dXkyZNuHcKAAAAwBXN40DlFBAQILvdXlqHAwAAAACfVyqB6vjx4/ryyy918OBB5eTk6IUXXpAknTx5UocPH5bdbleVKlVK41QAAAAA4DM8WuVPkhYsWKDY2FhNmjRJ8+fP1+LFi11tJ06c0B133KHPPvvM09MAAAAAgM/xKFCtXr1akyZNkt1u1+zZs3XnnXe6tV977bUKDw/XypUrPSoSAAAAAHyRR1P+3nrrLdWvX1/z5s1TYGCgdu3aVaiP3W7X1q1bPTkNAAAAAPgkj65QJSUlqWvXrgoMDLxgn7p16+rEiROenAYAAAAAfJJHgcowDFWqdPGLXCdOnGD5dAAAAABXJI8CVWhoqLZt23bB9ry8PG3dupXl1AEAAABckTwKVLfccot2796tGTNmFGrLz8/XSy+9pNTUVA0YMMCT0wAAAACAT/JoUYphw4Zp9erVmjlzpj7//HPX1L7HHntMO3fu1JEjR9SpUycNGjSoVIoFAAAAAF/i0RWqgIAAvfXWW7r//vuVlpamffv2yTAMffHFF0pPT9d9992n2bNny2KxlFa9AAAAAOAzPLpCJUlWq1Xjxo3T2LFjlZycrPT0dFWrVk1hYWHy9/cvjRoBAAAAwCd5FKhiY2PVpUsXPfvss7JYLAoLCyutugAAAADA53k05e/UqVOqVq1aadUCAAAAAOWKR4EqPDxcKSkppVQKAAAAAJQvHgWq++67T2vWrNF3331XWvUAAAAAQLnh0T1UGRkZ6tSpk0aNGqXY2FhFRkaqTp06Ra7qx2dRAQAAALjSeBSo4uPjZbFYZBiGvvzyS3355ZeS5BaoDMOQxWIhUAEAAAC44pQ4UGVlZclqtcpqterFF18si5oAAAAAoFwocaBq3769xowZo4cfflgDBw6UJG3fvl3bt2/XiBEjSr1AAAAAAPBVJV6UwjAMGYbhtm3dunVcrQIAAABQ4Xi0yh8AAAAAVGQEKgAAAAAwiUAFAAAAACYRqAAAAADAJFOfQ/X5559r+/btrseHDh2SJN13331F9rdYLJozZ46ZUwEAAACAzzIVqH755Rf98ssvhbavW7euyP7nf9AvAAAAAFwpShyoVq1aVRZ1AAAAAEC5U+JA1aBBg7KoAwAAAADKHRalAAAAAACTCFQAAAAAYBKBCgAAAABMIlABAAAAgEkEKgAAAAAwyecC1dq1azVs2DDFxMSoVatWio2N1YsvvqjMzEy3fqtXr1b//v0VGRmpXr166eOPP/ZSxQAAAAAqKlMf7FuW0tLS1Lp1aw0fPlzBwcHat2+fpk+frn379untt9+WJG3dulVjxozRoEGD9NRTT+m7777T008/rapVq6p3795efgYAAAAAKgqfC1S33nqr2+Po6GhZrVZNnDhRx44dU926dTV79my1bt1azz//vCQpJiZGqampmjZtGoEKAAAAwGXjc1P+ihIcHCxJys3NlcPh0KZNmwoFpz59+ujAgQM6fPiwFyoEAAAAUBH5bKDKz8/X2bNntWvXLs2cOVPdu3dXw4YNdejQIeXm5qpp06Zu/cPCwiRJycnJ3igXAAAAQAXkc1P+nLp166Zjx45Jkm688UZNnTpVkpSeni5JCgoKcuvvfOxsN8MwDGVnZ5vev7Q4HA7ZbDYVGAXKzze8Xc4lFRScy+VGQYHy8/O9XM2llbt6/1diTk6ODMP8eMjJyXH7CpQGxhXKAuMKZYFxhZIwDEMWi6VYfX02UM2ZM0c5OTnav3+/Zs+erdGjR2vu3Llles7c3FwlJSWV6TmKw2azKTg4WI6zDmXn5Hq7nEtyVLZJks6cdSg7+6yXq7m08lZvoBEgqYYOHjxYKr8EUlJSPD4G8GeMK5QFxhXKAuMKxWW1WovVz2cDVfPmzSVJbdq0UWRkpG699VZ99dVXatasmSQVWkY9IyNDklSjRg3T5wwICHAd35scDockyVrZqkBLgJeruTSr9VyNVSpbFWj4e7maSytv9VapfK7G0NBQj69QpaSkqEmTJrLZbKVVHio4xhXKAuMKZYFxhZLYv39/sfv6bKA6X3h4uAICAnTo0CF1795dAQEBSk5O1o033ujq47x36s/3VpWExWJRYGCgx/V6ynl50c/iJ3/ff78vP79zU+gsftRbFvz8z9VbWj/8bTabT4xzXFkYVygLjCuUBcYViqO40/0kH16U4nzbt29Xbm6uGjZsKKvVqujoaH3xxRdufRITExUWFqaGDRt6qUoAAAAAFY3PXaEaM2aMWrVqpfDwcFWpUkV79uzRW2+9pfDwcPXo0UOS9OCDD2rEiBH65z//qbi4OG3atElLly7Vq6++6uXqAQAAAFQkPheoWrdurcTERM2ZM0eGYahBgwYaPHiwRo0a5boxrF27dpo+fbpee+01LVq0SPXr19fkyZMVFxfn5eoBAAAAVCQ+F6juv/9+3X///ZfsFxsbq9jY2MtQEQAAAAAUrVzcQwUAAAAAvohABQAAAAAmEagAAAAAwCQCFQAAAACYRKACAAAAAJMIVAAAAABgEoEKAAAAAEwiUAEAAACASQQqAAAAADCJQAUAAAAAJhGoAAAAAMAkAhUAAAAAmESgAgAAAACTCFQAAAAAYBKBCgAAAABMIlABAAAAgEkEKgAAAAAwiUAFAAAAACYRqAAAAADAJAIVAAAAAJhEoAIAAAAAkwhUAAAAAGASgQoAAAAATCJQAQAAAIBJBCoAAAAAMIlABQAAAAAmEagAAAAAwCQCFQAAAACYRKACAAAAAJMIVAAAAABgEoEKAAAAAEwiUAEAAACASQQqAAAAADCJQAUAAAAAJhGoAAAAAMAkAhUAAAAAmESgAgAAAACTCFRABWGxWGSz2WSxWLxdCgAAwBWjkrcLAHBxVfwtMgzD4yBks9kUERFRSlVdXGnUCwAAUB4QqAAfZ/WzyGKxaMPRbKU78k0fpyC/QGfOnFGVKlXk5192F6drWP11Q73AMjs+AACALyFQAeVEuiNfp84WmN4/Pz9f2Tm5CrQEyN+/FAsDAACowLiHCgAAAABMIlABAAAAgEk+F6iWL1+uBx98UF26dFFUVJRuvfVWLVq0SIZhuPVbuHChevXqpcjISPXv319r1qzxUsUAAAAAKiqfC1TvvPOObDab4uPjNXv2bHXp0kUTJ07UzJkzXX2WLVumiRMnKi4uTgkJCYqKitKYMWP0448/eq9wAAAAABWOzy1KMXv2bNWqVcv1uGPHjkpLS9PcuXP10EMPyc/PT9OmTVPfvn01duxYSVJMTIz27t2rmTNnKiEhwUuVAwAAAKhofO4K1flhyqlFixbKyspSdna2UlNTlZKSori4OLc+ffr00caNG+VwOC5XqQAAAAAqOJ8LVEXZtm2b6tatq2rVqik5OVmSFBoa6tYnLCxMubm5Sk1N9UaJAAAAACogn5vy92dbt25VYmKiJkyYIElKT0+XJAUFBbn1cz52tpthGIays7NN719aHA6HbDabCowC5ecbl97BywoKzuVyo6BA+fnmP3j2cqmo9RYUFLh9LSsF/ysxJyen0GIyuPLk5OS4fQVKA+MKZYFxhZIwDEMWi6VYfX06UB09elTjxo1TdHS0RowYUebny83NVVJSUpmf51JsNpuCg4PlOOtQdk6ut8u5JEdlmyTpzFmHsrPPermaS6vo9Z45c8bjY1xMoBEgqYYOHjzIL60KJCUlxdsl4ArEuEJZYFyhuKxWa7H6+WygysjI0H333afg4GBNnz5dfn7n/pe+Ro0akqTMzEyFhIS49T+/3YyAgAA1a9bMg6pLh/M+MGtlqwItAV6u5tKs1nM1VqlsVaDh7+VqLq2i1ltQUKAzZ86oSpUqrn9PZaFK5XM1hoaGcoWqAsjJyVFKSoqaNGkim83m7XJwhWBcoSwwrlAS+/fvL3ZfnwxUZ86c0QMPPKDMzEz997//VfXq1V1tTZs2lSQlJye7/u58HBAQoEaNGpk+r8ViUWBgoPnCS4nz8qKfxU/+vv9+3/Xm3OJHvWWhtOv18/OTfxk+cT//c/Xyy6pisdlsPvHzE1cWxhXKAuMKxVHc6X6SDy5KkZeXp7Fjxyo5OVlvvvmm6tat69beqFEjNWnSRCtWrHDbnpiYqI4dOxb70hwAAAAAeMrnrlA999xzWrNmjeLj45WVleX2Yb0RERGyWq165JFHNH78eDVu3FjR0dFKTEzUjh07NH/+fO8VDgAAAKDC8blAtX79eknSlClTCrWtWrVKDRs2VL9+/ZSTk6OEhATNmTNHoaGhmjFjhtq0aXO5ywUAAABQgflcoFq9enWx+g0ePFiDBw8u42oAAAAA4MJ87h4qAAAAACgvCFQAAAAAYBKBCgAAAABMIlABAAAAgEkEKgClqoq/RYZheLuMEiuPNQMAAO/zuVX+AJRvVj+LLBaLNhzNVroj39vlFEsNq79uqBfo7TIAAEA5RKACUCbSHfk6dbbA22UAAACUKab8AQAAAIBJBCoAAAAAMIlABQAAAAAmEagAAGWuvK2iWN7qBQB4D4tSAADKXHla+ZFVHwEAJUGgAgBcFqz8CAC4EjHlDwAAAABMIlABAAAAgEkEKgAVXhV/S7lbhKC81QsAwJWKe6gAVHhWPwuLJgAAAFMIVADwPyyaAAAASoopfwAAAABgEoEKAAAAAEwiUAEAAACASQQqAAAAADCJQAUAAAAAJhGoAAAAAMAkAhUAAAAAmESgAgAAAACTCFQAAAAAYBKBCgAAAABMIlABAAAAgEkEKgAAAAAwiUAFAAAAACYRqAAAAADAJAIVAAAAAJhEoAIAAAAAkwhUAAAAAGASgQoAAAAATCJQAQAAAIBJBCoAAAAAMIlABQAAAAAmEagAAAAAwCQCFQAAAACYRKACAAAAAJMIVAAA0ywWi2w2mywWi7dLAQDAK3wuUP3yyy965plndOuttyoiIkL9+vUrst/ChQvVq1cvRUZGqn///lqzZs1lrhQAvKOKv0WGYXi7DEmSzWZTRESEbDabt0sBAMArKnm7gD/bt2+f1q5dq+uuu04FBQVFvmlYtmyZJk6cqNGjRysmJkaJiYkaM2aMFixYoKioqMtfNABcRlY/iywWizYczVa6I9+rtRTkF+jMmTOqUqWK/PyL/j+6+oGVdF0dAhcA4Mrkc4Gqe/fu6tGjhyQpPj5eO3fuLNRn2rRp6tu3r8aOHStJiomJ0d69ezVz5kwlJCRcznIBwGvSHfk6dbbAqzXk5+crOydXgZYA+fsX3ScowLs1AgBQlnxuyp+f38VLSk1NVUpKiuLi4ty29+nTRxs3bpTD4SjL8gAAAADAxecC1aUkJydLkkJDQ922h4WFKTc3V6mpqd4oCwAAAEAF5HNT/i4lPT1dkhQUFOS23fnY2W6GYRjKzs42X1wpcTgcstlsKjAKlJ/vGzeeX0xBwblcbhQUKD/fu/dzFEdFrbegoMDta1kpb6+vVP5q9qV6izOufKne4ij4X4k5OTk+s/jHpZTHVRYv9trm5OS4fQVKA+MKJWEYRrF/tpa7QFWWcnNzlZSU5O0yZLPZFBwcLMdZh7Jzcr1dziU5Kp+72fzMWYeys896uZpLq+j1njlzxuNjXEx5e32l8lezL9Z7sXHli/VeTKARIKmGDh48WC7eeAUEBKhly1byv8CiIL4oP79Au3btVG7uxX/HpaSkXJ6CUKEwrlBcVqu1WP3KXaCqUaOGJCkzM1MhISGu7RkZGW7tZgQEBKhZs2aeFVgKnPeBWStbFWgJ8HI1l2a1nquxSmWrAo0L3JXuQypqvQUF563Gdol7FT1R3l5fqfzV7Ev1Fmdc+VK9xVGl8rkaQ0NDy8UVKovFIn9/P337a5bSz/r+FcAalf3VuX41XXvttRd8fXNycpSSkqImTZqwJD9KDeMKJbF///5i9y13gapp06aSzt1L5fy783FAQIAaNWpk+tgWi0WBgYEe1+gp5+VFP4vfBVfN8iXON1EWP+otC6Vdr5+fn/zL8ImXt9dXKn81+2K9FxtXvljvxTiXfy9vb7gy8wyl53m7ikvz8z8Xoorz+tpsNp/4vYwrC+MKxVGSqdTlZ37A/zRq1EhNmjTRihUr3LYnJiaqY8eOxb40BwAAAACe8rkrVDk5OVq7dq0k6ciRI8rKynKFpw4dOqhWrVp65JFHNH78eDVu3FjR0dFKTEzUjh07NH/+fG+WDgAAAKCC8blAdeLECT322GNu25yP582bp+joaPXr1085OTlKSEjQnDlzFBoaqhkzZqhNmzbeKBkAAABABeVzgaphw4b6+eefL9lv8ODBGjx48GWoCAAAAACKVu7uoQIAAAAAX0GgAgAAAACTCFQAAAAAYBKBCgAAAABMIlABAAAAgEkEKgAAAAAwiUAFAAAAACYRqAAAAADAJAIVAAAAAJhEoAIAAAAAkwhUAAAAAGASgQoAAAAATCJQAQAAAIBJBCoAAAAAMIlABQAAAAAmEagAAAAAwCQCFQAAAACYRKACAAAAAJMIVAAAAABgEoEKAAAAAEwiUAEAAFxBDMPwdgklUt7qBf6skrcLAAAAQOmxWCzacDRb6Y58b5dySTWs/rqhXqC3ywA8QqACAAC4wqQ78nXqbIG3ywAqBKb8AQAAAIBJBCoAAAAAMIlABQAAAAAmEagAADhPFX8Lq44BAIqNRSkAADiP1c9SrlZJqx9YSdfVsXm7DACosAhUAAAUobyskhYU4Ps1AsCVjCl/AAAAAGASgQoAAAAATCJQAQAAn2KxWGSz2WSxWLxdCgBcEvdQAQCAy8a5iuLFwpLNZlNERMRlrOriCgxDfoQ7ABdAoAIAAJdNcVZRLMgv0JkzZ1SlShX5+Xt3Mo1zFUVWfQRwIQQqAABw2V1sFcX8/Hxl5+Qq0BIgf//LXNifOFdRZNVHABfCPVQAAAAAYBKBCgAAAABMIlABAAAAgEkEKgAAAAAwiUAFAAAAACYRqAAAAADAJAIVAAAAAJhUbgPVgQMHdM899ygqKkqdOnXSyy+/LIfD4e2yAAAAAFQg5fKDfdPT03X33XerSZMmmj59uo4dO6YpU6bozJkzeuaZZ7xdHgAAAOATDMOQxWLxdhnFVt7qlcppoPrwww91+vRpzZgxQ8HBwZLOfar6c889pwceeEB169b1boEAAACAD7BYLNpwNFvpjnxvl3JJNaz+uqFeoLfLKLFyGai++eYbdezY0RWmJCkuLk7PPvus1q9fr9tuu817xQEAAAA+JN2Rr1NnC7xdxhWrXN5DlZycrKZNm7ptCwoKUkhIiJKTk71UFQAAAICKplxeocrIyFBQUFCh7TVq1FB6erqpY+bm5sowDO3YscPT8jxmGIb8/Px0Vb6hOobh7XIuqdJpi376w6I6+YZqUW+pK816DRmy5JTtvOTy9vpK5a9mX6v3UuPK1+q9FOotW8Wt93L8vCqOK/X19RV+py36Kd0i4zLU6rw3Z9++feXuHh1PWCyMBzNyc3OLPU7KZaAqC84XzBf+gTlrqOJvkeT9eoqLessW9Za98lYz9ZYt6i1b1Fu2ylu9l+P9l8VikZ9fuZyc5THGg7karuhAFRQUpMzMzELb09PTVaNGDVPHbNOmjadlAQAAAKhgymVMb9q0aaF7pTIzM3X8+PFC91YBAAAAQFkpl4GqS5cu2rBhgzIyMlzbVqxYIT8/P3Xq1MmLlQEAAACoSCyGL9z1VULp6enq27evQkND9cADD7g+2PeWW27hg30BAAAAXDblMlBJ0oEDBzRp0iT98MMPqlq1qm699VaNGzdOVqvV26UBAAAAqCDKbaACAAAAAG8rl/dQAQAAAIAvIFABAAAAgEkEKgAAAAAwiUAFAAAAACYRqAAAAADAJAIVAAAAAJhEoAIAAAAAkwhUPubAgQO65557FBUVpU6dOunll1+Ww+HwdlkoJ5YvX64HH3xQXbp0UVRUlG699VYtWrRIf/64uYULF6pXr16KjIxU//79tWbNGi9VjPLm9OnT6tKli8LDw/XTTz+5tTGuUFJLlizRgAEDFBkZqejoaN177706c+aMq3316tXq37+/IiMj1atXL3388cderBblwapVqzR48GC1adNGnTt31mOPPabU1NRC/fh5hdJEoPIh6enpuvvuu5Wbm6vp06dr3Lhx+uijjzRlyhRvl4Zy4p133pHNZlN8fLxmz56tLl26aOLEiZo5c6arz7JlyzRx4kTFxcUpISFBUVFRGjNmjH788UfvFY5yY9asWcrPzy+0nXGFkpo9e7YmTZqkPn366K233tLzzz+vhg0busbX1q1bNWbMGEVFRSkhIUFxcXF6+umntWLFCi9XDl+1adMmjRkzRs2aNdPMmTP11FNPac+ePRo5cqRbUOfnFUqdAZ/xxhtvGFFRUcapU6dc2z788EOjRYsWxtGjR71XGMqNEydOFNr2j3/8w2jbtq2Rn59vGIZh3Hzzzcbf/vY3tz533HGHce+9916WGlF+7d+/34iKijI++OADw263Gzt27HC1Ma5QEgcOHDAiIiKMr7/++oJ9Ro4cadxxxx1u2/72t78ZcXFxZV0eyqmJEyca3bt3NwoKClzbNm7caNjtdmPLli2ubfy8QmnjCpUP+eabb9SxY0cFBwe7tsXFxamgoEDr16/3XmEoN2rVqlVoW4sWLZSVlaXs7GylpqYqJSVFcXFxbn369OmjjRs3Mr0UFzV58mQNGTJEoaGhbtsZVyipxYsXq2HDhuratWuR7Q6HQ5s2bVLv3r3dtvfp00cHDhzQ4cOHL0eZKGfy8vJUtWpVWSwW17bq1atLkmvqOz+vUBYIVD4kOTlZTZs2ddsWFBSkkJAQJScne6kqlHfbtm1T3bp1Va1aNdc4+vMb4rCwMOXm5hY5zxyQpBUrVmjv3r16+OGHC7UxrlBS27dvl91u16xZs9SxY0e1atVKQ4YM0fbt2yVJhw4dUm5ubqHfiWFhYZLE70QU6bbbbtOBAwe0YMECZWZmKjU1Va+88ooiIiLUtm1bSfy8QtkgUPmQjIwMBQUFFdpeo0YNpaene6EilHdbt25VYmKiRo4cKUmucfTnceZ8zDhDUXJycjRlyhSNGzdO1apVK9TOuEJJHT9+XN9++60+/fRTPfvss5o5c6YsFotGjhypEydOMKZgSrt27TRjxgxNnTpV7dq1U48ePXTixAklJCTI399fEj+vUDYIVMAV6ujRoxo3bpyio6M1YsQIb5eDcmz27NmqXbu2/vKXv3i7FFwhDMNQdna2Xn/9dfXu3Vtdu3bV7NmzZRiG5s+f7+3yUE59//33+vvf/67bb79d7777rl5//XUVFBTo/vvvd1uUAihtBCofEhQUpMzMzELb09PTVaNGDS9UhPIqIyND9913n4KDgzV9+nT5+Z37p+4cR38eZxkZGW7tgNORI0f09ttv69FHH1VmZqYyMjKUnZ0tScrOztbp06cZVyixoKAgBQcHq3nz5q5twcHBioiI0P79+xlTMGXy5MmKiYlRfHy8YmJi1Lt3b82ZM0e7d+/Wp59+KonfgygbBCof0rRp00LzwjMzM3X8+PFC88iBCzlz5oweeOABZWZm6s0333TdkCvJNY7+PM6Sk5MVEBCgRo0aXdZa4fsOHz6s3Nxc3X///Wrfvr3at2+v0aNHS5JGjBihe+65h3GFEmvWrNkF286ePavGjRsrICCgyDElid+JKNKBAwfcQrok1atXTzVr1tShQ4ck8XsQZYNA5UO6dOmiDRs2uP6XRDp3I7ifn586derkxcpQXuTl5Wns2LFKTk7Wm2++qbp167q1N2rUSE2aNCn0OS6JiYnq2LGjrFbr5SwX5UCLFi00b948tz9PPvmkJOm5557Ts88+y7hCiXXr1k1paWlKSkpybTt16pR27dqlli1bymq1Kjo6Wl988YXbfomJiQoLC1PDhg0vd8koB+rXr6/du3e7bTty5IhOnTqlBg0aSOL3IMpGJW8XgP9vyJAheu+99/Twww/rgQce0LFjx/Tyyy9ryJAhhd4YA0V57rnntGbNGsXHxysrK8vtQwojIiJktVr1yCOPaPz48WrcuLGio6OVmJioHTt2cN8CihQUFKTo6Ogi21q2bKmWLVtKEuMKJdKjRw9FRkbq0Ucf1bhx41S5cmXNmTNHVqtVd911lyTpwQcf1IgRI/TPf/5TcXFx2rRpk5YuXapXX33Vy9XDVw0ZMkT/+te/NHnyZHXv3l1paWmue0DPXyadn1cobRbDuTA/fMKBAwc0adIk/fDDD6patapuvfVWjRs3jv8xQbF0795dR44cKbJt1apVrv/VXbhwoRISEvTrr78qNDRUf/vb39StW7fLWSrKsU2bNmnEiBFatGiRIiMjXdsZVyiJkydP6sUXX9SaNWuUm5urdu3a6cknn3SbDrhq1Sq99tprOnjwoOrXr6/7779fgwYN8mLV8GWGYejDDz/UBx98oNTUVFWtWlVRUVEaN26ca8l9J35eoTQRqAAAAADAJO6hAgAAAACTCFQAAAAAYBKBCgAAAABMIlABAAAAgEkEKgAAAAAwiUAFAAAAACYRqAAAAADAJAIVAFQQx48f14QJE9S1a1e1aNFC4eHhysjIKPXzdO/eXd27dy/145bUpk2bFB4erunTp3u7lDI3fPhwhYeHF9qelZWlyZMnq3v37mrZsqXCw8OVlJR0yTYAQPFV8nYBAHC5ZWdna968efriiy+UkpKi3Nxc1apVSw0bNtT111+vwYMHq3Hjxt4us9TFx8dr/fr16tu3r6655hpZLBZVrly52Ps/+eSTWrx4sYKDg7Vu3TpZrdYyrNZ3xMfHa8mSJVq1apUaNmxY5udx8vf3V9WqVRUSEqIWLVqoZ8+e6t69e4le95dffln//e9/1a1bN/Xv31/+/v6qU6fOJdsAAMVHoAJQoWRlZemuu+7Szz//rGuuuUa33HKLatasqVOnTmnHjh2aM2eOGjdufMUFKofDoQ0bNuiGG27Q1KlTS7x/VlaWVqxYIYvForS0NK1cuVJ9+vQpg0oxaNAg1atXT4ZhKCsrS7/88ovWrFmjpUuXKiwsTK+88oqaN2/uts9LL72knJycQsf6+uuv1aRJE73xxhslagMAFB+BCkCF8u677+rnn3/W4MGDNWnSJFksFrf21NRUORwOL1VXdv744w8VFBToqquuMrX/8uXLlZ2drXvuuUfvvvuuFi1aRKAqI4MHD1ZUVJTbtqysLE2fPl3vvPOORo0apcWLF6tu3bqu9vr16xd5rN9//13t27cvcRsAoPi4hwpAhfLjjz9KkoYOHVooTElSo0aNFBYW5rYtPDxcw4cPL/J4Rd0vFB8fr/DwcKWmpuqtt95Sr1691Lp1a/Xp00fLli2TdO6K0auvvqru3bsrMjJSt9xyi9auXVui55Kdna1p06apd+/eioyMVIcOHXT//fdr27Ztbv2GDx+ubt26SZKWLFmi8PBwhYeHKz4+vtjnWrRokSpVqqR7771X0dHR2rhxo44cOXLRfTIyMvTMM8+oU6dOioyM1IABA7R06dJC/c6ePau3335b/fv31/XXX6+oqCh1795djz32mPbs2ePWNy8vT3PnzlX//v3VunVrXX/99Ro+fLhWr15d7OdSku9n9+7dXdPwYmNjXa/dn/dPTU3V008/rZtuukmtWrVS586dFR8ff8nXqLiqVaumJ598Urfddpv++OMPzZ492639z/dQOcegYRjavHmzW90XazvfypUrdffdd6t9+/aKjIxUv3799NZbbyk/P9+t3+LFixUeHq7Fixdr9erVGjJkiNq0aeP2OjocDs2dO1cDBw5UVFSU2rRpo7vuukurVq0q9FzP//czb9489e7dW61atVK3bt00Y8YMFRQUFPkarVy5UiNHjlR0dLQiIyPVvXt3PfHEE9q7d69bv5LUkpmZqddff119+vRRmzZt1LZtW/Xs2VMTJkwote8tgPKPK1QAKpTg4GBJ0sGDB9WiRYsyPdeLL76oHTt2qFu3bvLz81NiYqIef/xxBQUFaf78+dq/f7+6du2qs2fPaunSpXr44YeVmJhYrOmGZ8+e1d13360dO3aoZcuWuvvuu3XixAklJibq22+/1dSpUxUXFydJGjhwoJo3b6558+apefPm6tGjhyQV+/nv379fP/74o7p27ao6depowIAB2rhxoxYvXqxHHnmkyH0cDof++te/Kjs7W/3791dOTo6WL1+uxx9/XKdOnXJ78z5hwgQtX75c4eHhuu2222S1WnX06FFt2rRJP/30k2t6m2EYevTRR7Vq1So1adJEQ4cOVXZ2tpYvX64HH3xQTz75pP76178W6zkV14gRI7RkyRLt2bNHI0aMUFBQkCSpQYMGrj7bt2/XqFGjlJOTo5tuuknXXHONjhw5os8//1zffPON/vvf/6pRo0alUs9DDz2kxYsXa/ny5Xr22WeL/E8BSerRo4caNGigGTNmqEGDBho4cKCr7qCgoAu2OU2dOlVz5sxR3bp11bNnT1WvXl1bt27Vyy+/rO3bt2vatGmFzrlixQqtX79eN910k+666y5lZWVJOjcWRo0apc2bN6tFixYaNGiQcnNztXbtWj300EOaOHGihg0bVuh4//73v7V582Z169ZNnTt31qpVqzR9+nTl5uZq3Lhxbn2nTJmiuXPnKjg4WLGxsapdu7Z+++03bdy4US1btpTdbi9xLYZhaNSoUdq+fbvatm2rG2+8UX5+fjpy5IhWr16tW2+91e01A1CBGQBQgaxcudKw2+1GmzZtjClTphjr1q0zTp48edF97Ha7MWzYsCLbunXrZnTr1s1t24QJEwy73W7cfPPNxokTJ1zbt2/fbtjtdqNdu3bGnXfeaZw+fdrVtmzZMsNutxuTJk0q1vOYPn26Ybfbjccff9woKChwbd+1a5fRsmVLo127dkZmZqZre2pqqmG3240JEyYU6/jne/HFFw273W4sXbrUMAzDyMrKMqKiooybbrrJyM/PL9S/W7duht1uN4YOHWqcPXvWtf23334zoqOjjVatWhlHjx41DMMwMjIyjPDwcGPgwIFGXl6e23Hy8vKM9PR01+MlS5a4vhfnH/fIkSNGdHS0ERERYRw6dMi1/bvvvjPsdrsxbdo0t+Oa/X6mpqYW6u9wOIxu3boZbdq0MXbt2uXWtmXLFqNFixbGAw88UOS5/sx5nh9++OGi/bp27WrY7Xa35zps2DDDbrcX6nux53qhtm+//daw2+3GyJEj3cZoQUGB8cwzzxh2u91YsWKFa/vHH39s2O12o3nz5sb69esLHe+VV14x7Ha78dprr7mN1czMTOO2224zWrZs6RoP578O3bt3N44dO+bafuLECaNdu3ZGmzZt3L7/q1evNux2u9GvX79C/5Zzc3ON48ePm6plz549ht1uNx566KFCz+ns2bNGVlZWoe0AKiam/AGoUGJjYxUfHy/DMPT2229r1KhRiomJUc+ePfX8888rJSWl1M714IMPqlatWq7HrVu3VqNGjZSRkaFx48YpMDDQ1darVy8FBAQUmuJ2IZ988okCAgI0fvx4t6sUERERGjhwoDIyMrRy5UqPn0Nubq4+/fRTVatWzXVlq2rVqurRo4d+/fVXbdiw4YL7jhs3zm1Funr16mnEiBFyOByuqY8Wi0WGYahy5cry83P/leTv7++6IiTJNfXuiSeecDtu/fr19de//lV5eXn67LPPPH7OJfH111/ryJEjGjVqlCIiItza2rVrp9jYWK1du9Z1taY0OO+DO3XqVKkd83zz58+XJE2aNMltjFosFtd4c37/zhcbG6sbbrjBbVtBQYE++OADNW7cWI8++qjbWK1WrZoefvhh5ebm6quvvip0vIceesjtnr9atWopNjZWp0+f1sGDB13b33//fUnS008/rZo1a7odo1KlSq6VC83WUqVKlUK1Wa1WVa1atdB2ABUTU/4AVDj33HOPBg8erHXr1umHH37Qzp07tWPHDi1YsECLFi3Sq6++qtjYWI/P8+eV2CQpJCREqamphabb+fv7q1atWvr9998vedysrCylpqYqLCxM9erVK9QeHR2tjz76qNjh7GJWrVqlkydPatCgQW5LrA8YMECfffaZFi1apM6dOxfar1KlSmrTpk2h7e3atZMk7d69W9K5N7Jdu3bV2rVrNXDgQPXu3VsdOnRQZGSkAgIC3PZNSkqSzWZT69atCx03OjpakkrlOZeE8568gwcPFvl5V8ePH1dBQYEOHjyoyMjIy1qbWdu3b1dgYKA+/vjjIturVKmi5OTkQtuL+r4cPHhQ6enpuuqqqzRjxoxC7SdPnpSkIo/XsmXLQtucC3FkZma6tu3YsUNWq1UdOnS4wDMyV0tYWJjCw8O1dOlSHT16VD169FCHDh3UokWLQuEfQMVGoAJQIVWrVk1xcXGu+4wyMzP1yiuv6P3339fTTz+tG2+80ePPWapWrVqhbZUqVbpoW15e3iWP67zaUbt27SLbQ0JC3Pp5YtGiRZLOBajzdezYUXXr1tWqVauUlpbmujfNqWbNmkW+6XTWfH5tr7/+ut544w0tXbpUr776qqRzr89tt92mv/3tb7LZbK59igqQUuk+55JIT0+XJH3++ecX7VfUkuZmOUP3n6/GlJb09HTl5eUVGTqcsrOzC20rajympaVJkvbt26d9+/Zd8HhFvT4X+/dz/sIYWVlZqlu37iVDTklrqVSpkt59913NmDFDX3zxhaZMmSLp3JWyoUOH6sEHH5S/v/9FzwmgYiBQAYCk6tWr65lnntHatWt15MgR7d27V61atZJ0bqrThYJOZmamqlevfjlLdb3RPHHiRJHtf/zxh1s/s3777TetX79ekopcNMDps88+04gRI9y2nTp1SgUFBYXe5DprPr82m82mcePGady4cUpNTdWmTZv04Ycfat68eTp79qyef/551z7Oqwh/VpLnXJrfT+f53njjDddKimUpNTVVv/32m+uDqMuC8zlt2rSpRPsVtUCG81i9evUqciGL0lC9enXXlcCLhSoztdSsWVMTJ07UP/7xDyUnJ+u7777Te++9p+nTpysgIEAPPPBAqTwHAOUb16wB4H8sFovrasj5atSooWPHjhXafvjwYWVkZFyO0txUq1ZNjRo10qFDh4qsy/lGuKgphyWxePFiFRQU6Prrr9egQYMK/XGuDue8inW+vLw8/fDDD4W2b926VZIK3W/k1KhRIw0aNEjz589XYGCg23LoLVq0UE5Ojnbs2FFov82bN0sq3nMu6ffT+Sa9qOW6ndPcnFP/ytqsWbMkSX369LngCn+eat26tdLS0krlfsKwsDBVq1ZNO3fuVG5urufFFaF169ZyOByuMVAWtVgsFoWFhWno0KGaO3euJJVoqX4AVzYCFYAK5cMPPyzyDbl07nNsDhw4oKCgINcyy5LUqlUrHTlyxO0Nm8PhcE0B8oYBAwYoNzdXU6dOlWEYru179uzRkiVLVL16ddciEmYYhqHFixfLYrHopZde0gsvvFDoz5QpU9SmTRv9/PPP+umnnwod49VXX3X7kOSjR49q3rx5slqt6tu3r6Rz9638+XOCpHPTznJzc92mXToD3NSpU93eEP/222+aO3euKlWqpP79+1/yuZX0+1mjRg3Xef6sR48eql+/vubOnastW7YUas/NzXWFSE+cPn1aU6ZM0eLFixUSElKmV0acS9o/9dRTRS58cfz4cR04cKBYx6pUqZLuvPNOHTlyRC+99FKRQWbv3r0XvNpaHEOHDpUkvfDCC65pfU55eXmuq5clreXw4cM6fPhwoT7O43k6JRjAlYMpfwAqlG+++UbPPvusrrnmGrVt21ZXXXWVsrOzlZSUpK1bt8rPz0/PPvus25ule+65R+vXr9f999+vvn37ymazaf369QoKCnLdu3O53XfffVq7dq0+/fRTHThwQB07dtSJEye0fPly5efna9KkSR5N+fvuu+90+PBhdejQ4aKfoXTbbbfphx9+0KJFi9wWXQgJCXF9BlW3bt1cn0OVlpamf/zjH67FBY4dO6YBAwaoefPmCg8PV926dZWWlqZVq1YpNzdXo0aNch3z1ltv1ZdffqlVq1apf//+uummm9yOGx8fX6zPeyrp9zMmJkZvv/22nnnmGd18882y2WyqX7++BgwYIKvVqtdff1333Xefhg0bppiYGNntdlksFv3666/aunWrgoODtWLFimK/9gsXLtS6detkGIZOnz6tX375RZs3b9bp06d17bXX6pVXXnFb/a60denSRQ899JBmzZqlm2++WTfeeKPq16+vtLQ0/fLLL9q2bZvGjh1b6AOwL+TRRx/V7t279d5772nt2rVq166dateurWPHjmnv3r3as2eP/vvf/17wnsBL6dq1q0aOHKm3335bvXr1Uo8ePVzH37hxo0aOHOn6fLKS1LJnzx6NGTNGrVu3VlhYmEJCQnTs2DGtXLlSfn5+pf6ZZwDKLwIVgApl/Pjxatu2rTZs2KAtW7bo+PHjks6tHjZw4EANGzbMde+UU+fOnfXaa69p5syZ+vTTTxUcHKzevXtr3LhxuuWWW7zxNFS5cmW9++67SkhIUGJiot555x3ZbDa1b99eDzzwgGs1PbOc0/icV4UupE+fPnrhhRe0bNkyPfnkk64lpq1Wq+bOnaupU6fqs88+U0ZGhpo2baqJEyeqX79+rv0bNGigRx55RN999502bNigtLQ01axZUxERERoxYoS6dOni6muxWDRt2jTNmzdPS5Ys0fz58xUQEKCWLVvqr3/9a7FXZizp97Nr16564okntHDhQs2dO1e5ubnq0KGDa6GO1q1b67PPPtObb76pb775Rt9//72sVqvq1q2rHj16uK7GFZfztff391fVqlV11VVXqXv37urRo4diY2MLrX5YFh577DG1b99e8+bN08aNG5WZmang4GA1bNhQY8aMKdG4t1qtSkhI0KJFi/TJJ5/oyy+/lMPhUJ06dRQWFqYhQ4a4XRE2Y8KECWrTpo3mz5+vL774QmfPnlVISIhiYmLUqVMnU7W0atVK9913nzZv3qy1a9cqIyNDISEhuuGGGzRq1ChFRUV5VDOAK4fFOH+uCAAAAACg2LiHCgAAAABMIlABAAAAgEkEKgAAAAAwiUAFAAAAACYRqAAAAADAJAIVAAAAAJhEoAIAAAAAkwhUAAAAAGASgQoAAAAATCJQAQAAAIBJBCoAAAAAMIlABQAAAAAmEagAAAAAwKT/Bzui8RekVZ/PAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "adv[y_pred==0][0][:200]- mal_x_batch[y_pred==0][0][:200]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c47fcefd-cf57-4539-9f2f-e7ebee542b2c",
        "id": "t02h5OPXsoo4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00, -2.3711e-01,  0.0000e+00,\n",
              "         0.0000e+00, -5.6602e-03,  0.0000e+00,  1.6560e-01,  0.0000e+00,\n",
              "         4.0701e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         3.8184e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  1.6689e-06,  0.0000e+00,  0.0000e+00,\n",
              "         1.2139e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  4.9442e-05,  0.0000e+00,  0.0000e+00,  6.1479e-02,\n",
              "         4.5905e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  6.5267e-05,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  3.0577e-04,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  7.1108e-02,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  1.5214e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  1.1340e-01,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  2.9802e-08,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  9.2387e-07,  0.0000e+00,\n",
              "         4.0913e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  1.2005e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         5.8609e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  1.0163e-05,  0.0000e+00,  7.8633e-02,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  2.0216e-03,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  1.9357e-04,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  6.8039e-05,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "adv[y_pred==0][1][:200] - mal_x_batch[y_pred==0][1][:200]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7bd451b5-7390-40c6-cd46-c8605c4510f9",
        "id": "zY0duRdbsoo4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.0000e+00,  1.0000e+00,  0.0000e+00,  0.0000e+00,  4.7386e-06,\n",
              "         0.0000e+00,  6.6370e-05,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         4.0435e-03,  1.0000e+00,  7.3716e-04,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  9.9986e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  5.3458e-02,  9.9902e-01,  1.0000e+00,  1.0133e-06,\n",
              "         0.0000e+00,  1.5241e-04,  0.0000e+00,  0.0000e+00,  9.9999e-01,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  9.9263e-01, -1.1683e-04,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  2.9802e-08,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  6.6340e-05,\n",
              "         0.0000e+00,  0.0000e+00, -4.6888e-01,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  3.7909e-05,\n",
              "        -9.9988e-01,  3.2783e-07,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,  1.1027e-05,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00, -5.1332e-04,  0.0000e+00,\n",
              "         0.0000e+00, -1.0000e+00,  2.3550e-03, -6.5565e-07, -2.6703e-01,\n",
              "         4.9865e-04,  3.7094e-01,  3.2783e-07,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  2.6107e-05,  5.8413e-06,  9.1636e-01,\n",
              "         0.0000e+00,  1.6502e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  3.2187e-06,  1.0000e+00,\n",
              "         0.0000e+00,  9.9901e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         5.9605e-08,  0.0000e+00,  5.9605e-08,  1.0000e+00,  0.0000e+00,\n",
              "         2.9802e-08,  1.2904e-05,  9.2387e-07,  0.0000e+00,  0.0000e+00,\n",
              "         5.5114e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  2.7418e-06,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  9.9979e-01,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  2.9802e-08,  1.9372e-06,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.1921e-07,\n",
              "         0.0000e+00,  1.6361e-05,  0.0000e+00,  0.0000e+00,  1.0000e+00,\n",
              "         4.5505e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#perturbation^2 / (perturbation^2 + epsilon)"
      ],
      "metadata": {
        "id": "IBZxiBzXwkN9"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dHTtE7LTwkfq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "from torch.optim.lr_scheduler import CyclicLR\n",
        "\n",
        "def compare(output, target, confidence):\n",
        "    if not isinstance(output, (float, int, torch.LongTensor, torch.cuda.LongTensor)):\n",
        "        output = torch.clone(output).detach()\n",
        "        output[target] -= confidence  # Slightly reduce the confidence for targeted attacks\n",
        "        output = torch.argmax(output, dim=1)\n",
        "    return output == target\n",
        "\n",
        "def compare_round(mal_x_batch,newimg, model,removal_array):\n",
        "\n",
        "    adv_rounded = final_rounding(mal_x_batch, newimg, model,removal_array)\n",
        "    #l0dist = torch.sum((mal_x_batch - newimg)!=0, dim=1)\n",
        "    l0dist = torch.sum(torch.abs(adv_rounded - mal_x_batch), dim=1)\n",
        "    logits = model(adv_rounded)\n",
        "    output = torch.argmax(logits, dim=1)\n",
        "\n",
        "    return output == 0, l0dist\n",
        "\n",
        "def sin_func_torch(input_tensor):\n",
        "    \"\"\"\n",
        "    A sine-based transformation applied to the input tensor.\n",
        "\n",
        "    The function applies a sine transformation and rescales the output to [0, 1].\n",
        "    \"\"\"\n",
        "    return (torch.sin((input_tensor - 0.5) * torch.pi) + 1) / 2.0\n",
        "\n",
        "def CW_l1(imgs, y, model, removal_array, max_iterations=1000, max_learning_rate=0.1,\n",
        "       binary_search_steps=3, confidence=0.01, initial_const=0.1,\n",
        "       cons_increase_factor=2, device='cuda'):\n",
        "    \"\"\"\n",
        "    Implements the Carlini & Wagner (CW) L1 attack.\n",
        "\n",
        "    Parameters:\n",
        "    - imgs: The original images to be attacked.\n",
        "    - y: The true labels of the images.\n",
        "    - model: The target model to attack.\n",
        "    - removal_array: A mask array specifying which features are fixed.\n",
        "    - max_iterations: The maximum number of iterations for the attack.\n",
        "    - learning_rate: The learning rate for the optimizer.\n",
        "    - binary_search_steps: The number of steps for the binary search over the constant.\n",
        "    - confidence: Confidence value for the attack.\n",
        "    - initial_const: The initial constant for the attack loss.\n",
        "    - cons_increase_factor: The factor by which the constant is increased during binary search.\n",
        "    - device: The device on which to perform computations.\n",
        "\n",
        "    Returns:\n",
        "    - final_output: The perturbed images after the attack.\n",
        "    \"\"\"\n",
        "\n",
        "    labs = torch.zeros_like(y.squeeze(), dtype=torch.int64)\n",
        "    batch_size = imgs.shape[0]\n",
        "\n",
        "    # Expand removal array to match the batch size\n",
        "    expanded_removal_array = removal_array.expand(imgs.shape[0], -1)\n",
        "    non_fixed_features_mask = torch.bitwise_or(expanded_removal_array, 1 - imgs.to(torch.uint8))\n",
        "\n",
        "    mask_0 = non_fixed_features_mask == 0\n",
        "    mask_1 = non_fixed_features_mask == 1\n",
        "\n",
        "    imgs_tensor = imgs.clone().detach().to(dtype=torch.float32, device=device)\n",
        "    labs_tensor = labs.clone().detach().to(dtype=torch.int64, device=device)\n",
        "\n",
        "    lower_bound = torch.zeros(batch_size, dtype=torch.float32, device=device)\n",
        "    CONST = torch.full((batch_size,), initial_const, dtype=torch.float32, device=device)\n",
        "    upper_bound = torch.full((batch_size,), 1e10, dtype=torch.float32, device=device)\n",
        "\n",
        "    o_bestl0 = torch.full((batch_size,), 1e10, device=device)\n",
        "    o_bestattack = imgs.clone().detach()\n",
        "\n",
        "    o_best_loss1 = torch.full((batch_size,), float('inf'), device=device)\n",
        "    o_best_unsuccessfull_attack = imgs.clone().detach()\n",
        "    o_success_attack = torch.zeros(batch_size, dtype=torch.bool, device=device)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    for outer_step in range(binary_search_steps):\n",
        "        print(CONST)\n",
        "\n",
        "        modifier = torch.zeros_like(imgs_tensor, requires_grad=True).to(device)\n",
        "        optimizer = torch.optim.Adam([modifier], lr=max_learning_rate)\n",
        "\n",
        "        def grad_hook(grad):\n",
        "            return grad * non_fixed_features_mask\n",
        "        modifier.register_hook(grad_hook)\n",
        "\n",
        "        scheduler = CyclicLR(optimizer, base_lr=0.0001, max_lr=max_learning_rate, step_size_up=5,\n",
        "                             step_size_down=4995, mode='triangular2')\n",
        "\n",
        "        best_loss = torch.full((batch_size,), float('inf'), device=device)\n",
        "        no_improvement_count = torch.zeros(batch_size, dtype=torch.int, device=device)\n",
        "        active_mask = torch.ones(batch_size, dtype=torch.bool, device=device)\n",
        "\n",
        "        bestl0 = torch.full((batch_size,), 1e10, device=device)\n",
        "        bestscore = torch.full((batch_size,), -1, device=device)\n",
        "        success_mask = torch.zeros(batch_size, dtype=torch.bool, device=device)\n",
        "\n",
        "        for iteration in range(max_iterations):\n",
        "\n",
        "            active_indices = torch.where(active_mask)[0]\n",
        "            active_modifier = modifier[active_mask]\n",
        "            active_imgs_tensor = imgs_tensor[active_mask]\n",
        "            active_labs_tensor = labs_tensor[active_mask]\n",
        "            active_CONST = CONST[active_mask]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            modified_input = active_imgs_tensor + active_modifier\n",
        "            newimg = sin_func_torch(modified_input)\n",
        "            output = model(newimg)\n",
        "\n",
        "            eps = 0.01\n",
        "            perturbation_2 = (active_imgs_tensor - newimg)**2\n",
        "\n",
        "            #l2dist = torch.sum((1 - (perturbation - 1)**2 + eps)**0.5 - eps**0.5 , dim=1)\n",
        "            #l2dist = torch.sum((newimg - active_imgs_tensor) ** 2, dim=1)\n",
        "            l2dist = torch.sum((perturbation_2 / (perturbation_2 + eps)), dim=1)\n",
        "\n",
        "            real = output[torch.arange(active_indices.size(0)), active_labs_tensor]\n",
        "            other = output[torch.arange(active_indices.size(0)), active_labs_tensor + 1]\n",
        "\n",
        "            loss1 = torch.clamp((other - real + confidence), min=0.0)\n",
        "            #loss2 = l2dist\n",
        "\n",
        "            if outer_step != (binary_search_steps - 1):\n",
        "                loss = (active_CONST * loss1) + l2dist\n",
        "            else:\n",
        "                loss = loss1\n",
        "\n",
        "            total_loss = torch.sum(loss)\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            if (iteration + 1) % 10 == 0:\n",
        "                with torch.no_grad():\n",
        "                    success_mask_inloop, l0dist = compare_round(active_imgs_tensor,newimg, model,removal_array)\n",
        "                    success_mask  = torch.logical_or(success_mask_inloop, success_mask)\n",
        "                    current_lr = scheduler.get_last_lr()[0]\n",
        "\n",
        "                    improved_mask2 = (loss1 < o_best_loss1[active_mask])\n",
        "                    o_best_loss1[active_mask] = torch.where(improved_mask2, loss1, o_best_loss1[active_mask])\n",
        "                    o_best_unsuccessfull_attack[active_mask] = torch.where(improved_mask2.unsqueeze(-1), newimg, o_best_unsuccessfull_attack[active_mask])\n",
        "\n",
        "                    update_mask = (l0dist < bestl0[active_mask]) & success_mask_inloop\n",
        "                    bestl0[active_mask] = torch.where(update_mask, l0dist, bestl0[active_mask])\n",
        "                    bestscore[active_mask] = torch.where(update_mask, torch.argmax(output, dim=1), bestscore[active_mask])\n",
        "\n",
        "                    update_mask2 = (l0dist < o_bestl0[active_mask]) & success_mask_inloop\n",
        "                    o_bestl0[active_mask] = torch.where(update_mask2, l0dist, o_bestl0[active_mask])\n",
        "                    o_bestattack[active_mask] = torch.where(update_mask2.unsqueeze(-1), newimg, o_bestattack[active_mask])\n",
        "\n",
        "                    # Skip the rest of the loop for samples that haven't improved in 50 iterations\n",
        "                    #active_mask[active_mask.clone()] = no_improvement_count[active_mask.clone()] < 50\n",
        "\n",
        "                    if (iteration + 1) % 100 == 0:\n",
        "                        print(f\"Iteration {iteration + 1}: Current success : {success_mask_inloop.sum()} \\t All success : {success_mask.sum()} \\t mean(l0) : {o_bestl0[success_mask].mean():.6f} \\t Current Learning Rate: {current_lr:.3f}\")\n",
        "\n",
        "        if outer_step != (binary_search_steps - 1):\n",
        "            #o_success_attack  = torch.logical_or(o_success_attack, success_mask)\n",
        "            o_success_attack, o_l0dist = compare_round(imgs,o_bestattack, model,removal_array)\n",
        "            print(f\"outer_step {outer_step + 1}: all success : {o_success_attack.sum()} \\t mean(l0)(success) : {o_l0dist[o_success_attack].mean():.4f}\")\n",
        "            #print('o_success_attack ', o_success_attack.sum())\n",
        "            print('-------------------------------------------------------------')\n",
        "            failure_mask = ~success_mask\n",
        "            upper_bound[success_mask] = torch.minimum(upper_bound[success_mask], CONST[success_mask])\n",
        "            lower_bound[failure_mask] = torch.maximum(lower_bound[failure_mask], CONST[failure_mask])\n",
        "\n",
        "            upper_limit_mask = upper_bound < 1e10\n",
        "            CONST[upper_limit_mask] = (lower_bound[upper_limit_mask] + upper_bound[upper_limit_mask]) / 2\n",
        "            CONST[failure_mask & ~upper_limit_mask] *= cons_increase_factor\n",
        "\n",
        "    success_attack_mask = (o_bestl0 != 1e10)\n",
        "    final_output = torch.where(success_attack_mask.unsqueeze(-1), o_bestattack, o_best_unsuccessfull_attack)\n",
        "\n",
        "    return final_output\n"
      ],
      "metadata": {
        "id": "AcIw5kEfwkuB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attack_params =  {'device':device, 'confidence':0.5, 'removal_array':removal_array,'max_iterations': 20000, 'max_learning_rate': 0.1, 'initial_const': 1 , 'binary_search_steps' : 7, 'cons_increase_factor' : 10}\n",
        "adv = adv_predict(test_loader, combined_model, CW_l1, **attack_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a110f35-261b-4f0d-8481-c65a0fd25f45",
        "id": "Wl-FI3cgwkuC"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0')\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-15-86a2713630d1>:43: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at ../aten/src/ATen/native/TensorCompare.cpp:530.)\n",
            "  final[active_indices, indices] = torch.where(non_fixed_features_mask[active_indices, indices],\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 100: Current success : 29 \t All success : 53 \t mean(l0) : 4.132075 \t Current Learning Rate: 0.098\n",
            "Iteration 200: Current success : 34 \t All success : 56 \t mean(l0) : 4.125000 \t Current Learning Rate: 0.096\n",
            "Iteration 300: Current success : 37 \t All success : 57 \t mean(l0) : 4.245614 \t Current Learning Rate: 0.094\n",
            "Iteration 400: Current success : 40 \t All success : 58 \t mean(l0) : 4.258621 \t Current Learning Rate: 0.092\n",
            "Iteration 500: Current success : 37 \t All success : 58 \t mean(l0) : 4.258621 \t Current Learning Rate: 0.090\n",
            "Iteration 600: Current success : 38 \t All success : 58 \t mean(l0) : 4.258621 \t Current Learning Rate: 0.088\n",
            "Iteration 700: Current success : 42 \t All success : 65 \t mean(l0) : 4.246154 \t Current Learning Rate: 0.086\n",
            "Iteration 800: Current success : 49 \t All success : 68 \t mean(l0) : 4.500000 \t Current Learning Rate: 0.084\n",
            "Iteration 900: Current success : 47 \t All success : 68 \t mean(l0) : 4.367647 \t Current Learning Rate: 0.082\n",
            "Iteration 1000: Current success : 58 \t All success : 71 \t mean(l0) : 4.690141 \t Current Learning Rate: 0.080\n",
            "Iteration 1100: Current success : 55 \t All success : 71 \t mean(l0) : 4.661972 \t Current Learning Rate: 0.078\n",
            "Iteration 1200: Current success : 55 \t All success : 71 \t mean(l0) : 4.661972 \t Current Learning Rate: 0.076\n",
            "Iteration 1300: Current success : 53 \t All success : 72 \t mean(l0) : 4.736111 \t Current Learning Rate: 0.074\n",
            "Iteration 1400: Current success : 52 \t All success : 72 \t mean(l0) : 4.736111 \t Current Learning Rate: 0.072\n",
            "Iteration 1500: Current success : 58 \t All success : 72 \t mean(l0) : 4.722222 \t Current Learning Rate: 0.070\n",
            "Iteration 1600: Current success : 57 \t All success : 72 \t mean(l0) : 4.708333 \t Current Learning Rate: 0.068\n",
            "Iteration 1700: Current success : 55 \t All success : 72 \t mean(l0) : 4.708333 \t Current Learning Rate: 0.066\n",
            "Iteration 1800: Current success : 57 \t All success : 72 \t mean(l0) : 4.694445 \t Current Learning Rate: 0.064\n",
            "Iteration 1900: Current success : 59 \t All success : 73 \t mean(l0) : 4.712329 \t Current Learning Rate: 0.062\n",
            "Iteration 2000: Current success : 61 \t All success : 73 \t mean(l0) : 4.698630 \t Current Learning Rate: 0.060\n",
            "Iteration 2100: Current success : 64 \t All success : 73 \t mean(l0) : 4.698630 \t Current Learning Rate: 0.058\n",
            "Iteration 2200: Current success : 66 \t All success : 73 \t mean(l0) : 4.671233 \t Current Learning Rate: 0.056\n",
            "Iteration 2300: Current success : 66 \t All success : 73 \t mean(l0) : 4.671233 \t Current Learning Rate: 0.054\n",
            "Iteration 2400: Current success : 66 \t All success : 73 \t mean(l0) : 4.671233 \t Current Learning Rate: 0.052\n",
            "Iteration 2500: Current success : 67 \t All success : 73 \t mean(l0) : 4.671233 \t Current Learning Rate: 0.050\n",
            "Iteration 2600: Current success : 66 \t All success : 73 \t mean(l0) : 4.671233 \t Current Learning Rate: 0.048\n",
            "Iteration 2700: Current success : 66 \t All success : 73 \t mean(l0) : 4.671233 \t Current Learning Rate: 0.046\n",
            "Iteration 2800: Current success : 67 \t All success : 73 \t mean(l0) : 4.671233 \t Current Learning Rate: 0.044\n",
            "Iteration 2900: Current success : 66 \t All success : 73 \t mean(l0) : 4.671233 \t Current Learning Rate: 0.042\n",
            "Iteration 3000: Current success : 67 \t All success : 73 \t mean(l0) : 4.671233 \t Current Learning Rate: 0.040\n",
            "Iteration 3100: Current success : 66 \t All success : 73 \t mean(l0) : 4.671233 \t Current Learning Rate: 0.038\n",
            "Iteration 3200: Current success : 67 \t All success : 73 \t mean(l0) : 4.671233 \t Current Learning Rate: 0.036\n",
            "Iteration 3300: Current success : 67 \t All success : 73 \t mean(l0) : 4.671233 \t Current Learning Rate: 0.034\n",
            "Iteration 3400: Current success : 66 \t All success : 73 \t mean(l0) : 4.671233 \t Current Learning Rate: 0.032\n",
            "Iteration 3500: Current success : 66 \t All success : 73 \t mean(l0) : 4.671233 \t Current Learning Rate: 0.030\n",
            "Iteration 3600: Current success : 66 \t All success : 73 \t mean(l0) : 4.671233 \t Current Learning Rate: 0.028\n",
            "Iteration 3700: Current success : 66 \t All success : 73 \t mean(l0) : 4.671233 \t Current Learning Rate: 0.026\n",
            "Iteration 3800: Current success : 66 \t All success : 73 \t mean(l0) : 4.671233 \t Current Learning Rate: 0.024\n",
            "Iteration 3900: Current success : 66 \t All success : 73 \t mean(l0) : 4.671233 \t Current Learning Rate: 0.022\n",
            "Iteration 4000: Current success : 67 \t All success : 73 \t mean(l0) : 4.671233 \t Current Learning Rate: 0.020\n",
            "Iteration 4100: Current success : 66 \t All success : 73 \t mean(l0) : 4.671233 \t Current Learning Rate: 0.018\n",
            "Iteration 4200: Current success : 66 \t All success : 73 \t mean(l0) : 4.671233 \t Current Learning Rate: 0.016\n",
            "Iteration 4300: Current success : 67 \t All success : 73 \t mean(l0) : 4.671233 \t Current Learning Rate: 0.014\n",
            "Iteration 4400: Current success : 67 \t All success : 73 \t mean(l0) : 4.671233 \t Current Learning Rate: 0.012\n",
            "Iteration 4500: Current success : 67 \t All success : 73 \t mean(l0) : 4.671233 \t Current Learning Rate: 0.010\n",
            "Iteration 4600: Current success : 67 \t All success : 73 \t mean(l0) : 4.671233 \t Current Learning Rate: 0.008\n",
            "Iteration 4700: Current success : 66 \t All success : 73 \t mean(l0) : 4.671233 \t Current Learning Rate: 0.006\n",
            "Iteration 4800: Current success : 66 \t All success : 73 \t mean(l0) : 4.671233 \t Current Learning Rate: 0.004\n",
            "Iteration 4900: Current success : 66 \t All success : 73 \t mean(l0) : 4.671233 \t Current Learning Rate: 0.002\n",
            "Iteration 5000: Current success : 66 \t All success : 73 \t mean(l0) : 4.671233 \t Current Learning Rate: 0.000\n",
            "Iteration 5100: Current success : 49 \t All success : 73 \t mean(l0) : 4.506849 \t Current Learning Rate: 0.049\n",
            "Iteration 5200: Current success : 69 \t All success : 85 \t mean(l0) : 5.423530 \t Current Learning Rate: 0.048\n",
            "Iteration 5300: Current success : 77 \t All success : 94 \t mean(l0) : 6.276596 \t Current Learning Rate: 0.047\n",
            "Iteration 5400: Current success : 83 \t All success : 95 \t mean(l0) : 6.126316 \t Current Learning Rate: 0.046\n",
            "Iteration 5500: Current success : 84 \t All success : 95 \t mean(l0) : 6.094737 \t Current Learning Rate: 0.045\n",
            "Iteration 5600: Current success : 85 \t All success : 95 \t mean(l0) : 6.094737 \t Current Learning Rate: 0.044\n",
            "Iteration 5700: Current success : 85 \t All success : 95 \t mean(l0) : 6.000000 \t Current Learning Rate: 0.043\n",
            "Iteration 5800: Current success : 85 \t All success : 95 \t mean(l0) : 5.968421 \t Current Learning Rate: 0.042\n",
            "Iteration 5900: Current success : 85 \t All success : 95 \t mean(l0) : 5.968421 \t Current Learning Rate: 0.041\n",
            "Iteration 6000: Current success : 84 \t All success : 95 \t mean(l0) : 5.968421 \t Current Learning Rate: 0.040\n",
            "Iteration 6100: Current success : 83 \t All success : 95 \t mean(l0) : 5.968421 \t Current Learning Rate: 0.039\n",
            "Iteration 6200: Current success : 84 \t All success : 95 \t mean(l0) : 5.968421 \t Current Learning Rate: 0.038\n",
            "Iteration 6300: Current success : 84 \t All success : 95 \t mean(l0) : 5.968421 \t Current Learning Rate: 0.037\n",
            "Iteration 6400: Current success : 83 \t All success : 95 \t mean(l0) : 5.926316 \t Current Learning Rate: 0.036\n",
            "Iteration 6500: Current success : 84 \t All success : 95 \t mean(l0) : 5.926316 \t Current Learning Rate: 0.035\n",
            "Iteration 6600: Current success : 84 \t All success : 95 \t mean(l0) : 5.926316 \t Current Learning Rate: 0.034\n",
            "Iteration 6700: Current success : 83 \t All success : 95 \t mean(l0) : 5.926316 \t Current Learning Rate: 0.033\n",
            "Iteration 6800: Current success : 84 \t All success : 95 \t mean(l0) : 5.926316 \t Current Learning Rate: 0.032\n",
            "Iteration 6900: Current success : 84 \t All success : 95 \t mean(l0) : 5.926316 \t Current Learning Rate: 0.031\n",
            "Iteration 7000: Current success : 84 \t All success : 95 \t mean(l0) : 5.926316 \t Current Learning Rate: 0.030\n",
            "Iteration 7100: Current success : 84 \t All success : 95 \t mean(l0) : 5.926316 \t Current Learning Rate: 0.029\n",
            "Iteration 7200: Current success : 84 \t All success : 95 \t mean(l0) : 5.926316 \t Current Learning Rate: 0.028\n",
            "Iteration 7300: Current success : 84 \t All success : 95 \t mean(l0) : 5.926316 \t Current Learning Rate: 0.027\n",
            "Iteration 7400: Current success : 84 \t All success : 95 \t mean(l0) : 5.926316 \t Current Learning Rate: 0.026\n",
            "Iteration 7500: Current success : 84 \t All success : 95 \t mean(l0) : 5.926316 \t Current Learning Rate: 0.025\n",
            "Iteration 7600: Current success : 84 \t All success : 95 \t mean(l0) : 5.926316 \t Current Learning Rate: 0.024\n",
            "Iteration 7700: Current success : 84 \t All success : 95 \t mean(l0) : 5.926316 \t Current Learning Rate: 0.023\n",
            "Iteration 7800: Current success : 84 \t All success : 95 \t mean(l0) : 5.926316 \t Current Learning Rate: 0.022\n",
            "Iteration 7900: Current success : 84 \t All success : 95 \t mean(l0) : 5.926316 \t Current Learning Rate: 0.021\n",
            "Iteration 8000: Current success : 84 \t All success : 95 \t mean(l0) : 5.926316 \t Current Learning Rate: 0.020\n",
            "Iteration 8100: Current success : 84 \t All success : 95 \t mean(l0) : 5.926316 \t Current Learning Rate: 0.019\n",
            "Iteration 8200: Current success : 84 \t All success : 95 \t mean(l0) : 5.926316 \t Current Learning Rate: 0.018\n",
            "Iteration 8300: Current success : 84 \t All success : 95 \t mean(l0) : 5.926316 \t Current Learning Rate: 0.017\n",
            "Iteration 8400: Current success : 84 \t All success : 95 \t mean(l0) : 5.926316 \t Current Learning Rate: 0.016\n",
            "Iteration 8500: Current success : 84 \t All success : 95 \t mean(l0) : 5.926316 \t Current Learning Rate: 0.015\n",
            "Iteration 8600: Current success : 84 \t All success : 95 \t mean(l0) : 5.926316 \t Current Learning Rate: 0.014\n",
            "Iteration 8700: Current success : 84 \t All success : 95 \t mean(l0) : 5.926316 \t Current Learning Rate: 0.013\n",
            "Iteration 8800: Current success : 84 \t All success : 95 \t mean(l0) : 5.926316 \t Current Learning Rate: 0.012\n",
            "Iteration 8900: Current success : 84 \t All success : 95 \t mean(l0) : 5.926316 \t Current Learning Rate: 0.011\n",
            "Iteration 9000: Current success : 84 \t All success : 95 \t mean(l0) : 5.926316 \t Current Learning Rate: 0.010\n",
            "Iteration 9100: Current success : 84 \t All success : 95 \t mean(l0) : 5.926316 \t Current Learning Rate: 0.009\n",
            "Iteration 9200: Current success : 84 \t All success : 95 \t mean(l0) : 5.926316 \t Current Learning Rate: 0.008\n",
            "Iteration 9300: Current success : 84 \t All success : 95 \t mean(l0) : 5.926316 \t Current Learning Rate: 0.007\n",
            "Iteration 9400: Current success : 84 \t All success : 95 \t mean(l0) : 5.926316 \t Current Learning Rate: 0.006\n",
            "Iteration 9500: Current success : 84 \t All success : 95 \t mean(l0) : 5.926316 \t Current Learning Rate: 0.005\n",
            "Iteration 9600: Current success : 84 \t All success : 95 \t mean(l0) : 5.926316 \t Current Learning Rate: 0.004\n",
            "Iteration 9700: Current success : 84 \t All success : 95 \t mean(l0) : 5.926316 \t Current Learning Rate: 0.003\n",
            "Iteration 9800: Current success : 84 \t All success : 95 \t mean(l0) : 5.926316 \t Current Learning Rate: 0.002\n",
            "Iteration 9900: Current success : 84 \t All success : 95 \t mean(l0) : 5.926316 \t Current Learning Rate: 0.001\n",
            "Iteration 10000: Current success : 84 \t All success : 95 \t mean(l0) : 5.926316 \t Current Learning Rate: 0.000\n",
            "Iteration 10100: Current success : 85 \t All success : 96 \t mean(l0) : 5.927083 \t Current Learning Rate: 0.025\n",
            "Iteration 10200: Current success : 87 \t All success : 101 \t mean(l0) : 6.207921 \t Current Learning Rate: 0.024\n",
            "Iteration 10300: Current success : 87 \t All success : 101 \t mean(l0) : 6.000000 \t Current Learning Rate: 0.024\n",
            "Iteration 10400: Current success : 87 \t All success : 101 \t mean(l0) : 6.000000 \t Current Learning Rate: 0.023\n",
            "Iteration 10500: Current success : 87 \t All success : 101 \t mean(l0) : 6.000000 \t Current Learning Rate: 0.023\n",
            "Iteration 10600: Current success : 87 \t All success : 101 \t mean(l0) : 6.000000 \t Current Learning Rate: 0.022\n",
            "Iteration 10700: Current success : 87 \t All success : 101 \t mean(l0) : 6.000000 \t Current Learning Rate: 0.022\n",
            "Iteration 10800: Current success : 87 \t All success : 101 \t mean(l0) : 6.000000 \t Current Learning Rate: 0.021\n",
            "Iteration 10900: Current success : 88 \t All success : 101 \t mean(l0) : 6.000000 \t Current Learning Rate: 0.021\n",
            "Iteration 11000: Current success : 88 \t All success : 101 \t mean(l0) : 6.000000 \t Current Learning Rate: 0.020\n",
            "Iteration 11100: Current success : 88 \t All success : 101 \t mean(l0) : 6.000000 \t Current Learning Rate: 0.020\n",
            "Iteration 11200: Current success : 88 \t All success : 101 \t mean(l0) : 5.990099 \t Current Learning Rate: 0.019\n",
            "Iteration 11300: Current success : 88 \t All success : 101 \t mean(l0) : 5.990099 \t Current Learning Rate: 0.019\n",
            "Iteration 11400: Current success : 88 \t All success : 101 \t mean(l0) : 5.990099 \t Current Learning Rate: 0.018\n",
            "Iteration 11500: Current success : 88 \t All success : 101 \t mean(l0) : 5.990099 \t Current Learning Rate: 0.018\n",
            "Iteration 11600: Current success : 88 \t All success : 101 \t mean(l0) : 5.990099 \t Current Learning Rate: 0.017\n",
            "Iteration 11700: Current success : 88 \t All success : 101 \t mean(l0) : 5.990099 \t Current Learning Rate: 0.017\n",
            "Iteration 11800: Current success : 88 \t All success : 101 \t mean(l0) : 5.990099 \t Current Learning Rate: 0.016\n",
            "Iteration 11900: Current success : 88 \t All success : 101 \t mean(l0) : 5.990099 \t Current Learning Rate: 0.016\n",
            "Iteration 12000: Current success : 88 \t All success : 101 \t mean(l0) : 5.990099 \t Current Learning Rate: 0.015\n",
            "Iteration 12100: Current success : 88 \t All success : 101 \t mean(l0) : 5.990099 \t Current Learning Rate: 0.015\n",
            "Iteration 12200: Current success : 88 \t All success : 101 \t mean(l0) : 5.990099 \t Current Learning Rate: 0.014\n",
            "Iteration 12300: Current success : 88 \t All success : 101 \t mean(l0) : 5.990099 \t Current Learning Rate: 0.014\n",
            "Iteration 12400: Current success : 88 \t All success : 101 \t mean(l0) : 5.990099 \t Current Learning Rate: 0.013\n",
            "Iteration 12500: Current success : 88 \t All success : 101 \t mean(l0) : 5.990099 \t Current Learning Rate: 0.013\n",
            "Iteration 12600: Current success : 88 \t All success : 101 \t mean(l0) : 5.990099 \t Current Learning Rate: 0.012\n",
            "Iteration 12700: Current success : 88 \t All success : 101 \t mean(l0) : 5.990099 \t Current Learning Rate: 0.012\n",
            "Iteration 12800: Current success : 88 \t All success : 101 \t mean(l0) : 5.990099 \t Current Learning Rate: 0.011\n",
            "Iteration 12900: Current success : 88 \t All success : 101 \t mean(l0) : 5.990099 \t Current Learning Rate: 0.011\n",
            "Iteration 13000: Current success : 88 \t All success : 101 \t mean(l0) : 5.990099 \t Current Learning Rate: 0.010\n",
            "Iteration 13100: Current success : 88 \t All success : 101 \t mean(l0) : 5.990099 \t Current Learning Rate: 0.010\n",
            "Iteration 13200: Current success : 88 \t All success : 101 \t mean(l0) : 5.990099 \t Current Learning Rate: 0.009\n",
            "Iteration 13300: Current success : 88 \t All success : 101 \t mean(l0) : 5.990099 \t Current Learning Rate: 0.009\n",
            "Iteration 13400: Current success : 88 \t All success : 101 \t mean(l0) : 5.990099 \t Current Learning Rate: 0.008\n",
            "Iteration 13500: Current success : 88 \t All success : 101 \t mean(l0) : 5.990099 \t Current Learning Rate: 0.008\n",
            "Iteration 13600: Current success : 88 \t All success : 101 \t mean(l0) : 5.990099 \t Current Learning Rate: 0.007\n",
            "Iteration 13700: Current success : 88 \t All success : 101 \t mean(l0) : 5.990099 \t Current Learning Rate: 0.007\n",
            "Iteration 13800: Current success : 88 \t All success : 101 \t mean(l0) : 5.990099 \t Current Learning Rate: 0.006\n",
            "Iteration 13900: Current success : 88 \t All success : 101 \t mean(l0) : 5.990099 \t Current Learning Rate: 0.006\n",
            "Iteration 14000: Current success : 88 \t All success : 101 \t mean(l0) : 5.990099 \t Current Learning Rate: 0.005\n",
            "Iteration 14100: Current success : 88 \t All success : 101 \t mean(l0) : 5.990099 \t Current Learning Rate: 0.005\n",
            "Iteration 14200: Current success : 88 \t All success : 101 \t mean(l0) : 5.990099 \t Current Learning Rate: 0.004\n",
            "Iteration 14300: Current success : 88 \t All success : 101 \t mean(l0) : 5.990099 \t Current Learning Rate: 0.004\n",
            "Iteration 14400: Current success : 88 \t All success : 101 \t mean(l0) : 5.990099 \t Current Learning Rate: 0.003\n",
            "Iteration 14500: Current success : 88 \t All success : 101 \t mean(l0) : 5.990099 \t Current Learning Rate: 0.003\n",
            "Iteration 14600: Current success : 88 \t All success : 101 \t mean(l0) : 5.990099 \t Current Learning Rate: 0.002\n",
            "Iteration 14700: Current success : 88 \t All success : 101 \t mean(l0) : 5.990099 \t Current Learning Rate: 0.002\n",
            "Iteration 14800: Current success : 88 \t All success : 101 \t mean(l0) : 5.990099 \t Current Learning Rate: 0.001\n",
            "Iteration 14900: Current success : 88 \t All success : 101 \t mean(l0) : 5.990099 \t Current Learning Rate: 0.001\n",
            "Iteration 15000: Current success : 88 \t All success : 101 \t mean(l0) : 5.990099 \t Current Learning Rate: 0.000\n",
            "Iteration 15100: Current success : 88 \t All success : 101 \t mean(l0) : 5.990099 \t Current Learning Rate: 0.012\n",
            "Iteration 15200: Current success : 88 \t All success : 101 \t mean(l0) : 5.990099 \t Current Learning Rate: 0.012\n",
            "Iteration 15300: Current success : 88 \t All success : 101 \t mean(l0) : 5.980198 \t Current Learning Rate: 0.012\n",
            "Iteration 15400: Current success : 88 \t All success : 101 \t mean(l0) : 5.980198 \t Current Learning Rate: 0.012\n",
            "Iteration 15500: Current success : 88 \t All success : 101 \t mean(l0) : 5.980198 \t Current Learning Rate: 0.011\n",
            "Iteration 15600: Current success : 88 \t All success : 101 \t mean(l0) : 5.970297 \t Current Learning Rate: 0.011\n",
            "Iteration 15700: Current success : 88 \t All success : 101 \t mean(l0) : 5.970297 \t Current Learning Rate: 0.011\n",
            "Iteration 15800: Current success : 88 \t All success : 101 \t mean(l0) : 5.970297 \t Current Learning Rate: 0.011\n",
            "Iteration 15900: Current success : 88 \t All success : 101 \t mean(l0) : 5.970297 \t Current Learning Rate: 0.010\n",
            "Iteration 16000: Current success : 88 \t All success : 101 \t mean(l0) : 5.970297 \t Current Learning Rate: 0.010\n",
            "Iteration 16100: Current success : 88 \t All success : 101 \t mean(l0) : 5.970297 \t Current Learning Rate: 0.010\n",
            "Iteration 16200: Current success : 88 \t All success : 101 \t mean(l0) : 5.970297 \t Current Learning Rate: 0.010\n",
            "Iteration 16300: Current success : 88 \t All success : 101 \t mean(l0) : 5.970297 \t Current Learning Rate: 0.009\n",
            "Iteration 16400: Current success : 88 \t All success : 101 \t mean(l0) : 5.970297 \t Current Learning Rate: 0.009\n",
            "Iteration 16500: Current success : 88 \t All success : 101 \t mean(l0) : 5.970297 \t Current Learning Rate: 0.009\n",
            "Iteration 16600: Current success : 88 \t All success : 101 \t mean(l0) : 5.970297 \t Current Learning Rate: 0.009\n",
            "Iteration 16700: Current success : 88 \t All success : 101 \t mean(l0) : 5.970297 \t Current Learning Rate: 0.008\n",
            "Iteration 16800: Current success : 88 \t All success : 101 \t mean(l0) : 5.970297 \t Current Learning Rate: 0.008\n",
            "Iteration 16900: Current success : 88 \t All success : 101 \t mean(l0) : 5.970297 \t Current Learning Rate: 0.008\n",
            "Iteration 17000: Current success : 88 \t All success : 101 \t mean(l0) : 5.970297 \t Current Learning Rate: 0.008\n",
            "Iteration 17100: Current success : 88 \t All success : 101 \t mean(l0) : 5.970297 \t Current Learning Rate: 0.007\n",
            "Iteration 17200: Current success : 88 \t All success : 101 \t mean(l0) : 5.970297 \t Current Learning Rate: 0.007\n",
            "Iteration 17300: Current success : 88 \t All success : 101 \t mean(l0) : 5.970297 \t Current Learning Rate: 0.007\n",
            "Iteration 17400: Current success : 88 \t All success : 101 \t mean(l0) : 5.970297 \t Current Learning Rate: 0.007\n",
            "Iteration 17500: Current success : 88 \t All success : 101 \t mean(l0) : 5.970297 \t Current Learning Rate: 0.006\n",
            "Iteration 17600: Current success : 88 \t All success : 101 \t mean(l0) : 5.970297 \t Current Learning Rate: 0.006\n",
            "Iteration 17700: Current success : 88 \t All success : 101 \t mean(l0) : 5.970297 \t Current Learning Rate: 0.006\n",
            "Iteration 17800: Current success : 88 \t All success : 101 \t mean(l0) : 5.970297 \t Current Learning Rate: 0.006\n",
            "Iteration 17900: Current success : 88 \t All success : 101 \t mean(l0) : 5.970297 \t Current Learning Rate: 0.005\n",
            "Iteration 18000: Current success : 88 \t All success : 101 \t mean(l0) : 5.970297 \t Current Learning Rate: 0.005\n",
            "Iteration 18100: Current success : 88 \t All success : 101 \t mean(l0) : 5.970297 \t Current Learning Rate: 0.005\n",
            "Iteration 18200: Current success : 88 \t All success : 101 \t mean(l0) : 5.970297 \t Current Learning Rate: 0.005\n",
            "Iteration 18300: Current success : 88 \t All success : 101 \t mean(l0) : 5.970297 \t Current Learning Rate: 0.004\n",
            "Iteration 18400: Current success : 88 \t All success : 101 \t mean(l0) : 5.970297 \t Current Learning Rate: 0.004\n",
            "Iteration 18500: Current success : 88 \t All success : 101 \t mean(l0) : 5.970297 \t Current Learning Rate: 0.004\n",
            "Iteration 18600: Current success : 88 \t All success : 101 \t mean(l0) : 5.970297 \t Current Learning Rate: 0.004\n",
            "Iteration 18700: Current success : 88 \t All success : 101 \t mean(l0) : 5.970297 \t Current Learning Rate: 0.003\n",
            "Iteration 18800: Current success : 88 \t All success : 101 \t mean(l0) : 5.970297 \t Current Learning Rate: 0.003\n",
            "Iteration 18900: Current success : 88 \t All success : 101 \t mean(l0) : 5.970297 \t Current Learning Rate: 0.003\n",
            "Iteration 19000: Current success : 88 \t All success : 101 \t mean(l0) : 5.970297 \t Current Learning Rate: 0.003\n",
            "Iteration 19100: Current success : 88 \t All success : 101 \t mean(l0) : 5.970297 \t Current Learning Rate: 0.002\n",
            "Iteration 19200: Current success : 88 \t All success : 101 \t mean(l0) : 5.970297 \t Current Learning Rate: 0.002\n",
            "Iteration 19300: Current success : 88 \t All success : 101 \t mean(l0) : 5.970297 \t Current Learning Rate: 0.002\n",
            "Iteration 19400: Current success : 88 \t All success : 101 \t mean(l0) : 5.970297 \t Current Learning Rate: 0.002\n",
            "Iteration 19500: Current success : 88 \t All success : 101 \t mean(l0) : 5.970297 \t Current Learning Rate: 0.001\n",
            "Iteration 19600: Current success : 88 \t All success : 101 \t mean(l0) : 5.970297 \t Current Learning Rate: 0.001\n",
            "Iteration 19700: Current success : 88 \t All success : 101 \t mean(l0) : 5.970297 \t Current Learning Rate: 0.001\n",
            "Iteration 19800: Current success : 88 \t All success : 101 \t mean(l0) : 5.970297 \t Current Learning Rate: 0.001\n",
            "Iteration 19900: Current success : 88 \t All success : 101 \t mean(l0) : 5.970297 \t Current Learning Rate: 0.000\n",
            "Iteration 20000: Current success : 88 \t All success : 101 \t mean(l0) : 5.970297 \t Current Learning Rate: 0.000\n",
            "outer_step 1: all success : 101 \t mean(l0)(success) : 5.9703\n",
            "-------------------------------------------------------------\n",
            "tensor([10., 10., 10.,  ..., 10., 10., 10.], device='cuda:0')\n",
            "Iteration 100: Current success : 31 \t All success : 62 \t mean(l0) : 4.870967 \t Current Learning Rate: 0.098\n",
            "Iteration 200: Current success : 31 \t All success : 68 \t mean(l0) : 6.191176 \t Current Learning Rate: 0.096\n",
            "Iteration 300: Current success : 34 \t All success : 71 \t mean(l0) : 6.619718 \t Current Learning Rate: 0.094\n",
            "Iteration 400: Current success : 37 \t All success : 74 \t mean(l0) : 7.297297 \t Current Learning Rate: 0.092\n",
            "Iteration 500: Current success : 40 \t All success : 77 \t mean(l0) : 7.974026 \t Current Learning Rate: 0.090\n",
            "Iteration 600: Current success : 42 \t All success : 77 \t mean(l0) : 7.649351 \t Current Learning Rate: 0.088\n",
            "Iteration 700: Current success : 46 \t All success : 78 \t mean(l0) : 7.730769 \t Current Learning Rate: 0.086\n",
            "Iteration 800: Current success : 47 \t All success : 79 \t mean(l0) : 7.607595 \t Current Learning Rate: 0.084\n",
            "Iteration 900: Current success : 49 \t All success : 79 \t mean(l0) : 7.594937 \t Current Learning Rate: 0.082\n",
            "Iteration 1000: Current success : 47 \t All success : 79 \t mean(l0) : 7.594937 \t Current Learning Rate: 0.080\n",
            "Iteration 1100: Current success : 48 \t All success : 79 \t mean(l0) : 7.594937 \t Current Learning Rate: 0.078\n",
            "Iteration 1200: Current success : 47 \t All success : 79 \t mean(l0) : 7.594937 \t Current Learning Rate: 0.076\n",
            "Iteration 1300: Current success : 48 \t All success : 79 \t mean(l0) : 7.594937 \t Current Learning Rate: 0.074\n",
            "Iteration 1400: Current success : 49 \t All success : 79 \t mean(l0) : 7.594937 \t Current Learning Rate: 0.072\n",
            "Iteration 1500: Current success : 51 \t All success : 79 \t mean(l0) : 7.594937 \t Current Learning Rate: 0.070\n",
            "Iteration 1600: Current success : 52 \t All success : 79 \t mean(l0) : 7.594937 \t Current Learning Rate: 0.068\n",
            "Iteration 1700: Current success : 52 \t All success : 79 \t mean(l0) : 7.594937 \t Current Learning Rate: 0.066\n",
            "Iteration 1800: Current success : 54 \t All success : 80 \t mean(l0) : 7.562500 \t Current Learning Rate: 0.064\n",
            "Iteration 1900: Current success : 55 \t All success : 81 \t mean(l0) : 7.543210 \t Current Learning Rate: 0.062\n",
            "Iteration 2000: Current success : 56 \t All success : 81 \t mean(l0) : 7.530864 \t Current Learning Rate: 0.060\n",
            "Iteration 2100: Current success : 55 \t All success : 81 \t mean(l0) : 7.530864 \t Current Learning Rate: 0.058\n",
            "Iteration 2200: Current success : 58 \t All success : 81 \t mean(l0) : 7.530864 \t Current Learning Rate: 0.056\n",
            "Iteration 2300: Current success : 58 \t All success : 81 \t mean(l0) : 7.530864 \t Current Learning Rate: 0.054\n",
            "Iteration 2400: Current success : 58 \t All success : 81 \t mean(l0) : 7.530864 \t Current Learning Rate: 0.052\n",
            "Iteration 2500: Current success : 57 \t All success : 81 \t mean(l0) : 7.530864 \t Current Learning Rate: 0.050\n",
            "Iteration 2600: Current success : 58 \t All success : 81 \t mean(l0) : 7.530864 \t Current Learning Rate: 0.048\n",
            "Iteration 2700: Current success : 58 \t All success : 81 \t mean(l0) : 7.530864 \t Current Learning Rate: 0.046\n",
            "Iteration 2800: Current success : 57 \t All success : 81 \t mean(l0) : 7.530864 \t Current Learning Rate: 0.044\n",
            "Iteration 2900: Current success : 58 \t All success : 81 \t mean(l0) : 7.530864 \t Current Learning Rate: 0.042\n",
            "Iteration 3000: Current success : 58 \t All success : 81 \t mean(l0) : 7.530864 \t Current Learning Rate: 0.040\n",
            "Iteration 3100: Current success : 58 \t All success : 81 \t mean(l0) : 7.530864 \t Current Learning Rate: 0.038\n",
            "Iteration 3200: Current success : 58 \t All success : 81 \t mean(l0) : 7.530864 \t Current Learning Rate: 0.036\n",
            "Iteration 3300: Current success : 58 \t All success : 81 \t mean(l0) : 7.530864 \t Current Learning Rate: 0.034\n",
            "Iteration 3400: Current success : 58 \t All success : 81 \t mean(l0) : 7.530864 \t Current Learning Rate: 0.032\n",
            "Iteration 3500: Current success : 58 \t All success : 81 \t mean(l0) : 7.530864 \t Current Learning Rate: 0.030\n",
            "Iteration 3600: Current success : 58 \t All success : 81 \t mean(l0) : 7.530864 \t Current Learning Rate: 0.028\n",
            "Iteration 3700: Current success : 58 \t All success : 81 \t mean(l0) : 7.530864 \t Current Learning Rate: 0.026\n",
            "Iteration 3800: Current success : 58 \t All success : 81 \t mean(l0) : 7.530864 \t Current Learning Rate: 0.024\n",
            "Iteration 3900: Current success : 58 \t All success : 81 \t mean(l0) : 7.530864 \t Current Learning Rate: 0.022\n",
            "Iteration 4000: Current success : 58 \t All success : 81 \t mean(l0) : 7.530864 \t Current Learning Rate: 0.020\n",
            "Iteration 4100: Current success : 58 \t All success : 81 \t mean(l0) : 7.530864 \t Current Learning Rate: 0.018\n",
            "Iteration 4200: Current success : 58 \t All success : 81 \t mean(l0) : 7.530864 \t Current Learning Rate: 0.016\n",
            "Iteration 4300: Current success : 58 \t All success : 81 \t mean(l0) : 7.530864 \t Current Learning Rate: 0.014\n",
            "Iteration 4400: Current success : 58 \t All success : 81 \t mean(l0) : 7.530864 \t Current Learning Rate: 0.012\n",
            "Iteration 4500: Current success : 58 \t All success : 81 \t mean(l0) : 7.530864 \t Current Learning Rate: 0.010\n",
            "Iteration 4600: Current success : 58 \t All success : 81 \t mean(l0) : 7.530864 \t Current Learning Rate: 0.008\n",
            "Iteration 4700: Current success : 58 \t All success : 81 \t mean(l0) : 7.530864 \t Current Learning Rate: 0.006\n",
            "Iteration 4800: Current success : 58 \t All success : 81 \t mean(l0) : 7.530864 \t Current Learning Rate: 0.004\n",
            "Iteration 4900: Current success : 58 \t All success : 81 \t mean(l0) : 7.530864 \t Current Learning Rate: 0.002\n",
            "Iteration 5000: Current success : 58 \t All success : 81 \t mean(l0) : 7.530864 \t Current Learning Rate: 0.000\n",
            "Iteration 5100: Current success : 44 \t All success : 83 \t mean(l0) : 8.192771 \t Current Learning Rate: 0.049\n",
            "Iteration 5200: Current success : 71 \t All success : 102 \t mean(l0) : 10.725491 \t Current Learning Rate: 0.048\n",
            "Iteration 5300: Current success : 66 \t All success : 103 \t mean(l0) : 10.475728 \t Current Learning Rate: 0.047\n",
            "Iteration 5400: Current success : 66 \t All success : 103 \t mean(l0) : 10.194175 \t Current Learning Rate: 0.046\n",
            "Iteration 5500: Current success : 66 \t All success : 103 \t mean(l0) : 10.184466 \t Current Learning Rate: 0.045\n",
            "Iteration 5600: Current success : 66 \t All success : 103 \t mean(l0) : 10.184466 \t Current Learning Rate: 0.044\n",
            "Iteration 5700: Current success : 65 \t All success : 104 \t mean(l0) : 10.115385 \t Current Learning Rate: 0.043\n",
            "Iteration 5800: Current success : 67 \t All success : 105 \t mean(l0) : 10.314286 \t Current Learning Rate: 0.042\n",
            "Iteration 5900: Current success : 66 \t All success : 105 \t mean(l0) : 10.295238 \t Current Learning Rate: 0.041\n",
            "Iteration 6000: Current success : 67 \t All success : 105 \t mean(l0) : 10.295238 \t Current Learning Rate: 0.040\n",
            "Iteration 6100: Current success : 65 \t All success : 105 \t mean(l0) : 10.285715 \t Current Learning Rate: 0.039\n",
            "Iteration 6200: Current success : 65 \t All success : 105 \t mean(l0) : 10.276191 \t Current Learning Rate: 0.038\n",
            "Iteration 6300: Current success : 67 \t All success : 105 \t mean(l0) : 10.276191 \t Current Learning Rate: 0.037\n",
            "Iteration 6400: Current success : 67 \t All success : 105 \t mean(l0) : 10.276191 \t Current Learning Rate: 0.036\n",
            "Iteration 6500: Current success : 68 \t All success : 105 \t mean(l0) : 10.276191 \t Current Learning Rate: 0.035\n",
            "Iteration 6600: Current success : 66 \t All success : 105 \t mean(l0) : 10.276191 \t Current Learning Rate: 0.034\n",
            "Iteration 6700: Current success : 67 \t All success : 105 \t mean(l0) : 10.276191 \t Current Learning Rate: 0.033\n",
            "Iteration 6800: Current success : 68 \t All success : 105 \t mean(l0) : 10.276191 \t Current Learning Rate: 0.032\n",
            "Iteration 6900: Current success : 68 \t All success : 105 \t mean(l0) : 10.266667 \t Current Learning Rate: 0.031\n",
            "Iteration 7000: Current success : 67 \t All success : 105 \t mean(l0) : 10.266667 \t Current Learning Rate: 0.030\n",
            "Iteration 7100: Current success : 68 \t All success : 105 \t mean(l0) : 10.266667 \t Current Learning Rate: 0.029\n",
            "Iteration 7200: Current success : 68 \t All success : 105 \t mean(l0) : 10.266667 \t Current Learning Rate: 0.028\n",
            "Iteration 7300: Current success : 68 \t All success : 105 \t mean(l0) : 10.266667 \t Current Learning Rate: 0.027\n",
            "Iteration 7400: Current success : 68 \t All success : 105 \t mean(l0) : 10.266667 \t Current Learning Rate: 0.026\n",
            "Iteration 7500: Current success : 68 \t All success : 105 \t mean(l0) : 10.266667 \t Current Learning Rate: 0.025\n",
            "Iteration 7600: Current success : 68 \t All success : 105 \t mean(l0) : 10.266667 \t Current Learning Rate: 0.024\n",
            "Iteration 7700: Current success : 68 \t All success : 105 \t mean(l0) : 10.266667 \t Current Learning Rate: 0.023\n",
            "Iteration 7800: Current success : 69 \t All success : 105 \t mean(l0) : 10.266667 \t Current Learning Rate: 0.022\n",
            "Iteration 7900: Current success : 69 \t All success : 105 \t mean(l0) : 10.266667 \t Current Learning Rate: 0.021\n",
            "Iteration 8000: Current success : 69 \t All success : 105 \t mean(l0) : 10.266667 \t Current Learning Rate: 0.020\n",
            "Iteration 8100: Current success : 69 \t All success : 105 \t mean(l0) : 10.266667 \t Current Learning Rate: 0.019\n",
            "Iteration 8200: Current success : 69 \t All success : 105 \t mean(l0) : 10.266667 \t Current Learning Rate: 0.018\n",
            "Iteration 8300: Current success : 69 \t All success : 105 \t mean(l0) : 10.266667 \t Current Learning Rate: 0.017\n",
            "Iteration 8400: Current success : 69 \t All success : 105 \t mean(l0) : 10.266667 \t Current Learning Rate: 0.016\n",
            "Iteration 8500: Current success : 69 \t All success : 105 \t mean(l0) : 10.266667 \t Current Learning Rate: 0.015\n",
            "Iteration 8600: Current success : 69 \t All success : 105 \t mean(l0) : 10.266667 \t Current Learning Rate: 0.014\n",
            "Iteration 8700: Current success : 69 \t All success : 105 \t mean(l0) : 10.266667 \t Current Learning Rate: 0.013\n",
            "Iteration 8800: Current success : 69 \t All success : 105 \t mean(l0) : 10.266667 \t Current Learning Rate: 0.012\n",
            "Iteration 8900: Current success : 69 \t All success : 105 \t mean(l0) : 10.266667 \t Current Learning Rate: 0.011\n",
            "Iteration 9000: Current success : 69 \t All success : 105 \t mean(l0) : 10.266667 \t Current Learning Rate: 0.010\n",
            "Iteration 9100: Current success : 69 \t All success : 105 \t mean(l0) : 10.266667 \t Current Learning Rate: 0.009\n",
            "Iteration 9200: Current success : 69 \t All success : 105 \t mean(l0) : 10.266667 \t Current Learning Rate: 0.008\n",
            "Iteration 9300: Current success : 69 \t All success : 105 \t mean(l0) : 10.266667 \t Current Learning Rate: 0.007\n",
            "Iteration 9400: Current success : 69 \t All success : 105 \t mean(l0) : 10.266667 \t Current Learning Rate: 0.006\n",
            "Iteration 9500: Current success : 69 \t All success : 105 \t mean(l0) : 10.266667 \t Current Learning Rate: 0.005\n",
            "Iteration 9600: Current success : 69 \t All success : 105 \t mean(l0) : 10.266667 \t Current Learning Rate: 0.004\n",
            "Iteration 9700: Current success : 69 \t All success : 105 \t mean(l0) : 10.266667 \t Current Learning Rate: 0.003\n",
            "Iteration 9800: Current success : 69 \t All success : 105 \t mean(l0) : 10.266667 \t Current Learning Rate: 0.002\n",
            "Iteration 9900: Current success : 69 \t All success : 105 \t mean(l0) : 10.266667 \t Current Learning Rate: 0.001\n",
            "Iteration 10000: Current success : 69 \t All success : 105 \t mean(l0) : 10.266667 \t Current Learning Rate: 0.000\n",
            "Iteration 10100: Current success : 67 \t All success : 110 \t mean(l0) : 11.236363 \t Current Learning Rate: 0.025\n",
            "Iteration 10200: Current success : 76 \t All success : 114 \t mean(l0) : 11.842105 \t Current Learning Rate: 0.024\n",
            "Iteration 10300: Current success : 73 \t All success : 114 \t mean(l0) : 11.771930 \t Current Learning Rate: 0.024\n",
            "Iteration 10400: Current success : 74 \t All success : 114 \t mean(l0) : 11.771930 \t Current Learning Rate: 0.023\n",
            "Iteration 10500: Current success : 74 \t All success : 114 \t mean(l0) : 11.754386 \t Current Learning Rate: 0.023\n",
            "Iteration 10600: Current success : 75 \t All success : 114 \t mean(l0) : 11.754386 \t Current Learning Rate: 0.022\n",
            "Iteration 10700: Current success : 74 \t All success : 114 \t mean(l0) : 11.754386 \t Current Learning Rate: 0.022\n",
            "Iteration 10800: Current success : 74 \t All success : 114 \t mean(l0) : 11.754386 \t Current Learning Rate: 0.021\n",
            "Iteration 10900: Current success : 75 \t All success : 114 \t mean(l0) : 11.754386 \t Current Learning Rate: 0.021\n",
            "Iteration 11000: Current success : 74 \t All success : 114 \t mean(l0) : 11.754386 \t Current Learning Rate: 0.020\n",
            "Iteration 11100: Current success : 75 \t All success : 114 \t mean(l0) : 11.754386 \t Current Learning Rate: 0.020\n",
            "Iteration 11200: Current success : 75 \t All success : 114 \t mean(l0) : 11.754386 \t Current Learning Rate: 0.019\n",
            "Iteration 11300: Current success : 75 \t All success : 114 \t mean(l0) : 11.754386 \t Current Learning Rate: 0.019\n",
            "Iteration 11400: Current success : 75 \t All success : 114 \t mean(l0) : 11.754386 \t Current Learning Rate: 0.018\n",
            "Iteration 11500: Current success : 75 \t All success : 114 \t mean(l0) : 11.754386 \t Current Learning Rate: 0.018\n",
            "Iteration 11600: Current success : 75 \t All success : 114 \t mean(l0) : 11.754386 \t Current Learning Rate: 0.017\n",
            "Iteration 11700: Current success : 75 \t All success : 114 \t mean(l0) : 11.754386 \t Current Learning Rate: 0.017\n",
            "Iteration 11800: Current success : 75 \t All success : 114 \t mean(l0) : 11.754386 \t Current Learning Rate: 0.016\n",
            "Iteration 11900: Current success : 75 \t All success : 114 \t mean(l0) : 11.754386 \t Current Learning Rate: 0.016\n",
            "Iteration 12000: Current success : 75 \t All success : 114 \t mean(l0) : 11.754386 \t Current Learning Rate: 0.015\n",
            "Iteration 12100: Current success : 75 \t All success : 114 \t mean(l0) : 11.754386 \t Current Learning Rate: 0.015\n",
            "Iteration 12200: Current success : 75 \t All success : 114 \t mean(l0) : 11.754386 \t Current Learning Rate: 0.014\n",
            "Iteration 12300: Current success : 75 \t All success : 114 \t mean(l0) : 11.754386 \t Current Learning Rate: 0.014\n",
            "Iteration 12400: Current success : 75 \t All success : 114 \t mean(l0) : 11.754386 \t Current Learning Rate: 0.013\n",
            "Iteration 12500: Current success : 75 \t All success : 114 \t mean(l0) : 11.754386 \t Current Learning Rate: 0.013\n",
            "Iteration 12600: Current success : 75 \t All success : 114 \t mean(l0) : 11.754386 \t Current Learning Rate: 0.012\n",
            "Iteration 12700: Current success : 75 \t All success : 114 \t mean(l0) : 11.754386 \t Current Learning Rate: 0.012\n",
            "Iteration 12800: Current success : 75 \t All success : 114 \t mean(l0) : 11.754386 \t Current Learning Rate: 0.011\n",
            "Iteration 12900: Current success : 75 \t All success : 114 \t mean(l0) : 11.754386 \t Current Learning Rate: 0.011\n",
            "Iteration 13000: Current success : 75 \t All success : 114 \t mean(l0) : 11.754386 \t Current Learning Rate: 0.010\n",
            "Iteration 13100: Current success : 75 \t All success : 114 \t mean(l0) : 11.754386 \t Current Learning Rate: 0.010\n",
            "Iteration 13200: Current success : 75 \t All success : 114 \t mean(l0) : 11.754386 \t Current Learning Rate: 0.009\n",
            "Iteration 13300: Current success : 75 \t All success : 114 \t mean(l0) : 11.754386 \t Current Learning Rate: 0.009\n",
            "Iteration 13400: Current success : 75 \t All success : 114 \t mean(l0) : 11.754386 \t Current Learning Rate: 0.008\n",
            "Iteration 13500: Current success : 75 \t All success : 114 \t mean(l0) : 11.754386 \t Current Learning Rate: 0.008\n",
            "Iteration 13600: Current success : 75 \t All success : 114 \t mean(l0) : 11.754386 \t Current Learning Rate: 0.007\n",
            "Iteration 13700: Current success : 75 \t All success : 114 \t mean(l0) : 11.754386 \t Current Learning Rate: 0.007\n",
            "Iteration 13800: Current success : 75 \t All success : 114 \t mean(l0) : 11.754386 \t Current Learning Rate: 0.006\n",
            "Iteration 13900: Current success : 75 \t All success : 114 \t mean(l0) : 11.754386 \t Current Learning Rate: 0.006\n",
            "Iteration 14000: Current success : 75 \t All success : 114 \t mean(l0) : 11.754386 \t Current Learning Rate: 0.005\n",
            "Iteration 14100: Current success : 75 \t All success : 114 \t mean(l0) : 11.754386 \t Current Learning Rate: 0.005\n",
            "Iteration 14200: Current success : 75 \t All success : 114 \t mean(l0) : 11.754386 \t Current Learning Rate: 0.004\n",
            "Iteration 14300: Current success : 75 \t All success : 114 \t mean(l0) : 11.754386 \t Current Learning Rate: 0.004\n",
            "Iteration 14400: Current success : 75 \t All success : 114 \t mean(l0) : 11.754386 \t Current Learning Rate: 0.003\n",
            "Iteration 14500: Current success : 75 \t All success : 114 \t mean(l0) : 11.754386 \t Current Learning Rate: 0.003\n",
            "Iteration 14600: Current success : 75 \t All success : 114 \t mean(l0) : 11.754386 \t Current Learning Rate: 0.002\n",
            "Iteration 14700: Current success : 75 \t All success : 114 \t mean(l0) : 11.754386 \t Current Learning Rate: 0.002\n",
            "Iteration 14800: Current success : 75 \t All success : 114 \t mean(l0) : 11.754386 \t Current Learning Rate: 0.001\n",
            "Iteration 14900: Current success : 75 \t All success : 114 \t mean(l0) : 11.754386 \t Current Learning Rate: 0.001\n",
            "Iteration 15000: Current success : 75 \t All success : 114 \t mean(l0) : 11.754386 \t Current Learning Rate: 0.000\n",
            "Iteration 15100: Current success : 77 \t All success : 116 \t mean(l0) : 11.801724 \t Current Learning Rate: 0.012\n",
            "Iteration 15200: Current success : 79 \t All success : 116 \t mean(l0) : 11.758620 \t Current Learning Rate: 0.012\n",
            "Iteration 15300: Current success : 78 \t All success : 117 \t mean(l0) : 11.606838 \t Current Learning Rate: 0.012\n",
            "Iteration 15400: Current success : 79 \t All success : 117 \t mean(l0) : 11.606838 \t Current Learning Rate: 0.012\n",
            "Iteration 15500: Current success : 79 \t All success : 117 \t mean(l0) : 11.606838 \t Current Learning Rate: 0.011\n",
            "Iteration 15600: Current success : 79 \t All success : 117 \t mean(l0) : 11.606838 \t Current Learning Rate: 0.011\n",
            "Iteration 15700: Current success : 79 \t All success : 117 \t mean(l0) : 11.606838 \t Current Learning Rate: 0.011\n",
            "Iteration 15800: Current success : 79 \t All success : 117 \t mean(l0) : 11.606838 \t Current Learning Rate: 0.011\n",
            "Iteration 15900: Current success : 79 \t All success : 117 \t mean(l0) : 11.606838 \t Current Learning Rate: 0.010\n",
            "Iteration 16000: Current success : 79 \t All success : 117 \t mean(l0) : 11.606838 \t Current Learning Rate: 0.010\n",
            "Iteration 16100: Current success : 79 \t All success : 117 \t mean(l0) : 11.606838 \t Current Learning Rate: 0.010\n",
            "Iteration 16200: Current success : 79 \t All success : 117 \t mean(l0) : 11.606838 \t Current Learning Rate: 0.010\n",
            "Iteration 16300: Current success : 79 \t All success : 117 \t mean(l0) : 11.606838 \t Current Learning Rate: 0.009\n",
            "Iteration 16400: Current success : 79 \t All success : 117 \t mean(l0) : 11.606838 \t Current Learning Rate: 0.009\n",
            "Iteration 16500: Current success : 79 \t All success : 117 \t mean(l0) : 11.606838 \t Current Learning Rate: 0.009\n",
            "Iteration 16600: Current success : 79 \t All success : 117 \t mean(l0) : 11.606838 \t Current Learning Rate: 0.009\n",
            "Iteration 16700: Current success : 79 \t All success : 117 \t mean(l0) : 11.606838 \t Current Learning Rate: 0.008\n",
            "Iteration 16800: Current success : 79 \t All success : 117 \t mean(l0) : 11.606838 \t Current Learning Rate: 0.008\n",
            "Iteration 16900: Current success : 79 \t All success : 117 \t mean(l0) : 11.606838 \t Current Learning Rate: 0.008\n",
            "Iteration 17000: Current success : 79 \t All success : 117 \t mean(l0) : 11.606838 \t Current Learning Rate: 0.008\n",
            "Iteration 17100: Current success : 79 \t All success : 117 \t mean(l0) : 11.606838 \t Current Learning Rate: 0.007\n",
            "Iteration 17200: Current success : 79 \t All success : 117 \t mean(l0) : 11.606838 \t Current Learning Rate: 0.007\n",
            "Iteration 17300: Current success : 79 \t All success : 117 \t mean(l0) : 11.606838 \t Current Learning Rate: 0.007\n",
            "Iteration 17400: Current success : 79 \t All success : 117 \t mean(l0) : 11.606838 \t Current Learning Rate: 0.007\n",
            "Iteration 17500: Current success : 79 \t All success : 117 \t mean(l0) : 11.606838 \t Current Learning Rate: 0.006\n",
            "Iteration 17600: Current success : 79 \t All success : 117 \t mean(l0) : 11.606838 \t Current Learning Rate: 0.006\n",
            "Iteration 17700: Current success : 79 \t All success : 117 \t mean(l0) : 11.606838 \t Current Learning Rate: 0.006\n",
            "Iteration 17800: Current success : 79 \t All success : 117 \t mean(l0) : 11.606838 \t Current Learning Rate: 0.006\n",
            "Iteration 17900: Current success : 79 \t All success : 117 \t mean(l0) : 11.606838 \t Current Learning Rate: 0.005\n",
            "Iteration 18000: Current success : 79 \t All success : 117 \t mean(l0) : 11.606838 \t Current Learning Rate: 0.005\n",
            "Iteration 18100: Current success : 79 \t All success : 117 \t mean(l0) : 11.606838 \t Current Learning Rate: 0.005\n",
            "Iteration 18200: Current success : 79 \t All success : 117 \t mean(l0) : 11.606838 \t Current Learning Rate: 0.005\n",
            "Iteration 18300: Current success : 79 \t All success : 117 \t mean(l0) : 11.606838 \t Current Learning Rate: 0.004\n",
            "Iteration 18400: Current success : 79 \t All success : 117 \t mean(l0) : 11.606838 \t Current Learning Rate: 0.004\n",
            "Iteration 18500: Current success : 79 \t All success : 117 \t mean(l0) : 11.606838 \t Current Learning Rate: 0.004\n",
            "Iteration 18600: Current success : 79 \t All success : 117 \t mean(l0) : 11.606838 \t Current Learning Rate: 0.004\n",
            "Iteration 18700: Current success : 79 \t All success : 117 \t mean(l0) : 11.606838 \t Current Learning Rate: 0.003\n",
            "Iteration 18800: Current success : 79 \t All success : 117 \t mean(l0) : 11.606838 \t Current Learning Rate: 0.003\n",
            "Iteration 18900: Current success : 79 \t All success : 117 \t mean(l0) : 11.606838 \t Current Learning Rate: 0.003\n",
            "Iteration 19000: Current success : 79 \t All success : 117 \t mean(l0) : 11.606838 \t Current Learning Rate: 0.003\n",
            "Iteration 19100: Current success : 79 \t All success : 117 \t mean(l0) : 11.606838 \t Current Learning Rate: 0.002\n",
            "Iteration 19200: Current success : 79 \t All success : 117 \t mean(l0) : 11.606838 \t Current Learning Rate: 0.002\n",
            "Iteration 19300: Current success : 79 \t All success : 117 \t mean(l0) : 11.606838 \t Current Learning Rate: 0.002\n",
            "Iteration 19400: Current success : 79 \t All success : 117 \t mean(l0) : 11.606838 \t Current Learning Rate: 0.002\n",
            "Iteration 19500: Current success : 79 \t All success : 117 \t mean(l0) : 11.606838 \t Current Learning Rate: 0.001\n",
            "Iteration 19600: Current success : 79 \t All success : 117 \t mean(l0) : 11.606838 \t Current Learning Rate: 0.001\n",
            "Iteration 19700: Current success : 79 \t All success : 117 \t mean(l0) : 11.606838 \t Current Learning Rate: 0.001\n",
            "Iteration 19800: Current success : 79 \t All success : 117 \t mean(l0) : 11.606838 \t Current Learning Rate: 0.001\n",
            "Iteration 19900: Current success : 79 \t All success : 117 \t mean(l0) : 11.606838 \t Current Learning Rate: 0.000\n",
            "Iteration 20000: Current success : 79 \t All success : 117 \t mean(l0) : 11.606838 \t Current Learning Rate: 0.000\n",
            "outer_step 2: all success : 146 \t mean(l0)(success) : 10.2192\n",
            "-------------------------------------------------------------\n",
            "tensor([100., 100., 100.,  ..., 100., 100., 100.], device='cuda:0')\n",
            "Iteration 100: Current success : 28 \t All success : 82 \t mean(l0) : 9.353658 \t Current Learning Rate: 0.098\n",
            "Iteration 200: Current success : 32 \t All success : 86 \t mean(l0) : 9.453488 \t Current Learning Rate: 0.096\n",
            "Iteration 300: Current success : 37 \t All success : 89 \t mean(l0) : 9.831461 \t Current Learning Rate: 0.094\n",
            "Iteration 400: Current success : 42 \t All success : 93 \t mean(l0) : 11.408602 \t Current Learning Rate: 0.092\n",
            "Iteration 500: Current success : 41 \t All success : 97 \t mean(l0) : 11.443298 \t Current Learning Rate: 0.090\n",
            "Iteration 600: Current success : 40 \t All success : 97 \t mean(l0) : 11.443298 \t Current Learning Rate: 0.088\n",
            "Iteration 700: Current success : 47 \t All success : 101 \t mean(l0) : 12.336634 \t Current Learning Rate: 0.086\n",
            "Iteration 800: Current success : 46 \t All success : 103 \t mean(l0) : 12.417476 \t Current Learning Rate: 0.084\n",
            "Iteration 900: Current success : 46 \t All success : 103 \t mean(l0) : 12.417476 \t Current Learning Rate: 0.082\n",
            "Iteration 1000: Current success : 50 \t All success : 104 \t mean(l0) : 12.682693 \t Current Learning Rate: 0.080\n",
            "Iteration 1100: Current success : 48 \t All success : 105 \t mean(l0) : 12.752381 \t Current Learning Rate: 0.078\n",
            "Iteration 1200: Current success : 53 \t All success : 108 \t mean(l0) : 12.537037 \t Current Learning Rate: 0.076\n",
            "Iteration 1300: Current success : 55 \t All success : 111 \t mean(l0) : 12.684685 \t Current Learning Rate: 0.074\n",
            "Iteration 1400: Current success : 52 \t All success : 112 \t mean(l0) : 12.598215 \t Current Learning Rate: 0.072\n",
            "Iteration 1500: Current success : 56 \t All success : 112 \t mean(l0) : 12.598215 \t Current Learning Rate: 0.070\n",
            "Iteration 1600: Current success : 55 \t All success : 115 \t mean(l0) : 12.426086 \t Current Learning Rate: 0.068\n",
            "Iteration 1700: Current success : 62 \t All success : 115 \t mean(l0) : 12.182609 \t Current Learning Rate: 0.066\n",
            "Iteration 1800: Current success : 62 \t All success : 115 \t mean(l0) : 12.182609 \t Current Learning Rate: 0.064\n",
            "Iteration 1900: Current success : 60 \t All success : 116 \t mean(l0) : 12.318966 \t Current Learning Rate: 0.062\n",
            "Iteration 2000: Current success : 66 \t All success : 116 \t mean(l0) : 12.318966 \t Current Learning Rate: 0.060\n",
            "Iteration 2100: Current success : 67 \t All success : 116 \t mean(l0) : 12.301724 \t Current Learning Rate: 0.058\n",
            "Iteration 2200: Current success : 75 \t All success : 122 \t mean(l0) : 12.942622 \t Current Learning Rate: 0.056\n",
            "Iteration 2300: Current success : 69 \t All success : 122 \t mean(l0) : 12.942622 \t Current Learning Rate: 0.054\n",
            "Iteration 2400: Current success : 72 \t All success : 126 \t mean(l0) : 15.611112 \t Current Learning Rate: 0.052\n",
            "Iteration 2500: Current success : 73 \t All success : 126 \t mean(l0) : 15.611112 \t Current Learning Rate: 0.050\n",
            "Iteration 2600: Current success : 79 \t All success : 126 \t mean(l0) : 15.611112 \t Current Learning Rate: 0.048\n",
            "Iteration 2700: Current success : 73 \t All success : 126 \t mean(l0) : 15.603175 \t Current Learning Rate: 0.046\n",
            "Iteration 2800: Current success : 73 \t All success : 126 \t mean(l0) : 15.603175 \t Current Learning Rate: 0.044\n",
            "Iteration 2900: Current success : 73 \t All success : 126 \t mean(l0) : 15.603175 \t Current Learning Rate: 0.042\n",
            "Iteration 3000: Current success : 74 \t All success : 126 \t mean(l0) : 15.603175 \t Current Learning Rate: 0.040\n",
            "Iteration 3100: Current success : 73 \t All success : 126 \t mean(l0) : 15.603175 \t Current Learning Rate: 0.038\n",
            "Iteration 3200: Current success : 73 \t All success : 126 \t mean(l0) : 15.603175 \t Current Learning Rate: 0.036\n",
            "Iteration 3300: Current success : 76 \t All success : 126 \t mean(l0) : 15.603175 \t Current Learning Rate: 0.034\n",
            "Iteration 3400: Current success : 76 \t All success : 126 \t mean(l0) : 15.603175 \t Current Learning Rate: 0.032\n",
            "Iteration 3500: Current success : 73 \t All success : 126 \t mean(l0) : 15.603175 \t Current Learning Rate: 0.030\n",
            "Iteration 3600: Current success : 80 \t All success : 126 \t mean(l0) : 15.603175 \t Current Learning Rate: 0.028\n",
            "Iteration 3700: Current success : 76 \t All success : 126 \t mean(l0) : 15.603175 \t Current Learning Rate: 0.026\n",
            "Iteration 3800: Current success : 73 \t All success : 126 \t mean(l0) : 15.603175 \t Current Learning Rate: 0.024\n",
            "Iteration 3900: Current success : 76 \t All success : 126 \t mean(l0) : 15.603175 \t Current Learning Rate: 0.022\n",
            "Iteration 4000: Current success : 76 \t All success : 126 \t mean(l0) : 15.603175 \t Current Learning Rate: 0.020\n",
            "Iteration 4100: Current success : 76 \t All success : 126 \t mean(l0) : 15.603175 \t Current Learning Rate: 0.018\n",
            "Iteration 4200: Current success : 73 \t All success : 126 \t mean(l0) : 15.603175 \t Current Learning Rate: 0.016\n",
            "Iteration 4300: Current success : 73 \t All success : 126 \t mean(l0) : 15.603175 \t Current Learning Rate: 0.014\n",
            "Iteration 4400: Current success : 73 \t All success : 126 \t mean(l0) : 15.603175 \t Current Learning Rate: 0.012\n",
            "Iteration 4500: Current success : 73 \t All success : 126 \t mean(l0) : 15.603175 \t Current Learning Rate: 0.010\n",
            "Iteration 4600: Current success : 73 \t All success : 126 \t mean(l0) : 15.603175 \t Current Learning Rate: 0.008\n",
            "Iteration 4700: Current success : 73 \t All success : 126 \t mean(l0) : 15.603175 \t Current Learning Rate: 0.006\n",
            "Iteration 4800: Current success : 73 \t All success : 126 \t mean(l0) : 15.603175 \t Current Learning Rate: 0.004\n",
            "Iteration 4900: Current success : 73 \t All success : 126 \t mean(l0) : 15.603175 \t Current Learning Rate: 0.002\n",
            "Iteration 5000: Current success : 73 \t All success : 126 \t mean(l0) : 15.603175 \t Current Learning Rate: 0.000\n",
            "Iteration 5100: Current success : 87 \t All success : 142 \t mean(l0) : 19.852112 \t Current Learning Rate: 0.049\n",
            "Iteration 5200: Current success : 109 \t All success : 147 \t mean(l0) : 18.863945 \t Current Learning Rate: 0.048\n",
            "Iteration 5300: Current success : 115 \t All success : 154 \t mean(l0) : 18.883118 \t Current Learning Rate: 0.047\n",
            "Iteration 5400: Current success : 118 \t All success : 154 \t mean(l0) : 18.857143 \t Current Learning Rate: 0.046\n",
            "Iteration 5500: Current success : 114 \t All success : 154 \t mean(l0) : 18.805195 \t Current Learning Rate: 0.045\n",
            "Iteration 5600: Current success : 114 \t All success : 154 \t mean(l0) : 18.805195 \t Current Learning Rate: 0.044\n",
            "Iteration 5700: Current success : 118 \t All success : 154 \t mean(l0) : 18.727272 \t Current Learning Rate: 0.043\n",
            "Iteration 5800: Current success : 119 \t All success : 154 \t mean(l0) : 18.688313 \t Current Learning Rate: 0.042\n",
            "Iteration 5900: Current success : 112 \t All success : 154 \t mean(l0) : 18.649351 \t Current Learning Rate: 0.041\n",
            "Iteration 6000: Current success : 119 \t All success : 154 \t mean(l0) : 18.649351 \t Current Learning Rate: 0.040\n",
            "Iteration 6100: Current success : 117 \t All success : 154 \t mean(l0) : 18.649351 \t Current Learning Rate: 0.039\n",
            "Iteration 6200: Current success : 117 \t All success : 154 \t mean(l0) : 18.649351 \t Current Learning Rate: 0.038\n",
            "Iteration 6300: Current success : 119 \t All success : 154 \t mean(l0) : 18.629869 \t Current Learning Rate: 0.037\n",
            "Iteration 6400: Current success : 120 \t All success : 154 \t mean(l0) : 18.610390 \t Current Learning Rate: 0.036\n",
            "Iteration 6500: Current success : 120 \t All success : 154 \t mean(l0) : 18.610390 \t Current Learning Rate: 0.035\n",
            "Iteration 6600: Current success : 120 \t All success : 154 \t mean(l0) : 18.610390 \t Current Learning Rate: 0.034\n",
            "Iteration 6700: Current success : 119 \t All success : 154 \t mean(l0) : 18.610390 \t Current Learning Rate: 0.033\n",
            "Iteration 6800: Current success : 121 \t All success : 154 \t mean(l0) : 18.610390 \t Current Learning Rate: 0.032\n",
            "Iteration 6900: Current success : 120 \t All success : 154 \t mean(l0) : 18.610390 \t Current Learning Rate: 0.031\n",
            "Iteration 7000: Current success : 121 \t All success : 154 \t mean(l0) : 18.610390 \t Current Learning Rate: 0.030\n",
            "Iteration 7100: Current success : 121 \t All success : 154 \t mean(l0) : 18.610390 \t Current Learning Rate: 0.029\n",
            "Iteration 7200: Current success : 121 \t All success : 154 \t mean(l0) : 18.610390 \t Current Learning Rate: 0.028\n",
            "Iteration 7300: Current success : 121 \t All success : 154 \t mean(l0) : 18.610390 \t Current Learning Rate: 0.027\n",
            "Iteration 7400: Current success : 124 \t All success : 154 \t mean(l0) : 18.610390 \t Current Learning Rate: 0.026\n",
            "Iteration 7500: Current success : 124 \t All success : 154 \t mean(l0) : 18.610390 \t Current Learning Rate: 0.025\n",
            "Iteration 7600: Current success : 124 \t All success : 154 \t mean(l0) : 18.610390 \t Current Learning Rate: 0.024\n",
            "Iteration 7700: Current success : 124 \t All success : 154 \t mean(l0) : 18.610390 \t Current Learning Rate: 0.023\n",
            "Iteration 7800: Current success : 124 \t All success : 154 \t mean(l0) : 18.610390 \t Current Learning Rate: 0.022\n",
            "Iteration 7900: Current success : 124 \t All success : 154 \t mean(l0) : 18.610390 \t Current Learning Rate: 0.021\n",
            "Iteration 8000: Current success : 124 \t All success : 154 \t mean(l0) : 18.610390 \t Current Learning Rate: 0.020\n",
            "Iteration 8100: Current success : 124 \t All success : 154 \t mean(l0) : 18.610390 \t Current Learning Rate: 0.019\n",
            "Iteration 8200: Current success : 124 \t All success : 154 \t mean(l0) : 18.610390 \t Current Learning Rate: 0.018\n",
            "Iteration 8300: Current success : 124 \t All success : 154 \t mean(l0) : 18.610390 \t Current Learning Rate: 0.017\n",
            "Iteration 8400: Current success : 124 \t All success : 154 \t mean(l0) : 18.610390 \t Current Learning Rate: 0.016\n",
            "Iteration 8500: Current success : 124 \t All success : 154 \t mean(l0) : 18.610390 \t Current Learning Rate: 0.015\n",
            "Iteration 8600: Current success : 124 \t All success : 154 \t mean(l0) : 18.610390 \t Current Learning Rate: 0.014\n",
            "Iteration 8700: Current success : 124 \t All success : 154 \t mean(l0) : 18.610390 \t Current Learning Rate: 0.013\n",
            "Iteration 8800: Current success : 124 \t All success : 154 \t mean(l0) : 18.610390 \t Current Learning Rate: 0.012\n",
            "Iteration 8900: Current success : 124 \t All success : 154 \t mean(l0) : 18.610390 \t Current Learning Rate: 0.011\n",
            "Iteration 9000: Current success : 124 \t All success : 154 \t mean(l0) : 18.610390 \t Current Learning Rate: 0.010\n",
            "Iteration 9100: Current success : 124 \t All success : 154 \t mean(l0) : 18.610390 \t Current Learning Rate: 0.009\n",
            "Iteration 9200: Current success : 124 \t All success : 154 \t mean(l0) : 18.610390 \t Current Learning Rate: 0.008\n",
            "Iteration 9300: Current success : 124 \t All success : 154 \t mean(l0) : 18.610390 \t Current Learning Rate: 0.007\n",
            "Iteration 9400: Current success : 124 \t All success : 154 \t mean(l0) : 18.610390 \t Current Learning Rate: 0.006\n",
            "Iteration 9500: Current success : 124 \t All success : 154 \t mean(l0) : 18.610390 \t Current Learning Rate: 0.005\n",
            "Iteration 9600: Current success : 124 \t All success : 154 \t mean(l0) : 18.610390 \t Current Learning Rate: 0.004\n",
            "Iteration 9700: Current success : 124 \t All success : 154 \t mean(l0) : 18.610390 \t Current Learning Rate: 0.003\n",
            "Iteration 9800: Current success : 124 \t All success : 154 \t mean(l0) : 18.610390 \t Current Learning Rate: 0.002\n",
            "Iteration 9900: Current success : 124 \t All success : 154 \t mean(l0) : 18.610390 \t Current Learning Rate: 0.001\n",
            "Iteration 10000: Current success : 124 \t All success : 154 \t mean(l0) : 18.610390 \t Current Learning Rate: 0.000\n",
            "Iteration 10100: Current success : 122 \t All success : 159 \t mean(l0) : 19.779873 \t Current Learning Rate: 0.025\n",
            "Iteration 10200: Current success : 127 \t All success : 160 \t mean(l0) : 19.756250 \t Current Learning Rate: 0.024\n",
            "Iteration 10300: Current success : 125 \t All success : 160 \t mean(l0) : 19.731251 \t Current Learning Rate: 0.024\n",
            "Iteration 10400: Current success : 125 \t All success : 160 \t mean(l0) : 19.725000 \t Current Learning Rate: 0.023\n",
            "Iteration 10500: Current success : 125 \t All success : 160 \t mean(l0) : 19.537500 \t Current Learning Rate: 0.023\n",
            "Iteration 10600: Current success : 125 \t All success : 160 \t mean(l0) : 19.312500 \t Current Learning Rate: 0.022\n",
            "Iteration 10700: Current success : 125 \t All success : 160 \t mean(l0) : 19.312500 \t Current Learning Rate: 0.022\n",
            "Iteration 10800: Current success : 126 \t All success : 160 \t mean(l0) : 19.293751 \t Current Learning Rate: 0.021\n",
            "Iteration 10900: Current success : 125 \t All success : 160 \t mean(l0) : 19.293751 \t Current Learning Rate: 0.021\n",
            "Iteration 11000: Current success : 125 \t All success : 160 \t mean(l0) : 19.281250 \t Current Learning Rate: 0.020\n",
            "Iteration 11100: Current success : 125 \t All success : 160 \t mean(l0) : 19.281250 \t Current Learning Rate: 0.020\n",
            "Iteration 11200: Current success : 125 \t All success : 160 \t mean(l0) : 19.281250 \t Current Learning Rate: 0.019\n",
            "Iteration 11300: Current success : 125 \t All success : 160 \t mean(l0) : 19.281250 \t Current Learning Rate: 0.019\n",
            "Iteration 11400: Current success : 126 \t All success : 160 \t mean(l0) : 19.281250 \t Current Learning Rate: 0.018\n",
            "Iteration 11500: Current success : 126 \t All success : 160 \t mean(l0) : 19.281250 \t Current Learning Rate: 0.018\n",
            "Iteration 11600: Current success : 125 \t All success : 160 \t mean(l0) : 19.281250 \t Current Learning Rate: 0.017\n",
            "Iteration 11700: Current success : 126 \t All success : 160 \t mean(l0) : 19.281250 \t Current Learning Rate: 0.017\n",
            "Iteration 11800: Current success : 125 \t All success : 160 \t mean(l0) : 19.281250 \t Current Learning Rate: 0.016\n",
            "Iteration 11900: Current success : 125 \t All success : 160 \t mean(l0) : 19.281250 \t Current Learning Rate: 0.016\n",
            "Iteration 12000: Current success : 126 \t All success : 160 \t mean(l0) : 19.281250 \t Current Learning Rate: 0.015\n",
            "Iteration 12100: Current success : 126 \t All success : 160 \t mean(l0) : 19.281250 \t Current Learning Rate: 0.015\n",
            "Iteration 12200: Current success : 127 \t All success : 161 \t mean(l0) : 19.503105 \t Current Learning Rate: 0.014\n",
            "Iteration 12300: Current success : 127 \t All success : 161 \t mean(l0) : 19.478262 \t Current Learning Rate: 0.014\n",
            "Iteration 12400: Current success : 127 \t All success : 161 \t mean(l0) : 19.472050 \t Current Learning Rate: 0.013\n",
            "Iteration 12500: Current success : 127 \t All success : 161 \t mean(l0) : 19.472050 \t Current Learning Rate: 0.013\n",
            "Iteration 12600: Current success : 127 \t All success : 161 \t mean(l0) : 19.472050 \t Current Learning Rate: 0.012\n",
            "Iteration 12700: Current success : 127 \t All success : 161 \t mean(l0) : 19.472050 \t Current Learning Rate: 0.012\n",
            "Iteration 12800: Current success : 127 \t All success : 162 \t mean(l0) : 19.512346 \t Current Learning Rate: 0.011\n",
            "Iteration 12900: Current success : 130 \t All success : 162 \t mean(l0) : 19.512346 \t Current Learning Rate: 0.011\n",
            "Iteration 13000: Current success : 130 \t All success : 162 \t mean(l0) : 19.512346 \t Current Learning Rate: 0.010\n",
            "Iteration 13100: Current success : 130 \t All success : 162 \t mean(l0) : 19.506172 \t Current Learning Rate: 0.010\n",
            "Iteration 13200: Current success : 130 \t All success : 162 \t mean(l0) : 19.506172 \t Current Learning Rate: 0.009\n",
            "Iteration 13300: Current success : 130 \t All success : 162 \t mean(l0) : 19.506172 \t Current Learning Rate: 0.009\n",
            "Iteration 13400: Current success : 130 \t All success : 162 \t mean(l0) : 19.506172 \t Current Learning Rate: 0.008\n",
            "Iteration 13500: Current success : 130 \t All success : 162 \t mean(l0) : 19.506172 \t Current Learning Rate: 0.008\n",
            "Iteration 13600: Current success : 131 \t All success : 162 \t mean(l0) : 19.506172 \t Current Learning Rate: 0.007\n",
            "Iteration 13700: Current success : 130 \t All success : 162 \t mean(l0) : 19.506172 \t Current Learning Rate: 0.007\n",
            "Iteration 13800: Current success : 130 \t All success : 162 \t mean(l0) : 19.506172 \t Current Learning Rate: 0.006\n",
            "Iteration 13900: Current success : 131 \t All success : 162 \t mean(l0) : 19.506172 \t Current Learning Rate: 0.006\n",
            "Iteration 14000: Current success : 130 \t All success : 162 \t mean(l0) : 19.506172 \t Current Learning Rate: 0.005\n",
            "Iteration 14100: Current success : 131 \t All success : 162 \t mean(l0) : 19.506172 \t Current Learning Rate: 0.005\n",
            "Iteration 14200: Current success : 130 \t All success : 162 \t mean(l0) : 19.506172 \t Current Learning Rate: 0.004\n",
            "Iteration 14300: Current success : 130 \t All success : 162 \t mean(l0) : 19.506172 \t Current Learning Rate: 0.004\n",
            "Iteration 14400: Current success : 130 \t All success : 162 \t mean(l0) : 19.506172 \t Current Learning Rate: 0.003\n",
            "Iteration 14500: Current success : 130 \t All success : 162 \t mean(l0) : 19.506172 \t Current Learning Rate: 0.003\n",
            "Iteration 14600: Current success : 130 \t All success : 162 \t mean(l0) : 19.506172 \t Current Learning Rate: 0.002\n",
            "Iteration 14700: Current success : 131 \t All success : 162 \t mean(l0) : 19.506172 \t Current Learning Rate: 0.002\n",
            "Iteration 14800: Current success : 130 \t All success : 162 \t mean(l0) : 19.506172 \t Current Learning Rate: 0.001\n",
            "Iteration 14900: Current success : 130 \t All success : 162 \t mean(l0) : 19.506172 \t Current Learning Rate: 0.001\n",
            "Iteration 15000: Current success : 130 \t All success : 162 \t mean(l0) : 19.506172 \t Current Learning Rate: 0.000\n",
            "Iteration 15100: Current success : 125 \t All success : 163 \t mean(l0) : 19.417177 \t Current Learning Rate: 0.012\n",
            "Iteration 15200: Current success : 127 \t All success : 163 \t mean(l0) : 19.411043 \t Current Learning Rate: 0.012\n",
            "Iteration 15300: Current success : 126 \t All success : 163 \t mean(l0) : 19.411043 \t Current Learning Rate: 0.012\n",
            "Iteration 15400: Current success : 130 \t All success : 163 \t mean(l0) : 19.300613 \t Current Learning Rate: 0.012\n",
            "Iteration 15500: Current success : 130 \t All success : 163 \t mean(l0) : 19.300613 \t Current Learning Rate: 0.011\n",
            "Iteration 15600: Current success : 126 \t All success : 163 \t mean(l0) : 19.300613 \t Current Learning Rate: 0.011\n",
            "Iteration 15700: Current success : 130 \t All success : 163 \t mean(l0) : 19.300613 \t Current Learning Rate: 0.011\n",
            "Iteration 15800: Current success : 126 \t All success : 163 \t mean(l0) : 19.300613 \t Current Learning Rate: 0.011\n",
            "Iteration 15900: Current success : 130 \t All success : 163 \t mean(l0) : 19.300613 \t Current Learning Rate: 0.010\n",
            "Iteration 16000: Current success : 130 \t All success : 163 \t mean(l0) : 19.300613 \t Current Learning Rate: 0.010\n",
            "Iteration 16100: Current success : 130 \t All success : 163 \t mean(l0) : 19.300613 \t Current Learning Rate: 0.010\n",
            "Iteration 16200: Current success : 125 \t All success : 163 \t mean(l0) : 19.300613 \t Current Learning Rate: 0.010\n",
            "Iteration 16300: Current success : 130 \t All success : 163 \t mean(l0) : 19.300613 \t Current Learning Rate: 0.009\n",
            "Iteration 16400: Current success : 130 \t All success : 163 \t mean(l0) : 19.300613 \t Current Learning Rate: 0.009\n",
            "Iteration 16500: Current success : 130 \t All success : 163 \t mean(l0) : 19.300613 \t Current Learning Rate: 0.009\n",
            "Iteration 16600: Current success : 126 \t All success : 163 \t mean(l0) : 19.300613 \t Current Learning Rate: 0.009\n",
            "Iteration 16700: Current success : 133 \t All success : 163 \t mean(l0) : 19.300613 \t Current Learning Rate: 0.008\n",
            "Iteration 16800: Current success : 129 \t All success : 163 \t mean(l0) : 19.300613 \t Current Learning Rate: 0.008\n",
            "Iteration 16900: Current success : 129 \t All success : 163 \t mean(l0) : 19.300613 \t Current Learning Rate: 0.008\n",
            "Iteration 17000: Current success : 133 \t All success : 163 \t mean(l0) : 19.300613 \t Current Learning Rate: 0.008\n",
            "Iteration 17100: Current success : 133 \t All success : 163 \t mean(l0) : 19.300613 \t Current Learning Rate: 0.007\n",
            "Iteration 17200: Current success : 133 \t All success : 163 \t mean(l0) : 19.300613 \t Current Learning Rate: 0.007\n",
            "Iteration 17300: Current success : 129 \t All success : 163 \t mean(l0) : 19.300613 \t Current Learning Rate: 0.007\n",
            "Iteration 17400: Current success : 133 \t All success : 163 \t mean(l0) : 19.300613 \t Current Learning Rate: 0.007\n",
            "Iteration 17500: Current success : 129 \t All success : 163 \t mean(l0) : 19.300613 \t Current Learning Rate: 0.006\n",
            "Iteration 17600: Current success : 129 \t All success : 163 \t mean(l0) : 19.300613 \t Current Learning Rate: 0.006\n",
            "Iteration 17700: Current success : 133 \t All success : 163 \t mean(l0) : 19.300613 \t Current Learning Rate: 0.006\n",
            "Iteration 17800: Current success : 129 \t All success : 163 \t mean(l0) : 19.300613 \t Current Learning Rate: 0.006\n",
            "Iteration 17900: Current success : 133 \t All success : 163 \t mean(l0) : 19.300613 \t Current Learning Rate: 0.005\n",
            "Iteration 18000: Current success : 133 \t All success : 163 \t mean(l0) : 19.300613 \t Current Learning Rate: 0.005\n",
            "Iteration 18100: Current success : 133 \t All success : 163 \t mean(l0) : 19.300613 \t Current Learning Rate: 0.005\n",
            "Iteration 18200: Current success : 133 \t All success : 163 \t mean(l0) : 19.300613 \t Current Learning Rate: 0.005\n",
            "Iteration 18300: Current success : 133 \t All success : 163 \t mean(l0) : 19.300613 \t Current Learning Rate: 0.004\n",
            "Iteration 18400: Current success : 129 \t All success : 163 \t mean(l0) : 19.300613 \t Current Learning Rate: 0.004\n",
            "Iteration 18500: Current success : 129 \t All success : 163 \t mean(l0) : 19.300613 \t Current Learning Rate: 0.004\n",
            "Iteration 18600: Current success : 133 \t All success : 163 \t mean(l0) : 19.300613 \t Current Learning Rate: 0.004\n",
            "Iteration 18700: Current success : 133 \t All success : 163 \t mean(l0) : 19.300613 \t Current Learning Rate: 0.003\n",
            "Iteration 18800: Current success : 129 \t All success : 163 \t mean(l0) : 19.300613 \t Current Learning Rate: 0.003\n",
            "Iteration 18900: Current success : 133 \t All success : 163 \t mean(l0) : 19.300613 \t Current Learning Rate: 0.003\n",
            "Iteration 19000: Current success : 129 \t All success : 163 \t mean(l0) : 19.300613 \t Current Learning Rate: 0.003\n",
            "Iteration 19100: Current success : 133 \t All success : 163 \t mean(l0) : 19.300613 \t Current Learning Rate: 0.002\n",
            "Iteration 19200: Current success : 129 \t All success : 163 \t mean(l0) : 19.300613 \t Current Learning Rate: 0.002\n",
            "Iteration 19300: Current success : 133 \t All success : 163 \t mean(l0) : 19.300613 \t Current Learning Rate: 0.002\n",
            "Iteration 19400: Current success : 129 \t All success : 163 \t mean(l0) : 19.300613 \t Current Learning Rate: 0.002\n",
            "Iteration 19500: Current success : 133 \t All success : 163 \t mean(l0) : 19.300613 \t Current Learning Rate: 0.001\n",
            "Iteration 19600: Current success : 133 \t All success : 163 \t mean(l0) : 19.300613 \t Current Learning Rate: 0.001\n",
            "Iteration 19700: Current success : 133 \t All success : 163 \t mean(l0) : 19.300613 \t Current Learning Rate: 0.001\n",
            "Iteration 19800: Current success : 133 \t All success : 163 \t mean(l0) : 19.300613 \t Current Learning Rate: 0.001\n",
            "Iteration 19900: Current success : 133 \t All success : 163 \t mean(l0) : 19.300613 \t Current Learning Rate: 0.000\n",
            "Iteration 20000: Current success : 133 \t All success : 163 \t mean(l0) : 19.300613 \t Current Learning Rate: 0.000\n",
            "outer_step 3: all success : 228 \t mean(l0)(success) : 18.2763\n",
            "-------------------------------------------------------------\n",
            "tensor([  55., 1000., 1000.,  ..., 1000., 1000., 1000.], device='cuda:0')\n",
            "Iteration 100: Current success : 57 \t All success : 70 \t mean(l0) : 10.671429 \t Current Learning Rate: 0.098\n",
            "Iteration 200: Current success : 36 \t All success : 81 \t mean(l0) : 12.209877 \t Current Learning Rate: 0.096\n",
            "Iteration 300: Current success : 39 \t All success : 90 \t mean(l0) : 13.855556 \t Current Learning Rate: 0.094\n",
            "Iteration 400: Current success : 43 \t All success : 97 \t mean(l0) : 13.804123 \t Current Learning Rate: 0.092\n",
            "Iteration 500: Current success : 43 \t All success : 99 \t mean(l0) : 14.010101 \t Current Learning Rate: 0.090\n",
            "Iteration 600: Current success : 50 \t All success : 103 \t mean(l0) : 14.514564 \t Current Learning Rate: 0.088\n",
            "Iteration 700: Current success : 54 \t All success : 110 \t mean(l0) : 15.163636 \t Current Learning Rate: 0.086\n",
            "Iteration 800: Current success : 57 \t All success : 115 \t mean(l0) : 16.295650 \t Current Learning Rate: 0.084\n",
            "Iteration 900: Current success : 52 \t All success : 117 \t mean(l0) : 15.905984 \t Current Learning Rate: 0.082\n",
            "Iteration 1000: Current success : 57 \t All success : 117 \t mean(l0) : 15.897437 \t Current Learning Rate: 0.080\n",
            "Iteration 1100: Current success : 61 \t All success : 126 \t mean(l0) : 15.214287 \t Current Learning Rate: 0.078\n",
            "Iteration 1200: Current success : 60 \t All success : 127 \t mean(l0) : 15.102362 \t Current Learning Rate: 0.076\n",
            "Iteration 1300: Current success : 63 \t All success : 130 \t mean(l0) : 14.984615 \t Current Learning Rate: 0.074\n",
            "Iteration 1400: Current success : 68 \t All success : 130 \t mean(l0) : 14.984615 \t Current Learning Rate: 0.072\n",
            "Iteration 1500: Current success : 71 \t All success : 131 \t mean(l0) : 14.893129 \t Current Learning Rate: 0.070\n",
            "Iteration 1600: Current success : 67 \t All success : 131 \t mean(l0) : 14.893129 \t Current Learning Rate: 0.068\n",
            "Iteration 1700: Current success : 63 \t All success : 131 \t mean(l0) : 14.893129 \t Current Learning Rate: 0.066\n",
            "Iteration 1800: Current success : 62 \t All success : 133 \t mean(l0) : 14.729323 \t Current Learning Rate: 0.064\n",
            "Iteration 1900: Current success : 66 \t All success : 133 \t mean(l0) : 14.729323 \t Current Learning Rate: 0.062\n",
            "Iteration 2000: Current success : 70 \t All success : 134 \t mean(l0) : 14.626865 \t Current Learning Rate: 0.060\n",
            "Iteration 2100: Current success : 65 \t All success : 135 \t mean(l0) : 14.533333 \t Current Learning Rate: 0.058\n",
            "Iteration 2200: Current success : 71 \t All success : 135 \t mean(l0) : 14.533333 \t Current Learning Rate: 0.056\n",
            "Iteration 2300: Current success : 73 \t All success : 135 \t mean(l0) : 14.533333 \t Current Learning Rate: 0.054\n",
            "Iteration 2400: Current success : 75 \t All success : 135 \t mean(l0) : 14.533333 \t Current Learning Rate: 0.052\n",
            "Iteration 2500: Current success : 74 \t All success : 135 \t mean(l0) : 14.533333 \t Current Learning Rate: 0.050\n",
            "Iteration 2600: Current success : 75 \t All success : 136 \t mean(l0) : 14.625000 \t Current Learning Rate: 0.048\n",
            "Iteration 2700: Current success : 77 \t All success : 137 \t mean(l0) : 14.846715 \t Current Learning Rate: 0.046\n",
            "Iteration 2800: Current success : 79 \t All success : 137 \t mean(l0) : 14.817518 \t Current Learning Rate: 0.044\n",
            "Iteration 2900: Current success : 77 \t All success : 137 \t mean(l0) : 14.817518 \t Current Learning Rate: 0.042\n",
            "Iteration 3000: Current success : 77 \t All success : 137 \t mean(l0) : 14.817518 \t Current Learning Rate: 0.040\n",
            "Iteration 3100: Current success : 79 \t All success : 137 \t mean(l0) : 14.817518 \t Current Learning Rate: 0.038\n",
            "Iteration 3200: Current success : 81 \t All success : 137 \t mean(l0) : 14.817518 \t Current Learning Rate: 0.036\n",
            "Iteration 3300: Current success : 79 \t All success : 137 \t mean(l0) : 14.817518 \t Current Learning Rate: 0.034\n",
            "Iteration 3400: Current success : 77 \t All success : 137 \t mean(l0) : 14.817518 \t Current Learning Rate: 0.032\n",
            "Iteration 3500: Current success : 78 \t All success : 137 \t mean(l0) : 14.817518 \t Current Learning Rate: 0.030\n",
            "Iteration 3600: Current success : 77 \t All success : 137 \t mean(l0) : 14.817518 \t Current Learning Rate: 0.028\n",
            "Iteration 3700: Current success : 77 \t All success : 137 \t mean(l0) : 14.817518 \t Current Learning Rate: 0.026\n",
            "Iteration 3800: Current success : 77 \t All success : 137 \t mean(l0) : 14.817518 \t Current Learning Rate: 0.024\n",
            "Iteration 3900: Current success : 77 \t All success : 137 \t mean(l0) : 14.817518 \t Current Learning Rate: 0.022\n",
            "Iteration 4000: Current success : 77 \t All success : 137 \t mean(l0) : 14.817518 \t Current Learning Rate: 0.020\n",
            "Iteration 4100: Current success : 78 \t All success : 137 \t mean(l0) : 14.817518 \t Current Learning Rate: 0.018\n",
            "Iteration 4200: Current success : 77 \t All success : 137 \t mean(l0) : 14.817518 \t Current Learning Rate: 0.016\n",
            "Iteration 4300: Current success : 78 \t All success : 137 \t mean(l0) : 14.817518 \t Current Learning Rate: 0.014\n",
            "Iteration 4400: Current success : 77 \t All success : 137 \t mean(l0) : 14.817518 \t Current Learning Rate: 0.012\n",
            "Iteration 4500: Current success : 78 \t All success : 137 \t mean(l0) : 14.817518 \t Current Learning Rate: 0.010\n",
            "Iteration 4600: Current success : 77 \t All success : 137 \t mean(l0) : 14.817518 \t Current Learning Rate: 0.008\n",
            "Iteration 4700: Current success : 77 \t All success : 137 \t mean(l0) : 14.817518 \t Current Learning Rate: 0.006\n",
            "Iteration 4800: Current success : 77 \t All success : 137 \t mean(l0) : 14.817518 \t Current Learning Rate: 0.004\n",
            "Iteration 4900: Current success : 77 \t All success : 137 \t mean(l0) : 14.817518 \t Current Learning Rate: 0.002\n",
            "Iteration 5000: Current success : 77 \t All success : 137 \t mean(l0) : 14.817518 \t Current Learning Rate: 0.000\n",
            "Iteration 5100: Current success : 47 \t All success : 142 \t mean(l0) : 15.112676 \t Current Learning Rate: 0.049\n",
            "Iteration 5200: Current success : 67 \t All success : 146 \t mean(l0) : 15.363013 \t Current Learning Rate: 0.048\n",
            "Iteration 5300: Current success : 76 \t All success : 152 \t mean(l0) : 14.980264 \t Current Learning Rate: 0.047\n",
            "Iteration 5400: Current success : 78 \t All success : 152 \t mean(l0) : 14.980264 \t Current Learning Rate: 0.046\n",
            "Iteration 5500: Current success : 76 \t All success : 153 \t mean(l0) : 15.137255 \t Current Learning Rate: 0.045\n",
            "Iteration 5600: Current success : 79 \t All success : 153 \t mean(l0) : 15.137255 \t Current Learning Rate: 0.044\n",
            "Iteration 5700: Current success : 71 \t All success : 153 \t mean(l0) : 15.137255 \t Current Learning Rate: 0.043\n",
            "Iteration 5800: Current success : 78 \t All success : 153 \t mean(l0) : 15.137255 \t Current Learning Rate: 0.042\n",
            "Iteration 5900: Current success : 76 \t All success : 153 \t mean(l0) : 15.137255 \t Current Learning Rate: 0.041\n",
            "Iteration 6000: Current success : 76 \t All success : 153 \t mean(l0) : 15.137255 \t Current Learning Rate: 0.040\n",
            "Iteration 6100: Current success : 75 \t All success : 153 \t mean(l0) : 15.137255 \t Current Learning Rate: 0.039\n",
            "Iteration 6200: Current success : 84 \t All success : 153 \t mean(l0) : 15.137255 \t Current Learning Rate: 0.038\n",
            "Iteration 6300: Current success : 84 \t All success : 153 \t mean(l0) : 15.137255 \t Current Learning Rate: 0.037\n",
            "Iteration 6400: Current success : 79 \t All success : 153 \t mean(l0) : 15.104575 \t Current Learning Rate: 0.036\n",
            "Iteration 6500: Current success : 82 \t All success : 153 \t mean(l0) : 15.104575 \t Current Learning Rate: 0.035\n",
            "Iteration 6600: Current success : 83 \t All success : 153 \t mean(l0) : 15.104575 \t Current Learning Rate: 0.034\n",
            "Iteration 6700: Current success : 81 \t All success : 153 \t mean(l0) : 15.104575 \t Current Learning Rate: 0.033\n",
            "Iteration 6800: Current success : 85 \t All success : 153 \t mean(l0) : 15.104575 \t Current Learning Rate: 0.032\n",
            "Iteration 6900: Current success : 89 \t All success : 153 \t mean(l0) : 15.104575 \t Current Learning Rate: 0.031\n",
            "Iteration 7000: Current success : 85 \t All success : 153 \t mean(l0) : 15.104575 \t Current Learning Rate: 0.030\n",
            "Iteration 7100: Current success : 83 \t All success : 153 \t mean(l0) : 15.104575 \t Current Learning Rate: 0.029\n",
            "Iteration 7200: Current success : 87 \t All success : 153 \t mean(l0) : 15.104575 \t Current Learning Rate: 0.028\n",
            "Iteration 7300: Current success : 86 \t All success : 153 \t mean(l0) : 15.104575 \t Current Learning Rate: 0.027\n",
            "Iteration 7400: Current success : 86 \t All success : 153 \t mean(l0) : 15.104575 \t Current Learning Rate: 0.026\n",
            "Iteration 7500: Current success : 87 \t All success : 153 \t mean(l0) : 15.104575 \t Current Learning Rate: 0.025\n",
            "Iteration 7600: Current success : 86 \t All success : 153 \t mean(l0) : 15.104575 \t Current Learning Rate: 0.024\n",
            "Iteration 7700: Current success : 86 \t All success : 153 \t mean(l0) : 15.104575 \t Current Learning Rate: 0.023\n",
            "Iteration 7800: Current success : 87 \t All success : 153 \t mean(l0) : 15.104575 \t Current Learning Rate: 0.022\n",
            "Iteration 7900: Current success : 86 \t All success : 153 \t mean(l0) : 15.104575 \t Current Learning Rate: 0.021\n",
            "Iteration 8000: Current success : 87 \t All success : 153 \t mean(l0) : 15.104575 \t Current Learning Rate: 0.020\n",
            "Iteration 8100: Current success : 87 \t All success : 153 \t mean(l0) : 15.104575 \t Current Learning Rate: 0.019\n",
            "Iteration 8200: Current success : 86 \t All success : 153 \t mean(l0) : 15.104575 \t Current Learning Rate: 0.018\n",
            "Iteration 8300: Current success : 86 \t All success : 153 \t mean(l0) : 15.104575 \t Current Learning Rate: 0.017\n",
            "Iteration 8400: Current success : 87 \t All success : 153 \t mean(l0) : 15.104575 \t Current Learning Rate: 0.016\n",
            "Iteration 8500: Current success : 86 \t All success : 153 \t mean(l0) : 15.104575 \t Current Learning Rate: 0.015\n",
            "Iteration 8600: Current success : 86 \t All success : 153 \t mean(l0) : 15.104575 \t Current Learning Rate: 0.014\n",
            "Iteration 8700: Current success : 87 \t All success : 153 \t mean(l0) : 15.104575 \t Current Learning Rate: 0.013\n",
            "Iteration 8800: Current success : 86 \t All success : 153 \t mean(l0) : 15.104575 \t Current Learning Rate: 0.012\n",
            "Iteration 8900: Current success : 87 \t All success : 153 \t mean(l0) : 15.104575 \t Current Learning Rate: 0.011\n",
            "Iteration 9000: Current success : 86 \t All success : 153 \t mean(l0) : 15.104575 \t Current Learning Rate: 0.010\n",
            "Iteration 9100: Current success : 86 \t All success : 153 \t mean(l0) : 15.104575 \t Current Learning Rate: 0.009\n",
            "Iteration 9200: Current success : 86 \t All success : 153 \t mean(l0) : 15.104575 \t Current Learning Rate: 0.008\n",
            "Iteration 9300: Current success : 86 \t All success : 153 \t mean(l0) : 15.104575 \t Current Learning Rate: 0.007\n",
            "Iteration 9400: Current success : 86 \t All success : 153 \t mean(l0) : 15.104575 \t Current Learning Rate: 0.006\n",
            "Iteration 9500: Current success : 86 \t All success : 153 \t mean(l0) : 15.104575 \t Current Learning Rate: 0.005\n",
            "Iteration 9600: Current success : 86 \t All success : 153 \t mean(l0) : 15.104575 \t Current Learning Rate: 0.004\n",
            "Iteration 9700: Current success : 86 \t All success : 153 \t mean(l0) : 15.104575 \t Current Learning Rate: 0.003\n",
            "Iteration 9800: Current success : 86 \t All success : 153 \t mean(l0) : 15.104575 \t Current Learning Rate: 0.002\n",
            "Iteration 9900: Current success : 86 \t All success : 153 \t mean(l0) : 15.104575 \t Current Learning Rate: 0.001\n",
            "Iteration 10000: Current success : 86 \t All success : 153 \t mean(l0) : 15.104575 \t Current Learning Rate: 0.000\n",
            "Iteration 10100: Current success : 67 \t All success : 163 \t mean(l0) : 16.631901 \t Current Learning Rate: 0.025\n",
            "Iteration 10200: Current success : 83 \t All success : 164 \t mean(l0) : 16.567072 \t Current Learning Rate: 0.024\n",
            "Iteration 10300: Current success : 82 \t All success : 164 \t mean(l0) : 16.560974 \t Current Learning Rate: 0.024\n",
            "Iteration 10400: Current success : 82 \t All success : 164 \t mean(l0) : 16.560974 \t Current Learning Rate: 0.023\n",
            "Iteration 10500: Current success : 82 \t All success : 164 \t mean(l0) : 16.554878 \t Current Learning Rate: 0.023\n",
            "Iteration 10600: Current success : 85 \t All success : 164 \t mean(l0) : 16.554878 \t Current Learning Rate: 0.022\n",
            "Iteration 10700: Current success : 84 \t All success : 164 \t mean(l0) : 16.554878 \t Current Learning Rate: 0.022\n",
            "Iteration 10800: Current success : 80 \t All success : 164 \t mean(l0) : 16.371950 \t Current Learning Rate: 0.021\n",
            "Iteration 10900: Current success : 84 \t All success : 164 \t mean(l0) : 16.371950 \t Current Learning Rate: 0.021\n",
            "Iteration 11000: Current success : 83 \t All success : 164 \t mean(l0) : 16.371950 \t Current Learning Rate: 0.020\n",
            "Iteration 11100: Current success : 84 \t All success : 164 \t mean(l0) : 16.371950 \t Current Learning Rate: 0.020\n",
            "Iteration 11200: Current success : 81 \t All success : 164 \t mean(l0) : 16.371950 \t Current Learning Rate: 0.019\n",
            "Iteration 11300: Current success : 83 \t All success : 164 \t mean(l0) : 16.371950 \t Current Learning Rate: 0.019\n",
            "Iteration 11400: Current success : 83 \t All success : 164 \t mean(l0) : 16.371950 \t Current Learning Rate: 0.018\n",
            "Iteration 11500: Current success : 83 \t All success : 164 \t mean(l0) : 16.371950 \t Current Learning Rate: 0.018\n",
            "Iteration 11600: Current success : 85 \t All success : 164 \t mean(l0) : 16.371950 \t Current Learning Rate: 0.017\n",
            "Iteration 11700: Current success : 84 \t All success : 164 \t mean(l0) : 16.371950 \t Current Learning Rate: 0.017\n",
            "Iteration 11800: Current success : 84 \t All success : 164 \t mean(l0) : 16.371950 \t Current Learning Rate: 0.016\n",
            "Iteration 11900: Current success : 85 \t All success : 164 \t mean(l0) : 16.371950 \t Current Learning Rate: 0.016\n",
            "Iteration 12000: Current success : 86 \t All success : 164 \t mean(l0) : 16.371950 \t Current Learning Rate: 0.015\n",
            "Iteration 12100: Current success : 85 \t All success : 164 \t mean(l0) : 16.371950 \t Current Learning Rate: 0.015\n",
            "Iteration 12200: Current success : 86 \t All success : 164 \t mean(l0) : 16.371950 \t Current Learning Rate: 0.014\n",
            "Iteration 12300: Current success : 86 \t All success : 164 \t mean(l0) : 16.371950 \t Current Learning Rate: 0.014\n",
            "Iteration 12400: Current success : 87 \t All success : 164 \t mean(l0) : 16.371950 \t Current Learning Rate: 0.013\n",
            "Iteration 12500: Current success : 87 \t All success : 164 \t mean(l0) : 16.371950 \t Current Learning Rate: 0.013\n",
            "Iteration 12600: Current success : 86 \t All success : 164 \t mean(l0) : 16.371950 \t Current Learning Rate: 0.012\n",
            "Iteration 12700: Current success : 86 \t All success : 164 \t mean(l0) : 16.371950 \t Current Learning Rate: 0.012\n",
            "Iteration 12800: Current success : 86 \t All success : 164 \t mean(l0) : 16.371950 \t Current Learning Rate: 0.011\n",
            "Iteration 12900: Current success : 86 \t All success : 164 \t mean(l0) : 16.371950 \t Current Learning Rate: 0.011\n",
            "Iteration 13000: Current success : 86 \t All success : 164 \t mean(l0) : 16.371950 \t Current Learning Rate: 0.010\n",
            "Iteration 13100: Current success : 86 \t All success : 164 \t mean(l0) : 16.371950 \t Current Learning Rate: 0.010\n",
            "Iteration 13200: Current success : 86 \t All success : 164 \t mean(l0) : 16.371950 \t Current Learning Rate: 0.009\n",
            "Iteration 13300: Current success : 86 \t All success : 164 \t mean(l0) : 16.371950 \t Current Learning Rate: 0.009\n",
            "Iteration 13400: Current success : 86 \t All success : 164 \t mean(l0) : 16.341463 \t Current Learning Rate: 0.008\n",
            "Iteration 13500: Current success : 86 \t All success : 164 \t mean(l0) : 16.298780 \t Current Learning Rate: 0.008\n",
            "Iteration 13600: Current success : 86 \t All success : 164 \t mean(l0) : 16.298780 \t Current Learning Rate: 0.007\n",
            "Iteration 13700: Current success : 86 \t All success : 164 \t mean(l0) : 16.298780 \t Current Learning Rate: 0.007\n",
            "Iteration 13800: Current success : 86 \t All success : 164 \t mean(l0) : 16.298780 \t Current Learning Rate: 0.006\n",
            "Iteration 13900: Current success : 86 \t All success : 164 \t mean(l0) : 16.298780 \t Current Learning Rate: 0.006\n",
            "Iteration 14000: Current success : 86 \t All success : 164 \t mean(l0) : 16.298780 \t Current Learning Rate: 0.005\n",
            "Iteration 14100: Current success : 86 \t All success : 164 \t mean(l0) : 16.292683 \t Current Learning Rate: 0.005\n",
            "Iteration 14200: Current success : 86 \t All success : 164 \t mean(l0) : 16.292683 \t Current Learning Rate: 0.004\n",
            "Iteration 14300: Current success : 86 \t All success : 164 \t mean(l0) : 16.292683 \t Current Learning Rate: 0.004\n",
            "Iteration 14400: Current success : 86 \t All success : 164 \t mean(l0) : 16.292683 \t Current Learning Rate: 0.003\n",
            "Iteration 14500: Current success : 86 \t All success : 164 \t mean(l0) : 16.292683 \t Current Learning Rate: 0.003\n",
            "Iteration 14600: Current success : 86 \t All success : 164 \t mean(l0) : 16.292683 \t Current Learning Rate: 0.002\n",
            "Iteration 14700: Current success : 86 \t All success : 164 \t mean(l0) : 16.292683 \t Current Learning Rate: 0.002\n",
            "Iteration 14800: Current success : 86 \t All success : 164 \t mean(l0) : 16.292683 \t Current Learning Rate: 0.001\n",
            "Iteration 14900: Current success : 86 \t All success : 164 \t mean(l0) : 16.292683 \t Current Learning Rate: 0.001\n",
            "Iteration 15000: Current success : 86 \t All success : 164 \t mean(l0) : 16.292683 \t Current Learning Rate: 0.000\n",
            "Iteration 15100: Current success : 88 \t All success : 167 \t mean(l0) : 16.425150 \t Current Learning Rate: 0.012\n",
            "Iteration 15200: Current success : 86 \t All success : 167 \t mean(l0) : 16.419163 \t Current Learning Rate: 0.012\n",
            "Iteration 15300: Current success : 84 \t All success : 167 \t mean(l0) : 16.419163 \t Current Learning Rate: 0.012\n",
            "Iteration 15400: Current success : 89 \t All success : 167 \t mean(l0) : 16.419163 \t Current Learning Rate: 0.012\n",
            "Iteration 15500: Current success : 86 \t All success : 167 \t mean(l0) : 16.419163 \t Current Learning Rate: 0.011\n",
            "Iteration 15600: Current success : 87 \t All success : 167 \t mean(l0) : 16.407187 \t Current Learning Rate: 0.011\n",
            "Iteration 15700: Current success : 86 \t All success : 167 \t mean(l0) : 16.395210 \t Current Learning Rate: 0.011\n",
            "Iteration 15800: Current success : 87 \t All success : 167 \t mean(l0) : 16.395210 \t Current Learning Rate: 0.011\n",
            "Iteration 15900: Current success : 86 \t All success : 167 \t mean(l0) : 16.395210 \t Current Learning Rate: 0.010\n",
            "Iteration 16000: Current success : 86 \t All success : 167 \t mean(l0) : 16.395210 \t Current Learning Rate: 0.010\n",
            "Iteration 16100: Current success : 86 \t All success : 167 \t mean(l0) : 16.395210 \t Current Learning Rate: 0.010\n",
            "Iteration 16200: Current success : 86 \t All success : 167 \t mean(l0) : 16.395210 \t Current Learning Rate: 0.010\n",
            "Iteration 16300: Current success : 86 \t All success : 167 \t mean(l0) : 16.395210 \t Current Learning Rate: 0.009\n",
            "Iteration 16400: Current success : 86 \t All success : 167 \t mean(l0) : 16.395210 \t Current Learning Rate: 0.009\n",
            "Iteration 16500: Current success : 83 \t All success : 167 \t mean(l0) : 16.395210 \t Current Learning Rate: 0.009\n",
            "Iteration 16600: Current success : 86 \t All success : 167 \t mean(l0) : 16.395210 \t Current Learning Rate: 0.009\n",
            "Iteration 16700: Current success : 88 \t All success : 167 \t mean(l0) : 16.395210 \t Current Learning Rate: 0.008\n",
            "Iteration 16800: Current success : 86 \t All success : 167 \t mean(l0) : 16.395210 \t Current Learning Rate: 0.008\n",
            "Iteration 16900: Current success : 87 \t All success : 167 \t mean(l0) : 16.395210 \t Current Learning Rate: 0.008\n",
            "Iteration 17000: Current success : 87 \t All success : 167 \t mean(l0) : 16.395210 \t Current Learning Rate: 0.008\n",
            "Iteration 17100: Current success : 87 \t All success : 167 \t mean(l0) : 16.395210 \t Current Learning Rate: 0.007\n",
            "Iteration 17200: Current success : 86 \t All success : 167 \t mean(l0) : 16.395210 \t Current Learning Rate: 0.007\n",
            "Iteration 17300: Current success : 84 \t All success : 167 \t mean(l0) : 16.395210 \t Current Learning Rate: 0.007\n",
            "Iteration 17400: Current success : 86 \t All success : 167 \t mean(l0) : 16.395210 \t Current Learning Rate: 0.007\n",
            "Iteration 17500: Current success : 88 \t All success : 167 \t mean(l0) : 16.395210 \t Current Learning Rate: 0.006\n",
            "Iteration 17600: Current success : 86 \t All success : 167 \t mean(l0) : 16.395210 \t Current Learning Rate: 0.006\n",
            "Iteration 17700: Current success : 86 \t All success : 167 \t mean(l0) : 16.395210 \t Current Learning Rate: 0.006\n",
            "Iteration 17800: Current success : 86 \t All success : 167 \t mean(l0) : 16.395210 \t Current Learning Rate: 0.006\n",
            "Iteration 17900: Current success : 87 \t All success : 167 \t mean(l0) : 16.395210 \t Current Learning Rate: 0.005\n",
            "Iteration 18000: Current success : 86 \t All success : 167 \t mean(l0) : 16.395210 \t Current Learning Rate: 0.005\n",
            "Iteration 18100: Current success : 86 \t All success : 167 \t mean(l0) : 16.395210 \t Current Learning Rate: 0.005\n",
            "Iteration 18200: Current success : 86 \t All success : 167 \t mean(l0) : 16.395210 \t Current Learning Rate: 0.005\n",
            "Iteration 18300: Current success : 86 \t All success : 167 \t mean(l0) : 16.395210 \t Current Learning Rate: 0.004\n",
            "Iteration 18400: Current success : 86 \t All success : 167 \t mean(l0) : 16.395210 \t Current Learning Rate: 0.004\n",
            "Iteration 18500: Current success : 86 \t All success : 167 \t mean(l0) : 16.395210 \t Current Learning Rate: 0.004\n",
            "Iteration 18600: Current success : 86 \t All success : 167 \t mean(l0) : 16.395210 \t Current Learning Rate: 0.004\n",
            "Iteration 18700: Current success : 86 \t All success : 167 \t mean(l0) : 16.395210 \t Current Learning Rate: 0.003\n",
            "Iteration 18800: Current success : 86 \t All success : 167 \t mean(l0) : 16.395210 \t Current Learning Rate: 0.003\n",
            "Iteration 18900: Current success : 86 \t All success : 167 \t mean(l0) : 16.395210 \t Current Learning Rate: 0.003\n",
            "Iteration 19000: Current success : 86 \t All success : 167 \t mean(l0) : 16.395210 \t Current Learning Rate: 0.003\n",
            "Iteration 19100: Current success : 86 \t All success : 167 \t mean(l0) : 16.395210 \t Current Learning Rate: 0.002\n",
            "Iteration 19200: Current success : 86 \t All success : 167 \t mean(l0) : 16.395210 \t Current Learning Rate: 0.002\n",
            "Iteration 19300: Current success : 86 \t All success : 167 \t mean(l0) : 16.395210 \t Current Learning Rate: 0.002\n",
            "Iteration 19400: Current success : 86 \t All success : 167 \t mean(l0) : 16.395210 \t Current Learning Rate: 0.002\n",
            "Iteration 19500: Current success : 86 \t All success : 167 \t mean(l0) : 16.395210 \t Current Learning Rate: 0.001\n",
            "Iteration 19600: Current success : 86 \t All success : 167 \t mean(l0) : 16.395210 \t Current Learning Rate: 0.001\n",
            "Iteration 19700: Current success : 86 \t All success : 167 \t mean(l0) : 16.395210 \t Current Learning Rate: 0.001\n",
            "Iteration 19800: Current success : 86 \t All success : 167 \t mean(l0) : 16.395210 \t Current Learning Rate: 0.001\n",
            "Iteration 19900: Current success : 86 \t All success : 167 \t mean(l0) : 16.395210 \t Current Learning Rate: 0.000\n",
            "Iteration 20000: Current success : 86 \t All success : 167 \t mean(l0) : 16.395210 \t Current Learning Rate: 0.000\n",
            "outer_step 4: all success : 253 \t mean(l0)(success) : 20.6166\n",
            "-------------------------------------------------------------\n",
            "tensor([   32.5000, 10000.0000, 10000.0000,  ..., 10000.0000, 10000.0000,\n",
            "        10000.0000], device='cuda:0')\n",
            "Iteration 100: Current success : 25 \t All success : 27 \t mean(l0) : 8.407408 \t Current Learning Rate: 0.098\n",
            "Iteration 200: Current success : 36 \t All success : 41 \t mean(l0) : 14.951219 \t Current Learning Rate: 0.096\n",
            "Iteration 300: Current success : 37 \t All success : 46 \t mean(l0) : 16.891304 \t Current Learning Rate: 0.094\n",
            "Iteration 400: Current success : 35 \t All success : 50 \t mean(l0) : 17.039999 \t Current Learning Rate: 0.092\n",
            "Iteration 500: Current success : 42 \t All success : 56 \t mean(l0) : 17.982143 \t Current Learning Rate: 0.090\n",
            "Iteration 600: Current success : 41 \t All success : 61 \t mean(l0) : 17.000000 \t Current Learning Rate: 0.088\n",
            "Iteration 700: Current success : 39 \t All success : 63 \t mean(l0) : 17.777779 \t Current Learning Rate: 0.086\n",
            "Iteration 800: Current success : 36 \t All success : 65 \t mean(l0) : 17.661539 \t Current Learning Rate: 0.084\n",
            "Iteration 900: Current success : 44 \t All success : 71 \t mean(l0) : 16.591549 \t Current Learning Rate: 0.082\n",
            "Iteration 1000: Current success : 50 \t All success : 75 \t mean(l0) : 17.733334 \t Current Learning Rate: 0.080\n",
            "Iteration 1100: Current success : 50 \t All success : 76 \t mean(l0) : 17.552631 \t Current Learning Rate: 0.078\n",
            "Iteration 1200: Current success : 49 \t All success : 76 \t mean(l0) : 17.552631 \t Current Learning Rate: 0.076\n",
            "Iteration 1300: Current success : 50 \t All success : 78 \t mean(l0) : 17.192308 \t Current Learning Rate: 0.074\n",
            "Iteration 1400: Current success : 51 \t All success : 80 \t mean(l0) : 16.925001 \t Current Learning Rate: 0.072\n",
            "Iteration 1500: Current success : 53 \t All success : 83 \t mean(l0) : 16.433735 \t Current Learning Rate: 0.070\n",
            "Iteration 1600: Current success : 54 \t All success : 83 \t mean(l0) : 16.433735 \t Current Learning Rate: 0.068\n",
            "Iteration 1700: Current success : 58 \t All success : 85 \t mean(l0) : 17.564707 \t Current Learning Rate: 0.066\n",
            "Iteration 1800: Current success : 56 \t All success : 88 \t mean(l0) : 17.056818 \t Current Learning Rate: 0.064\n",
            "Iteration 1900: Current success : 56 \t All success : 88 \t mean(l0) : 17.056818 \t Current Learning Rate: 0.062\n",
            "Iteration 2000: Current success : 59 \t All success : 89 \t mean(l0) : 17.629213 \t Current Learning Rate: 0.060\n",
            "Iteration 2100: Current success : 60 \t All success : 92 \t mean(l0) : 17.315218 \t Current Learning Rate: 0.058\n",
            "Iteration 2200: Current success : 62 \t All success : 92 \t mean(l0) : 17.315218 \t Current Learning Rate: 0.056\n",
            "Iteration 2300: Current success : 60 \t All success : 92 \t mean(l0) : 17.315218 \t Current Learning Rate: 0.054\n",
            "Iteration 2400: Current success : 59 \t All success : 92 \t mean(l0) : 17.315218 \t Current Learning Rate: 0.052\n",
            "Iteration 2500: Current success : 63 \t All success : 92 \t mean(l0) : 17.315218 \t Current Learning Rate: 0.050\n",
            "Iteration 2600: Current success : 63 \t All success : 92 \t mean(l0) : 17.315218 \t Current Learning Rate: 0.048\n",
            "Iteration 2700: Current success : 62 \t All success : 92 \t mean(l0) : 17.315218 \t Current Learning Rate: 0.046\n",
            "Iteration 2800: Current success : 63 \t All success : 92 \t mean(l0) : 17.315218 \t Current Learning Rate: 0.044\n",
            "Iteration 2900: Current success : 62 \t All success : 92 \t mean(l0) : 17.315218 \t Current Learning Rate: 0.042\n",
            "Iteration 3000: Current success : 63 \t All success : 92 \t mean(l0) : 17.315218 \t Current Learning Rate: 0.040\n",
            "Iteration 3100: Current success : 62 \t All success : 92 \t mean(l0) : 17.315218 \t Current Learning Rate: 0.038\n",
            "Iteration 3200: Current success : 63 \t All success : 92 \t mean(l0) : 17.315218 \t Current Learning Rate: 0.036\n",
            "Iteration 3300: Current success : 64 \t All success : 92 \t mean(l0) : 17.315218 \t Current Learning Rate: 0.034\n",
            "Iteration 3400: Current success : 62 \t All success : 92 \t mean(l0) : 17.315218 \t Current Learning Rate: 0.032\n",
            "Iteration 3500: Current success : 63 \t All success : 92 \t mean(l0) : 17.315218 \t Current Learning Rate: 0.030\n",
            "Iteration 3600: Current success : 64 \t All success : 92 \t mean(l0) : 17.315218 \t Current Learning Rate: 0.028\n",
            "Iteration 3700: Current success : 63 \t All success : 92 \t mean(l0) : 17.315218 \t Current Learning Rate: 0.026\n",
            "Iteration 3800: Current success : 64 \t All success : 92 \t mean(l0) : 17.315218 \t Current Learning Rate: 0.024\n",
            "Iteration 3900: Current success : 64 \t All success : 92 \t mean(l0) : 17.315218 \t Current Learning Rate: 0.022\n",
            "Iteration 4000: Current success : 63 \t All success : 92 \t mean(l0) : 17.315218 \t Current Learning Rate: 0.020\n",
            "Iteration 4100: Current success : 64 \t All success : 92 \t mean(l0) : 17.315218 \t Current Learning Rate: 0.018\n",
            "Iteration 4200: Current success : 64 \t All success : 92 \t mean(l0) : 17.315218 \t Current Learning Rate: 0.016\n",
            "Iteration 4300: Current success : 64 \t All success : 92 \t mean(l0) : 17.315218 \t Current Learning Rate: 0.014\n",
            "Iteration 4400: Current success : 64 \t All success : 92 \t mean(l0) : 17.315218 \t Current Learning Rate: 0.012\n",
            "Iteration 4500: Current success : 64 \t All success : 92 \t mean(l0) : 17.315218 \t Current Learning Rate: 0.010\n",
            "Iteration 4600: Current success : 64 \t All success : 92 \t mean(l0) : 17.315218 \t Current Learning Rate: 0.008\n",
            "Iteration 4700: Current success : 64 \t All success : 92 \t mean(l0) : 17.315218 \t Current Learning Rate: 0.006\n",
            "Iteration 4800: Current success : 64 \t All success : 92 \t mean(l0) : 17.315218 \t Current Learning Rate: 0.004\n",
            "Iteration 4900: Current success : 64 \t All success : 92 \t mean(l0) : 17.315218 \t Current Learning Rate: 0.002\n",
            "Iteration 5000: Current success : 64 \t All success : 92 \t mean(l0) : 17.315218 \t Current Learning Rate: 0.000\n",
            "Iteration 5100: Current success : 43 \t All success : 100 \t mean(l0) : 18.119999 \t Current Learning Rate: 0.049\n",
            "Iteration 5200: Current success : 63 \t All success : 107 \t mean(l0) : 18.261683 \t Current Learning Rate: 0.048\n",
            "Iteration 5300: Current success : 70 \t All success : 109 \t mean(l0) : 18.752293 \t Current Learning Rate: 0.047\n",
            "Iteration 5400: Current success : 72 \t All success : 109 \t mean(l0) : 18.733944 \t Current Learning Rate: 0.046\n",
            "Iteration 5500: Current success : 69 \t All success : 110 \t mean(l0) : 18.581818 \t Current Learning Rate: 0.045\n",
            "Iteration 5600: Current success : 71 \t All success : 110 \t mean(l0) : 18.581818 \t Current Learning Rate: 0.044\n",
            "Iteration 5700: Current success : 70 \t All success : 110 \t mean(l0) : 18.581818 \t Current Learning Rate: 0.043\n",
            "Iteration 5800: Current success : 73 \t All success : 110 \t mean(l0) : 18.581818 \t Current Learning Rate: 0.042\n",
            "Iteration 5900: Current success : 71 \t All success : 110 \t mean(l0) : 18.581818 \t Current Learning Rate: 0.041\n",
            "Iteration 6000: Current success : 72 \t All success : 111 \t mean(l0) : 18.459459 \t Current Learning Rate: 0.040\n",
            "Iteration 6100: Current success : 69 \t All success : 111 \t mean(l0) : 18.459459 \t Current Learning Rate: 0.039\n",
            "Iteration 6200: Current success : 71 \t All success : 111 \t mean(l0) : 18.459459 \t Current Learning Rate: 0.038\n",
            "Iteration 6300: Current success : 70 \t All success : 111 \t mean(l0) : 18.459459 \t Current Learning Rate: 0.037\n",
            "Iteration 6400: Current success : 74 \t All success : 111 \t mean(l0) : 18.459459 \t Current Learning Rate: 0.036\n",
            "Iteration 6500: Current success : 72 \t All success : 111 \t mean(l0) : 18.459459 \t Current Learning Rate: 0.035\n",
            "Iteration 6600: Current success : 70 \t All success : 111 \t mean(l0) : 18.459459 \t Current Learning Rate: 0.034\n",
            "Iteration 6700: Current success : 72 \t All success : 111 \t mean(l0) : 18.459459 \t Current Learning Rate: 0.033\n",
            "Iteration 6800: Current success : 104 \t All success : 140 \t mean(l0) : 16.085714 \t Current Learning Rate: 0.032\n",
            "Iteration 6900: Current success : 76 \t All success : 140 \t mean(l0) : 16.085714 \t Current Learning Rate: 0.031\n",
            "Iteration 7000: Current success : 107 \t All success : 140 \t mean(l0) : 16.085714 \t Current Learning Rate: 0.030\n",
            "Iteration 7100: Current success : 106 \t All success : 140 \t mean(l0) : 16.085714 \t Current Learning Rate: 0.029\n",
            "Iteration 7200: Current success : 78 \t All success : 140 \t mean(l0) : 16.085714 \t Current Learning Rate: 0.028\n",
            "Iteration 7300: Current success : 107 \t All success : 140 \t mean(l0) : 16.085714 \t Current Learning Rate: 0.027\n",
            "Iteration 7400: Current success : 74 \t All success : 140 \t mean(l0) : 16.085714 \t Current Learning Rate: 0.026\n",
            "Iteration 7500: Current success : 105 \t All success : 140 \t mean(l0) : 16.085714 \t Current Learning Rate: 0.025\n",
            "Iteration 7600: Current success : 103 \t All success : 140 \t mean(l0) : 16.085714 \t Current Learning Rate: 0.024\n",
            "Iteration 7700: Current success : 75 \t All success : 140 \t mean(l0) : 16.085714 \t Current Learning Rate: 0.023\n",
            "Iteration 7800: Current success : 104 \t All success : 140 \t mean(l0) : 16.085714 \t Current Learning Rate: 0.022\n",
            "Iteration 7900: Current success : 108 \t All success : 140 \t mean(l0) : 16.085714 \t Current Learning Rate: 0.021\n",
            "Iteration 8000: Current success : 108 \t All success : 140 \t mean(l0) : 16.085714 \t Current Learning Rate: 0.020\n",
            "Iteration 8100: Current success : 108 \t All success : 140 \t mean(l0) : 16.085714 \t Current Learning Rate: 0.019\n",
            "Iteration 8200: Current success : 79 \t All success : 140 \t mean(l0) : 16.085714 \t Current Learning Rate: 0.018\n",
            "Iteration 8300: Current success : 108 \t All success : 140 \t mean(l0) : 16.085714 \t Current Learning Rate: 0.017\n",
            "Iteration 8400: Current success : 108 \t All success : 140 \t mean(l0) : 16.085714 \t Current Learning Rate: 0.016\n",
            "Iteration 8500: Current success : 108 \t All success : 140 \t mean(l0) : 16.085714 \t Current Learning Rate: 0.015\n",
            "Iteration 8600: Current success : 104 \t All success : 140 \t mean(l0) : 16.085714 \t Current Learning Rate: 0.014\n",
            "Iteration 8700: Current success : 104 \t All success : 140 \t mean(l0) : 16.085714 \t Current Learning Rate: 0.013\n",
            "Iteration 8800: Current success : 104 \t All success : 140 \t mean(l0) : 16.085714 \t Current Learning Rate: 0.012\n",
            "Iteration 8900: Current success : 75 \t All success : 140 \t mean(l0) : 16.085714 \t Current Learning Rate: 0.011\n",
            "Iteration 9000: Current success : 75 \t All success : 140 \t mean(l0) : 16.085714 \t Current Learning Rate: 0.010\n",
            "Iteration 9100: Current success : 104 \t All success : 140 \t mean(l0) : 16.085714 \t Current Learning Rate: 0.009\n",
            "Iteration 9200: Current success : 104 \t All success : 140 \t mean(l0) : 16.085714 \t Current Learning Rate: 0.008\n",
            "Iteration 9300: Current success : 75 \t All success : 140 \t mean(l0) : 16.085714 \t Current Learning Rate: 0.007\n",
            "Iteration 9400: Current success : 104 \t All success : 140 \t mean(l0) : 16.085714 \t Current Learning Rate: 0.006\n",
            "Iteration 9500: Current success : 104 \t All success : 140 \t mean(l0) : 16.085714 \t Current Learning Rate: 0.005\n",
            "Iteration 9600: Current success : 104 \t All success : 140 \t mean(l0) : 16.085714 \t Current Learning Rate: 0.004\n",
            "Iteration 9700: Current success : 104 \t All success : 140 \t mean(l0) : 16.085714 \t Current Learning Rate: 0.003\n",
            "Iteration 9800: Current success : 104 \t All success : 140 \t mean(l0) : 16.085714 \t Current Learning Rate: 0.002\n",
            "Iteration 9900: Current success : 104 \t All success : 140 \t mean(l0) : 16.085714 \t Current Learning Rate: 0.001\n",
            "Iteration 10000: Current success : 104 \t All success : 140 \t mean(l0) : 16.085714 \t Current Learning Rate: 0.000\n",
            "Iteration 10100: Current success : 59 \t All success : 152 \t mean(l0) : 16.611843 \t Current Learning Rate: 0.025\n",
            "Iteration 10200: Current success : 75 \t All success : 154 \t mean(l0) : 16.448051 \t Current Learning Rate: 0.024\n",
            "Iteration 10300: Current success : 75 \t All success : 154 \t mean(l0) : 16.441559 \t Current Learning Rate: 0.024\n",
            "Iteration 10400: Current success : 72 \t All success : 154 \t mean(l0) : 16.441559 \t Current Learning Rate: 0.023\n",
            "Iteration 10500: Current success : 76 \t All success : 154 \t mean(l0) : 16.441559 \t Current Learning Rate: 0.023\n",
            "Iteration 10600: Current success : 77 \t All success : 154 \t mean(l0) : 16.441559 \t Current Learning Rate: 0.022\n",
            "Iteration 10700: Current success : 78 \t All success : 154 \t mean(l0) : 16.422077 \t Current Learning Rate: 0.022\n",
            "Iteration 10800: Current success : 77 \t All success : 154 \t mean(l0) : 16.422077 \t Current Learning Rate: 0.021\n",
            "Iteration 10900: Current success : 79 \t All success : 158 \t mean(l0) : 17.525316 \t Current Learning Rate: 0.021\n",
            "Iteration 11000: Current success : 82 \t All success : 158 \t mean(l0) : 17.525316 \t Current Learning Rate: 0.020\n",
            "Iteration 11100: Current success : 77 \t All success : 158 \t mean(l0) : 17.525316 \t Current Learning Rate: 0.020\n",
            "Iteration 11200: Current success : 78 \t All success : 158 \t mean(l0) : 17.525316 \t Current Learning Rate: 0.019\n",
            "Iteration 11300: Current success : 79 \t All success : 158 \t mean(l0) : 17.525316 \t Current Learning Rate: 0.019\n",
            "Iteration 11400: Current success : 83 \t All success : 158 \t mean(l0) : 17.474684 \t Current Learning Rate: 0.018\n",
            "Iteration 11500: Current success : 76 \t All success : 158 \t mean(l0) : 17.474684 \t Current Learning Rate: 0.018\n",
            "Iteration 11600: Current success : 82 \t All success : 159 \t mean(l0) : 17.358490 \t Current Learning Rate: 0.017\n",
            "Iteration 11700: Current success : 79 \t All success : 159 \t mean(l0) : 17.358490 \t Current Learning Rate: 0.017\n",
            "Iteration 11800: Current success : 78 \t All success : 159 \t mean(l0) : 17.358490 \t Current Learning Rate: 0.016\n",
            "Iteration 11900: Current success : 79 \t All success : 159 \t mean(l0) : 17.358490 \t Current Learning Rate: 0.016\n",
            "Iteration 12000: Current success : 83 \t All success : 159 \t mean(l0) : 17.358490 \t Current Learning Rate: 0.015\n",
            "Iteration 12100: Current success : 83 \t All success : 159 \t mean(l0) : 17.358490 \t Current Learning Rate: 0.015\n",
            "Iteration 12200: Current success : 83 \t All success : 159 \t mean(l0) : 17.358490 \t Current Learning Rate: 0.014\n",
            "Iteration 12300: Current success : 82 \t All success : 159 \t mean(l0) : 17.358490 \t Current Learning Rate: 0.014\n",
            "Iteration 12400: Current success : 83 \t All success : 159 \t mean(l0) : 17.358490 \t Current Learning Rate: 0.013\n",
            "Iteration 12500: Current success : 83 \t All success : 159 \t mean(l0) : 17.358490 \t Current Learning Rate: 0.013\n",
            "Iteration 12600: Current success : 83 \t All success : 159 \t mean(l0) : 17.352201 \t Current Learning Rate: 0.012\n",
            "Iteration 12700: Current success : 85 \t All success : 159 \t mean(l0) : 17.352201 \t Current Learning Rate: 0.012\n",
            "Iteration 12800: Current success : 85 \t All success : 159 \t mean(l0) : 17.352201 \t Current Learning Rate: 0.011\n",
            "Iteration 12900: Current success : 85 \t All success : 159 \t mean(l0) : 17.352201 \t Current Learning Rate: 0.011\n",
            "Iteration 13000: Current success : 81 \t All success : 159 \t mean(l0) : 17.352201 \t Current Learning Rate: 0.010\n",
            "Iteration 13100: Current success : 81 \t All success : 159 \t mean(l0) : 17.352201 \t Current Learning Rate: 0.010\n",
            "Iteration 13200: Current success : 81 \t All success : 159 \t mean(l0) : 17.352201 \t Current Learning Rate: 0.009\n",
            "Iteration 13300: Current success : 81 \t All success : 159 \t mean(l0) : 17.339622 \t Current Learning Rate: 0.009\n",
            "Iteration 13400: Current success : 85 \t All success : 159 \t mean(l0) : 17.339622 \t Current Learning Rate: 0.008\n",
            "Iteration 13500: Current success : 81 \t All success : 159 \t mean(l0) : 17.339622 \t Current Learning Rate: 0.008\n",
            "Iteration 13600: Current success : 81 \t All success : 159 \t mean(l0) : 17.339622 \t Current Learning Rate: 0.007\n",
            "Iteration 13700: Current success : 80 \t All success : 159 \t mean(l0) : 17.339622 \t Current Learning Rate: 0.007\n",
            "Iteration 13800: Current success : 81 \t All success : 159 \t mean(l0) : 17.339622 \t Current Learning Rate: 0.006\n",
            "Iteration 13900: Current success : 81 \t All success : 159 \t mean(l0) : 17.339622 \t Current Learning Rate: 0.006\n",
            "Iteration 14000: Current success : 85 \t All success : 159 \t mean(l0) : 17.339622 \t Current Learning Rate: 0.005\n",
            "Iteration 14100: Current success : 85 \t All success : 159 \t mean(l0) : 17.339622 \t Current Learning Rate: 0.005\n",
            "Iteration 14200: Current success : 81 \t All success : 159 \t mean(l0) : 17.339622 \t Current Learning Rate: 0.004\n",
            "Iteration 14300: Current success : 81 \t All success : 159 \t mean(l0) : 17.339622 \t Current Learning Rate: 0.004\n",
            "Iteration 14400: Current success : 81 \t All success : 159 \t mean(l0) : 17.339622 \t Current Learning Rate: 0.003\n",
            "Iteration 14500: Current success : 81 \t All success : 159 \t mean(l0) : 17.339622 \t Current Learning Rate: 0.003\n",
            "Iteration 14600: Current success : 81 \t All success : 159 \t mean(l0) : 17.339622 \t Current Learning Rate: 0.002\n",
            "Iteration 14700: Current success : 81 \t All success : 159 \t mean(l0) : 17.339622 \t Current Learning Rate: 0.002\n",
            "Iteration 14800: Current success : 81 \t All success : 159 \t mean(l0) : 17.339622 \t Current Learning Rate: 0.001\n",
            "Iteration 14900: Current success : 81 \t All success : 159 \t mean(l0) : 17.339622 \t Current Learning Rate: 0.001\n",
            "Iteration 15000: Current success : 81 \t All success : 159 \t mean(l0) : 17.339622 \t Current Learning Rate: 0.000\n",
            "Iteration 15100: Current success : 80 \t All success : 161 \t mean(l0) : 17.062113 \t Current Learning Rate: 0.012\n",
            "Iteration 15200: Current success : 87 \t All success : 161 \t mean(l0) : 17.000000 \t Current Learning Rate: 0.012\n",
            "Iteration 15300: Current success : 82 \t All success : 161 \t mean(l0) : 17.000000 \t Current Learning Rate: 0.012\n",
            "Iteration 15400: Current success : 82 \t All success : 161 \t mean(l0) : 16.993790 \t Current Learning Rate: 0.012\n",
            "Iteration 15500: Current success : 83 \t All success : 161 \t mean(l0) : 16.993790 \t Current Learning Rate: 0.011\n",
            "Iteration 15600: Current success : 83 \t All success : 162 \t mean(l0) : 17.141975 \t Current Learning Rate: 0.011\n",
            "Iteration 15700: Current success : 83 \t All success : 164 \t mean(l0) : 17.298780 \t Current Learning Rate: 0.011\n",
            "Iteration 15800: Current success : 84 \t All success : 164 \t mean(l0) : 17.298780 \t Current Learning Rate: 0.011\n",
            "Iteration 15900: Current success : 83 \t All success : 164 \t mean(l0) : 17.298780 \t Current Learning Rate: 0.010\n",
            "Iteration 16000: Current success : 84 \t All success : 164 \t mean(l0) : 17.298780 \t Current Learning Rate: 0.010\n",
            "Iteration 16100: Current success : 83 \t All success : 164 \t mean(l0) : 17.292683 \t Current Learning Rate: 0.010\n",
            "Iteration 16200: Current success : 84 \t All success : 164 \t mean(l0) : 17.292683 \t Current Learning Rate: 0.010\n",
            "Iteration 16300: Current success : 84 \t All success : 164 \t mean(l0) : 17.292683 \t Current Learning Rate: 0.009\n",
            "Iteration 16400: Current success : 83 \t All success : 164 \t mean(l0) : 17.292683 \t Current Learning Rate: 0.009\n",
            "Iteration 16500: Current success : 84 \t All success : 164 \t mean(l0) : 17.292683 \t Current Learning Rate: 0.009\n",
            "Iteration 16600: Current success : 83 \t All success : 164 \t mean(l0) : 17.292683 \t Current Learning Rate: 0.009\n",
            "Iteration 16700: Current success : 84 \t All success : 164 \t mean(l0) : 17.292683 \t Current Learning Rate: 0.008\n",
            "Iteration 16800: Current success : 84 \t All success : 164 \t mean(l0) : 17.292683 \t Current Learning Rate: 0.008\n",
            "Iteration 16900: Current success : 84 \t All success : 164 \t mean(l0) : 17.292683 \t Current Learning Rate: 0.008\n",
            "Iteration 17000: Current success : 82 \t All success : 164 \t mean(l0) : 17.292683 \t Current Learning Rate: 0.008\n",
            "Iteration 17100: Current success : 84 \t All success : 164 \t mean(l0) : 17.292683 \t Current Learning Rate: 0.007\n",
            "Iteration 17200: Current success : 84 \t All success : 164 \t mean(l0) : 17.292683 \t Current Learning Rate: 0.007\n",
            "Iteration 17300: Current success : 84 \t All success : 165 \t mean(l0) : 17.278788 \t Current Learning Rate: 0.007\n",
            "Iteration 17400: Current success : 84 \t All success : 165 \t mean(l0) : 17.278788 \t Current Learning Rate: 0.007\n",
            "Iteration 17500: Current success : 83 \t All success : 165 \t mean(l0) : 17.278788 \t Current Learning Rate: 0.006\n",
            "Iteration 17600: Current success : 83 \t All success : 165 \t mean(l0) : 17.278788 \t Current Learning Rate: 0.006\n",
            "Iteration 17700: Current success : 84 \t All success : 165 \t mean(l0) : 17.278788 \t Current Learning Rate: 0.006\n",
            "Iteration 17800: Current success : 84 \t All success : 165 \t mean(l0) : 17.278788 \t Current Learning Rate: 0.006\n",
            "Iteration 17900: Current success : 84 \t All success : 165 \t mean(l0) : 17.278788 \t Current Learning Rate: 0.005\n",
            "Iteration 18000: Current success : 84 \t All success : 165 \t mean(l0) : 17.278788 \t Current Learning Rate: 0.005\n",
            "Iteration 18100: Current success : 84 \t All success : 165 \t mean(l0) : 17.278788 \t Current Learning Rate: 0.005\n",
            "Iteration 18200: Current success : 84 \t All success : 165 \t mean(l0) : 17.278788 \t Current Learning Rate: 0.005\n",
            "Iteration 18300: Current success : 84 \t All success : 165 \t mean(l0) : 17.278788 \t Current Learning Rate: 0.004\n",
            "Iteration 18400: Current success : 84 \t All success : 165 \t mean(l0) : 17.278788 \t Current Learning Rate: 0.004\n",
            "Iteration 18500: Current success : 84 \t All success : 165 \t mean(l0) : 17.278788 \t Current Learning Rate: 0.004\n",
            "Iteration 18600: Current success : 84 \t All success : 165 \t mean(l0) : 17.278788 \t Current Learning Rate: 0.004\n",
            "Iteration 18700: Current success : 84 \t All success : 165 \t mean(l0) : 17.278788 \t Current Learning Rate: 0.003\n",
            "Iteration 18800: Current success : 84 \t All success : 165 \t mean(l0) : 17.278788 \t Current Learning Rate: 0.003\n",
            "Iteration 18900: Current success : 84 \t All success : 165 \t mean(l0) : 17.278788 \t Current Learning Rate: 0.003\n",
            "Iteration 19000: Current success : 84 \t All success : 165 \t mean(l0) : 17.278788 \t Current Learning Rate: 0.003\n",
            "Iteration 19100: Current success : 84 \t All success : 165 \t mean(l0) : 17.278788 \t Current Learning Rate: 0.002\n",
            "Iteration 19200: Current success : 84 \t All success : 165 \t mean(l0) : 17.278788 \t Current Learning Rate: 0.002\n",
            "Iteration 19300: Current success : 84 \t All success : 165 \t mean(l0) : 17.278788 \t Current Learning Rate: 0.002\n",
            "Iteration 19400: Current success : 84 \t All success : 165 \t mean(l0) : 17.278788 \t Current Learning Rate: 0.002\n",
            "Iteration 19500: Current success : 84 \t All success : 165 \t mean(l0) : 17.278788 \t Current Learning Rate: 0.001\n",
            "Iteration 19600: Current success : 84 \t All success : 165 \t mean(l0) : 17.278788 \t Current Learning Rate: 0.001\n",
            "Iteration 19700: Current success : 84 \t All success : 165 \t mean(l0) : 17.278788 \t Current Learning Rate: 0.001\n",
            "Iteration 19800: Current success : 84 \t All success : 165 \t mean(l0) : 17.278788 \t Current Learning Rate: 0.001\n",
            "Iteration 19900: Current success : 84 \t All success : 165 \t mean(l0) : 17.278788 \t Current Learning Rate: 0.000\n",
            "Iteration 20000: Current success : 84 \t All success : 165 \t mean(l0) : 17.278788 \t Current Learning Rate: 0.000\n",
            "outer_step 5: all success : 263 \t mean(l0)(success) : 21.0418\n",
            "-------------------------------------------------------------\n",
            "tensor([2.1250e+01, 1.0000e+05, 1.0000e+05,  ..., 1.0000e+05, 1.0000e+05,\n",
            "        1.0000e+05], device='cuda:0')\n",
            "Iteration 100: Current success : 49 \t All success : 55 \t mean(l0) : 6.036364 \t Current Learning Rate: 0.098\n",
            "Iteration 200: Current success : 62 \t All success : 69 \t mean(l0) : 9.376812 \t Current Learning Rate: 0.096\n",
            "Iteration 300: Current success : 62 \t All success : 74 \t mean(l0) : 10.432433 \t Current Learning Rate: 0.094\n",
            "Iteration 400: Current success : 63 \t All success : 76 \t mean(l0) : 11.184211 \t Current Learning Rate: 0.092\n",
            "Iteration 500: Current success : 66 \t All success : 79 \t mean(l0) : 12.037975 \t Current Learning Rate: 0.090\n",
            "Iteration 600: Current success : 66 \t All success : 81 \t mean(l0) : 12.086420 \t Current Learning Rate: 0.088\n",
            "Iteration 700: Current success : 66 \t All success : 83 \t mean(l0) : 11.867470 \t Current Learning Rate: 0.086\n",
            "Iteration 800: Current success : 69 \t All success : 85 \t mean(l0) : 11.658824 \t Current Learning Rate: 0.084\n",
            "Iteration 900: Current success : 72 \t All success : 86 \t mean(l0) : 12.034883 \t Current Learning Rate: 0.082\n",
            "Iteration 1000: Current success : 74 \t All success : 90 \t mean(l0) : 12.500000 \t Current Learning Rate: 0.080\n",
            "Iteration 1100: Current success : 72 \t All success : 94 \t mean(l0) : 12.117021 \t Current Learning Rate: 0.078\n",
            "Iteration 1200: Current success : 75 \t All success : 97 \t mean(l0) : 11.824742 \t Current Learning Rate: 0.076\n",
            "Iteration 1300: Current success : 76 \t All success : 97 \t mean(l0) : 11.824742 \t Current Learning Rate: 0.074\n",
            "Iteration 1400: Current success : 78 \t All success : 98 \t mean(l0) : 11.714286 \t Current Learning Rate: 0.072\n",
            "Iteration 1500: Current success : 78 \t All success : 100 \t mean(l0) : 11.790000 \t Current Learning Rate: 0.070\n",
            "Iteration 1600: Current success : 77 \t All success : 101 \t mean(l0) : 11.702971 \t Current Learning Rate: 0.068\n",
            "Iteration 1700: Current success : 75 \t All success : 103 \t mean(l0) : 11.514564 \t Current Learning Rate: 0.066\n",
            "Iteration 1800: Current success : 75 \t All success : 104 \t mean(l0) : 11.432693 \t Current Learning Rate: 0.064\n",
            "Iteration 1900: Current success : 79 \t All success : 104 \t mean(l0) : 11.432693 \t Current Learning Rate: 0.062\n",
            "Iteration 2000: Current success : 81 \t All success : 106 \t mean(l0) : 11.292453 \t Current Learning Rate: 0.060\n",
            "Iteration 2100: Current success : 79 \t All success : 106 \t mean(l0) : 11.292453 \t Current Learning Rate: 0.058\n",
            "Iteration 2200: Current success : 80 \t All success : 106 \t mean(l0) : 11.292453 \t Current Learning Rate: 0.056\n",
            "Iteration 2300: Current success : 80 \t All success : 106 \t mean(l0) : 11.292453 \t Current Learning Rate: 0.054\n",
            "Iteration 2400: Current success : 84 \t All success : 106 \t mean(l0) : 11.292453 \t Current Learning Rate: 0.052\n",
            "Iteration 2500: Current success : 85 \t All success : 106 \t mean(l0) : 11.292453 \t Current Learning Rate: 0.050\n",
            "Iteration 2600: Current success : 86 \t All success : 106 \t mean(l0) : 11.292453 \t Current Learning Rate: 0.048\n",
            "Iteration 2700: Current success : 85 \t All success : 107 \t mean(l0) : 11.261682 \t Current Learning Rate: 0.046\n",
            "Iteration 2800: Current success : 87 \t All success : 107 \t mean(l0) : 11.261682 \t Current Learning Rate: 0.044\n",
            "Iteration 2900: Current success : 86 \t All success : 107 \t mean(l0) : 11.261682 \t Current Learning Rate: 0.042\n",
            "Iteration 3000: Current success : 87 \t All success : 107 \t mean(l0) : 11.261682 \t Current Learning Rate: 0.040\n",
            "Iteration 3100: Current success : 87 \t All success : 107 \t mean(l0) : 11.261682 \t Current Learning Rate: 0.038\n",
            "Iteration 3200: Current success : 87 \t All success : 107 \t mean(l0) : 11.261682 \t Current Learning Rate: 0.036\n",
            "Iteration 3300: Current success : 87 \t All success : 107 \t mean(l0) : 11.261682 \t Current Learning Rate: 0.034\n",
            "Iteration 3400: Current success : 87 \t All success : 108 \t mean(l0) : 11.250000 \t Current Learning Rate: 0.032\n",
            "Iteration 3500: Current success : 87 \t All success : 108 \t mean(l0) : 11.250000 \t Current Learning Rate: 0.030\n",
            "Iteration 3600: Current success : 86 \t All success : 108 \t mean(l0) : 11.250000 \t Current Learning Rate: 0.028\n",
            "Iteration 3700: Current success : 86 \t All success : 108 \t mean(l0) : 11.250000 \t Current Learning Rate: 0.026\n",
            "Iteration 3800: Current success : 87 \t All success : 108 \t mean(l0) : 11.250000 \t Current Learning Rate: 0.024\n",
            "Iteration 3900: Current success : 87 \t All success : 108 \t mean(l0) : 11.250000 \t Current Learning Rate: 0.022\n",
            "Iteration 4000: Current success : 87 \t All success : 108 \t mean(l0) : 11.250000 \t Current Learning Rate: 0.020\n",
            "Iteration 4100: Current success : 87 \t All success : 108 \t mean(l0) : 11.250000 \t Current Learning Rate: 0.018\n",
            "Iteration 4200: Current success : 87 \t All success : 108 \t mean(l0) : 11.250000 \t Current Learning Rate: 0.016\n",
            "Iteration 4300: Current success : 87 \t All success : 108 \t mean(l0) : 11.250000 \t Current Learning Rate: 0.014\n",
            "Iteration 4400: Current success : 87 \t All success : 108 \t mean(l0) : 11.250000 \t Current Learning Rate: 0.012\n",
            "Iteration 4500: Current success : 87 \t All success : 108 \t mean(l0) : 11.250000 \t Current Learning Rate: 0.010\n",
            "Iteration 4600: Current success : 87 \t All success : 108 \t mean(l0) : 11.250000 \t Current Learning Rate: 0.008\n",
            "Iteration 4700: Current success : 87 \t All success : 108 \t mean(l0) : 11.250000 \t Current Learning Rate: 0.006\n",
            "Iteration 4800: Current success : 87 \t All success : 108 \t mean(l0) : 11.250000 \t Current Learning Rate: 0.004\n",
            "Iteration 4900: Current success : 87 \t All success : 108 \t mean(l0) : 11.250000 \t Current Learning Rate: 0.002\n",
            "Iteration 5000: Current success : 87 \t All success : 108 \t mean(l0) : 11.250000 \t Current Learning Rate: 0.000\n",
            "Iteration 5100: Current success : 77 \t All success : 124 \t mean(l0) : 16.806452 \t Current Learning Rate: 0.049\n",
            "Iteration 5200: Current success : 106 \t All success : 141 \t mean(l0) : 16.262411 \t Current Learning Rate: 0.048\n",
            "Iteration 5300: Current success : 108 \t All success : 142 \t mean(l0) : 16.183098 \t Current Learning Rate: 0.047\n",
            "Iteration 5400: Current success : 103 \t All success : 142 \t mean(l0) : 16.183098 \t Current Learning Rate: 0.046\n",
            "Iteration 5500: Current success : 104 \t All success : 142 \t mean(l0) : 16.183098 \t Current Learning Rate: 0.045\n",
            "Iteration 5600: Current success : 107 \t All success : 143 \t mean(l0) : 16.265734 \t Current Learning Rate: 0.044\n",
            "Iteration 5700: Current success : 107 \t All success : 144 \t mean(l0) : 16.576389 \t Current Learning Rate: 0.043\n",
            "Iteration 5800: Current success : 104 \t All success : 144 \t mean(l0) : 16.576389 \t Current Learning Rate: 0.042\n",
            "Iteration 5900: Current success : 108 \t All success : 144 \t mean(l0) : 16.576389 \t Current Learning Rate: 0.041\n",
            "Iteration 6000: Current success : 104 \t All success : 144 \t mean(l0) : 16.576389 \t Current Learning Rate: 0.040\n",
            "Iteration 6100: Current success : 105 \t All success : 144 \t mean(l0) : 16.576389 \t Current Learning Rate: 0.039\n",
            "Iteration 6200: Current success : 106 \t All success : 144 \t mean(l0) : 16.576389 \t Current Learning Rate: 0.038\n",
            "Iteration 6300: Current success : 108 \t All success : 144 \t mean(l0) : 16.576389 \t Current Learning Rate: 0.037\n",
            "Iteration 6400: Current success : 107 \t All success : 144 \t mean(l0) : 16.576389 \t Current Learning Rate: 0.036\n",
            "Iteration 6500: Current success : 106 \t All success : 144 \t mean(l0) : 16.576389 \t Current Learning Rate: 0.035\n",
            "Iteration 6600: Current success : 108 \t All success : 144 \t mean(l0) : 16.576389 \t Current Learning Rate: 0.034\n",
            "Iteration 6700: Current success : 104 \t All success : 144 \t mean(l0) : 16.576389 \t Current Learning Rate: 0.033\n",
            "Iteration 6800: Current success : 107 \t All success : 144 \t mean(l0) : 16.576389 \t Current Learning Rate: 0.032\n",
            "Iteration 6900: Current success : 108 \t All success : 144 \t mean(l0) : 16.576389 \t Current Learning Rate: 0.031\n",
            "Iteration 7000: Current success : 107 \t All success : 144 \t mean(l0) : 16.576389 \t Current Learning Rate: 0.030\n",
            "Iteration 7100: Current success : 108 \t All success : 144 \t mean(l0) : 16.576389 \t Current Learning Rate: 0.029\n",
            "Iteration 7200: Current success : 109 \t All success : 144 \t mean(l0) : 16.576389 \t Current Learning Rate: 0.028\n",
            "Iteration 7300: Current success : 108 \t All success : 144 \t mean(l0) : 16.576389 \t Current Learning Rate: 0.027\n",
            "Iteration 7400: Current success : 110 \t All success : 144 \t mean(l0) : 16.576389 \t Current Learning Rate: 0.026\n",
            "Iteration 7500: Current success : 108 \t All success : 144 \t mean(l0) : 16.576389 \t Current Learning Rate: 0.025\n",
            "Iteration 7600: Current success : 110 \t All success : 144 \t mean(l0) : 16.576389 \t Current Learning Rate: 0.024\n",
            "Iteration 7700: Current success : 110 \t All success : 144 \t mean(l0) : 16.576389 \t Current Learning Rate: 0.023\n",
            "Iteration 7800: Current success : 110 \t All success : 144 \t mean(l0) : 16.576389 \t Current Learning Rate: 0.022\n",
            "Iteration 7900: Current success : 110 \t All success : 144 \t mean(l0) : 16.576389 \t Current Learning Rate: 0.021\n",
            "Iteration 8000: Current success : 110 \t All success : 144 \t mean(l0) : 16.576389 \t Current Learning Rate: 0.020\n",
            "Iteration 8100: Current success : 110 \t All success : 144 \t mean(l0) : 16.576389 \t Current Learning Rate: 0.019\n",
            "Iteration 8200: Current success : 110 \t All success : 144 \t mean(l0) : 16.576389 \t Current Learning Rate: 0.018\n",
            "Iteration 8300: Current success : 110 \t All success : 144 \t mean(l0) : 16.576389 \t Current Learning Rate: 0.017\n",
            "Iteration 8400: Current success : 110 \t All success : 144 \t mean(l0) : 16.576389 \t Current Learning Rate: 0.016\n",
            "Iteration 8500: Current success : 110 \t All success : 144 \t mean(l0) : 16.576389 \t Current Learning Rate: 0.015\n",
            "Iteration 8600: Current success : 110 \t All success : 144 \t mean(l0) : 16.576389 \t Current Learning Rate: 0.014\n",
            "Iteration 8700: Current success : 110 \t All success : 144 \t mean(l0) : 16.576389 \t Current Learning Rate: 0.013\n",
            "Iteration 8800: Current success : 110 \t All success : 144 \t mean(l0) : 16.576389 \t Current Learning Rate: 0.012\n",
            "Iteration 8900: Current success : 110 \t All success : 144 \t mean(l0) : 16.576389 \t Current Learning Rate: 0.011\n",
            "Iteration 9000: Current success : 110 \t All success : 144 \t mean(l0) : 16.576389 \t Current Learning Rate: 0.010\n",
            "Iteration 9100: Current success : 110 \t All success : 144 \t mean(l0) : 16.576389 \t Current Learning Rate: 0.009\n",
            "Iteration 9200: Current success : 110 \t All success : 144 \t mean(l0) : 16.576389 \t Current Learning Rate: 0.008\n",
            "Iteration 9300: Current success : 110 \t All success : 144 \t mean(l0) : 16.576389 \t Current Learning Rate: 0.007\n",
            "Iteration 9400: Current success : 110 \t All success : 144 \t mean(l0) : 16.576389 \t Current Learning Rate: 0.006\n",
            "Iteration 9500: Current success : 110 \t All success : 144 \t mean(l0) : 16.576389 \t Current Learning Rate: 0.005\n",
            "Iteration 9600: Current success : 110 \t All success : 144 \t mean(l0) : 16.576389 \t Current Learning Rate: 0.004\n",
            "Iteration 9700: Current success : 110 \t All success : 144 \t mean(l0) : 16.576389 \t Current Learning Rate: 0.003\n",
            "Iteration 9800: Current success : 110 \t All success : 144 \t mean(l0) : 16.576389 \t Current Learning Rate: 0.002\n",
            "Iteration 9900: Current success : 110 \t All success : 144 \t mean(l0) : 16.576389 \t Current Learning Rate: 0.001\n",
            "Iteration 10000: Current success : 110 \t All success : 144 \t mean(l0) : 16.576389 \t Current Learning Rate: 0.000\n",
            "Iteration 10100: Current success : 63 \t All success : 151 \t mean(l0) : 17.364239 \t Current Learning Rate: 0.025\n",
            "Iteration 10200: Current success : 70 \t All success : 151 \t mean(l0) : 17.337748 \t Current Learning Rate: 0.024\n",
            "Iteration 10300: Current success : 73 \t All success : 152 \t mean(l0) : 17.302631 \t Current Learning Rate: 0.024\n",
            "Iteration 10400: Current success : 73 \t All success : 152 \t mean(l0) : 17.302631 \t Current Learning Rate: 0.023\n",
            "Iteration 10500: Current success : 74 \t All success : 152 \t mean(l0) : 17.302631 \t Current Learning Rate: 0.023\n",
            "Iteration 10600: Current success : 75 \t All success : 152 \t mean(l0) : 17.302631 \t Current Learning Rate: 0.022\n",
            "Iteration 10700: Current success : 76 \t All success : 152 \t mean(l0) : 17.302631 \t Current Learning Rate: 0.022\n",
            "Iteration 10800: Current success : 74 \t All success : 152 \t mean(l0) : 17.302631 \t Current Learning Rate: 0.021\n",
            "Iteration 10900: Current success : 70 \t All success : 152 \t mean(l0) : 17.302631 \t Current Learning Rate: 0.021\n",
            "Iteration 11000: Current success : 74 \t All success : 152 \t mean(l0) : 17.302631 \t Current Learning Rate: 0.020\n",
            "Iteration 11100: Current success : 77 \t All success : 152 \t mean(l0) : 17.302631 \t Current Learning Rate: 0.020\n",
            "Iteration 11200: Current success : 76 \t All success : 152 \t mean(l0) : 17.302631 \t Current Learning Rate: 0.019\n",
            "Iteration 11300: Current success : 79 \t All success : 152 \t mean(l0) : 17.302631 \t Current Learning Rate: 0.019\n",
            "Iteration 11400: Current success : 80 \t All success : 152 \t mean(l0) : 17.302631 \t Current Learning Rate: 0.018\n",
            "Iteration 11500: Current success : 80 \t All success : 152 \t mean(l0) : 17.302631 \t Current Learning Rate: 0.018\n",
            "Iteration 11600: Current success : 79 \t All success : 152 \t mean(l0) : 17.302631 \t Current Learning Rate: 0.017\n",
            "Iteration 11700: Current success : 79 \t All success : 152 \t mean(l0) : 17.302631 \t Current Learning Rate: 0.017\n",
            "Iteration 11800: Current success : 83 \t All success : 152 \t mean(l0) : 17.302631 \t Current Learning Rate: 0.016\n",
            "Iteration 11900: Current success : 82 \t All success : 152 \t mean(l0) : 17.302631 \t Current Learning Rate: 0.016\n",
            "Iteration 12000: Current success : 82 \t All success : 152 \t mean(l0) : 17.302631 \t Current Learning Rate: 0.015\n",
            "Iteration 12100: Current success : 82 \t All success : 152 \t mean(l0) : 17.302631 \t Current Learning Rate: 0.015\n",
            "Iteration 12200: Current success : 83 \t All success : 153 \t mean(l0) : 17.215687 \t Current Learning Rate: 0.014\n",
            "Iteration 12300: Current success : 85 \t All success : 153 \t mean(l0) : 17.215687 \t Current Learning Rate: 0.014\n",
            "Iteration 12400: Current success : 84 \t All success : 153 \t mean(l0) : 17.215687 \t Current Learning Rate: 0.013\n",
            "Iteration 12500: Current success : 83 \t All success : 153 \t mean(l0) : 17.215687 \t Current Learning Rate: 0.013\n",
            "Iteration 12600: Current success : 87 \t All success : 153 \t mean(l0) : 17.215687 \t Current Learning Rate: 0.012\n",
            "Iteration 12700: Current success : 87 \t All success : 153 \t mean(l0) : 17.215687 \t Current Learning Rate: 0.012\n",
            "Iteration 12800: Current success : 87 \t All success : 153 \t mean(l0) : 17.215687 \t Current Learning Rate: 0.011\n",
            "Iteration 12900: Current success : 87 \t All success : 153 \t mean(l0) : 17.215687 \t Current Learning Rate: 0.011\n",
            "Iteration 13000: Current success : 87 \t All success : 153 \t mean(l0) : 17.215687 \t Current Learning Rate: 0.010\n",
            "Iteration 13100: Current success : 87 \t All success : 153 \t mean(l0) : 17.215687 \t Current Learning Rate: 0.010\n",
            "Iteration 13200: Current success : 85 \t All success : 153 \t mean(l0) : 17.215687 \t Current Learning Rate: 0.009\n",
            "Iteration 13300: Current success : 86 \t All success : 153 \t mean(l0) : 17.215687 \t Current Learning Rate: 0.009\n",
            "Iteration 13400: Current success : 86 \t All success : 153 \t mean(l0) : 17.215687 \t Current Learning Rate: 0.008\n",
            "Iteration 13500: Current success : 86 \t All success : 153 \t mean(l0) : 17.215687 \t Current Learning Rate: 0.008\n",
            "Iteration 13600: Current success : 86 \t All success : 153 \t mean(l0) : 17.215687 \t Current Learning Rate: 0.007\n",
            "Iteration 13700: Current success : 86 \t All success : 153 \t mean(l0) : 17.215687 \t Current Learning Rate: 0.007\n",
            "Iteration 13800: Current success : 86 \t All success : 153 \t mean(l0) : 17.215687 \t Current Learning Rate: 0.006\n",
            "Iteration 13900: Current success : 86 \t All success : 153 \t mean(l0) : 17.215687 \t Current Learning Rate: 0.006\n",
            "Iteration 14000: Current success : 86 \t All success : 153 \t mean(l0) : 17.215687 \t Current Learning Rate: 0.005\n",
            "Iteration 14100: Current success : 86 \t All success : 153 \t mean(l0) : 17.215687 \t Current Learning Rate: 0.005\n",
            "Iteration 14200: Current success : 86 \t All success : 153 \t mean(l0) : 17.215687 \t Current Learning Rate: 0.004\n",
            "Iteration 14300: Current success : 86 \t All success : 153 \t mean(l0) : 17.215687 \t Current Learning Rate: 0.004\n",
            "Iteration 14400: Current success : 86 \t All success : 153 \t mean(l0) : 17.215687 \t Current Learning Rate: 0.003\n",
            "Iteration 14500: Current success : 86 \t All success : 153 \t mean(l0) : 17.215687 \t Current Learning Rate: 0.003\n",
            "Iteration 14600: Current success : 86 \t All success : 153 \t mean(l0) : 17.215687 \t Current Learning Rate: 0.002\n",
            "Iteration 14700: Current success : 86 \t All success : 153 \t mean(l0) : 17.215687 \t Current Learning Rate: 0.002\n",
            "Iteration 14800: Current success : 86 \t All success : 153 \t mean(l0) : 17.215687 \t Current Learning Rate: 0.001\n",
            "Iteration 14900: Current success : 86 \t All success : 153 \t mean(l0) : 17.215687 \t Current Learning Rate: 0.001\n",
            "Iteration 15000: Current success : 86 \t All success : 153 \t mean(l0) : 17.215687 \t Current Learning Rate: 0.000\n",
            "Iteration 15100: Current success : 90 \t All success : 153 \t mean(l0) : 17.202614 \t Current Learning Rate: 0.012\n",
            "Iteration 15200: Current success : 92 \t All success : 154 \t mean(l0) : 17.363636 \t Current Learning Rate: 0.012\n",
            "Iteration 15300: Current success : 92 \t All success : 154 \t mean(l0) : 17.363636 \t Current Learning Rate: 0.012\n",
            "Iteration 15400: Current success : 92 \t All success : 154 \t mean(l0) : 17.363636 \t Current Learning Rate: 0.012\n",
            "Iteration 15500: Current success : 121 \t All success : 154 \t mean(l0) : 17.363636 \t Current Learning Rate: 0.011\n",
            "Iteration 15600: Current success : 121 \t All success : 154 \t mean(l0) : 17.363636 \t Current Learning Rate: 0.011\n",
            "Iteration 15700: Current success : 92 \t All success : 154 \t mean(l0) : 17.363636 \t Current Learning Rate: 0.011\n",
            "Iteration 15800: Current success : 92 \t All success : 154 \t mean(l0) : 17.363636 \t Current Learning Rate: 0.011\n",
            "Iteration 15900: Current success : 92 \t All success : 154 \t mean(l0) : 17.350649 \t Current Learning Rate: 0.010\n",
            "Iteration 16000: Current success : 92 \t All success : 154 \t mean(l0) : 17.344156 \t Current Learning Rate: 0.010\n",
            "Iteration 16100: Current success : 92 \t All success : 154 \t mean(l0) : 17.337662 \t Current Learning Rate: 0.010\n",
            "Iteration 16200: Current success : 92 \t All success : 154 \t mean(l0) : 17.337662 \t Current Learning Rate: 0.010\n",
            "Iteration 16300: Current success : 92 \t All success : 154 \t mean(l0) : 17.337662 \t Current Learning Rate: 0.009\n",
            "Iteration 16400: Current success : 92 \t All success : 154 \t mean(l0) : 17.337662 \t Current Learning Rate: 0.009\n",
            "Iteration 16500: Current success : 92 \t All success : 154 \t mean(l0) : 17.337662 \t Current Learning Rate: 0.009\n",
            "Iteration 16600: Current success : 92 \t All success : 154 \t mean(l0) : 17.337662 \t Current Learning Rate: 0.009\n",
            "Iteration 16700: Current success : 92 \t All success : 154 \t mean(l0) : 17.337662 \t Current Learning Rate: 0.008\n",
            "Iteration 16800: Current success : 92 \t All success : 154 \t mean(l0) : 17.337662 \t Current Learning Rate: 0.008\n",
            "Iteration 16900: Current success : 92 \t All success : 154 \t mean(l0) : 17.337662 \t Current Learning Rate: 0.008\n",
            "Iteration 17000: Current success : 92 \t All success : 154 \t mean(l0) : 17.337662 \t Current Learning Rate: 0.008\n",
            "Iteration 17100: Current success : 92 \t All success : 154 \t mean(l0) : 17.337662 \t Current Learning Rate: 0.007\n",
            "Iteration 17200: Current success : 94 \t All success : 155 \t mean(l0) : 17.541935 \t Current Learning Rate: 0.007\n",
            "Iteration 17300: Current success : 93 \t All success : 155 \t mean(l0) : 17.541935 \t Current Learning Rate: 0.007\n",
            "Iteration 17400: Current success : 93 \t All success : 155 \t mean(l0) : 17.541935 \t Current Learning Rate: 0.007\n",
            "Iteration 17500: Current success : 93 \t All success : 155 \t mean(l0) : 17.541935 \t Current Learning Rate: 0.006\n",
            "Iteration 17600: Current success : 93 \t All success : 155 \t mean(l0) : 17.541935 \t Current Learning Rate: 0.006\n",
            "Iteration 17700: Current success : 93 \t All success : 155 \t mean(l0) : 17.541935 \t Current Learning Rate: 0.006\n",
            "Iteration 17800: Current success : 94 \t All success : 155 \t mean(l0) : 17.541935 \t Current Learning Rate: 0.006\n",
            "Iteration 17900: Current success : 94 \t All success : 155 \t mean(l0) : 17.541935 \t Current Learning Rate: 0.005\n",
            "Iteration 18000: Current success : 94 \t All success : 155 \t mean(l0) : 17.541935 \t Current Learning Rate: 0.005\n",
            "Iteration 18100: Current success : 94 \t All success : 155 \t mean(l0) : 17.541935 \t Current Learning Rate: 0.005\n",
            "Iteration 18200: Current success : 94 \t All success : 155 \t mean(l0) : 17.541935 \t Current Learning Rate: 0.005\n",
            "Iteration 18300: Current success : 94 \t All success : 155 \t mean(l0) : 17.541935 \t Current Learning Rate: 0.004\n",
            "Iteration 18400: Current success : 94 \t All success : 155 \t mean(l0) : 17.541935 \t Current Learning Rate: 0.004\n",
            "Iteration 18500: Current success : 94 \t All success : 155 \t mean(l0) : 17.541935 \t Current Learning Rate: 0.004\n",
            "Iteration 18600: Current success : 94 \t All success : 155 \t mean(l0) : 17.541935 \t Current Learning Rate: 0.004\n",
            "Iteration 18700: Current success : 94 \t All success : 155 \t mean(l0) : 17.541935 \t Current Learning Rate: 0.003\n",
            "Iteration 18800: Current success : 94 \t All success : 155 \t mean(l0) : 17.541935 \t Current Learning Rate: 0.003\n",
            "Iteration 18900: Current success : 94 \t All success : 155 \t mean(l0) : 17.541935 \t Current Learning Rate: 0.003\n",
            "Iteration 19000: Current success : 94 \t All success : 155 \t mean(l0) : 17.541935 \t Current Learning Rate: 0.003\n",
            "Iteration 19100: Current success : 94 \t All success : 155 \t mean(l0) : 17.541935 \t Current Learning Rate: 0.002\n",
            "Iteration 19200: Current success : 94 \t All success : 155 \t mean(l0) : 17.541935 \t Current Learning Rate: 0.002\n",
            "Iteration 19300: Current success : 94 \t All success : 155 \t mean(l0) : 17.541935 \t Current Learning Rate: 0.002\n",
            "Iteration 19400: Current success : 94 \t All success : 155 \t mean(l0) : 17.541935 \t Current Learning Rate: 0.002\n",
            "Iteration 19500: Current success : 94 \t All success : 155 \t mean(l0) : 17.541935 \t Current Learning Rate: 0.001\n",
            "Iteration 19600: Current success : 94 \t All success : 155 \t mean(l0) : 17.541935 \t Current Learning Rate: 0.001\n",
            "Iteration 19700: Current success : 94 \t All success : 155 \t mean(l0) : 17.541935 \t Current Learning Rate: 0.001\n",
            "Iteration 19800: Current success : 94 \t All success : 155 \t mean(l0) : 17.541935 \t Current Learning Rate: 0.001\n",
            "Iteration 19900: Current success : 94 \t All success : 155 \t mean(l0) : 17.541935 \t Current Learning Rate: 0.000\n",
            "Iteration 20000: Current success : 94 \t All success : 155 \t mean(l0) : 17.541935 \t Current Learning Rate: 0.000\n",
            "outer_step 6: all success : 266 \t mean(l0)(success) : 21.2068\n",
            "-------------------------------------------------------------\n",
            "tensor([1.5625e+01, 1.0000e+06, 1.0000e+06,  ..., 1.0000e+06, 1.0000e+06,\n",
            "        1.0000e+06], device='cuda:0')\n",
            "Iteration 100: Current success : 119 \t All success : 121 \t mean(l0) : 9.404959 \t Current Learning Rate: 0.098\n",
            "Iteration 200: Current success : 116 \t All success : 132 \t mean(l0) : 11.371212 \t Current Learning Rate: 0.096\n",
            "Iteration 300: Current success : 126 \t All success : 180 \t mean(l0) : 13.216667 \t Current Learning Rate: 0.094\n",
            "Iteration 400: Current success : 161 \t All success : 185 \t mean(l0) : 13.491892 \t Current Learning Rate: 0.092\n",
            "Iteration 500: Current success : 164 \t All success : 187 \t mean(l0) : 13.679145 \t Current Learning Rate: 0.090\n",
            "Iteration 600: Current success : 165 \t All success : 188 \t mean(l0) : 13.808510 \t Current Learning Rate: 0.088\n",
            "Iteration 700: Current success : 169 \t All success : 192 \t mean(l0) : 14.276042 \t Current Learning Rate: 0.086\n",
            "Iteration 800: Current success : 170 \t All success : 192 \t mean(l0) : 14.276042 \t Current Learning Rate: 0.084\n",
            "Iteration 900: Current success : 166 \t All success : 192 \t mean(l0) : 14.270834 \t Current Learning Rate: 0.082\n",
            "Iteration 1000: Current success : 166 \t All success : 192 \t mean(l0) : 14.270834 \t Current Learning Rate: 0.080\n",
            "Iteration 1100: Current success : 169 \t All success : 192 \t mean(l0) : 14.270834 \t Current Learning Rate: 0.078\n",
            "Iteration 1200: Current success : 169 \t All success : 192 \t mean(l0) : 14.260417 \t Current Learning Rate: 0.076\n",
            "Iteration 1300: Current success : 168 \t All success : 192 \t mean(l0) : 14.260417 \t Current Learning Rate: 0.074\n",
            "Iteration 1400: Current success : 169 \t All success : 193 \t mean(l0) : 14.502590 \t Current Learning Rate: 0.072\n",
            "Iteration 1500: Current success : 170 \t All success : 193 \t mean(l0) : 14.502590 \t Current Learning Rate: 0.070\n",
            "Iteration 1600: Current success : 170 \t All success : 193 \t mean(l0) : 14.502590 \t Current Learning Rate: 0.068\n",
            "Iteration 1700: Current success : 170 \t All success : 193 \t mean(l0) : 14.502590 \t Current Learning Rate: 0.066\n",
            "Iteration 1800: Current success : 172 \t All success : 195 \t mean(l0) : 14.938462 \t Current Learning Rate: 0.064\n",
            "Iteration 1900: Current success : 172 \t All success : 195 \t mean(l0) : 14.938462 \t Current Learning Rate: 0.062\n",
            "Iteration 2000: Current success : 172 \t All success : 195 \t mean(l0) : 14.938462 \t Current Learning Rate: 0.060\n",
            "Iteration 2100: Current success : 172 \t All success : 195 \t mean(l0) : 14.938462 \t Current Learning Rate: 0.058\n",
            "Iteration 2200: Current success : 172 \t All success : 195 \t mean(l0) : 14.938462 \t Current Learning Rate: 0.056\n",
            "Iteration 2300: Current success : 172 \t All success : 195 \t mean(l0) : 14.938462 \t Current Learning Rate: 0.054\n",
            "Iteration 2400: Current success : 174 \t All success : 197 \t mean(l0) : 14.969543 \t Current Learning Rate: 0.052\n",
            "Iteration 2500: Current success : 174 \t All success : 197 \t mean(l0) : 14.969543 \t Current Learning Rate: 0.050\n",
            "Iteration 2600: Current success : 174 \t All success : 197 \t mean(l0) : 14.969543 \t Current Learning Rate: 0.048\n",
            "Iteration 2700: Current success : 174 \t All success : 197 \t mean(l0) : 14.969543 \t Current Learning Rate: 0.046\n",
            "Iteration 2800: Current success : 174 \t All success : 197 \t mean(l0) : 14.969543 \t Current Learning Rate: 0.044\n",
            "Iteration 2900: Current success : 174 \t All success : 197 \t mean(l0) : 14.969543 \t Current Learning Rate: 0.042\n",
            "Iteration 3000: Current success : 174 \t All success : 197 \t mean(l0) : 14.969543 \t Current Learning Rate: 0.040\n",
            "Iteration 3100: Current success : 174 \t All success : 197 \t mean(l0) : 14.969543 \t Current Learning Rate: 0.038\n",
            "Iteration 3200: Current success : 175 \t All success : 198 \t mean(l0) : 15.101010 \t Current Learning Rate: 0.036\n",
            "Iteration 3300: Current success : 175 \t All success : 198 \t mean(l0) : 15.101010 \t Current Learning Rate: 0.034\n",
            "Iteration 3400: Current success : 174 \t All success : 198 \t mean(l0) : 15.101010 \t Current Learning Rate: 0.032\n",
            "Iteration 3500: Current success : 175 \t All success : 198 \t mean(l0) : 15.101010 \t Current Learning Rate: 0.030\n",
            "Iteration 3600: Current success : 175 \t All success : 198 \t mean(l0) : 15.101010 \t Current Learning Rate: 0.028\n",
            "Iteration 3700: Current success : 175 \t All success : 198 \t mean(l0) : 15.101010 \t Current Learning Rate: 0.026\n",
            "Iteration 3800: Current success : 175 \t All success : 198 \t mean(l0) : 15.101010 \t Current Learning Rate: 0.024\n",
            "Iteration 3900: Current success : 175 \t All success : 198 \t mean(l0) : 15.101010 \t Current Learning Rate: 0.022\n",
            "Iteration 4000: Current success : 175 \t All success : 198 \t mean(l0) : 15.101010 \t Current Learning Rate: 0.020\n",
            "Iteration 4100: Current success : 175 \t All success : 198 \t mean(l0) : 15.101010 \t Current Learning Rate: 0.018\n",
            "Iteration 4200: Current success : 175 \t All success : 198 \t mean(l0) : 15.101010 \t Current Learning Rate: 0.016\n",
            "Iteration 4300: Current success : 175 \t All success : 198 \t mean(l0) : 15.101010 \t Current Learning Rate: 0.014\n",
            "Iteration 4400: Current success : 175 \t All success : 198 \t mean(l0) : 15.101010 \t Current Learning Rate: 0.012\n",
            "Iteration 4500: Current success : 175 \t All success : 198 \t mean(l0) : 15.101010 \t Current Learning Rate: 0.010\n",
            "Iteration 4600: Current success : 175 \t All success : 198 \t mean(l0) : 15.101010 \t Current Learning Rate: 0.008\n",
            "Iteration 4700: Current success : 175 \t All success : 198 \t mean(l0) : 15.101010 \t Current Learning Rate: 0.006\n",
            "Iteration 4800: Current success : 175 \t All success : 198 \t mean(l0) : 15.101010 \t Current Learning Rate: 0.004\n",
            "Iteration 4900: Current success : 175 \t All success : 198 \t mean(l0) : 15.101010 \t Current Learning Rate: 0.002\n",
            "Iteration 5000: Current success : 175 \t All success : 198 \t mean(l0) : 15.101010 \t Current Learning Rate: 0.000\n",
            "Iteration 5100: Current success : 198 \t All success : 213 \t mean(l0) : 15.478873 \t Current Learning Rate: 0.049\n",
            "Iteration 5200: Current success : 198 \t All success : 219 \t mean(l0) : 16.401825 \t Current Learning Rate: 0.048\n",
            "Iteration 5300: Current success : 198 \t All success : 219 \t mean(l0) : 16.401825 \t Current Learning Rate: 0.047\n",
            "Iteration 5400: Current success : 201 \t All success : 220 \t mean(l0) : 16.704544 \t Current Learning Rate: 0.046\n",
            "Iteration 5500: Current success : 201 \t All success : 221 \t mean(l0) : 16.696833 \t Current Learning Rate: 0.045\n",
            "Iteration 5600: Current success : 200 \t All success : 221 \t mean(l0) : 16.696833 \t Current Learning Rate: 0.044\n",
            "Iteration 5700: Current success : 200 \t All success : 221 \t mean(l0) : 16.696833 \t Current Learning Rate: 0.043\n",
            "Iteration 5800: Current success : 201 \t All success : 221 \t mean(l0) : 16.696833 \t Current Learning Rate: 0.042\n",
            "Iteration 5900: Current success : 201 \t All success : 221 \t mean(l0) : 16.696833 \t Current Learning Rate: 0.041\n",
            "Iteration 6000: Current success : 201 \t All success : 221 \t mean(l0) : 16.696833 \t Current Learning Rate: 0.040\n",
            "Iteration 6100: Current success : 201 \t All success : 221 \t mean(l0) : 16.696833 \t Current Learning Rate: 0.039\n",
            "Iteration 6200: Current success : 201 \t All success : 221 \t mean(l0) : 16.696833 \t Current Learning Rate: 0.038\n",
            "Iteration 6300: Current success : 201 \t All success : 221 \t mean(l0) : 16.696833 \t Current Learning Rate: 0.037\n",
            "Iteration 6400: Current success : 201 \t All success : 221 \t mean(l0) : 16.696833 \t Current Learning Rate: 0.036\n",
            "Iteration 6500: Current success : 201 \t All success : 221 \t mean(l0) : 16.696833 \t Current Learning Rate: 0.035\n",
            "Iteration 6600: Current success : 201 \t All success : 221 \t mean(l0) : 16.696833 \t Current Learning Rate: 0.034\n",
            "Iteration 6700: Current success : 201 \t All success : 221 \t mean(l0) : 16.696833 \t Current Learning Rate: 0.033\n",
            "Iteration 6800: Current success : 201 \t All success : 221 \t mean(l0) : 16.696833 \t Current Learning Rate: 0.032\n",
            "Iteration 6900: Current success : 201 \t All success : 221 \t mean(l0) : 16.696833 \t Current Learning Rate: 0.031\n",
            "Iteration 7000: Current success : 201 \t All success : 221 \t mean(l0) : 16.696833 \t Current Learning Rate: 0.030\n",
            "Iteration 7100: Current success : 201 \t All success : 221 \t mean(l0) : 16.696833 \t Current Learning Rate: 0.029\n",
            "Iteration 7200: Current success : 201 \t All success : 221 \t mean(l0) : 16.696833 \t Current Learning Rate: 0.028\n",
            "Iteration 7300: Current success : 201 \t All success : 221 \t mean(l0) : 16.696833 \t Current Learning Rate: 0.027\n",
            "Iteration 7400: Current success : 201 \t All success : 221 \t mean(l0) : 16.696833 \t Current Learning Rate: 0.026\n",
            "Iteration 7500: Current success : 201 \t All success : 221 \t mean(l0) : 16.696833 \t Current Learning Rate: 0.025\n",
            "Iteration 7600: Current success : 201 \t All success : 221 \t mean(l0) : 16.696833 \t Current Learning Rate: 0.024\n",
            "Iteration 7700: Current success : 201 \t All success : 221 \t mean(l0) : 16.696833 \t Current Learning Rate: 0.023\n",
            "Iteration 7800: Current success : 201 \t All success : 221 \t mean(l0) : 16.696833 \t Current Learning Rate: 0.022\n",
            "Iteration 7900: Current success : 201 \t All success : 221 \t mean(l0) : 16.696833 \t Current Learning Rate: 0.021\n",
            "Iteration 8000: Current success : 201 \t All success : 221 \t mean(l0) : 16.696833 \t Current Learning Rate: 0.020\n",
            "Iteration 8100: Current success : 201 \t All success : 221 \t mean(l0) : 16.696833 \t Current Learning Rate: 0.019\n",
            "Iteration 8200: Current success : 201 \t All success : 221 \t mean(l0) : 16.696833 \t Current Learning Rate: 0.018\n",
            "Iteration 8300: Current success : 201 \t All success : 221 \t mean(l0) : 16.696833 \t Current Learning Rate: 0.017\n",
            "Iteration 8400: Current success : 201 \t All success : 221 \t mean(l0) : 16.696833 \t Current Learning Rate: 0.016\n",
            "Iteration 8500: Current success : 201 \t All success : 221 \t mean(l0) : 16.696833 \t Current Learning Rate: 0.015\n",
            "Iteration 8600: Current success : 201 \t All success : 221 \t mean(l0) : 16.696833 \t Current Learning Rate: 0.014\n",
            "Iteration 8700: Current success : 201 \t All success : 221 \t mean(l0) : 16.696833 \t Current Learning Rate: 0.013\n",
            "Iteration 8800: Current success : 201 \t All success : 221 \t mean(l0) : 16.696833 \t Current Learning Rate: 0.012\n",
            "Iteration 8900: Current success : 201 \t All success : 221 \t mean(l0) : 16.696833 \t Current Learning Rate: 0.011\n",
            "Iteration 9000: Current success : 201 \t All success : 221 \t mean(l0) : 16.696833 \t Current Learning Rate: 0.010\n",
            "Iteration 9100: Current success : 201 \t All success : 221 \t mean(l0) : 16.696833 \t Current Learning Rate: 0.009\n",
            "Iteration 9200: Current success : 201 \t All success : 221 \t mean(l0) : 16.696833 \t Current Learning Rate: 0.008\n",
            "Iteration 9300: Current success : 201 \t All success : 221 \t mean(l0) : 16.696833 \t Current Learning Rate: 0.007\n",
            "Iteration 9400: Current success : 201 \t All success : 221 \t mean(l0) : 16.696833 \t Current Learning Rate: 0.006\n",
            "Iteration 9500: Current success : 201 \t All success : 221 \t mean(l0) : 16.696833 \t Current Learning Rate: 0.005\n",
            "Iteration 9600: Current success : 201 \t All success : 221 \t mean(l0) : 16.696833 \t Current Learning Rate: 0.004\n",
            "Iteration 9700: Current success : 201 \t All success : 221 \t mean(l0) : 16.696833 \t Current Learning Rate: 0.003\n",
            "Iteration 9800: Current success : 201 \t All success : 221 \t mean(l0) : 16.696833 \t Current Learning Rate: 0.002\n",
            "Iteration 9900: Current success : 201 \t All success : 221 \t mean(l0) : 16.696833 \t Current Learning Rate: 0.001\n",
            "Iteration 10000: Current success : 201 \t All success : 221 \t mean(l0) : 16.696833 \t Current Learning Rate: 0.000\n",
            "Iteration 10100: Current success : 197 \t All success : 221 \t mean(l0) : 16.696833 \t Current Learning Rate: 0.025\n",
            "Iteration 10200: Current success : 198 \t All success : 221 \t mean(l0) : 16.696833 \t Current Learning Rate: 0.024\n",
            "Iteration 10300: Current success : 199 \t All success : 221 \t mean(l0) : 16.696833 \t Current Learning Rate: 0.024\n",
            "Iteration 10400: Current success : 199 \t All success : 221 \t mean(l0) : 16.696833 \t Current Learning Rate: 0.023\n",
            "Iteration 10500: Current success : 199 \t All success : 221 \t mean(l0) : 16.696833 \t Current Learning Rate: 0.023\n",
            "Iteration 10600: Current success : 199 \t All success : 221 \t mean(l0) : 16.696833 \t Current Learning Rate: 0.022\n",
            "Iteration 10700: Current success : 200 \t All success : 222 \t mean(l0) : 16.648649 \t Current Learning Rate: 0.022\n",
            "Iteration 10800: Current success : 200 \t All success : 222 \t mean(l0) : 16.648649 \t Current Learning Rate: 0.021\n",
            "Iteration 10900: Current success : 200 \t All success : 222 \t mean(l0) : 16.648649 \t Current Learning Rate: 0.021\n",
            "Iteration 11000: Current success : 200 \t All success : 222 \t mean(l0) : 16.648649 \t Current Learning Rate: 0.020\n",
            "Iteration 11100: Current success : 200 \t All success : 222 \t mean(l0) : 16.648649 \t Current Learning Rate: 0.020\n",
            "Iteration 11200: Current success : 200 \t All success : 222 \t mean(l0) : 16.644144 \t Current Learning Rate: 0.019\n",
            "Iteration 11300: Current success : 200 \t All success : 222 \t mean(l0) : 16.644144 \t Current Learning Rate: 0.019\n",
            "Iteration 11400: Current success : 200 \t All success : 223 \t mean(l0) : 16.600897 \t Current Learning Rate: 0.018\n",
            "Iteration 11500: Current success : 200 \t All success : 223 \t mean(l0) : 16.600897 \t Current Learning Rate: 0.018\n",
            "Iteration 11600: Current success : 200 \t All success : 223 \t mean(l0) : 16.600897 \t Current Learning Rate: 0.017\n",
            "Iteration 11700: Current success : 200 \t All success : 223 \t mean(l0) : 16.600897 \t Current Learning Rate: 0.017\n",
            "Iteration 11800: Current success : 200 \t All success : 223 \t mean(l0) : 16.600897 \t Current Learning Rate: 0.016\n",
            "Iteration 11900: Current success : 200 \t All success : 223 \t mean(l0) : 16.600897 \t Current Learning Rate: 0.016\n",
            "Iteration 12000: Current success : 200 \t All success : 223 \t mean(l0) : 16.600897 \t Current Learning Rate: 0.015\n",
            "Iteration 12100: Current success : 200 \t All success : 223 \t mean(l0) : 16.600897 \t Current Learning Rate: 0.015\n",
            "Iteration 12200: Current success : 201 \t All success : 223 \t mean(l0) : 16.600897 \t Current Learning Rate: 0.014\n",
            "Iteration 12300: Current success : 201 \t All success : 223 \t mean(l0) : 16.600897 \t Current Learning Rate: 0.014\n",
            "Iteration 12400: Current success : 200 \t All success : 223 \t mean(l0) : 16.600897 \t Current Learning Rate: 0.013\n",
            "Iteration 12500: Current success : 201 \t All success : 223 \t mean(l0) : 16.600897 \t Current Learning Rate: 0.013\n",
            "Iteration 12600: Current success : 201 \t All success : 223 \t mean(l0) : 16.600897 \t Current Learning Rate: 0.012\n",
            "Iteration 12700: Current success : 201 \t All success : 223 \t mean(l0) : 16.600897 \t Current Learning Rate: 0.012\n",
            "Iteration 12800: Current success : 201 \t All success : 223 \t mean(l0) : 16.600897 \t Current Learning Rate: 0.011\n",
            "Iteration 12900: Current success : 201 \t All success : 223 \t mean(l0) : 16.600897 \t Current Learning Rate: 0.011\n",
            "Iteration 13000: Current success : 200 \t All success : 223 \t mean(l0) : 16.600897 \t Current Learning Rate: 0.010\n",
            "Iteration 13100: Current success : 200 \t All success : 223 \t mean(l0) : 16.600897 \t Current Learning Rate: 0.010\n",
            "Iteration 13200: Current success : 200 \t All success : 223 \t mean(l0) : 16.600897 \t Current Learning Rate: 0.009\n",
            "Iteration 13300: Current success : 200 \t All success : 223 \t mean(l0) : 16.600897 \t Current Learning Rate: 0.009\n",
            "Iteration 13400: Current success : 200 \t All success : 223 \t mean(l0) : 16.600897 \t Current Learning Rate: 0.008\n",
            "Iteration 13500: Current success : 200 \t All success : 223 \t mean(l0) : 16.600897 \t Current Learning Rate: 0.008\n",
            "Iteration 13600: Current success : 200 \t All success : 223 \t mean(l0) : 16.600897 \t Current Learning Rate: 0.007\n",
            "Iteration 13700: Current success : 200 \t All success : 223 \t mean(l0) : 16.600897 \t Current Learning Rate: 0.007\n",
            "Iteration 13800: Current success : 200 \t All success : 223 \t mean(l0) : 16.600897 \t Current Learning Rate: 0.006\n",
            "Iteration 13900: Current success : 200 \t All success : 223 \t mean(l0) : 16.600897 \t Current Learning Rate: 0.006\n",
            "Iteration 14000: Current success : 200 \t All success : 223 \t mean(l0) : 16.600897 \t Current Learning Rate: 0.005\n",
            "Iteration 14100: Current success : 200 \t All success : 223 \t mean(l0) : 16.600897 \t Current Learning Rate: 0.005\n",
            "Iteration 14200: Current success : 200 \t All success : 223 \t mean(l0) : 16.600897 \t Current Learning Rate: 0.004\n",
            "Iteration 14300: Current success : 200 \t All success : 223 \t mean(l0) : 16.600897 \t Current Learning Rate: 0.004\n",
            "Iteration 14400: Current success : 200 \t All success : 223 \t mean(l0) : 16.600897 \t Current Learning Rate: 0.003\n",
            "Iteration 14500: Current success : 200 \t All success : 223 \t mean(l0) : 16.600897 \t Current Learning Rate: 0.003\n",
            "Iteration 14600: Current success : 200 \t All success : 223 \t mean(l0) : 16.600897 \t Current Learning Rate: 0.002\n",
            "Iteration 14700: Current success : 200 \t All success : 223 \t mean(l0) : 16.600897 \t Current Learning Rate: 0.002\n",
            "Iteration 14800: Current success : 200 \t All success : 223 \t mean(l0) : 16.600897 \t Current Learning Rate: 0.001\n",
            "Iteration 14900: Current success : 200 \t All success : 223 \t mean(l0) : 16.600897 \t Current Learning Rate: 0.001\n",
            "Iteration 15000: Current success : 200 \t All success : 223 \t mean(l0) : 16.600897 \t Current Learning Rate: 0.000\n",
            "Iteration 15100: Current success : 201 \t All success : 223 \t mean(l0) : 16.600897 \t Current Learning Rate: 0.012\n",
            "Iteration 15200: Current success : 202 \t All success : 223 \t mean(l0) : 16.600897 \t Current Learning Rate: 0.012\n",
            "Iteration 15300: Current success : 201 \t All success : 223 \t mean(l0) : 16.600897 \t Current Learning Rate: 0.012\n",
            "Iteration 15400: Current success : 201 \t All success : 224 \t mean(l0) : 16.776787 \t Current Learning Rate: 0.012\n",
            "Iteration 15500: Current success : 201 \t All success : 224 \t mean(l0) : 16.776787 \t Current Learning Rate: 0.011\n",
            "Iteration 15600: Current success : 201 \t All success : 224 \t mean(l0) : 16.776787 \t Current Learning Rate: 0.011\n",
            "Iteration 15700: Current success : 201 \t All success : 224 \t mean(l0) : 16.776787 \t Current Learning Rate: 0.011\n",
            "Iteration 15800: Current success : 202 \t All success : 224 \t mean(l0) : 16.776787 \t Current Learning Rate: 0.011\n",
            "Iteration 15900: Current success : 202 \t All success : 224 \t mean(l0) : 16.776787 \t Current Learning Rate: 0.010\n",
            "Iteration 16000: Current success : 203 \t All success : 224 \t mean(l0) : 16.776787 \t Current Learning Rate: 0.010\n",
            "Iteration 16100: Current success : 203 \t All success : 224 \t mean(l0) : 16.776787 \t Current Learning Rate: 0.010\n",
            "Iteration 16200: Current success : 202 \t All success : 224 \t mean(l0) : 16.776787 \t Current Learning Rate: 0.010\n",
            "Iteration 16300: Current success : 203 \t All success : 224 \t mean(l0) : 16.776787 \t Current Learning Rate: 0.009\n",
            "Iteration 16400: Current success : 203 \t All success : 224 \t mean(l0) : 16.776787 \t Current Learning Rate: 0.009\n",
            "Iteration 16500: Current success : 203 \t All success : 224 \t mean(l0) : 16.776787 \t Current Learning Rate: 0.009\n",
            "Iteration 16600: Current success : 203 \t All success : 224 \t mean(l0) : 16.776787 \t Current Learning Rate: 0.009\n",
            "Iteration 16700: Current success : 203 \t All success : 224 \t mean(l0) : 16.776787 \t Current Learning Rate: 0.008\n",
            "Iteration 16800: Current success : 203 \t All success : 224 \t mean(l0) : 16.776787 \t Current Learning Rate: 0.008\n",
            "Iteration 16900: Current success : 203 \t All success : 224 \t mean(l0) : 16.776787 \t Current Learning Rate: 0.008\n",
            "Iteration 17000: Current success : 203 \t All success : 224 \t mean(l0) : 16.776787 \t Current Learning Rate: 0.008\n",
            "Iteration 17100: Current success : 203 \t All success : 224 \t mean(l0) : 16.776787 \t Current Learning Rate: 0.007\n",
            "Iteration 17200: Current success : 203 \t All success : 224 \t mean(l0) : 16.776787 \t Current Learning Rate: 0.007\n",
            "Iteration 17300: Current success : 202 \t All success : 224 \t mean(l0) : 16.776787 \t Current Learning Rate: 0.007\n",
            "Iteration 17400: Current success : 202 \t All success : 224 \t mean(l0) : 16.776787 \t Current Learning Rate: 0.007\n",
            "Iteration 17500: Current success : 202 \t All success : 224 \t mean(l0) : 16.776787 \t Current Learning Rate: 0.006\n",
            "Iteration 17600: Current success : 202 \t All success : 224 \t mean(l0) : 16.776787 \t Current Learning Rate: 0.006\n",
            "Iteration 17700: Current success : 202 \t All success : 224 \t mean(l0) : 16.776787 \t Current Learning Rate: 0.006\n",
            "Iteration 17800: Current success : 202 \t All success : 224 \t mean(l0) : 16.776787 \t Current Learning Rate: 0.006\n",
            "Iteration 17900: Current success : 202 \t All success : 224 \t mean(l0) : 16.776787 \t Current Learning Rate: 0.005\n",
            "Iteration 18000: Current success : 202 \t All success : 224 \t mean(l0) : 16.776787 \t Current Learning Rate: 0.005\n",
            "Iteration 18100: Current success : 202 \t All success : 224 \t mean(l0) : 16.776787 \t Current Learning Rate: 0.005\n",
            "Iteration 18200: Current success : 203 \t All success : 224 \t mean(l0) : 16.776787 \t Current Learning Rate: 0.005\n",
            "Iteration 18300: Current success : 203 \t All success : 224 \t mean(l0) : 16.776787 \t Current Learning Rate: 0.004\n",
            "Iteration 18400: Current success : 203 \t All success : 224 \t mean(l0) : 16.776787 \t Current Learning Rate: 0.004\n",
            "Iteration 18500: Current success : 202 \t All success : 224 \t mean(l0) : 16.776787 \t Current Learning Rate: 0.004\n",
            "Iteration 18600: Current success : 202 \t All success : 224 \t mean(l0) : 16.776787 \t Current Learning Rate: 0.004\n",
            "Iteration 18700: Current success : 203 \t All success : 224 \t mean(l0) : 16.776787 \t Current Learning Rate: 0.003\n",
            "Iteration 18800: Current success : 202 \t All success : 224 \t mean(l0) : 16.776787 \t Current Learning Rate: 0.003\n",
            "Iteration 18900: Current success : 202 \t All success : 224 \t mean(l0) : 16.776787 \t Current Learning Rate: 0.003\n",
            "Iteration 19000: Current success : 202 \t All success : 224 \t mean(l0) : 16.776787 \t Current Learning Rate: 0.003\n",
            "Iteration 19100: Current success : 201 \t All success : 224 \t mean(l0) : 16.776787 \t Current Learning Rate: 0.002\n",
            "Iteration 19200: Current success : 201 \t All success : 224 \t mean(l0) : 16.776787 \t Current Learning Rate: 0.002\n",
            "Iteration 19300: Current success : 201 \t All success : 224 \t mean(l0) : 16.776787 \t Current Learning Rate: 0.002\n",
            "Iteration 19400: Current success : 201 \t All success : 224 \t mean(l0) : 16.776787 \t Current Learning Rate: 0.002\n",
            "Iteration 19500: Current success : 201 \t All success : 224 \t mean(l0) : 16.776787 \t Current Learning Rate: 0.001\n",
            "Iteration 19600: Current success : 201 \t All success : 224 \t mean(l0) : 16.776787 \t Current Learning Rate: 0.001\n",
            "Iteration 19700: Current success : 201 \t All success : 224 \t mean(l0) : 16.776787 \t Current Learning Rate: 0.001\n",
            "Iteration 19800: Current success : 201 \t All success : 224 \t mean(l0) : 16.776787 \t Current Learning Rate: 0.001\n",
            "Iteration 19900: Current success : 201 \t All success : 224 \t mean(l0) : 16.776787 \t Current Learning Rate: 0.000\n",
            "Iteration 20000: Current success : 201 \t All success : 224 \t mean(l0) : 16.776787 \t Current Learning Rate: 0.000\n",
            "1130\n",
            "268\n",
            "Accuracy of just malwares (without attack): 98.85% | Under attack: 76.28%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xj5WTRSLwkuC"
      },
      "outputs": [],
      "source": [
        "for x_test, y_test in test_loader:\n",
        "    x_test, y_test = x_test.to(torch.float32).to(device), y_test.to(device)\n",
        "    mal_x_batch, mal_y_batch = x_test[y_test.squeeze() == 1], y_test[y_test.squeeze() == 1]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation on rounded adversarial examples\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "combined_model.eval()\n",
        "\n",
        "# Apply the final rounding solution to adversarial examples\n",
        "adv_rounded = final_rounding(mal_x_batch, adv, combined_model,removal_array)\n",
        "\n",
        "# Get the model's predictions on the rounded adversarial examples\n",
        "outputs = combined_model(adv_rounded)\n",
        "y_pred = outputs.argmax(dim=1)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "num_malwares = len(mal_y_batch)\n",
        "num_successful_attacks = (y_pred == 0).sum().item()\n",
        "total_difference = (torch.abs(adv_rounded[y_pred == 0] - mal_x_batch[y_pred == 0])).sum().item()\n",
        "accuracy = (y_pred == 1).sum().item() / num_malwares * 100\n",
        "\n",
        "# Display the results with clear formatting\n",
        "print(f\"Evaluation Results on Rounded Adversarial Examples\")\n",
        "print(f\"-----------------------------------------------\")\n",
        "print(f\"Number of malware samples: {num_malwares}\")\n",
        "print(f\"Number of successful attacks: {num_successful_attacks}\")\n",
        "print(f\"Total L1 norm difference (successful adversarial examples vs. original): {total_difference}\")\n",
        "print(f\"Accuracy of the model on malware under attack: {accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c70ffa27-65ef-4dff-bf0e-96dd6fbc0b99",
        "id": "XtGlUivfwkuD"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Results on Rounded Adversarial Examples\n",
            "-----------------------------------------------\n",
            "Number of malware samples: 1130\n",
            "Number of successful attacks: 268\n",
            "Total L1 norm difference (successful adversarial examples vs. original): 5578.0\n",
            "Accuracy of the model on malware under attack: 76.28%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure that fixed elements are not changed\n",
        "\n",
        "# Ensure `removal_array` is expanded to match the batch size\n",
        "expanded_removal_array = removal_array.expand(mal_x_batch.shape[0], -1)\n",
        "\n",
        "# Create a mask where non-fixed features are set to 1\n",
        "non_fixed_features_mask = torch.bitwise_or(expanded_removal_array, 1 - mal_x_batch.to(torch.uint8))\n",
        "\n",
        "# Calculate indices where the mask is 0 (fixed) or 1 (non-fixed)\n",
        "mask_fixed = non_fixed_features_mask == 0\n",
        "mask_non_fixed = non_fixed_features_mask == 1\n",
        "\n",
        "# Calculate and print the differences\n",
        "diff_adv_fixed = (torch.abs(adv - mal_x_batch) * mask_fixed).sum().item()\n",
        "diff_adv_non_fixed = (torch.abs(adv - mal_x_batch) * mask_non_fixed).sum().item()\n",
        "diff_adv_rounded_fixed = (torch.abs(adv_rounded - mal_x_batch) * mask_fixed).sum().item()\n",
        "diff_adv_rounded_non_fixed = (torch.abs(adv_rounded - mal_x_batch) * mask_non_fixed).sum().item()\n",
        "\n",
        "# Display results with clear formatting\n",
        "print(f\"Differences when mask is 0 (fixed features):\")\n",
        "print(f\"  Adv vs. Original: {diff_adv_fixed}\")\n",
        "print(f\"  Rounded Adv vs. Original: {diff_adv_rounded_fixed}\")\n",
        "print(f\"Differences when mask is 1 (non-fixed features):\")\n",
        "print(f\"  Adv vs. Original: {diff_adv_non_fixed}\")\n",
        "print(f\"  Rounded Adv vs. Original: {diff_adv_rounded_non_fixed}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c6664f7-e7d8-4f32-f358-bb95991aff95",
        "id": "gDY0NkXhwkuD"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Differences when mask is 0 (fixed features):\n",
            "  Adv vs. Original: 0.0\n",
            "  Rounded Adv vs. Original: 0.0\n",
            "Differences when mask is 1 (non-fixed features):\n",
            "  Adv vs. Original: 59956.19140625\n",
            "  Rounded Adv vs. Original: 77251.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming differences is already computed as in your code\n",
        "differences = (torch.abs(adv_rounded[y_pred == 0] - mal_x_batch[y_pred == 0])).sum(1)\n",
        "\n",
        "# Convert differences to a numpy array for plotting\n",
        "differences_np = differences.cpu().numpy()\n",
        "\n",
        "# Set the style of the plot\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# Create a histogram with Seaborn\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(differences_np, bins=16, kde=False, color='skyblue')\n",
        "#sns.histplot(differences_np, bins=30, kde=True, color='skyblue')\n",
        "\n",
        "# Set the title and labels\n",
        "plt.title('Abundance of Differences', fontsize=16)\n",
        "plt.xlabel('Sum of Absolute Differences', fontsize=14)\n",
        "plt.ylabel('Frequency', fontsize=14)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 576
        },
        "outputId": "5ee5aa25-c20a-45a1-8bc6-f0b896ed18aa",
        "id": "VvzfDiMmwkuD"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "adv[y_pred==0][0][:200]- mal_x_batch[y_pred==0][0][:200]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7217fd2-dbf6-469e-8dbc-7c703e78bab7",
        "id": "eZNRq3snwkuE"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.0000e+00,  9.9212e-05,  0.0000e+00, -7.1192e-03,  5.4151e-05,\n",
              "         2.0802e-05, -6.7011e-03,  2.9802e-08,  2.6822e-07,  8.4043e-06,\n",
              "         6.5863e-06,  2.2620e-05,  1.3308e-02,  5.7554e-04,  1.0442e-03,\n",
              "         2.6324e-03,  4.5493e-03,  2.9504e-06,  0.0000e+00,  9.9758e-01,\n",
              "         4.1127e-06,  2.1235e-03,  1.0669e-05,  1.3530e-05,  6.6459e-06,\n",
              "         9.9898e-01,  6.5565e-06,  2.9802e-08,  1.2994e-05,  5.2601e-05,\n",
              "         2.9802e-08,  9.6083e-05,  5.6028e-06,  0.0000e+00,  1.8835e-05,\n",
              "         2.6822e-06,  7.1824e-06,  1.4186e-05,  2.3842e-07,  1.5042e-03,\n",
              "         3.8743e-07,  2.0862e-07,  1.1921e-07,  2.9802e-08,  2.9802e-08,\n",
              "         4.4703e-07,  1.1921e-07,  1.1921e-07,  2.0862e-07,  6.0558e-05,\n",
              "         3.1164e-04,  0.0000e+00,  9.4831e-05,  0.0000e+00,  2.9802e-07,\n",
              "         2.9802e-08,  2.7299e-05,  1.4901e-07,  2.3842e-07,  2.0862e-07,\n",
              "         2.3842e-07,  5.9605e-08,  2.6822e-07,  0.0000e+00,  5.9605e-07,\n",
              "         1.7630e-03,  2.4348e-05,  1.3411e-06,  5.9605e-08,  2.3842e-07,\n",
              "         9.9805e-01,  1.7881e-07,  3.1562e-03,  6.5565e-06,  7.3314e-06,\n",
              "         3.8743e-07,  9.9994e-01,  7.7784e-06,  3.6359e-06,  2.2084e-05,\n",
              "         6.2585e-07, -8.5485e-04,  7.7486e-07,  5.9605e-07,  7.5102e-06,\n",
              "         2.9460e-02,  1.0103e-05,  9.3281e-06,  5.9605e-08,  8.9407e-08,\n",
              "         1.4901e-07,  8.9407e-08,  8.9407e-08,  2.0862e-07,  1.8477e-06,\n",
              "         5.6676e-03,  9.2387e-06,  1.3411e-06,  5.6624e-07,  1.1921e-07,\n",
              "         2.0862e-07,  2.0862e-07,  1.7881e-07,  5.9605e-08,  3.4451e-05,\n",
              "         2.3842e-06,  1.4088e-04,  1.3113e-06,  1.1921e-07,  8.9407e-08,\n",
              "         1.7881e-07,  2.9802e-08,  1.1712e-05,  0.0000e+00,  5.9605e-08,\n",
              "         2.4796e-05,  2.9802e-08,  3.1888e-06,  2.9802e-08,  5.9605e-08,\n",
              "         6.2585e-07,  3.2783e-07,  2.9802e-08,  8.0466e-07,  2.9802e-07,\n",
              "         5.9605e-08,  2.9802e-08,  5.9605e-08,  9.9999e-01,  8.9407e-06,\n",
              "         2.0862e-07,  1.4603e-06,  1.4970e-03,  6.5565e-07,  5.3942e-06,\n",
              "         4.1425e-06,  1.4901e-07,  2.0862e-07,  8.7691e-03,  2.0862e-07,\n",
              "         2.3842e-07,  5.9605e-08,  0.0000e+00,  2.3842e-07,  3.5763e-07,\n",
              "         4.7684e-07,  3.5763e-07,  1.5378e-05,  1.0133e-06,  1.7881e-07,\n",
              "         8.0466e-07,  3.2783e-07,  7.1526e-07,  8.9407e-08,  8.3447e-07,\n",
              "         7.1526e-07,  9.8348e-07,  4.1723e-07,  9.9877e-01,  1.4603e-06,\n",
              "         0.0000e+00,  2.0862e-07,  2.6822e-07,  1.6689e-06,  9.9998e-01,\n",
              "         5.3346e-06,  2.4843e-04,  4.1723e-07,  3.5763e-07,  1.0431e-06,\n",
              "         9.2387e-07,  2.6610e-04,  4.5347e-04,  3.4571e-06,  1.6987e-06,\n",
              "         6.2585e-07,  3.5763e-07,  5.9605e-08,  2.1458e-06,  0.0000e+00,\n",
              "         4.7684e-07,  2.6822e-07,  5.0068e-06,  3.1173e-05,  0.0000e+00,\n",
              "         1.1623e-06,  4.7684e-06,  2.6822e-07,  4.4703e-07,  0.0000e+00,\n",
              "         2.9802e-08,  1.1921e-07,  0.0000e+00,  1.4901e-07,  1.1921e-07,\n",
              "         2.9802e-08,  5.3644e-07,  3.5763e-07,  1.7881e-07,  1.6409e-03],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "adv[y_pred==0][1][:200] - mal_x_batch[y_pred==0][1][:200]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f154733a-ed4e-4c03-95ab-bd4e9ce26b80",
        "id": "9j4PnovbwkuE"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.0000e+00,  1.0000e+00,  0.0000e+00,  0.0000e+00,  4.7386e-06,\n",
              "         0.0000e+00,  6.6370e-05,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         4.0435e-03,  1.0000e+00,  7.3716e-04,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  9.9986e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  5.3458e-02,  9.9902e-01,  1.0000e+00,  1.0133e-06,\n",
              "         0.0000e+00,  1.5241e-04,  0.0000e+00,  0.0000e+00,  9.9999e-01,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  9.9263e-01, -1.1683e-04,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  2.9802e-08,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  6.6340e-05,\n",
              "         0.0000e+00,  0.0000e+00, -4.6888e-01,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  3.7909e-05,\n",
              "        -9.9988e-01,  3.2783e-07,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,  1.1027e-05,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00, -5.1332e-04,  0.0000e+00,\n",
              "         0.0000e+00, -1.0000e+00,  2.3550e-03, -6.5565e-07, -2.6703e-01,\n",
              "         4.9865e-04,  3.7094e-01,  3.2783e-07,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  2.6107e-05,  5.8413e-06,  9.1636e-01,\n",
              "         0.0000e+00,  1.6502e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  3.2187e-06,  1.0000e+00,\n",
              "         0.0000e+00,  9.9901e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         5.9605e-08,  0.0000e+00,  5.9605e-08,  1.0000e+00,  0.0000e+00,\n",
              "         2.9802e-08,  1.2904e-05,  9.2387e-07,  0.0000e+00,  0.0000e+00,\n",
              "         5.5114e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  2.7418e-06,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  9.9979e-01,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  2.9802e-08,  1.9372e-06,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.1921e-07,\n",
              "         0.0000e+00,  1.6361e-05,  0.0000e+00,  0.0000e+00,  1.0000e+00,\n",
              "         4.5505e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6YBE0PJCiYiJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# tanh(2 * perturbation_tensor)"
      ],
      "metadata": {
        "id": "KA5co4jqiL48"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eZRaFPgjiY7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "from torch.optim.lr_scheduler import CyclicLR\n",
        "\n",
        "def compare(output, target, confidence):\n",
        "    if not isinstance(output, (float, int, torch.LongTensor, torch.cuda.LongTensor)):\n",
        "        output = torch.clone(output).detach()\n",
        "        output[target] -= confidence  # Slightly reduce the confidence for targeted attacks\n",
        "        output = torch.argmax(output, dim=1)\n",
        "    return output == target\n",
        "\n",
        "def compare_round(mal_x_batch,newimg, model,removal_array):\n",
        "\n",
        "    adv_rounded = final_rounding(mal_x_batch, newimg, model,removal_array)\n",
        "    #l0dist = torch.sum((mal_x_batch - newimg)!=0, dim=1)\n",
        "    l0dist = torch.sum(torch.abs(adv_rounded - mal_x_batch), dim=1)\n",
        "    logits = model(adv_rounded)\n",
        "    output = torch.argmax(logits, dim=1)\n",
        "\n",
        "    return output == 0, l0dist\n",
        "\n",
        "def sin_func_torch(input_tensor):\n",
        "    \"\"\"\n",
        "    A sine-based transformation applied to the input tensor.\n",
        "\n",
        "    The function applies a sine transformation and rescales the output to [0, 1].\n",
        "    \"\"\"\n",
        "    return (torch.sin((input_tensor - 0.5) * torch.pi) + 1) / 2.0\n",
        "\n",
        "def CW_l1(imgs, y, model, removal_array, max_iterations=1000, max_learning_rate=0.1,\n",
        "       binary_search_steps=3, confidence=0.01, initial_const=0.1,\n",
        "       cons_increase_factor=2, device='cuda'):\n",
        "    \"\"\"\n",
        "    Implements the Carlini & Wagner (CW) L1 attack.\n",
        "\n",
        "    Parameters:\n",
        "    - imgs: The original images to be attacked.\n",
        "    - y: The true labels of the images.\n",
        "    - model: The target model to attack.\n",
        "    - removal_array: A mask array specifying which features are fixed.\n",
        "    - max_iterations: The maximum number of iterations for the attack.\n",
        "    - learning_rate: The learning rate for the optimizer.\n",
        "    - binary_search_steps: The number of steps for the binary search over the constant.\n",
        "    - confidence: Confidence value for the attack.\n",
        "    - initial_const: The initial constant for the attack loss.\n",
        "    - cons_increase_factor: The factor by which the constant is increased during binary search.\n",
        "    - device: The device on which to perform computations.\n",
        "\n",
        "    Returns:\n",
        "    - final_output: The perturbed images after the attack.\n",
        "    \"\"\"\n",
        "\n",
        "    labs = torch.zeros_like(y.squeeze(), dtype=torch.int64)\n",
        "    batch_size = imgs.shape[0]\n",
        "\n",
        "    # Expand removal array to match the batch size\n",
        "    expanded_removal_array = removal_array.expand(imgs.shape[0], -1)\n",
        "    non_fixed_features_mask = torch.bitwise_or(expanded_removal_array, 1 - imgs.to(torch.uint8))\n",
        "\n",
        "    mask_0 = non_fixed_features_mask == 0\n",
        "    mask_1 = non_fixed_features_mask == 1\n",
        "\n",
        "    imgs_tensor = imgs.clone().detach().to(dtype=torch.float32, device=device)\n",
        "    labs_tensor = labs.clone().detach().to(dtype=torch.int64, device=device)\n",
        "\n",
        "    lower_bound = torch.zeros(batch_size, dtype=torch.float32, device=device)\n",
        "    CONST = torch.full((batch_size,), initial_const, dtype=torch.float32, device=device)\n",
        "    upper_bound = torch.full((batch_size,), 1e10, dtype=torch.float32, device=device)\n",
        "\n",
        "    o_bestl0 = torch.full((batch_size,), 1e10, device=device)\n",
        "    o_bestattack = imgs.clone().detach()\n",
        "\n",
        "    o_best_loss1 = torch.full((batch_size,), float('inf'), device=device)\n",
        "    o_best_unsuccessfull_attack = imgs.clone().detach()\n",
        "    o_success_attack = torch.zeros(batch_size, dtype=torch.bool, device=device)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    for outer_step in range(binary_search_steps):\n",
        "        print(CONST)\n",
        "\n",
        "        modifier = torch.zeros_like(imgs_tensor, requires_grad=True).to(device)\n",
        "        optimizer = torch.optim.Adam([modifier], lr=max_learning_rate)\n",
        "\n",
        "        def grad_hook(grad):\n",
        "            return grad * non_fixed_features_mask\n",
        "        modifier.register_hook(grad_hook)\n",
        "\n",
        "        scheduler = CyclicLR(optimizer, base_lr=0.0001, max_lr=max_learning_rate, step_size_up=5,\n",
        "                             step_size_down=4995, mode='triangular2')\n",
        "\n",
        "        best_loss = torch.full((batch_size,), float('inf'), device=device)\n",
        "        no_improvement_count = torch.zeros(batch_size, dtype=torch.int, device=device)\n",
        "        active_mask = torch.ones(batch_size, dtype=torch.bool, device=device)\n",
        "\n",
        "        bestl0 = torch.full((batch_size,), 1e10, device=device)\n",
        "        bestscore = torch.full((batch_size,), -1, device=device)\n",
        "        success_mask = torch.zeros(batch_size, dtype=torch.bool, device=device)\n",
        "\n",
        "        for iteration in range(max_iterations):\n",
        "\n",
        "            active_indices = torch.where(active_mask)[0]\n",
        "            active_modifier = modifier[active_mask]\n",
        "            active_imgs_tensor = imgs_tensor[active_mask]\n",
        "            active_labs_tensor = labs_tensor[active_mask]\n",
        "            active_CONST = CONST[active_mask]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            modified_input = active_imgs_tensor + active_modifier\n",
        "            newimg = sin_func_torch(modified_input)\n",
        "            output = model(newimg)\n",
        "\n",
        "            #eps = 1e-2\n",
        "            perturbation = torch.where((active_imgs_tensor==1.), (active_imgs_tensor - newimg), (newimg - active_imgs_tensor))\n",
        "\n",
        "            #l2dist = torch.sum((1 - (perturbation - 1)**2 + eps)**0.5 - eps**0.5 , dim=1)\n",
        "            #l2dist = torch.sum((newimg - active_imgs_tensor) ** 2, dim=1)\n",
        "            l2dist = torch.sum(torch.tanh(2 * perturbation), dim=1)\n",
        "\n",
        "            real = output[torch.arange(active_indices.size(0)), active_labs_tensor]\n",
        "            other = output[torch.arange(active_indices.size(0)), active_labs_tensor + 1]\n",
        "\n",
        "            loss1 = torch.clamp((other - real + confidence), min=0.0)\n",
        "            #loss2 = l2dist\n",
        "\n",
        "            if outer_step != (binary_search_steps - 1):\n",
        "                loss = (active_CONST * loss1) + l2dist\n",
        "            else:\n",
        "                loss = loss1\n",
        "\n",
        "            total_loss = torch.sum(loss)\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            if (iteration + 1) % 10 == 0:\n",
        "                with torch.no_grad():\n",
        "                    success_mask_inloop, l0dist = compare_round(active_imgs_tensor,newimg, model,removal_array)\n",
        "                    success_mask  = torch.logical_or(success_mask_inloop, success_mask)\n",
        "                    current_lr = scheduler.get_last_lr()[0]\n",
        "\n",
        "                    improved_mask2 = (loss1 < o_best_loss1[active_mask])\n",
        "                    o_best_loss1[active_mask] = torch.where(improved_mask2, loss1, o_best_loss1[active_mask])\n",
        "                    o_best_unsuccessfull_attack[active_mask] = torch.where(improved_mask2.unsqueeze(-1), newimg, o_best_unsuccessfull_attack[active_mask])\n",
        "\n",
        "                    update_mask = (l0dist < bestl0[active_mask]) & success_mask_inloop\n",
        "                    bestl0[active_mask] = torch.where(update_mask, l0dist, bestl0[active_mask])\n",
        "                    bestscore[active_mask] = torch.where(update_mask, torch.argmax(output, dim=1), bestscore[active_mask])\n",
        "\n",
        "                    update_mask2 = (l0dist < o_bestl0[active_mask]) & success_mask_inloop\n",
        "                    o_bestl0[active_mask] = torch.where(update_mask2, l0dist, o_bestl0[active_mask])\n",
        "                    o_bestattack[active_mask] = torch.where(update_mask2.unsqueeze(-1), newimg, o_bestattack[active_mask])\n",
        "\n",
        "                    # Skip the rest of the loop for samples that haven't improved in 50 iterations\n",
        "                    #active_mask[active_mask.clone()] = no_improvement_count[active_mask.clone()] < 50\n",
        "\n",
        "                    if (iteration + 1) % 100 == 0:\n",
        "                        print(f\"Iteration {iteration + 1}: Current success : {success_mask_inloop.sum()} \\t All success : {success_mask.sum()} \\t mean(l0) : {o_bestl0[success_mask].mean():.6f} \\t Current Learning Rate: {current_lr:.3f}\")\n",
        "\n",
        "        if outer_step != (binary_search_steps - 1):\n",
        "            #o_success_attack  = torch.logical_or(o_success_attack, success_mask)\n",
        "            o_success_attack, o_l0dist = compare_round(imgs,o_bestattack, model,removal_array)\n",
        "            print(f\"outer_step {outer_step + 1}: all success : {o_success_attack.sum()} \\t mean(l0)(success) : {o_l0dist[o_success_attack].mean():.4f}\")\n",
        "            #print('o_success_attack ', o_success_attack.sum())\n",
        "            print('-------------------------------------------------------------')\n",
        "            failure_mask = ~success_mask\n",
        "            upper_bound[success_mask] = torch.minimum(upper_bound[success_mask], CONST[success_mask])\n",
        "            lower_bound[failure_mask] = torch.maximum(lower_bound[failure_mask], CONST[failure_mask])\n",
        "\n",
        "            upper_limit_mask = upper_bound < 1e10\n",
        "            CONST[upper_limit_mask] = (lower_bound[upper_limit_mask] + upper_bound[upper_limit_mask]) / 2\n",
        "            CONST[failure_mask & ~upper_limit_mask] *= cons_increase_factor\n",
        "\n",
        "    success_attack_mask = (o_bestl0 != 1e10)\n",
        "    final_output = torch.where(success_attack_mask.unsqueeze(-1), o_bestattack, o_best_unsuccessfull_attack)\n",
        "\n",
        "    return final_output\n"
      ],
      "metadata": {
        "id": "KIMKOVx7iZLE"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#l2dist = torch.sum(perturbation, dim=1)\n",
        "attack_params =  {'device':device, 'confidence':0.5, 'removal_array':removal_array,'max_iterations': 20000, 'max_learning_rate': 0.1, 'initial_const': 1 , 'binary_search_steps' : 7, 'cons_increase_factor' : 10}\n",
        "adv = adv_predict(test_loader, combined_model, CW_l1, **attack_params)"
      ],
      "metadata": {
        "id": "Fi1CCrTOiZLF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8819ef8-8f50-43a7-c5dc-185bff134eaf"
      },
      "execution_count": 19,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0')\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-15-86a2713630d1>:43: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at ../aten/src/ATen/native/TensorCompare.cpp:530.)\n",
            "  final[active_indices, indices] = torch.where(non_fixed_features_mask[active_indices, indices],\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 100: Current success : 37 \t All success : 53 \t mean(l0) : 3.207547 \t Current Learning Rate: 0.098\n",
            "Iteration 200: Current success : 35 \t All success : 54 \t mean(l0) : 3.333333 \t Current Learning Rate: 0.096\n",
            "Iteration 300: Current success : 36 \t All success : 56 \t mean(l0) : 3.482143 \t Current Learning Rate: 0.094\n",
            "Iteration 400: Current success : 35 \t All success : 56 \t mean(l0) : 3.482143 \t Current Learning Rate: 0.092\n",
            "Iteration 500: Current success : 35 \t All success : 56 \t mean(l0) : 3.482143 \t Current Learning Rate: 0.090\n",
            "Iteration 600: Current success : 35 \t All success : 56 \t mean(l0) : 3.482143 \t Current Learning Rate: 0.088\n",
            "Iteration 700: Current success : 35 \t All success : 56 \t mean(l0) : 3.482143 \t Current Learning Rate: 0.086\n",
            "Iteration 800: Current success : 35 \t All success : 56 \t mean(l0) : 3.482143 \t Current Learning Rate: 0.084\n",
            "Iteration 900: Current success : 37 \t All success : 56 \t mean(l0) : 3.446429 \t Current Learning Rate: 0.082\n",
            "Iteration 1000: Current success : 37 \t All success : 56 \t mean(l0) : 3.446429 \t Current Learning Rate: 0.080\n",
            "Iteration 1100: Current success : 37 \t All success : 57 \t mean(l0) : 3.561404 \t Current Learning Rate: 0.078\n",
            "Iteration 1200: Current success : 37 \t All success : 57 \t mean(l0) : 3.561404 \t Current Learning Rate: 0.076\n",
            "Iteration 1300: Current success : 37 \t All success : 57 \t mean(l0) : 3.561404 \t Current Learning Rate: 0.074\n",
            "Iteration 1400: Current success : 37 \t All success : 57 \t mean(l0) : 3.561404 \t Current Learning Rate: 0.072\n",
            "Iteration 1500: Current success : 37 \t All success : 57 \t mean(l0) : 3.561404 \t Current Learning Rate: 0.070\n",
            "Iteration 1600: Current success : 37 \t All success : 57 \t mean(l0) : 3.561404 \t Current Learning Rate: 0.068\n",
            "Iteration 1700: Current success : 37 \t All success : 57 \t mean(l0) : 3.561404 \t Current Learning Rate: 0.066\n",
            "Iteration 1800: Current success : 37 \t All success : 57 \t mean(l0) : 3.561404 \t Current Learning Rate: 0.064\n",
            "Iteration 1900: Current success : 37 \t All success : 57 \t mean(l0) : 3.561404 \t Current Learning Rate: 0.062\n",
            "Iteration 2000: Current success : 37 \t All success : 57 \t mean(l0) : 3.561404 \t Current Learning Rate: 0.060\n",
            "Iteration 2100: Current success : 37 \t All success : 57 \t mean(l0) : 3.561404 \t Current Learning Rate: 0.058\n",
            "Iteration 2200: Current success : 37 \t All success : 57 \t mean(l0) : 3.561404 \t Current Learning Rate: 0.056\n",
            "Iteration 2300: Current success : 37 \t All success : 57 \t mean(l0) : 3.561404 \t Current Learning Rate: 0.054\n",
            "Iteration 2400: Current success : 37 \t All success : 57 \t mean(l0) : 3.561404 \t Current Learning Rate: 0.052\n",
            "Iteration 2500: Current success : 37 \t All success : 57 \t mean(l0) : 3.561404 \t Current Learning Rate: 0.050\n",
            "Iteration 2600: Current success : 37 \t All success : 57 \t mean(l0) : 3.561404 \t Current Learning Rate: 0.048\n",
            "Iteration 2700: Current success : 47 \t All success : 59 \t mean(l0) : 3.593220 \t Current Learning Rate: 0.046\n",
            "Iteration 2800: Current success : 48 \t All success : 59 \t mean(l0) : 3.593220 \t Current Learning Rate: 0.044\n",
            "Iteration 2900: Current success : 48 \t All success : 59 \t mean(l0) : 3.593220 \t Current Learning Rate: 0.042\n",
            "Iteration 3000: Current success : 48 \t All success : 59 \t mean(l0) : 3.593220 \t Current Learning Rate: 0.040\n",
            "Iteration 3100: Current success : 50 \t All success : 59 \t mean(l0) : 3.593220 \t Current Learning Rate: 0.038\n",
            "Iteration 3200: Current success : 51 \t All success : 59 \t mean(l0) : 3.593220 \t Current Learning Rate: 0.036\n",
            "Iteration 3300: Current success : 51 \t All success : 59 \t mean(l0) : 3.593220 \t Current Learning Rate: 0.034\n",
            "Iteration 3400: Current success : 51 \t All success : 59 \t mean(l0) : 3.593220 \t Current Learning Rate: 0.032\n",
            "Iteration 3500: Current success : 51 \t All success : 59 \t mean(l0) : 3.593220 \t Current Learning Rate: 0.030\n",
            "Iteration 3600: Current success : 51 \t All success : 59 \t mean(l0) : 3.593220 \t Current Learning Rate: 0.028\n",
            "Iteration 3700: Current success : 51 \t All success : 59 \t mean(l0) : 3.593220 \t Current Learning Rate: 0.026\n",
            "Iteration 3800: Current success : 51 \t All success : 59 \t mean(l0) : 3.593220 \t Current Learning Rate: 0.024\n",
            "Iteration 3900: Current success : 51 \t All success : 59 \t mean(l0) : 3.593220 \t Current Learning Rate: 0.022\n",
            "Iteration 4000: Current success : 51 \t All success : 59 \t mean(l0) : 3.593220 \t Current Learning Rate: 0.020\n",
            "Iteration 4100: Current success : 51 \t All success : 59 \t mean(l0) : 3.593220 \t Current Learning Rate: 0.018\n",
            "Iteration 4200: Current success : 51 \t All success : 59 \t mean(l0) : 3.593220 \t Current Learning Rate: 0.016\n",
            "Iteration 4300: Current success : 51 \t All success : 59 \t mean(l0) : 3.593220 \t Current Learning Rate: 0.014\n",
            "Iteration 4400: Current success : 51 \t All success : 59 \t mean(l0) : 3.593220 \t Current Learning Rate: 0.012\n",
            "Iteration 4500: Current success : 51 \t All success : 59 \t mean(l0) : 3.593220 \t Current Learning Rate: 0.010\n",
            "Iteration 4600: Current success : 51 \t All success : 59 \t mean(l0) : 3.593220 \t Current Learning Rate: 0.008\n",
            "Iteration 4700: Current success : 51 \t All success : 59 \t mean(l0) : 3.593220 \t Current Learning Rate: 0.006\n",
            "Iteration 4800: Current success : 51 \t All success : 59 \t mean(l0) : 3.593220 \t Current Learning Rate: 0.004\n",
            "Iteration 4900: Current success : 51 \t All success : 59 \t mean(l0) : 3.593220 \t Current Learning Rate: 0.002\n",
            "Iteration 5000: Current success : 51 \t All success : 59 \t mean(l0) : 3.593220 \t Current Learning Rate: 0.000\n",
            "Iteration 5100: Current success : 49 \t All success : 64 \t mean(l0) : 3.750000 \t Current Learning Rate: 0.049\n",
            "Iteration 5200: Current success : 56 \t All success : 64 \t mean(l0) : 3.703125 \t Current Learning Rate: 0.048\n",
            "Iteration 5300: Current success : 57 \t All success : 64 \t mean(l0) : 3.687500 \t Current Learning Rate: 0.047\n",
            "Iteration 5400: Current success : 56 \t All success : 64 \t mean(l0) : 3.687500 \t Current Learning Rate: 0.046\n",
            "Iteration 5500: Current success : 56 \t All success : 64 \t mean(l0) : 3.687500 \t Current Learning Rate: 0.045\n",
            "Iteration 5600: Current success : 56 \t All success : 64 \t mean(l0) : 3.687500 \t Current Learning Rate: 0.044\n",
            "Iteration 5700: Current success : 54 \t All success : 64 \t mean(l0) : 3.687500 \t Current Learning Rate: 0.043\n",
            "Iteration 5800: Current success : 55 \t All success : 64 \t mean(l0) : 3.687500 \t Current Learning Rate: 0.042\n",
            "Iteration 5900: Current success : 58 \t All success : 65 \t mean(l0) : 3.769231 \t Current Learning Rate: 0.041\n",
            "Iteration 6000: Current success : 53 \t All success : 65 \t mean(l0) : 3.769231 \t Current Learning Rate: 0.040\n",
            "Iteration 6100: Current success : 52 \t All success : 65 \t mean(l0) : 3.769231 \t Current Learning Rate: 0.039\n",
            "Iteration 6200: Current success : 53 \t All success : 65 \t mean(l0) : 3.769231 \t Current Learning Rate: 0.038\n",
            "Iteration 6300: Current success : 53 \t All success : 65 \t mean(l0) : 3.769231 \t Current Learning Rate: 0.037\n",
            "Iteration 6400: Current success : 48 \t All success : 65 \t mean(l0) : 3.769231 \t Current Learning Rate: 0.036\n",
            "Iteration 6500: Current success : 52 \t All success : 65 \t mean(l0) : 3.769231 \t Current Learning Rate: 0.035\n",
            "Iteration 6600: Current success : 53 \t All success : 65 \t mean(l0) : 3.769231 \t Current Learning Rate: 0.034\n",
            "Iteration 6700: Current success : 46 \t All success : 66 \t mean(l0) : 3.848485 \t Current Learning Rate: 0.033\n",
            "Iteration 6800: Current success : 53 \t All success : 66 \t mean(l0) : 3.848485 \t Current Learning Rate: 0.032\n",
            "Iteration 6900: Current success : 51 \t All success : 66 \t mean(l0) : 3.848485 \t Current Learning Rate: 0.031\n",
            "Iteration 7000: Current success : 46 \t All success : 66 \t mean(l0) : 3.848485 \t Current Learning Rate: 0.030\n",
            "Iteration 7100: Current success : 51 \t All success : 67 \t mean(l0) : 3.850746 \t Current Learning Rate: 0.029\n",
            "Iteration 7200: Current success : 53 \t All success : 67 \t mean(l0) : 3.850746 \t Current Learning Rate: 0.028\n",
            "Iteration 7300: Current success : 51 \t All success : 67 \t mean(l0) : 3.850746 \t Current Learning Rate: 0.027\n",
            "Iteration 7400: Current success : 51 \t All success : 67 \t mean(l0) : 3.850746 \t Current Learning Rate: 0.026\n",
            "Iteration 7500: Current success : 49 \t All success : 67 \t mean(l0) : 3.850746 \t Current Learning Rate: 0.025\n",
            "Iteration 7600: Current success : 55 \t All success : 67 \t mean(l0) : 3.850746 \t Current Learning Rate: 0.024\n",
            "Iteration 7700: Current success : 53 \t All success : 67 \t mean(l0) : 3.850746 \t Current Learning Rate: 0.023\n",
            "Iteration 7800: Current success : 57 \t All success : 67 \t mean(l0) : 3.850746 \t Current Learning Rate: 0.022\n",
            "Iteration 7900: Current success : 56 \t All success : 67 \t mean(l0) : 3.850746 \t Current Learning Rate: 0.021\n",
            "Iteration 8000: Current success : 57 \t All success : 67 \t mean(l0) : 3.850746 \t Current Learning Rate: 0.020\n",
            "Iteration 8100: Current success : 57 \t All success : 67 \t mean(l0) : 3.850746 \t Current Learning Rate: 0.019\n",
            "Iteration 8200: Current success : 57 \t All success : 67 \t mean(l0) : 3.850746 \t Current Learning Rate: 0.018\n",
            "Iteration 8300: Current success : 57 \t All success : 67 \t mean(l0) : 3.850746 \t Current Learning Rate: 0.017\n",
            "Iteration 8400: Current success : 57 \t All success : 67 \t mean(l0) : 3.850746 \t Current Learning Rate: 0.016\n",
            "Iteration 8500: Current success : 57 \t All success : 67 \t mean(l0) : 3.850746 \t Current Learning Rate: 0.015\n",
            "Iteration 8600: Current success : 57 \t All success : 67 \t mean(l0) : 3.850746 \t Current Learning Rate: 0.014\n",
            "Iteration 8700: Current success : 57 \t All success : 67 \t mean(l0) : 3.850746 \t Current Learning Rate: 0.013\n",
            "Iteration 8800: Current success : 57 \t All success : 67 \t mean(l0) : 3.850746 \t Current Learning Rate: 0.012\n",
            "Iteration 8900: Current success : 57 \t All success : 67 \t mean(l0) : 3.850746 \t Current Learning Rate: 0.011\n",
            "Iteration 9000: Current success : 57 \t All success : 67 \t mean(l0) : 3.850746 \t Current Learning Rate: 0.010\n",
            "Iteration 9100: Current success : 57 \t All success : 67 \t mean(l0) : 3.850746 \t Current Learning Rate: 0.009\n",
            "Iteration 9200: Current success : 57 \t All success : 67 \t mean(l0) : 3.850746 \t Current Learning Rate: 0.008\n",
            "Iteration 9300: Current success : 57 \t All success : 67 \t mean(l0) : 3.850746 \t Current Learning Rate: 0.007\n",
            "Iteration 9400: Current success : 57 \t All success : 67 \t mean(l0) : 3.850746 \t Current Learning Rate: 0.006\n",
            "Iteration 9500: Current success : 57 \t All success : 67 \t mean(l0) : 3.850746 \t Current Learning Rate: 0.005\n",
            "Iteration 9600: Current success : 57 \t All success : 67 \t mean(l0) : 3.850746 \t Current Learning Rate: 0.004\n",
            "Iteration 9700: Current success : 57 \t All success : 67 \t mean(l0) : 3.850746 \t Current Learning Rate: 0.003\n",
            "Iteration 9800: Current success : 57 \t All success : 67 \t mean(l0) : 3.850746 \t Current Learning Rate: 0.002\n",
            "Iteration 9900: Current success : 57 \t All success : 67 \t mean(l0) : 3.850746 \t Current Learning Rate: 0.001\n",
            "Iteration 10000: Current success : 57 \t All success : 67 \t mean(l0) : 3.850746 \t Current Learning Rate: 0.000\n",
            "Iteration 10100: Current success : 46 \t All success : 67 \t mean(l0) : 3.850746 \t Current Learning Rate: 0.025\n",
            "Iteration 10200: Current success : 46 \t All success : 67 \t mean(l0) : 3.850746 \t Current Learning Rate: 0.024\n",
            "Iteration 10300: Current success : 46 \t All success : 67 \t mean(l0) : 3.850746 \t Current Learning Rate: 0.024\n",
            "Iteration 10400: Current success : 46 \t All success : 67 \t mean(l0) : 3.850746 \t Current Learning Rate: 0.023\n",
            "Iteration 10500: Current success : 46 \t All success : 67 \t mean(l0) : 3.850746 \t Current Learning Rate: 0.023\n",
            "Iteration 10600: Current success : 46 \t All success : 67 \t mean(l0) : 3.850746 \t Current Learning Rate: 0.022\n",
            "Iteration 10700: Current success : 46 \t All success : 67 \t mean(l0) : 3.850746 \t Current Learning Rate: 0.022\n",
            "Iteration 10800: Current success : 46 \t All success : 67 \t mean(l0) : 3.850746 \t Current Learning Rate: 0.021\n",
            "Iteration 10900: Current success : 46 \t All success : 67 \t mean(l0) : 3.850746 \t Current Learning Rate: 0.021\n",
            "Iteration 11000: Current success : 46 \t All success : 67 \t mean(l0) : 3.850746 \t Current Learning Rate: 0.020\n",
            "Iteration 11100: Current success : 46 \t All success : 67 \t mean(l0) : 3.850746 \t Current Learning Rate: 0.020\n",
            "Iteration 11200: Current success : 46 \t All success : 67 \t mean(l0) : 3.850746 \t Current Learning Rate: 0.019\n",
            "Iteration 11300: Current success : 46 \t All success : 67 \t mean(l0) : 3.850746 \t Current Learning Rate: 0.019\n",
            "Iteration 11400: Current success : 46 \t All success : 67 \t mean(l0) : 3.850746 \t Current Learning Rate: 0.018\n",
            "Iteration 11500: Current success : 46 \t All success : 67 \t mean(l0) : 3.850746 \t Current Learning Rate: 0.018\n",
            "Iteration 11600: Current success : 46 \t All success : 67 \t mean(l0) : 3.850746 \t Current Learning Rate: 0.017\n",
            "Iteration 11700: Current success : 46 \t All success : 67 \t mean(l0) : 3.850746 \t Current Learning Rate: 0.017\n",
            "Iteration 11800: Current success : 46 \t All success : 67 \t mean(l0) : 3.850746 \t Current Learning Rate: 0.016\n",
            "Iteration 11900: Current success : 46 \t All success : 67 \t mean(l0) : 3.850746 \t Current Learning Rate: 0.016\n",
            "Iteration 12000: Current success : 46 \t All success : 67 \t mean(l0) : 3.850746 \t Current Learning Rate: 0.015\n",
            "Iteration 12100: Current success : 46 \t All success : 67 \t mean(l0) : 3.850746 \t Current Learning Rate: 0.015\n",
            "Iteration 12200: Current success : 46 \t All success : 67 \t mean(l0) : 3.850746 \t Current Learning Rate: 0.014\n",
            "Iteration 12300: Current success : 46 \t All success : 67 \t mean(l0) : 3.850746 \t Current Learning Rate: 0.014\n",
            "Iteration 12400: Current success : 46 \t All success : 67 \t mean(l0) : 3.850746 \t Current Learning Rate: 0.013\n",
            "Iteration 12500: Current success : 46 \t All success : 67 \t mean(l0) : 3.850746 \t Current Learning Rate: 0.013\n",
            "Iteration 12600: Current success : 46 \t All success : 67 \t mean(l0) : 3.850746 \t Current Learning Rate: 0.012\n",
            "Iteration 12700: Current success : 46 \t All success : 67 \t mean(l0) : 3.850746 \t Current Learning Rate: 0.012\n",
            "Iteration 12800: Current success : 46 \t All success : 67 \t mean(l0) : 3.850746 \t Current Learning Rate: 0.011\n",
            "Iteration 12900: Current success : 46 \t All success : 67 \t mean(l0) : 3.850746 \t Current Learning Rate: 0.011\n",
            "Iteration 13000: Current success : 57 \t All success : 67 \t mean(l0) : 3.850746 \t Current Learning Rate: 0.010\n",
            "Iteration 13100: Current success : 57 \t All success : 67 \t mean(l0) : 3.850746 \t Current Learning Rate: 0.010\n",
            "Iteration 13200: Current success : 56 \t All success : 67 \t mean(l0) : 3.850746 \t Current Learning Rate: 0.009\n",
            "Iteration 13300: Current success : 56 \t All success : 67 \t mean(l0) : 3.850746 \t Current Learning Rate: 0.009\n",
            "Iteration 13400: Current success : 57 \t All success : 67 \t mean(l0) : 3.850746 \t Current Learning Rate: 0.008\n",
            "Iteration 13500: Current success : 57 \t All success : 67 \t mean(l0) : 3.850746 \t Current Learning Rate: 0.008\n",
            "Iteration 13600: Current success : 58 \t All success : 67 \t mean(l0) : 3.850746 \t Current Learning Rate: 0.007\n",
            "Iteration 13700: Current success : 58 \t All success : 67 \t mean(l0) : 3.850746 \t Current Learning Rate: 0.007\n",
            "Iteration 13800: Current success : 58 \t All success : 67 \t mean(l0) : 3.850746 \t Current Learning Rate: 0.006\n",
            "Iteration 13900: Current success : 58 \t All success : 67 \t mean(l0) : 3.850746 \t Current Learning Rate: 0.006\n",
            "Iteration 14000: Current success : 58 \t All success : 67 \t mean(l0) : 3.850746 \t Current Learning Rate: 0.005\n",
            "Iteration 14100: Current success : 58 \t All success : 67 \t mean(l0) : 3.850746 \t Current Learning Rate: 0.005\n",
            "Iteration 14200: Current success : 58 \t All success : 67 \t mean(l0) : 3.850746 \t Current Learning Rate: 0.004\n",
            "Iteration 14300: Current success : 58 \t All success : 67 \t mean(l0) : 3.850746 \t Current Learning Rate: 0.004\n",
            "Iteration 14400: Current success : 58 \t All success : 67 \t mean(l0) : 3.850746 \t Current Learning Rate: 0.003\n",
            "Iteration 14500: Current success : 58 \t All success : 67 \t mean(l0) : 3.850746 \t Current Learning Rate: 0.003\n",
            "Iteration 14600: Current success : 58 \t All success : 67 \t mean(l0) : 3.850746 \t Current Learning Rate: 0.002\n",
            "Iteration 14700: Current success : 58 \t All success : 67 \t mean(l0) : 3.850746 \t Current Learning Rate: 0.002\n",
            "Iteration 14800: Current success : 58 \t All success : 67 \t mean(l0) : 3.850746 \t Current Learning Rate: 0.001\n",
            "Iteration 14900: Current success : 58 \t All success : 67 \t mean(l0) : 3.850746 \t Current Learning Rate: 0.001\n",
            "Iteration 15000: Current success : 58 \t All success : 67 \t mean(l0) : 3.850746 \t Current Learning Rate: 0.000\n",
            "Iteration 15100: Current success : 52 \t All success : 68 \t mean(l0) : 3.897059 \t Current Learning Rate: 0.012\n",
            "Iteration 15200: Current success : 57 \t All success : 68 \t mean(l0) : 3.867647 \t Current Learning Rate: 0.012\n",
            "Iteration 15300: Current success : 57 \t All success : 68 \t mean(l0) : 3.867647 \t Current Learning Rate: 0.012\n",
            "Iteration 15400: Current success : 56 \t All success : 68 \t mean(l0) : 3.867647 \t Current Learning Rate: 0.012\n",
            "Iteration 15500: Current success : 57 \t All success : 68 \t mean(l0) : 3.867647 \t Current Learning Rate: 0.011\n",
            "Iteration 15600: Current success : 56 \t All success : 68 \t mean(l0) : 3.867647 \t Current Learning Rate: 0.011\n",
            "Iteration 15700: Current success : 56 \t All success : 68 \t mean(l0) : 3.867647 \t Current Learning Rate: 0.011\n",
            "Iteration 15800: Current success : 56 \t All success : 68 \t mean(l0) : 3.867647 \t Current Learning Rate: 0.011\n",
            "Iteration 15900: Current success : 56 \t All success : 68 \t mean(l0) : 3.867647 \t Current Learning Rate: 0.010\n",
            "Iteration 16000: Current success : 56 \t All success : 68 \t mean(l0) : 3.867647 \t Current Learning Rate: 0.010\n",
            "Iteration 16100: Current success : 56 \t All success : 68 \t mean(l0) : 3.867647 \t Current Learning Rate: 0.010\n",
            "Iteration 16200: Current success : 57 \t All success : 68 \t mean(l0) : 3.867647 \t Current Learning Rate: 0.010\n",
            "Iteration 16300: Current success : 56 \t All success : 68 \t mean(l0) : 3.867647 \t Current Learning Rate: 0.009\n",
            "Iteration 16400: Current success : 55 \t All success : 68 \t mean(l0) : 3.867647 \t Current Learning Rate: 0.009\n",
            "Iteration 16500: Current success : 57 \t All success : 68 \t mean(l0) : 3.867647 \t Current Learning Rate: 0.009\n",
            "Iteration 16600: Current success : 55 \t All success : 68 \t mean(l0) : 3.867647 \t Current Learning Rate: 0.009\n",
            "Iteration 16700: Current success : 55 \t All success : 68 \t mean(l0) : 3.867647 \t Current Learning Rate: 0.008\n",
            "Iteration 16800: Current success : 55 \t All success : 68 \t mean(l0) : 3.867647 \t Current Learning Rate: 0.008\n",
            "Iteration 16900: Current success : 55 \t All success : 68 \t mean(l0) : 3.867647 \t Current Learning Rate: 0.008\n",
            "Iteration 17000: Current success : 56 \t All success : 68 \t mean(l0) : 3.867647 \t Current Learning Rate: 0.008\n",
            "Iteration 17100: Current success : 55 \t All success : 68 \t mean(l0) : 3.867647 \t Current Learning Rate: 0.007\n",
            "Iteration 17200: Current success : 55 \t All success : 68 \t mean(l0) : 3.867647 \t Current Learning Rate: 0.007\n",
            "Iteration 17300: Current success : 54 \t All success : 68 \t mean(l0) : 3.867647 \t Current Learning Rate: 0.007\n",
            "Iteration 17400: Current success : 56 \t All success : 68 \t mean(l0) : 3.867647 \t Current Learning Rate: 0.007\n",
            "Iteration 17500: Current success : 56 \t All success : 68 \t mean(l0) : 3.867647 \t Current Learning Rate: 0.006\n",
            "Iteration 17600: Current success : 55 \t All success : 68 \t mean(l0) : 3.867647 \t Current Learning Rate: 0.006\n",
            "Iteration 17700: Current success : 54 \t All success : 68 \t mean(l0) : 3.867647 \t Current Learning Rate: 0.006\n",
            "Iteration 17800: Current success : 55 \t All success : 68 \t mean(l0) : 3.867647 \t Current Learning Rate: 0.006\n",
            "Iteration 17900: Current success : 57 \t All success : 68 \t mean(l0) : 3.867647 \t Current Learning Rate: 0.005\n",
            "Iteration 18000: Current success : 57 \t All success : 68 \t mean(l0) : 3.867647 \t Current Learning Rate: 0.005\n",
            "Iteration 18100: Current success : 57 \t All success : 68 \t mean(l0) : 3.867647 \t Current Learning Rate: 0.005\n",
            "Iteration 18200: Current success : 57 \t All success : 68 \t mean(l0) : 3.867647 \t Current Learning Rate: 0.005\n",
            "Iteration 18300: Current success : 57 \t All success : 68 \t mean(l0) : 3.867647 \t Current Learning Rate: 0.004\n",
            "Iteration 18400: Current success : 57 \t All success : 68 \t mean(l0) : 3.867647 \t Current Learning Rate: 0.004\n",
            "Iteration 18500: Current success : 56 \t All success : 68 \t mean(l0) : 3.867647 \t Current Learning Rate: 0.004\n",
            "Iteration 18600: Current success : 57 \t All success : 68 \t mean(l0) : 3.867647 \t Current Learning Rate: 0.004\n",
            "Iteration 18700: Current success : 57 \t All success : 68 \t mean(l0) : 3.867647 \t Current Learning Rate: 0.003\n",
            "Iteration 18800: Current success : 57 \t All success : 68 \t mean(l0) : 3.867647 \t Current Learning Rate: 0.003\n",
            "Iteration 18900: Current success : 57 \t All success : 68 \t mean(l0) : 3.867647 \t Current Learning Rate: 0.003\n",
            "Iteration 19000: Current success : 57 \t All success : 68 \t mean(l0) : 3.867647 \t Current Learning Rate: 0.003\n",
            "Iteration 19100: Current success : 57 \t All success : 68 \t mean(l0) : 3.867647 \t Current Learning Rate: 0.002\n",
            "Iteration 19200: Current success : 57 \t All success : 68 \t mean(l0) : 3.867647 \t Current Learning Rate: 0.002\n",
            "Iteration 19300: Current success : 57 \t All success : 68 \t mean(l0) : 3.867647 \t Current Learning Rate: 0.002\n",
            "Iteration 19400: Current success : 57 \t All success : 68 \t mean(l0) : 3.867647 \t Current Learning Rate: 0.002\n",
            "Iteration 19500: Current success : 57 \t All success : 68 \t mean(l0) : 3.867647 \t Current Learning Rate: 0.001\n",
            "Iteration 19600: Current success : 57 \t All success : 68 \t mean(l0) : 3.867647 \t Current Learning Rate: 0.001\n",
            "Iteration 19700: Current success : 57 \t All success : 68 \t mean(l0) : 3.867647 \t Current Learning Rate: 0.001\n",
            "Iteration 19800: Current success : 57 \t All success : 68 \t mean(l0) : 3.867647 \t Current Learning Rate: 0.001\n",
            "Iteration 19900: Current success : 57 \t All success : 68 \t mean(l0) : 3.867647 \t Current Learning Rate: 0.000\n",
            "Iteration 20000: Current success : 57 \t All success : 68 \t mean(l0) : 3.867647 \t Current Learning Rate: 0.000\n",
            "outer_step 1: all success : 68 \t mean(l0)(success) : 3.8676\n",
            "-------------------------------------------------------------\n",
            "tensor([10., 10., 10.,  ..., 10., 10., 10.], device='cuda:0')\n",
            "Iteration 100: Current success : 64 \t All success : 77 \t mean(l0) : 6.909091 \t Current Learning Rate: 0.098\n",
            "Iteration 200: Current success : 64 \t All success : 80 \t mean(l0) : 7.137500 \t Current Learning Rate: 0.096\n",
            "Iteration 300: Current success : 63 \t All success : 109 \t mean(l0) : 7.522935 \t Current Learning Rate: 0.094\n",
            "Iteration 400: Current success : 69 \t All success : 116 \t mean(l0) : 8.206897 \t Current Learning Rate: 0.092\n",
            "Iteration 500: Current success : 69 \t All success : 116 \t mean(l0) : 8.206897 \t Current Learning Rate: 0.090\n",
            "Iteration 600: Current success : 64 \t All success : 116 \t mean(l0) : 8.206897 \t Current Learning Rate: 0.088\n",
            "Iteration 700: Current success : 65 \t All success : 116 \t mean(l0) : 8.206897 \t Current Learning Rate: 0.086\n",
            "Iteration 800: Current success : 65 \t All success : 117 \t mean(l0) : 8.170940 \t Current Learning Rate: 0.084\n",
            "Iteration 900: Current success : 65 \t All success : 117 \t mean(l0) : 8.170940 \t Current Learning Rate: 0.082\n",
            "Iteration 1000: Current success : 65 \t All success : 117 \t mean(l0) : 8.170940 \t Current Learning Rate: 0.080\n",
            "Iteration 1100: Current success : 65 \t All success : 118 \t mean(l0) : 8.118644 \t Current Learning Rate: 0.078\n",
            "Iteration 1200: Current success : 65 \t All success : 118 \t mean(l0) : 8.084745 \t Current Learning Rate: 0.076\n",
            "Iteration 1300: Current success : 66 \t All success : 119 \t mean(l0) : 8.092438 \t Current Learning Rate: 0.074\n",
            "Iteration 1400: Current success : 66 \t All success : 119 \t mean(l0) : 8.092438 \t Current Learning Rate: 0.072\n",
            "Iteration 1500: Current success : 66 \t All success : 119 \t mean(l0) : 8.092438 \t Current Learning Rate: 0.070\n",
            "Iteration 1600: Current success : 66 \t All success : 119 \t mean(l0) : 8.092438 \t Current Learning Rate: 0.068\n",
            "Iteration 1700: Current success : 66 \t All success : 119 \t mean(l0) : 8.092438 \t Current Learning Rate: 0.066\n",
            "Iteration 1800: Current success : 67 \t All success : 119 \t mean(l0) : 8.092438 \t Current Learning Rate: 0.064\n",
            "Iteration 1900: Current success : 66 \t All success : 119 \t mean(l0) : 8.092438 \t Current Learning Rate: 0.062\n",
            "Iteration 2000: Current success : 66 \t All success : 119 \t mean(l0) : 8.092438 \t Current Learning Rate: 0.060\n",
            "Iteration 2100: Current success : 67 \t All success : 119 \t mean(l0) : 8.092438 \t Current Learning Rate: 0.058\n",
            "Iteration 2200: Current success : 67 \t All success : 119 \t mean(l0) : 8.092438 \t Current Learning Rate: 0.056\n",
            "Iteration 2300: Current success : 67 \t All success : 119 \t mean(l0) : 8.092438 \t Current Learning Rate: 0.054\n",
            "Iteration 2400: Current success : 67 \t All success : 119 \t mean(l0) : 8.092438 \t Current Learning Rate: 0.052\n",
            "Iteration 2500: Current success : 67 \t All success : 119 \t mean(l0) : 8.092438 \t Current Learning Rate: 0.050\n",
            "Iteration 2600: Current success : 67 \t All success : 119 \t mean(l0) : 8.092438 \t Current Learning Rate: 0.048\n",
            "Iteration 2700: Current success : 74 \t All success : 122 \t mean(l0) : 8.008196 \t Current Learning Rate: 0.046\n",
            "Iteration 2800: Current success : 76 \t All success : 123 \t mean(l0) : 8.000000 \t Current Learning Rate: 0.044\n",
            "Iteration 2900: Current success : 76 \t All success : 123 \t mean(l0) : 8.000000 \t Current Learning Rate: 0.042\n",
            "Iteration 3000: Current success : 76 \t All success : 123 \t mean(l0) : 8.000000 \t Current Learning Rate: 0.040\n",
            "Iteration 3100: Current success : 76 \t All success : 123 \t mean(l0) : 8.000000 \t Current Learning Rate: 0.038\n",
            "Iteration 3200: Current success : 77 \t All success : 123 \t mean(l0) : 8.000000 \t Current Learning Rate: 0.036\n",
            "Iteration 3300: Current success : 78 \t All success : 123 \t mean(l0) : 7.975609 \t Current Learning Rate: 0.034\n",
            "Iteration 3400: Current success : 77 \t All success : 123 \t mean(l0) : 7.975609 \t Current Learning Rate: 0.032\n",
            "Iteration 3500: Current success : 77 \t All success : 123 \t mean(l0) : 7.975609 \t Current Learning Rate: 0.030\n",
            "Iteration 3600: Current success : 78 \t All success : 123 \t mean(l0) : 7.975609 \t Current Learning Rate: 0.028\n",
            "Iteration 3700: Current success : 78 \t All success : 123 \t mean(l0) : 7.975609 \t Current Learning Rate: 0.026\n",
            "Iteration 3800: Current success : 78 \t All success : 123 \t mean(l0) : 7.975609 \t Current Learning Rate: 0.024\n",
            "Iteration 3900: Current success : 78 \t All success : 123 \t mean(l0) : 7.975609 \t Current Learning Rate: 0.022\n",
            "Iteration 4000: Current success : 78 \t All success : 123 \t mean(l0) : 7.975609 \t Current Learning Rate: 0.020\n",
            "Iteration 4100: Current success : 78 \t All success : 123 \t mean(l0) : 7.975609 \t Current Learning Rate: 0.018\n",
            "Iteration 4200: Current success : 78 \t All success : 123 \t mean(l0) : 7.975609 \t Current Learning Rate: 0.016\n",
            "Iteration 4300: Current success : 78 \t All success : 123 \t mean(l0) : 7.975609 \t Current Learning Rate: 0.014\n",
            "Iteration 4400: Current success : 78 \t All success : 123 \t mean(l0) : 7.975609 \t Current Learning Rate: 0.012\n",
            "Iteration 4500: Current success : 78 \t All success : 123 \t mean(l0) : 7.975609 \t Current Learning Rate: 0.010\n",
            "Iteration 4600: Current success : 78 \t All success : 123 \t mean(l0) : 7.975609 \t Current Learning Rate: 0.008\n",
            "Iteration 4700: Current success : 78 \t All success : 123 \t mean(l0) : 7.975609 \t Current Learning Rate: 0.006\n",
            "Iteration 4800: Current success : 78 \t All success : 123 \t mean(l0) : 7.975609 \t Current Learning Rate: 0.004\n",
            "Iteration 4900: Current success : 78 \t All success : 123 \t mean(l0) : 7.975609 \t Current Learning Rate: 0.002\n",
            "Iteration 5000: Current success : 78 \t All success : 123 \t mean(l0) : 7.975609 \t Current Learning Rate: 0.000\n",
            "Iteration 5100: Current success : 78 \t All success : 132 \t mean(l0) : 9.242425 \t Current Learning Rate: 0.049\n",
            "Iteration 5200: Current success : 83 \t All success : 137 \t mean(l0) : 10.153285 \t Current Learning Rate: 0.048\n",
            "Iteration 5300: Current success : 85 \t All success : 138 \t mean(l0) : 10.079710 \t Current Learning Rate: 0.047\n",
            "Iteration 5400: Current success : 81 \t All success : 138 \t mean(l0) : 10.079710 \t Current Learning Rate: 0.046\n",
            "Iteration 5500: Current success : 81 \t All success : 139 \t mean(l0) : 10.366907 \t Current Learning Rate: 0.045\n",
            "Iteration 5600: Current success : 81 \t All success : 139 \t mean(l0) : 10.359713 \t Current Learning Rate: 0.044\n",
            "Iteration 5700: Current success : 82 \t All success : 139 \t mean(l0) : 10.359713 \t Current Learning Rate: 0.043\n",
            "Iteration 5800: Current success : 83 \t All success : 139 \t mean(l0) : 10.359713 \t Current Learning Rate: 0.042\n",
            "Iteration 5900: Current success : 81 \t All success : 139 \t mean(l0) : 10.359713 \t Current Learning Rate: 0.041\n",
            "Iteration 6000: Current success : 81 \t All success : 139 \t mean(l0) : 10.359713 \t Current Learning Rate: 0.040\n",
            "Iteration 6100: Current success : 79 \t All success : 139 \t mean(l0) : 10.359713 \t Current Learning Rate: 0.039\n",
            "Iteration 6200: Current success : 79 \t All success : 139 \t mean(l0) : 10.359713 \t Current Learning Rate: 0.038\n",
            "Iteration 6300: Current success : 79 \t All success : 140 \t mean(l0) : 10.321428 \t Current Learning Rate: 0.037\n",
            "Iteration 6400: Current success : 78 \t All success : 140 \t mean(l0) : 10.321428 \t Current Learning Rate: 0.036\n",
            "Iteration 6500: Current success : 79 \t All success : 140 \t mean(l0) : 10.321428 \t Current Learning Rate: 0.035\n",
            "Iteration 6600: Current success : 79 \t All success : 140 \t mean(l0) : 10.321428 \t Current Learning Rate: 0.034\n",
            "Iteration 6700: Current success : 78 \t All success : 140 \t mean(l0) : 10.321428 \t Current Learning Rate: 0.033\n",
            "Iteration 6800: Current success : 78 \t All success : 140 \t mean(l0) : 10.321428 \t Current Learning Rate: 0.032\n",
            "Iteration 6900: Current success : 77 \t All success : 140 \t mean(l0) : 10.321428 \t Current Learning Rate: 0.031\n",
            "Iteration 7000: Current success : 79 \t All success : 140 \t mean(l0) : 10.321428 \t Current Learning Rate: 0.030\n",
            "Iteration 7100: Current success : 79 \t All success : 140 \t mean(l0) : 10.321428 \t Current Learning Rate: 0.029\n",
            "Iteration 7200: Current success : 80 \t All success : 140 \t mean(l0) : 10.321428 \t Current Learning Rate: 0.028\n",
            "Iteration 7300: Current success : 79 \t All success : 140 \t mean(l0) : 10.321428 \t Current Learning Rate: 0.027\n",
            "Iteration 7400: Current success : 77 \t All success : 140 \t mean(l0) : 10.321428 \t Current Learning Rate: 0.026\n",
            "Iteration 7500: Current success : 79 \t All success : 140 \t mean(l0) : 10.321428 \t Current Learning Rate: 0.025\n",
            "Iteration 7600: Current success : 80 \t All success : 140 \t mean(l0) : 10.321428 \t Current Learning Rate: 0.024\n",
            "Iteration 7700: Current success : 81 \t All success : 140 \t mean(l0) : 10.321428 \t Current Learning Rate: 0.023\n",
            "Iteration 7800: Current success : 83 \t All success : 140 \t mean(l0) : 10.321428 \t Current Learning Rate: 0.022\n",
            "Iteration 7900: Current success : 83 \t All success : 140 \t mean(l0) : 10.321428 \t Current Learning Rate: 0.021\n",
            "Iteration 8000: Current success : 83 \t All success : 140 \t mean(l0) : 10.321428 \t Current Learning Rate: 0.020\n",
            "Iteration 8100: Current success : 83 \t All success : 140 \t mean(l0) : 10.321428 \t Current Learning Rate: 0.019\n",
            "Iteration 8200: Current success : 83 \t All success : 140 \t mean(l0) : 10.321428 \t Current Learning Rate: 0.018\n",
            "Iteration 8300: Current success : 83 \t All success : 140 \t mean(l0) : 10.321428 \t Current Learning Rate: 0.017\n",
            "Iteration 8400: Current success : 83 \t All success : 140 \t mean(l0) : 10.321428 \t Current Learning Rate: 0.016\n",
            "Iteration 8500: Current success : 83 \t All success : 140 \t mean(l0) : 10.321428 \t Current Learning Rate: 0.015\n",
            "Iteration 8600: Current success : 83 \t All success : 140 \t mean(l0) : 10.321428 \t Current Learning Rate: 0.014\n",
            "Iteration 8700: Current success : 83 \t All success : 140 \t mean(l0) : 10.321428 \t Current Learning Rate: 0.013\n",
            "Iteration 8800: Current success : 83 \t All success : 140 \t mean(l0) : 10.321428 \t Current Learning Rate: 0.012\n",
            "Iteration 8900: Current success : 83 \t All success : 140 \t mean(l0) : 10.321428 \t Current Learning Rate: 0.011\n",
            "Iteration 9000: Current success : 83 \t All success : 140 \t mean(l0) : 10.321428 \t Current Learning Rate: 0.010\n",
            "Iteration 9100: Current success : 83 \t All success : 140 \t mean(l0) : 10.321428 \t Current Learning Rate: 0.009\n",
            "Iteration 9200: Current success : 83 \t All success : 140 \t mean(l0) : 10.321428 \t Current Learning Rate: 0.008\n",
            "Iteration 9300: Current success : 83 \t All success : 140 \t mean(l0) : 10.321428 \t Current Learning Rate: 0.007\n",
            "Iteration 9400: Current success : 83 \t All success : 140 \t mean(l0) : 10.321428 \t Current Learning Rate: 0.006\n",
            "Iteration 9500: Current success : 83 \t All success : 140 \t mean(l0) : 10.321428 \t Current Learning Rate: 0.005\n",
            "Iteration 9600: Current success : 83 \t All success : 140 \t mean(l0) : 10.321428 \t Current Learning Rate: 0.004\n",
            "Iteration 9700: Current success : 83 \t All success : 140 \t mean(l0) : 10.321428 \t Current Learning Rate: 0.003\n",
            "Iteration 9800: Current success : 83 \t All success : 140 \t mean(l0) : 10.321428 \t Current Learning Rate: 0.002\n",
            "Iteration 9900: Current success : 83 \t All success : 140 \t mean(l0) : 10.321428 \t Current Learning Rate: 0.001\n",
            "Iteration 10000: Current success : 83 \t All success : 140 \t mean(l0) : 10.321428 \t Current Learning Rate: 0.000\n",
            "Iteration 10100: Current success : 62 \t All success : 141 \t mean(l0) : 10.482269 \t Current Learning Rate: 0.025\n",
            "Iteration 10200: Current success : 63 \t All success : 142 \t mean(l0) : 10.408450 \t Current Learning Rate: 0.024\n",
            "Iteration 10300: Current success : 64 \t All success : 142 \t mean(l0) : 10.380281 \t Current Learning Rate: 0.024\n",
            "Iteration 10400: Current success : 65 \t All success : 142 \t mean(l0) : 10.366197 \t Current Learning Rate: 0.023\n",
            "Iteration 10500: Current success : 64 \t All success : 142 \t mean(l0) : 10.366197 \t Current Learning Rate: 0.023\n",
            "Iteration 10600: Current success : 64 \t All success : 142 \t mean(l0) : 10.366197 \t Current Learning Rate: 0.022\n",
            "Iteration 10700: Current success : 64 \t All success : 142 \t mean(l0) : 10.359155 \t Current Learning Rate: 0.022\n",
            "Iteration 10800: Current success : 65 \t All success : 142 \t mean(l0) : 10.359155 \t Current Learning Rate: 0.021\n",
            "Iteration 10900: Current success : 64 \t All success : 142 \t mean(l0) : 10.352113 \t Current Learning Rate: 0.021\n",
            "Iteration 11000: Current success : 64 \t All success : 142 \t mean(l0) : 10.352113 \t Current Learning Rate: 0.020\n",
            "Iteration 11100: Current success : 64 \t All success : 142 \t mean(l0) : 10.352113 \t Current Learning Rate: 0.020\n",
            "Iteration 11200: Current success : 64 \t All success : 142 \t mean(l0) : 10.352113 \t Current Learning Rate: 0.019\n",
            "Iteration 11300: Current success : 64 \t All success : 142 \t mean(l0) : 10.352113 \t Current Learning Rate: 0.019\n",
            "Iteration 11400: Current success : 64 \t All success : 142 \t mean(l0) : 10.352113 \t Current Learning Rate: 0.018\n",
            "Iteration 11500: Current success : 64 \t All success : 142 \t mean(l0) : 10.352113 \t Current Learning Rate: 0.018\n",
            "Iteration 11600: Current success : 64 \t All success : 142 \t mean(l0) : 10.330986 \t Current Learning Rate: 0.017\n",
            "Iteration 11700: Current success : 64 \t All success : 142 \t mean(l0) : 10.330986 \t Current Learning Rate: 0.017\n",
            "Iteration 11800: Current success : 64 \t All success : 142 \t mean(l0) : 10.330986 \t Current Learning Rate: 0.016\n",
            "Iteration 11900: Current success : 65 \t All success : 142 \t mean(l0) : 10.330986 \t Current Learning Rate: 0.016\n",
            "Iteration 12000: Current success : 63 \t All success : 142 \t mean(l0) : 10.330986 \t Current Learning Rate: 0.015\n",
            "Iteration 12100: Current success : 64 \t All success : 142 \t mean(l0) : 10.330986 \t Current Learning Rate: 0.015\n",
            "Iteration 12200: Current success : 63 \t All success : 142 \t mean(l0) : 10.330986 \t Current Learning Rate: 0.014\n",
            "Iteration 12300: Current success : 63 \t All success : 142 \t mean(l0) : 10.330986 \t Current Learning Rate: 0.014\n",
            "Iteration 12400: Current success : 64 \t All success : 142 \t mean(l0) : 10.330986 \t Current Learning Rate: 0.013\n",
            "Iteration 12500: Current success : 63 \t All success : 142 \t mean(l0) : 10.330986 \t Current Learning Rate: 0.013\n",
            "Iteration 12600: Current success : 63 \t All success : 142 \t mean(l0) : 10.330986 \t Current Learning Rate: 0.012\n",
            "Iteration 12700: Current success : 63 \t All success : 142 \t mean(l0) : 10.330986 \t Current Learning Rate: 0.012\n",
            "Iteration 12800: Current success : 63 \t All success : 142 \t mean(l0) : 10.330986 \t Current Learning Rate: 0.011\n",
            "Iteration 12900: Current success : 65 \t All success : 142 \t mean(l0) : 10.330986 \t Current Learning Rate: 0.011\n",
            "Iteration 13000: Current success : 76 \t All success : 142 \t mean(l0) : 10.330986 \t Current Learning Rate: 0.010\n",
            "Iteration 13100: Current success : 76 \t All success : 142 \t mean(l0) : 10.330986 \t Current Learning Rate: 0.010\n",
            "Iteration 13200: Current success : 77 \t All success : 142 \t mean(l0) : 10.330986 \t Current Learning Rate: 0.009\n",
            "Iteration 13300: Current success : 77 \t All success : 142 \t mean(l0) : 10.330986 \t Current Learning Rate: 0.009\n",
            "Iteration 13400: Current success : 77 \t All success : 142 \t mean(l0) : 10.330986 \t Current Learning Rate: 0.008\n",
            "Iteration 13500: Current success : 77 \t All success : 142 \t mean(l0) : 10.330986 \t Current Learning Rate: 0.008\n",
            "Iteration 13600: Current success : 77 \t All success : 142 \t mean(l0) : 10.330986 \t Current Learning Rate: 0.007\n",
            "Iteration 13700: Current success : 77 \t All success : 142 \t mean(l0) : 10.330986 \t Current Learning Rate: 0.007\n",
            "Iteration 13800: Current success : 77 \t All success : 142 \t mean(l0) : 10.330986 \t Current Learning Rate: 0.006\n",
            "Iteration 13900: Current success : 77 \t All success : 142 \t mean(l0) : 10.330986 \t Current Learning Rate: 0.006\n",
            "Iteration 14000: Current success : 77 \t All success : 142 \t mean(l0) : 10.330986 \t Current Learning Rate: 0.005\n",
            "Iteration 14100: Current success : 77 \t All success : 142 \t mean(l0) : 10.330986 \t Current Learning Rate: 0.005\n",
            "Iteration 14200: Current success : 77 \t All success : 142 \t mean(l0) : 10.330986 \t Current Learning Rate: 0.004\n",
            "Iteration 14300: Current success : 77 \t All success : 142 \t mean(l0) : 10.330986 \t Current Learning Rate: 0.004\n",
            "Iteration 14400: Current success : 77 \t All success : 142 \t mean(l0) : 10.330986 \t Current Learning Rate: 0.003\n",
            "Iteration 14500: Current success : 77 \t All success : 142 \t mean(l0) : 10.330986 \t Current Learning Rate: 0.003\n",
            "Iteration 14600: Current success : 77 \t All success : 142 \t mean(l0) : 10.330986 \t Current Learning Rate: 0.002\n",
            "Iteration 14700: Current success : 77 \t All success : 142 \t mean(l0) : 10.330986 \t Current Learning Rate: 0.002\n",
            "Iteration 14800: Current success : 77 \t All success : 142 \t mean(l0) : 10.330986 \t Current Learning Rate: 0.001\n",
            "Iteration 14900: Current success : 77 \t All success : 142 \t mean(l0) : 10.330986 \t Current Learning Rate: 0.001\n",
            "Iteration 15000: Current success : 77 \t All success : 142 \t mean(l0) : 10.330986 \t Current Learning Rate: 0.000\n",
            "Iteration 15100: Current success : 70 \t All success : 145 \t mean(l0) : 10.917241 \t Current Learning Rate: 0.012\n",
            "Iteration 15200: Current success : 87 \t All success : 146 \t mean(l0) : 11.123287 \t Current Learning Rate: 0.012\n",
            "Iteration 15300: Current success : 83 \t All success : 147 \t mean(l0) : 11.129251 \t Current Learning Rate: 0.012\n",
            "Iteration 15400: Current success : 86 \t All success : 148 \t mean(l0) : 11.250000 \t Current Learning Rate: 0.012\n",
            "Iteration 15500: Current success : 85 \t All success : 148 \t mean(l0) : 11.229730 \t Current Learning Rate: 0.011\n",
            "Iteration 15600: Current success : 85 \t All success : 148 \t mean(l0) : 11.222973 \t Current Learning Rate: 0.011\n",
            "Iteration 15700: Current success : 85 \t All success : 148 \t mean(l0) : 11.216216 \t Current Learning Rate: 0.011\n",
            "Iteration 15800: Current success : 85 \t All success : 148 \t mean(l0) : 11.216216 \t Current Learning Rate: 0.011\n",
            "Iteration 15900: Current success : 84 \t All success : 148 \t mean(l0) : 11.216216 \t Current Learning Rate: 0.010\n",
            "Iteration 16000: Current success : 84 \t All success : 148 \t mean(l0) : 11.216216 \t Current Learning Rate: 0.010\n",
            "Iteration 16100: Current success : 84 \t All success : 148 \t mean(l0) : 11.216216 \t Current Learning Rate: 0.010\n",
            "Iteration 16200: Current success : 82 \t All success : 148 \t mean(l0) : 11.216216 \t Current Learning Rate: 0.010\n",
            "Iteration 16300: Current success : 85 \t All success : 148 \t mean(l0) : 11.216216 \t Current Learning Rate: 0.009\n",
            "Iteration 16400: Current success : 84 \t All success : 148 \t mean(l0) : 11.216216 \t Current Learning Rate: 0.009\n",
            "Iteration 16500: Current success : 82 \t All success : 148 \t mean(l0) : 11.216216 \t Current Learning Rate: 0.009\n",
            "Iteration 16600: Current success : 83 \t All success : 148 \t mean(l0) : 11.216216 \t Current Learning Rate: 0.009\n",
            "Iteration 16700: Current success : 83 \t All success : 148 \t mean(l0) : 11.209459 \t Current Learning Rate: 0.008\n",
            "Iteration 16800: Current success : 82 \t All success : 148 \t mean(l0) : 11.209459 \t Current Learning Rate: 0.008\n",
            "Iteration 16900: Current success : 84 \t All success : 148 \t mean(l0) : 11.209459 \t Current Learning Rate: 0.008\n",
            "Iteration 17000: Current success : 83 \t All success : 148 \t mean(l0) : 11.209459 \t Current Learning Rate: 0.008\n",
            "Iteration 17100: Current success : 83 \t All success : 148 \t mean(l0) : 11.189190 \t Current Learning Rate: 0.007\n",
            "Iteration 17200: Current success : 84 \t All success : 148 \t mean(l0) : 11.189190 \t Current Learning Rate: 0.007\n",
            "Iteration 17300: Current success : 83 \t All success : 148 \t mean(l0) : 11.189190 \t Current Learning Rate: 0.007\n",
            "Iteration 17400: Current success : 83 \t All success : 148 \t mean(l0) : 11.189190 \t Current Learning Rate: 0.007\n",
            "Iteration 17500: Current success : 82 \t All success : 148 \t mean(l0) : 11.189190 \t Current Learning Rate: 0.006\n",
            "Iteration 17600: Current success : 83 \t All success : 148 \t mean(l0) : 11.189190 \t Current Learning Rate: 0.006\n",
            "Iteration 17700: Current success : 82 \t All success : 148 \t mean(l0) : 11.189190 \t Current Learning Rate: 0.006\n",
            "Iteration 17800: Current success : 85 \t All success : 148 \t mean(l0) : 11.189190 \t Current Learning Rate: 0.006\n",
            "Iteration 17900: Current success : 84 \t All success : 148 \t mean(l0) : 11.189190 \t Current Learning Rate: 0.005\n",
            "Iteration 18000: Current success : 88 \t All success : 148 \t mean(l0) : 11.189190 \t Current Learning Rate: 0.005\n",
            "Iteration 18100: Current success : 88 \t All success : 148 \t mean(l0) : 11.189190 \t Current Learning Rate: 0.005\n",
            "Iteration 18200: Current success : 88 \t All success : 148 \t mean(l0) : 11.189190 \t Current Learning Rate: 0.005\n",
            "Iteration 18300: Current success : 88 \t All success : 148 \t mean(l0) : 11.189190 \t Current Learning Rate: 0.004\n",
            "Iteration 18400: Current success : 88 \t All success : 148 \t mean(l0) : 11.189190 \t Current Learning Rate: 0.004\n",
            "Iteration 18500: Current success : 88 \t All success : 148 \t mean(l0) : 11.189190 \t Current Learning Rate: 0.004\n",
            "Iteration 18600: Current success : 88 \t All success : 148 \t mean(l0) : 11.189190 \t Current Learning Rate: 0.004\n",
            "Iteration 18700: Current success : 88 \t All success : 148 \t mean(l0) : 11.189190 \t Current Learning Rate: 0.003\n",
            "Iteration 18800: Current success : 88 \t All success : 148 \t mean(l0) : 11.189190 \t Current Learning Rate: 0.003\n",
            "Iteration 18900: Current success : 88 \t All success : 148 \t mean(l0) : 11.189190 \t Current Learning Rate: 0.003\n",
            "Iteration 19000: Current success : 88 \t All success : 148 \t mean(l0) : 11.189190 \t Current Learning Rate: 0.003\n",
            "Iteration 19100: Current success : 88 \t All success : 148 \t mean(l0) : 11.189190 \t Current Learning Rate: 0.002\n",
            "Iteration 19200: Current success : 88 \t All success : 148 \t mean(l0) : 11.189190 \t Current Learning Rate: 0.002\n",
            "Iteration 19300: Current success : 88 \t All success : 148 \t mean(l0) : 11.189190 \t Current Learning Rate: 0.002\n",
            "Iteration 19400: Current success : 88 \t All success : 148 \t mean(l0) : 11.189190 \t Current Learning Rate: 0.002\n",
            "Iteration 19500: Current success : 88 \t All success : 148 \t mean(l0) : 11.189190 \t Current Learning Rate: 0.001\n",
            "Iteration 19600: Current success : 88 \t All success : 148 \t mean(l0) : 11.189190 \t Current Learning Rate: 0.001\n",
            "Iteration 19700: Current success : 88 \t All success : 148 \t mean(l0) : 11.189190 \t Current Learning Rate: 0.001\n",
            "Iteration 19800: Current success : 88 \t All success : 148 \t mean(l0) : 11.189190 \t Current Learning Rate: 0.001\n",
            "Iteration 19900: Current success : 88 \t All success : 148 \t mean(l0) : 11.189190 \t Current Learning Rate: 0.000\n",
            "Iteration 20000: Current success : 88 \t All success : 148 \t mean(l0) : 11.189190 \t Current Learning Rate: 0.000\n",
            "outer_step 2: all success : 162 \t mean(l0)(success) : 10.4938\n",
            "-------------------------------------------------------------\n",
            "tensor([100., 100., 100.,  ..., 100., 100., 100.], device='cuda:0')\n",
            "Iteration 100: Current success : 63 \t All success : 76 \t mean(l0) : 12.736842 \t Current Learning Rate: 0.098\n",
            "Iteration 200: Current success : 63 \t All success : 93 \t mean(l0) : 13.258064 \t Current Learning Rate: 0.096\n",
            "Iteration 300: Current success : 65 \t All success : 103 \t mean(l0) : 15.932039 \t Current Learning Rate: 0.094\n",
            "Iteration 400: Current success : 64 \t All success : 104 \t mean(l0) : 15.865385 \t Current Learning Rate: 0.092\n",
            "Iteration 500: Current success : 64 \t All success : 104 \t mean(l0) : 15.557693 \t Current Learning Rate: 0.090\n",
            "Iteration 600: Current success : 65 \t All success : 104 \t mean(l0) : 15.538462 \t Current Learning Rate: 0.088\n",
            "Iteration 700: Current success : 64 \t All success : 104 \t mean(l0) : 15.538462 \t Current Learning Rate: 0.086\n",
            "Iteration 800: Current success : 64 \t All success : 104 \t mean(l0) : 15.528847 \t Current Learning Rate: 0.084\n",
            "Iteration 900: Current success : 64 \t All success : 104 \t mean(l0) : 15.528847 \t Current Learning Rate: 0.082\n",
            "Iteration 1000: Current success : 64 \t All success : 104 \t mean(l0) : 15.528847 \t Current Learning Rate: 0.080\n",
            "Iteration 1100: Current success : 64 \t All success : 106 \t mean(l0) : 15.273585 \t Current Learning Rate: 0.078\n",
            "Iteration 1200: Current success : 64 \t All success : 106 \t mean(l0) : 15.273585 \t Current Learning Rate: 0.076\n",
            "Iteration 1300: Current success : 63 \t All success : 106 \t mean(l0) : 15.273585 \t Current Learning Rate: 0.074\n",
            "Iteration 1400: Current success : 64 \t All success : 106 \t mean(l0) : 15.273585 \t Current Learning Rate: 0.072\n",
            "Iteration 1500: Current success : 66 \t All success : 108 \t mean(l0) : 15.064815 \t Current Learning Rate: 0.070\n",
            "Iteration 1600: Current success : 65 \t All success : 108 \t mean(l0) : 15.064815 \t Current Learning Rate: 0.068\n",
            "Iteration 1700: Current success : 64 \t All success : 108 \t mean(l0) : 15.064815 \t Current Learning Rate: 0.066\n",
            "Iteration 1800: Current success : 65 \t All success : 108 \t mean(l0) : 15.064815 \t Current Learning Rate: 0.064\n",
            "Iteration 1900: Current success : 65 \t All success : 108 \t mean(l0) : 15.064815 \t Current Learning Rate: 0.062\n",
            "Iteration 2000: Current success : 65 \t All success : 111 \t mean(l0) : 14.846848 \t Current Learning Rate: 0.060\n",
            "Iteration 2100: Current success : 64 \t All success : 111 \t mean(l0) : 14.846848 \t Current Learning Rate: 0.058\n",
            "Iteration 2200: Current success : 66 \t All success : 111 \t mean(l0) : 14.846848 \t Current Learning Rate: 0.056\n",
            "Iteration 2300: Current success : 67 \t All success : 116 \t mean(l0) : 15.948276 \t Current Learning Rate: 0.054\n",
            "Iteration 2400: Current success : 67 \t All success : 116 \t mean(l0) : 15.948276 \t Current Learning Rate: 0.052\n",
            "Iteration 2500: Current success : 68 \t All success : 117 \t mean(l0) : 16.222223 \t Current Learning Rate: 0.050\n",
            "Iteration 2600: Current success : 68 \t All success : 118 \t mean(l0) : 16.042374 \t Current Learning Rate: 0.048\n",
            "Iteration 2700: Current success : 77 \t All success : 121 \t mean(l0) : 15.760330 \t Current Learning Rate: 0.046\n",
            "Iteration 2800: Current success : 79 \t All success : 121 \t mean(l0) : 15.760330 \t Current Learning Rate: 0.044\n",
            "Iteration 2900: Current success : 79 \t All success : 121 \t mean(l0) : 15.760330 \t Current Learning Rate: 0.042\n",
            "Iteration 3000: Current success : 79 \t All success : 121 \t mean(l0) : 15.760330 \t Current Learning Rate: 0.040\n",
            "Iteration 3100: Current success : 80 \t All success : 121 \t mean(l0) : 15.760330 \t Current Learning Rate: 0.038\n",
            "Iteration 3200: Current success : 80 \t All success : 121 \t mean(l0) : 15.760330 \t Current Learning Rate: 0.036\n",
            "Iteration 3300: Current success : 80 \t All success : 121 \t mean(l0) : 15.760330 \t Current Learning Rate: 0.034\n",
            "Iteration 3400: Current success : 80 \t All success : 121 \t mean(l0) : 15.760330 \t Current Learning Rate: 0.032\n",
            "Iteration 3500: Current success : 80 \t All success : 121 \t mean(l0) : 15.760330 \t Current Learning Rate: 0.030\n",
            "Iteration 3600: Current success : 80 \t All success : 121 \t mean(l0) : 15.760330 \t Current Learning Rate: 0.028\n",
            "Iteration 3700: Current success : 80 \t All success : 121 \t mean(l0) : 15.760330 \t Current Learning Rate: 0.026\n",
            "Iteration 3800: Current success : 80 \t All success : 121 \t mean(l0) : 15.760330 \t Current Learning Rate: 0.024\n",
            "Iteration 3900: Current success : 80 \t All success : 121 \t mean(l0) : 15.760330 \t Current Learning Rate: 0.022\n",
            "Iteration 4000: Current success : 80 \t All success : 121 \t mean(l0) : 15.760330 \t Current Learning Rate: 0.020\n",
            "Iteration 4100: Current success : 80 \t All success : 121 \t mean(l0) : 15.760330 \t Current Learning Rate: 0.018\n",
            "Iteration 4200: Current success : 80 \t All success : 121 \t mean(l0) : 15.760330 \t Current Learning Rate: 0.016\n",
            "Iteration 4300: Current success : 80 \t All success : 121 \t mean(l0) : 15.760330 \t Current Learning Rate: 0.014\n",
            "Iteration 4400: Current success : 80 \t All success : 121 \t mean(l0) : 15.760330 \t Current Learning Rate: 0.012\n",
            "Iteration 4500: Current success : 80 \t All success : 121 \t mean(l0) : 15.760330 \t Current Learning Rate: 0.010\n",
            "Iteration 4600: Current success : 80 \t All success : 121 \t mean(l0) : 15.760330 \t Current Learning Rate: 0.008\n",
            "Iteration 4700: Current success : 80 \t All success : 121 \t mean(l0) : 15.760330 \t Current Learning Rate: 0.006\n",
            "Iteration 4800: Current success : 80 \t All success : 121 \t mean(l0) : 15.760330 \t Current Learning Rate: 0.004\n",
            "Iteration 4900: Current success : 80 \t All success : 121 \t mean(l0) : 15.760330 \t Current Learning Rate: 0.002\n",
            "Iteration 5000: Current success : 80 \t All success : 121 \t mean(l0) : 15.760330 \t Current Learning Rate: 0.000\n",
            "Iteration 5100: Current success : 70 \t All success : 129 \t mean(l0) : 17.108526 \t Current Learning Rate: 0.049\n",
            "Iteration 5200: Current success : 81 \t All success : 137 \t mean(l0) : 17.737226 \t Current Learning Rate: 0.048\n",
            "Iteration 5300: Current success : 88 \t All success : 141 \t mean(l0) : 18.773048 \t Current Learning Rate: 0.047\n",
            "Iteration 5400: Current success : 85 \t All success : 141 \t mean(l0) : 18.716312 \t Current Learning Rate: 0.046\n",
            "Iteration 5500: Current success : 85 \t All success : 141 \t mean(l0) : 18.695034 \t Current Learning Rate: 0.045\n",
            "Iteration 5600: Current success : 85 \t All success : 141 \t mean(l0) : 18.680851 \t Current Learning Rate: 0.044\n",
            "Iteration 5700: Current success : 84 \t All success : 141 \t mean(l0) : 18.680851 \t Current Learning Rate: 0.043\n",
            "Iteration 5800: Current success : 84 \t All success : 141 \t mean(l0) : 18.666666 \t Current Learning Rate: 0.042\n",
            "Iteration 5900: Current success : 85 \t All success : 142 \t mean(l0) : 18.633802 \t Current Learning Rate: 0.041\n",
            "Iteration 6000: Current success : 85 \t All success : 142 \t mean(l0) : 18.619719 \t Current Learning Rate: 0.040\n",
            "Iteration 6100: Current success : 85 \t All success : 143 \t mean(l0) : 18.538462 \t Current Learning Rate: 0.039\n",
            "Iteration 6200: Current success : 83 \t All success : 143 \t mean(l0) : 18.538462 \t Current Learning Rate: 0.038\n",
            "Iteration 6300: Current success : 85 \t All success : 143 \t mean(l0) : 18.510489 \t Current Learning Rate: 0.037\n",
            "Iteration 6400: Current success : 85 \t All success : 143 \t mean(l0) : 18.510489 \t Current Learning Rate: 0.036\n",
            "Iteration 6500: Current success : 83 \t All success : 143 \t mean(l0) : 18.510489 \t Current Learning Rate: 0.035\n",
            "Iteration 6600: Current success : 84 \t All success : 143 \t mean(l0) : 18.510489 \t Current Learning Rate: 0.034\n",
            "Iteration 6700: Current success : 85 \t All success : 143 \t mean(l0) : 18.510489 \t Current Learning Rate: 0.033\n",
            "Iteration 6800: Current success : 84 \t All success : 143 \t mean(l0) : 18.510489 \t Current Learning Rate: 0.032\n",
            "Iteration 6900: Current success : 85 \t All success : 143 \t mean(l0) : 18.510489 \t Current Learning Rate: 0.031\n",
            "Iteration 7000: Current success : 84 \t All success : 143 \t mean(l0) : 18.510489 \t Current Learning Rate: 0.030\n",
            "Iteration 7100: Current success : 84 \t All success : 143 \t mean(l0) : 18.510489 \t Current Learning Rate: 0.029\n",
            "Iteration 7200: Current success : 85 \t All success : 143 \t mean(l0) : 18.510489 \t Current Learning Rate: 0.028\n",
            "Iteration 7300: Current success : 86 \t All success : 143 \t mean(l0) : 18.510489 \t Current Learning Rate: 0.027\n",
            "Iteration 7400: Current success : 85 \t All success : 143 \t mean(l0) : 18.510489 \t Current Learning Rate: 0.026\n",
            "Iteration 7500: Current success : 84 \t All success : 143 \t mean(l0) : 18.510489 \t Current Learning Rate: 0.025\n",
            "Iteration 7600: Current success : 86 \t All success : 143 \t mean(l0) : 18.510489 \t Current Learning Rate: 0.024\n",
            "Iteration 7700: Current success : 86 \t All success : 143 \t mean(l0) : 18.510489 \t Current Learning Rate: 0.023\n",
            "Iteration 7800: Current success : 87 \t All success : 143 \t mean(l0) : 18.510489 \t Current Learning Rate: 0.022\n",
            "Iteration 7900: Current success : 87 \t All success : 143 \t mean(l0) : 18.510489 \t Current Learning Rate: 0.021\n",
            "Iteration 8000: Current success : 87 \t All success : 143 \t mean(l0) : 18.510489 \t Current Learning Rate: 0.020\n",
            "Iteration 8100: Current success : 86 \t All success : 143 \t mean(l0) : 18.510489 \t Current Learning Rate: 0.019\n",
            "Iteration 8200: Current success : 86 \t All success : 143 \t mean(l0) : 18.510489 \t Current Learning Rate: 0.018\n",
            "Iteration 8300: Current success : 87 \t All success : 143 \t mean(l0) : 18.510489 \t Current Learning Rate: 0.017\n",
            "Iteration 8400: Current success : 87 \t All success : 143 \t mean(l0) : 18.510489 \t Current Learning Rate: 0.016\n",
            "Iteration 8500: Current success : 87 \t All success : 143 \t mean(l0) : 18.510489 \t Current Learning Rate: 0.015\n",
            "Iteration 8600: Current success : 87 \t All success : 143 \t mean(l0) : 18.510489 \t Current Learning Rate: 0.014\n",
            "Iteration 8700: Current success : 87 \t All success : 143 \t mean(l0) : 18.510489 \t Current Learning Rate: 0.013\n",
            "Iteration 8800: Current success : 87 \t All success : 143 \t mean(l0) : 18.510489 \t Current Learning Rate: 0.012\n",
            "Iteration 8900: Current success : 87 \t All success : 143 \t mean(l0) : 18.510489 \t Current Learning Rate: 0.011\n",
            "Iteration 9000: Current success : 87 \t All success : 143 \t mean(l0) : 18.510489 \t Current Learning Rate: 0.010\n",
            "Iteration 9100: Current success : 87 \t All success : 143 \t mean(l0) : 18.510489 \t Current Learning Rate: 0.009\n",
            "Iteration 9200: Current success : 87 \t All success : 143 \t mean(l0) : 18.510489 \t Current Learning Rate: 0.008\n",
            "Iteration 9300: Current success : 87 \t All success : 143 \t mean(l0) : 18.510489 \t Current Learning Rate: 0.007\n",
            "Iteration 9400: Current success : 87 \t All success : 143 \t mean(l0) : 18.510489 \t Current Learning Rate: 0.006\n",
            "Iteration 9500: Current success : 87 \t All success : 143 \t mean(l0) : 18.510489 \t Current Learning Rate: 0.005\n",
            "Iteration 9600: Current success : 87 \t All success : 143 \t mean(l0) : 18.510489 \t Current Learning Rate: 0.004\n",
            "Iteration 9700: Current success : 87 \t All success : 143 \t mean(l0) : 18.510489 \t Current Learning Rate: 0.003\n",
            "Iteration 9800: Current success : 87 \t All success : 143 \t mean(l0) : 18.510489 \t Current Learning Rate: 0.002\n",
            "Iteration 9900: Current success : 87 \t All success : 143 \t mean(l0) : 18.510489 \t Current Learning Rate: 0.001\n",
            "Iteration 10000: Current success : 87 \t All success : 143 \t mean(l0) : 18.510489 \t Current Learning Rate: 0.000\n",
            "Iteration 10100: Current success : 80 \t All success : 146 \t mean(l0) : 18.986301 \t Current Learning Rate: 0.025\n",
            "Iteration 10200: Current success : 83 \t All success : 146 \t mean(l0) : 18.917809 \t Current Learning Rate: 0.024\n",
            "Iteration 10300: Current success : 80 \t All success : 146 \t mean(l0) : 18.917809 \t Current Learning Rate: 0.024\n",
            "Iteration 10400: Current success : 79 \t All success : 146 \t mean(l0) : 18.917809 \t Current Learning Rate: 0.023\n",
            "Iteration 10500: Current success : 79 \t All success : 146 \t mean(l0) : 18.904110 \t Current Learning Rate: 0.023\n",
            "Iteration 10600: Current success : 78 \t All success : 146 \t mean(l0) : 18.904110 \t Current Learning Rate: 0.022\n",
            "Iteration 10700: Current success : 78 \t All success : 146 \t mean(l0) : 18.904110 \t Current Learning Rate: 0.022\n",
            "Iteration 10800: Current success : 78 \t All success : 146 \t mean(l0) : 18.904110 \t Current Learning Rate: 0.021\n",
            "Iteration 10900: Current success : 78 \t All success : 146 \t mean(l0) : 18.904110 \t Current Learning Rate: 0.021\n",
            "Iteration 11000: Current success : 79 \t All success : 146 \t mean(l0) : 18.904110 \t Current Learning Rate: 0.020\n",
            "Iteration 11100: Current success : 79 \t All success : 146 \t mean(l0) : 18.904110 \t Current Learning Rate: 0.020\n",
            "Iteration 11200: Current success : 78 \t All success : 146 \t mean(l0) : 18.904110 \t Current Learning Rate: 0.019\n",
            "Iteration 11300: Current success : 78 \t All success : 146 \t mean(l0) : 18.904110 \t Current Learning Rate: 0.019\n",
            "Iteration 11400: Current success : 78 \t All success : 146 \t mean(l0) : 18.904110 \t Current Learning Rate: 0.018\n",
            "Iteration 11500: Current success : 78 \t All success : 146 \t mean(l0) : 18.904110 \t Current Learning Rate: 0.018\n",
            "Iteration 11600: Current success : 78 \t All success : 146 \t mean(l0) : 18.904110 \t Current Learning Rate: 0.017\n",
            "Iteration 11700: Current success : 78 \t All success : 146 \t mean(l0) : 18.904110 \t Current Learning Rate: 0.017\n",
            "Iteration 11800: Current success : 78 \t All success : 146 \t mean(l0) : 18.904110 \t Current Learning Rate: 0.016\n",
            "Iteration 11900: Current success : 78 \t All success : 146 \t mean(l0) : 18.904110 \t Current Learning Rate: 0.016\n",
            "Iteration 12000: Current success : 79 \t All success : 146 \t mean(l0) : 18.904110 \t Current Learning Rate: 0.015\n",
            "Iteration 12100: Current success : 79 \t All success : 146 \t mean(l0) : 18.904110 \t Current Learning Rate: 0.015\n",
            "Iteration 12200: Current success : 79 \t All success : 146 \t mean(l0) : 18.904110 \t Current Learning Rate: 0.014\n",
            "Iteration 12300: Current success : 78 \t All success : 146 \t mean(l0) : 18.904110 \t Current Learning Rate: 0.014\n",
            "Iteration 12400: Current success : 78 \t All success : 146 \t mean(l0) : 18.904110 \t Current Learning Rate: 0.013\n",
            "Iteration 12500: Current success : 78 \t All success : 146 \t mean(l0) : 18.904110 \t Current Learning Rate: 0.013\n",
            "Iteration 12600: Current success : 78 \t All success : 146 \t mean(l0) : 18.904110 \t Current Learning Rate: 0.012\n",
            "Iteration 12700: Current success : 79 \t All success : 146 \t mean(l0) : 18.904110 \t Current Learning Rate: 0.012\n",
            "Iteration 12800: Current success : 78 \t All success : 146 \t mean(l0) : 18.904110 \t Current Learning Rate: 0.011\n",
            "Iteration 12900: Current success : 80 \t All success : 146 \t mean(l0) : 18.904110 \t Current Learning Rate: 0.011\n",
            "Iteration 13000: Current success : 89 \t All success : 146 \t mean(l0) : 18.904110 \t Current Learning Rate: 0.010\n",
            "Iteration 13100: Current success : 90 \t All success : 146 \t mean(l0) : 18.904110 \t Current Learning Rate: 0.010\n",
            "Iteration 13200: Current success : 91 \t All success : 146 \t mean(l0) : 18.904110 \t Current Learning Rate: 0.009\n",
            "Iteration 13300: Current success : 90 \t All success : 146 \t mean(l0) : 18.904110 \t Current Learning Rate: 0.009\n",
            "Iteration 13400: Current success : 91 \t All success : 146 \t mean(l0) : 18.904110 \t Current Learning Rate: 0.008\n",
            "Iteration 13500: Current success : 92 \t All success : 146 \t mean(l0) : 18.904110 \t Current Learning Rate: 0.008\n",
            "Iteration 13600: Current success : 92 \t All success : 146 \t mean(l0) : 18.904110 \t Current Learning Rate: 0.007\n",
            "Iteration 13700: Current success : 92 \t All success : 146 \t mean(l0) : 18.904110 \t Current Learning Rate: 0.007\n",
            "Iteration 13800: Current success : 92 \t All success : 146 \t mean(l0) : 18.904110 \t Current Learning Rate: 0.006\n",
            "Iteration 13900: Current success : 92 \t All success : 146 \t mean(l0) : 18.904110 \t Current Learning Rate: 0.006\n",
            "Iteration 14000: Current success : 92 \t All success : 146 \t mean(l0) : 18.904110 \t Current Learning Rate: 0.005\n",
            "Iteration 14100: Current success : 92 \t All success : 146 \t mean(l0) : 18.904110 \t Current Learning Rate: 0.005\n",
            "Iteration 14200: Current success : 92 \t All success : 146 \t mean(l0) : 18.904110 \t Current Learning Rate: 0.004\n",
            "Iteration 14300: Current success : 92 \t All success : 146 \t mean(l0) : 18.904110 \t Current Learning Rate: 0.004\n",
            "Iteration 14400: Current success : 92 \t All success : 146 \t mean(l0) : 18.904110 \t Current Learning Rate: 0.003\n",
            "Iteration 14500: Current success : 92 \t All success : 146 \t mean(l0) : 18.904110 \t Current Learning Rate: 0.003\n",
            "Iteration 14600: Current success : 92 \t All success : 146 \t mean(l0) : 18.904110 \t Current Learning Rate: 0.002\n",
            "Iteration 14700: Current success : 92 \t All success : 146 \t mean(l0) : 18.904110 \t Current Learning Rate: 0.002\n",
            "Iteration 14800: Current success : 92 \t All success : 146 \t mean(l0) : 18.904110 \t Current Learning Rate: 0.001\n",
            "Iteration 14900: Current success : 92 \t All success : 146 \t mean(l0) : 18.904110 \t Current Learning Rate: 0.001\n",
            "Iteration 15000: Current success : 92 \t All success : 146 \t mean(l0) : 18.904110 \t Current Learning Rate: 0.000\n",
            "Iteration 15100: Current success : 82 \t All success : 148 \t mean(l0) : 18.952703 \t Current Learning Rate: 0.012\n",
            "Iteration 15200: Current success : 92 \t All success : 148 \t mean(l0) : 18.878378 \t Current Learning Rate: 0.012\n",
            "Iteration 15300: Current success : 92 \t All success : 150 \t mean(l0) : 19.446667 \t Current Learning Rate: 0.012\n",
            "Iteration 15400: Current success : 91 \t All success : 150 \t mean(l0) : 19.413334 \t Current Learning Rate: 0.012\n",
            "Iteration 15500: Current success : 92 \t All success : 150 \t mean(l0) : 19.400000 \t Current Learning Rate: 0.011\n",
            "Iteration 15600: Current success : 91 \t All success : 150 \t mean(l0) : 19.400000 \t Current Learning Rate: 0.011\n",
            "Iteration 15700: Current success : 93 \t All success : 154 \t mean(l0) : 20.441559 \t Current Learning Rate: 0.011\n",
            "Iteration 15800: Current success : 95 \t All success : 154 \t mean(l0) : 20.441559 \t Current Learning Rate: 0.011\n",
            "Iteration 15900: Current success : 93 \t All success : 154 \t mean(l0) : 20.441559 \t Current Learning Rate: 0.010\n",
            "Iteration 16000: Current success : 95 \t All success : 154 \t mean(l0) : 20.441559 \t Current Learning Rate: 0.010\n",
            "Iteration 16100: Current success : 94 \t All success : 154 \t mean(l0) : 20.441559 \t Current Learning Rate: 0.010\n",
            "Iteration 16200: Current success : 95 \t All success : 154 \t mean(l0) : 20.435064 \t Current Learning Rate: 0.010\n",
            "Iteration 16300: Current success : 96 \t All success : 154 \t mean(l0) : 20.435064 \t Current Learning Rate: 0.009\n",
            "Iteration 16400: Current success : 93 \t All success : 154 \t mean(l0) : 20.428572 \t Current Learning Rate: 0.009\n",
            "Iteration 16500: Current success : 94 \t All success : 154 \t mean(l0) : 20.428572 \t Current Learning Rate: 0.009\n",
            "Iteration 16600: Current success : 92 \t All success : 154 \t mean(l0) : 20.428572 \t Current Learning Rate: 0.009\n",
            "Iteration 16700: Current success : 93 \t All success : 154 \t mean(l0) : 20.409090 \t Current Learning Rate: 0.008\n",
            "Iteration 16800: Current success : 93 \t All success : 154 \t mean(l0) : 20.350649 \t Current Learning Rate: 0.008\n",
            "Iteration 16900: Current success : 95 \t All success : 154 \t mean(l0) : 20.350649 \t Current Learning Rate: 0.008\n",
            "Iteration 17000: Current success : 97 \t All success : 154 \t mean(l0) : 20.350649 \t Current Learning Rate: 0.008\n",
            "Iteration 17100: Current success : 94 \t All success : 154 \t mean(l0) : 20.350649 \t Current Learning Rate: 0.007\n",
            "Iteration 17200: Current success : 93 \t All success : 154 \t mean(l0) : 20.324675 \t Current Learning Rate: 0.007\n",
            "Iteration 17300: Current success : 94 \t All success : 154 \t mean(l0) : 20.324675 \t Current Learning Rate: 0.007\n",
            "Iteration 17400: Current success : 95 \t All success : 154 \t mean(l0) : 20.324675 \t Current Learning Rate: 0.007\n",
            "Iteration 17500: Current success : 92 \t All success : 154 \t mean(l0) : 20.324675 \t Current Learning Rate: 0.006\n",
            "Iteration 17600: Current success : 96 \t All success : 154 \t mean(l0) : 20.324675 \t Current Learning Rate: 0.006\n",
            "Iteration 17700: Current success : 95 \t All success : 154 \t mean(l0) : 20.324675 \t Current Learning Rate: 0.006\n",
            "Iteration 17800: Current success : 96 \t All success : 154 \t mean(l0) : 20.324675 \t Current Learning Rate: 0.006\n",
            "Iteration 17900: Current success : 98 \t All success : 154 \t mean(l0) : 20.324675 \t Current Learning Rate: 0.005\n",
            "Iteration 18000: Current success : 97 \t All success : 154 \t mean(l0) : 20.324675 \t Current Learning Rate: 0.005\n",
            "Iteration 18100: Current success : 97 \t All success : 154 \t mean(l0) : 20.324675 \t Current Learning Rate: 0.005\n",
            "Iteration 18200: Current success : 97 \t All success : 154 \t mean(l0) : 20.324675 \t Current Learning Rate: 0.005\n",
            "Iteration 18300: Current success : 97 \t All success : 154 \t mean(l0) : 20.324675 \t Current Learning Rate: 0.004\n",
            "Iteration 18400: Current success : 98 \t All success : 154 \t mean(l0) : 20.324675 \t Current Learning Rate: 0.004\n",
            "Iteration 18500: Current success : 98 \t All success : 154 \t mean(l0) : 20.324675 \t Current Learning Rate: 0.004\n",
            "Iteration 18600: Current success : 98 \t All success : 154 \t mean(l0) : 20.324675 \t Current Learning Rate: 0.004\n",
            "Iteration 18700: Current success : 98 \t All success : 154 \t mean(l0) : 20.324675 \t Current Learning Rate: 0.003\n",
            "Iteration 18800: Current success : 98 \t All success : 154 \t mean(l0) : 20.324675 \t Current Learning Rate: 0.003\n",
            "Iteration 18900: Current success : 97 \t All success : 154 \t mean(l0) : 20.324675 \t Current Learning Rate: 0.003\n",
            "Iteration 19000: Current success : 98 \t All success : 154 \t mean(l0) : 20.324675 \t Current Learning Rate: 0.003\n",
            "Iteration 19100: Current success : 98 \t All success : 154 \t mean(l0) : 20.324675 \t Current Learning Rate: 0.002\n",
            "Iteration 19200: Current success : 98 \t All success : 154 \t mean(l0) : 20.324675 \t Current Learning Rate: 0.002\n",
            "Iteration 19300: Current success : 97 \t All success : 154 \t mean(l0) : 20.324675 \t Current Learning Rate: 0.002\n",
            "Iteration 19400: Current success : 98 \t All success : 154 \t mean(l0) : 20.324675 \t Current Learning Rate: 0.002\n",
            "Iteration 19500: Current success : 98 \t All success : 154 \t mean(l0) : 20.324675 \t Current Learning Rate: 0.001\n",
            "Iteration 19600: Current success : 98 \t All success : 154 \t mean(l0) : 20.324675 \t Current Learning Rate: 0.001\n",
            "Iteration 19700: Current success : 98 \t All success : 154 \t mean(l0) : 20.324675 \t Current Learning Rate: 0.001\n",
            "Iteration 19800: Current success : 98 \t All success : 154 \t mean(l0) : 20.324675 \t Current Learning Rate: 0.001\n",
            "Iteration 19900: Current success : 98 \t All success : 154 \t mean(l0) : 20.324675 \t Current Learning Rate: 0.000\n",
            "Iteration 20000: Current success : 98 \t All success : 154 \t mean(l0) : 20.324675 \t Current Learning Rate: 0.000\n",
            "outer_step 3: all success : 226 \t mean(l0)(success) : 18.3894\n",
            "-------------------------------------------------------------\n",
            "tensor([  55., 1000., 1000.,  ..., 1000., 1000., 1000.], device='cuda:0')\n",
            "Iteration 100: Current success : 33 \t All success : 42 \t mean(l0) : 10.238095 \t Current Learning Rate: 0.098\n",
            "Iteration 200: Current success : 46 \t All success : 57 \t mean(l0) : 14.052631 \t Current Learning Rate: 0.096\n",
            "Iteration 300: Current success : 54 \t All success : 71 \t mean(l0) : 17.140844 \t Current Learning Rate: 0.094\n",
            "Iteration 400: Current success : 53 \t All success : 77 \t mean(l0) : 16.792208 \t Current Learning Rate: 0.092\n",
            "Iteration 500: Current success : 59 \t All success : 83 \t mean(l0) : 18.915663 \t Current Learning Rate: 0.090\n",
            "Iteration 600: Current success : 55 \t All success : 84 \t mean(l0) : 19.142857 \t Current Learning Rate: 0.088\n",
            "Iteration 700: Current success : 60 \t All success : 85 \t mean(l0) : 19.623529 \t Current Learning Rate: 0.086\n",
            "Iteration 800: Current success : 59 \t All success : 86 \t mean(l0) : 19.837210 \t Current Learning Rate: 0.084\n",
            "Iteration 900: Current success : 59 \t All success : 86 \t mean(l0) : 19.802326 \t Current Learning Rate: 0.082\n",
            "Iteration 1000: Current success : 57 \t All success : 86 \t mean(l0) : 19.802326 \t Current Learning Rate: 0.080\n",
            "Iteration 1100: Current success : 56 \t All success : 87 \t mean(l0) : 19.586206 \t Current Learning Rate: 0.078\n",
            "Iteration 1200: Current success : 56 \t All success : 87 \t mean(l0) : 19.586206 \t Current Learning Rate: 0.076\n",
            "Iteration 1300: Current success : 55 \t All success : 87 \t mean(l0) : 19.540230 \t Current Learning Rate: 0.074\n",
            "Iteration 1400: Current success : 55 \t All success : 87 \t mean(l0) : 19.517241 \t Current Learning Rate: 0.072\n",
            "Iteration 1500: Current success : 54 \t All success : 88 \t mean(l0) : 19.318182 \t Current Learning Rate: 0.070\n",
            "Iteration 1600: Current success : 54 \t All success : 92 \t mean(l0) : 18.652174 \t Current Learning Rate: 0.068\n",
            "Iteration 1700: Current success : 55 \t All success : 92 \t mean(l0) : 18.652174 \t Current Learning Rate: 0.066\n",
            "Iteration 1800: Current success : 56 \t All success : 93 \t mean(l0) : 19.139786 \t Current Learning Rate: 0.064\n",
            "Iteration 1900: Current success : 54 \t All success : 93 \t mean(l0) : 19.129032 \t Current Learning Rate: 0.062\n",
            "Iteration 2000: Current success : 55 \t All success : 93 \t mean(l0) : 19.129032 \t Current Learning Rate: 0.060\n",
            "Iteration 2100: Current success : 55 \t All success : 93 \t mean(l0) : 19.129032 \t Current Learning Rate: 0.058\n",
            "Iteration 2200: Current success : 56 \t All success : 93 \t mean(l0) : 19.129032 \t Current Learning Rate: 0.056\n",
            "Iteration 2300: Current success : 56 \t All success : 93 \t mean(l0) : 19.129032 \t Current Learning Rate: 0.054\n",
            "Iteration 2400: Current success : 55 \t All success : 93 \t mean(l0) : 19.129032 \t Current Learning Rate: 0.052\n",
            "Iteration 2500: Current success : 55 \t All success : 93 \t mean(l0) : 19.129032 \t Current Learning Rate: 0.050\n",
            "Iteration 2600: Current success : 55 \t All success : 93 \t mean(l0) : 19.129032 \t Current Learning Rate: 0.048\n",
            "Iteration 2700: Current success : 64 \t All success : 97 \t mean(l0) : 18.567009 \t Current Learning Rate: 0.046\n",
            "Iteration 2800: Current success : 66 \t All success : 99 \t mean(l0) : 18.272728 \t Current Learning Rate: 0.044\n",
            "Iteration 2900: Current success : 66 \t All success : 99 \t mean(l0) : 18.272728 \t Current Learning Rate: 0.042\n",
            "Iteration 3000: Current success : 67 \t All success : 99 \t mean(l0) : 18.272728 \t Current Learning Rate: 0.040\n",
            "Iteration 3100: Current success : 67 \t All success : 99 \t mean(l0) : 18.272728 \t Current Learning Rate: 0.038\n",
            "Iteration 3200: Current success : 67 \t All success : 99 \t mean(l0) : 18.272728 \t Current Learning Rate: 0.036\n",
            "Iteration 3300: Current success : 67 \t All success : 99 \t mean(l0) : 18.272728 \t Current Learning Rate: 0.034\n",
            "Iteration 3400: Current success : 67 \t All success : 99 \t mean(l0) : 18.272728 \t Current Learning Rate: 0.032\n",
            "Iteration 3500: Current success : 67 \t All success : 99 \t mean(l0) : 18.272728 \t Current Learning Rate: 0.030\n",
            "Iteration 3600: Current success : 67 \t All success : 99 \t mean(l0) : 18.272728 \t Current Learning Rate: 0.028\n",
            "Iteration 3700: Current success : 67 \t All success : 99 \t mean(l0) : 18.272728 \t Current Learning Rate: 0.026\n",
            "Iteration 3800: Current success : 67 \t All success : 99 \t mean(l0) : 18.272728 \t Current Learning Rate: 0.024\n",
            "Iteration 3900: Current success : 67 \t All success : 99 \t mean(l0) : 18.272728 \t Current Learning Rate: 0.022\n",
            "Iteration 4000: Current success : 67 \t All success : 99 \t mean(l0) : 18.272728 \t Current Learning Rate: 0.020\n",
            "Iteration 4100: Current success : 67 \t All success : 99 \t mean(l0) : 18.272728 \t Current Learning Rate: 0.018\n",
            "Iteration 4200: Current success : 67 \t All success : 99 \t mean(l0) : 18.272728 \t Current Learning Rate: 0.016\n",
            "Iteration 4300: Current success : 67 \t All success : 99 \t mean(l0) : 18.272728 \t Current Learning Rate: 0.014\n",
            "Iteration 4400: Current success : 67 \t All success : 99 \t mean(l0) : 18.272728 \t Current Learning Rate: 0.012\n",
            "Iteration 4500: Current success : 67 \t All success : 99 \t mean(l0) : 18.272728 \t Current Learning Rate: 0.010\n",
            "Iteration 4600: Current success : 67 \t All success : 99 \t mean(l0) : 18.272728 \t Current Learning Rate: 0.008\n",
            "Iteration 4700: Current success : 67 \t All success : 99 \t mean(l0) : 18.272728 \t Current Learning Rate: 0.006\n",
            "Iteration 4800: Current success : 67 \t All success : 99 \t mean(l0) : 18.272728 \t Current Learning Rate: 0.004\n",
            "Iteration 4900: Current success : 67 \t All success : 99 \t mean(l0) : 18.272728 \t Current Learning Rate: 0.002\n",
            "Iteration 5000: Current success : 67 \t All success : 99 \t mean(l0) : 18.272728 \t Current Learning Rate: 0.000\n",
            "Iteration 5100: Current success : 73 \t All success : 119 \t mean(l0) : 20.798321 \t Current Learning Rate: 0.049\n",
            "Iteration 5200: Current success : 85 \t All success : 124 \t mean(l0) : 20.750000 \t Current Learning Rate: 0.048\n",
            "Iteration 5300: Current success : 96 \t All success : 134 \t mean(l0) : 20.888060 \t Current Learning Rate: 0.047\n",
            "Iteration 5400: Current success : 93 \t All success : 137 \t mean(l0) : 21.481752 \t Current Learning Rate: 0.046\n",
            "Iteration 5500: Current success : 93 \t All success : 137 \t mean(l0) : 21.481752 \t Current Learning Rate: 0.045\n",
            "Iteration 5600: Current success : 98 \t All success : 137 \t mean(l0) : 21.481752 \t Current Learning Rate: 0.044\n",
            "Iteration 5700: Current success : 96 \t All success : 137 \t mean(l0) : 21.474453 \t Current Learning Rate: 0.043\n",
            "Iteration 5800: Current success : 94 \t All success : 137 \t mean(l0) : 21.430656 \t Current Learning Rate: 0.042\n",
            "Iteration 5900: Current success : 93 \t All success : 137 \t mean(l0) : 21.430656 \t Current Learning Rate: 0.041\n",
            "Iteration 6000: Current success : 92 \t All success : 137 \t mean(l0) : 21.430656 \t Current Learning Rate: 0.040\n",
            "Iteration 6100: Current success : 92 \t All success : 137 \t mean(l0) : 21.430656 \t Current Learning Rate: 0.039\n",
            "Iteration 6200: Current success : 91 \t All success : 137 \t mean(l0) : 21.430656 \t Current Learning Rate: 0.038\n",
            "Iteration 6300: Current success : 91 \t All success : 137 \t mean(l0) : 21.430656 \t Current Learning Rate: 0.037\n",
            "Iteration 6400: Current success : 92 \t All success : 137 \t mean(l0) : 21.430656 \t Current Learning Rate: 0.036\n",
            "Iteration 6500: Current success : 90 \t All success : 138 \t mean(l0) : 21.594204 \t Current Learning Rate: 0.035\n",
            "Iteration 6600: Current success : 93 \t All success : 139 \t mean(l0) : 21.446043 \t Current Learning Rate: 0.034\n",
            "Iteration 6700: Current success : 92 \t All success : 139 \t mean(l0) : 21.446043 \t Current Learning Rate: 0.033\n",
            "Iteration 6800: Current success : 92 \t All success : 139 \t mean(l0) : 21.446043 \t Current Learning Rate: 0.032\n",
            "Iteration 6900: Current success : 92 \t All success : 139 \t mean(l0) : 21.446043 \t Current Learning Rate: 0.031\n",
            "Iteration 7000: Current success : 93 \t All success : 139 \t mean(l0) : 21.446043 \t Current Learning Rate: 0.030\n",
            "Iteration 7100: Current success : 91 \t All success : 139 \t mean(l0) : 21.446043 \t Current Learning Rate: 0.029\n",
            "Iteration 7200: Current success : 93 \t All success : 139 \t mean(l0) : 21.446043 \t Current Learning Rate: 0.028\n",
            "Iteration 7300: Current success : 93 \t All success : 139 \t mean(l0) : 21.446043 \t Current Learning Rate: 0.027\n",
            "Iteration 7400: Current success : 94 \t All success : 139 \t mean(l0) : 21.446043 \t Current Learning Rate: 0.026\n",
            "Iteration 7500: Current success : 91 \t All success : 139 \t mean(l0) : 21.446043 \t Current Learning Rate: 0.025\n",
            "Iteration 7600: Current success : 93 \t All success : 139 \t mean(l0) : 21.446043 \t Current Learning Rate: 0.024\n",
            "Iteration 7700: Current success : 95 \t All success : 139 \t mean(l0) : 21.402878 \t Current Learning Rate: 0.023\n",
            "Iteration 7800: Current success : 98 \t All success : 139 \t mean(l0) : 21.402878 \t Current Learning Rate: 0.022\n",
            "Iteration 7900: Current success : 98 \t All success : 139 \t mean(l0) : 21.402878 \t Current Learning Rate: 0.021\n",
            "Iteration 8000: Current success : 98 \t All success : 139 \t mean(l0) : 21.402878 \t Current Learning Rate: 0.020\n",
            "Iteration 8100: Current success : 98 \t All success : 139 \t mean(l0) : 21.402878 \t Current Learning Rate: 0.019\n",
            "Iteration 8200: Current success : 98 \t All success : 139 \t mean(l0) : 21.402878 \t Current Learning Rate: 0.018\n",
            "Iteration 8300: Current success : 98 \t All success : 139 \t mean(l0) : 21.402878 \t Current Learning Rate: 0.017\n",
            "Iteration 8400: Current success : 98 \t All success : 139 \t mean(l0) : 21.381296 \t Current Learning Rate: 0.016\n",
            "Iteration 8500: Current success : 98 \t All success : 139 \t mean(l0) : 21.338129 \t Current Learning Rate: 0.015\n",
            "Iteration 8600: Current success : 98 \t All success : 139 \t mean(l0) : 21.338129 \t Current Learning Rate: 0.014\n",
            "Iteration 8700: Current success : 98 \t All success : 139 \t mean(l0) : 21.338129 \t Current Learning Rate: 0.013\n",
            "Iteration 8800: Current success : 98 \t All success : 139 \t mean(l0) : 21.338129 \t Current Learning Rate: 0.012\n",
            "Iteration 8900: Current success : 98 \t All success : 139 \t mean(l0) : 21.338129 \t Current Learning Rate: 0.011\n",
            "Iteration 9000: Current success : 98 \t All success : 139 \t mean(l0) : 21.338129 \t Current Learning Rate: 0.010\n",
            "Iteration 9100: Current success : 98 \t All success : 139 \t mean(l0) : 21.338129 \t Current Learning Rate: 0.009\n",
            "Iteration 9200: Current success : 98 \t All success : 139 \t mean(l0) : 21.338129 \t Current Learning Rate: 0.008\n",
            "Iteration 9300: Current success : 98 \t All success : 139 \t mean(l0) : 21.338129 \t Current Learning Rate: 0.007\n",
            "Iteration 9400: Current success : 98 \t All success : 139 \t mean(l0) : 21.338129 \t Current Learning Rate: 0.006\n",
            "Iteration 9500: Current success : 98 \t All success : 139 \t mean(l0) : 21.338129 \t Current Learning Rate: 0.005\n",
            "Iteration 9600: Current success : 98 \t All success : 139 \t mean(l0) : 21.338129 \t Current Learning Rate: 0.004\n",
            "Iteration 9700: Current success : 98 \t All success : 139 \t mean(l0) : 21.338129 \t Current Learning Rate: 0.003\n",
            "Iteration 9800: Current success : 98 \t All success : 139 \t mean(l0) : 21.338129 \t Current Learning Rate: 0.002\n",
            "Iteration 9900: Current success : 98 \t All success : 139 \t mean(l0) : 21.338129 \t Current Learning Rate: 0.001\n",
            "Iteration 10000: Current success : 98 \t All success : 139 \t mean(l0) : 21.338129 \t Current Learning Rate: 0.000\n",
            "Iteration 10100: Current success : 84 \t All success : 149 \t mean(l0) : 20.389261 \t Current Learning Rate: 0.025\n",
            "Iteration 10200: Current success : 84 \t All success : 150 \t mean(l0) : 20.320000 \t Current Learning Rate: 0.024\n",
            "Iteration 10300: Current success : 83 \t All success : 150 \t mean(l0) : 20.213333 \t Current Learning Rate: 0.024\n",
            "Iteration 10400: Current success : 85 \t All success : 152 \t mean(l0) : 20.578947 \t Current Learning Rate: 0.023\n",
            "Iteration 10500: Current success : 85 \t All success : 152 \t mean(l0) : 20.539474 \t Current Learning Rate: 0.023\n",
            "Iteration 10600: Current success : 85 \t All success : 152 \t mean(l0) : 20.539474 \t Current Learning Rate: 0.022\n",
            "Iteration 10700: Current success : 85 \t All success : 152 \t mean(l0) : 20.539474 \t Current Learning Rate: 0.022\n",
            "Iteration 10800: Current success : 85 \t All success : 152 \t mean(l0) : 20.539474 \t Current Learning Rate: 0.021\n",
            "Iteration 10900: Current success : 85 \t All success : 152 \t mean(l0) : 20.539474 \t Current Learning Rate: 0.021\n",
            "Iteration 11000: Current success : 85 \t All success : 152 \t mean(l0) : 20.539474 \t Current Learning Rate: 0.020\n",
            "Iteration 11100: Current success : 87 \t All success : 152 \t mean(l0) : 20.539474 \t Current Learning Rate: 0.020\n",
            "Iteration 11200: Current success : 85 \t All success : 152 \t mean(l0) : 20.539474 \t Current Learning Rate: 0.019\n",
            "Iteration 11300: Current success : 86 \t All success : 152 \t mean(l0) : 20.532894 \t Current Learning Rate: 0.019\n",
            "Iteration 11400: Current success : 85 \t All success : 152 \t mean(l0) : 20.519737 \t Current Learning Rate: 0.018\n",
            "Iteration 11500: Current success : 85 \t All success : 152 \t mean(l0) : 20.519737 \t Current Learning Rate: 0.018\n",
            "Iteration 11600: Current success : 85 \t All success : 152 \t mean(l0) : 20.513159 \t Current Learning Rate: 0.017\n",
            "Iteration 11700: Current success : 85 \t All success : 152 \t mean(l0) : 20.513159 \t Current Learning Rate: 0.017\n",
            "Iteration 11800: Current success : 85 \t All success : 152 \t mean(l0) : 20.513159 \t Current Learning Rate: 0.016\n",
            "Iteration 11900: Current success : 85 \t All success : 152 \t mean(l0) : 20.513159 \t Current Learning Rate: 0.016\n",
            "Iteration 12000: Current success : 85 \t All success : 152 \t mean(l0) : 20.513159 \t Current Learning Rate: 0.015\n",
            "Iteration 12100: Current success : 85 \t All success : 152 \t mean(l0) : 20.513159 \t Current Learning Rate: 0.015\n",
            "Iteration 12200: Current success : 85 \t All success : 152 \t mean(l0) : 20.513159 \t Current Learning Rate: 0.014\n",
            "Iteration 12300: Current success : 85 \t All success : 152 \t mean(l0) : 20.513159 \t Current Learning Rate: 0.014\n",
            "Iteration 12400: Current success : 86 \t All success : 152 \t mean(l0) : 20.513159 \t Current Learning Rate: 0.013\n",
            "Iteration 12500: Current success : 85 \t All success : 152 \t mean(l0) : 20.513159 \t Current Learning Rate: 0.013\n",
            "Iteration 12600: Current success : 85 \t All success : 152 \t mean(l0) : 20.513159 \t Current Learning Rate: 0.012\n",
            "Iteration 12700: Current success : 85 \t All success : 152 \t mean(l0) : 20.513159 \t Current Learning Rate: 0.012\n",
            "Iteration 12800: Current success : 85 \t All success : 152 \t mean(l0) : 20.513159 \t Current Learning Rate: 0.011\n",
            "Iteration 12900: Current success : 85 \t All success : 152 \t mean(l0) : 20.513159 \t Current Learning Rate: 0.011\n",
            "Iteration 13000: Current success : 99 \t All success : 152 \t mean(l0) : 20.513159 \t Current Learning Rate: 0.010\n",
            "Iteration 13100: Current success : 99 \t All success : 152 \t mean(l0) : 20.506578 \t Current Learning Rate: 0.010\n",
            "Iteration 13200: Current success : 99 \t All success : 152 \t mean(l0) : 20.506578 \t Current Learning Rate: 0.009\n",
            "Iteration 13300: Current success : 99 \t All success : 152 \t mean(l0) : 20.506578 \t Current Learning Rate: 0.009\n",
            "Iteration 13400: Current success : 99 \t All success : 152 \t mean(l0) : 20.506578 \t Current Learning Rate: 0.008\n",
            "Iteration 13500: Current success : 99 \t All success : 152 \t mean(l0) : 20.506578 \t Current Learning Rate: 0.008\n",
            "Iteration 13600: Current success : 99 \t All success : 152 \t mean(l0) : 20.506578 \t Current Learning Rate: 0.007\n",
            "Iteration 13700: Current success : 99 \t All success : 152 \t mean(l0) : 20.506578 \t Current Learning Rate: 0.007\n",
            "Iteration 13800: Current success : 99 \t All success : 152 \t mean(l0) : 20.506578 \t Current Learning Rate: 0.006\n",
            "Iteration 13900: Current success : 99 \t All success : 152 \t mean(l0) : 20.506578 \t Current Learning Rate: 0.006\n",
            "Iteration 14000: Current success : 99 \t All success : 152 \t mean(l0) : 20.506578 \t Current Learning Rate: 0.005\n",
            "Iteration 14100: Current success : 99 \t All success : 152 \t mean(l0) : 20.506578 \t Current Learning Rate: 0.005\n",
            "Iteration 14200: Current success : 99 \t All success : 152 \t mean(l0) : 20.493422 \t Current Learning Rate: 0.004\n",
            "Iteration 14300: Current success : 99 \t All success : 152 \t mean(l0) : 20.493422 \t Current Learning Rate: 0.004\n",
            "Iteration 14400: Current success : 99 \t All success : 152 \t mean(l0) : 20.493422 \t Current Learning Rate: 0.003\n",
            "Iteration 14500: Current success : 99 \t All success : 152 \t mean(l0) : 20.493422 \t Current Learning Rate: 0.003\n",
            "Iteration 14600: Current success : 99 \t All success : 152 \t mean(l0) : 20.493422 \t Current Learning Rate: 0.002\n",
            "Iteration 14700: Current success : 99 \t All success : 152 \t mean(l0) : 20.493422 \t Current Learning Rate: 0.002\n",
            "Iteration 14800: Current success : 99 \t All success : 152 \t mean(l0) : 20.493422 \t Current Learning Rate: 0.001\n",
            "Iteration 14900: Current success : 99 \t All success : 152 \t mean(l0) : 20.493422 \t Current Learning Rate: 0.001\n",
            "Iteration 15000: Current success : 99 \t All success : 152 \t mean(l0) : 20.493422 \t Current Learning Rate: 0.000\n",
            "Iteration 15100: Current success : 96 \t All success : 155 \t mean(l0) : 20.180645 \t Current Learning Rate: 0.012\n",
            "Iteration 15200: Current success : 105 \t All success : 157 \t mean(l0) : 20.394905 \t Current Learning Rate: 0.012\n",
            "Iteration 15300: Current success : 107 \t All success : 157 \t mean(l0) : 20.369427 \t Current Learning Rate: 0.012\n",
            "Iteration 15400: Current success : 107 \t All success : 157 \t mean(l0) : 20.324841 \t Current Learning Rate: 0.012\n",
            "Iteration 15500: Current success : 107 \t All success : 157 \t mean(l0) : 20.324841 \t Current Learning Rate: 0.011\n",
            "Iteration 15600: Current success : 107 \t All success : 157 \t mean(l0) : 20.324841 \t Current Learning Rate: 0.011\n",
            "Iteration 15700: Current success : 107 \t All success : 157 \t mean(l0) : 20.318472 \t Current Learning Rate: 0.011\n",
            "Iteration 15800: Current success : 107 \t All success : 157 \t mean(l0) : 20.286625 \t Current Learning Rate: 0.011\n",
            "Iteration 15900: Current success : 107 \t All success : 157 \t mean(l0) : 20.286625 \t Current Learning Rate: 0.010\n",
            "Iteration 16000: Current success : 106 \t All success : 157 \t mean(l0) : 20.273886 \t Current Learning Rate: 0.010\n",
            "Iteration 16100: Current success : 107 \t All success : 158 \t mean(l0) : 20.411392 \t Current Learning Rate: 0.010\n",
            "Iteration 16200: Current success : 110 \t All success : 158 \t mean(l0) : 20.405064 \t Current Learning Rate: 0.010\n",
            "Iteration 16300: Current success : 106 \t All success : 158 \t mean(l0) : 20.405064 \t Current Learning Rate: 0.009\n",
            "Iteration 16400: Current success : 109 \t All success : 158 \t mean(l0) : 20.392406 \t Current Learning Rate: 0.009\n",
            "Iteration 16500: Current success : 103 \t All success : 158 \t mean(l0) : 20.392406 \t Current Learning Rate: 0.009\n",
            "Iteration 16600: Current success : 107 \t All success : 158 \t mean(l0) : 20.392406 \t Current Learning Rate: 0.009\n",
            "Iteration 16700: Current success : 107 \t All success : 158 \t mean(l0) : 20.392406 \t Current Learning Rate: 0.008\n",
            "Iteration 16800: Current success : 104 \t All success : 158 \t mean(l0) : 20.392406 \t Current Learning Rate: 0.008\n",
            "Iteration 16900: Current success : 106 \t All success : 158 \t mean(l0) : 20.392406 \t Current Learning Rate: 0.008\n",
            "Iteration 17000: Current success : 106 \t All success : 158 \t mean(l0) : 20.392406 \t Current Learning Rate: 0.008\n",
            "Iteration 17100: Current success : 103 \t All success : 158 \t mean(l0) : 20.392406 \t Current Learning Rate: 0.007\n",
            "Iteration 17200: Current success : 106 \t All success : 158 \t mean(l0) : 20.392406 \t Current Learning Rate: 0.007\n",
            "Iteration 17300: Current success : 105 \t All success : 158 \t mean(l0) : 20.392406 \t Current Learning Rate: 0.007\n",
            "Iteration 17400: Current success : 103 \t All success : 158 \t mean(l0) : 20.392406 \t Current Learning Rate: 0.007\n",
            "Iteration 17500: Current success : 107 \t All success : 158 \t mean(l0) : 20.386076 \t Current Learning Rate: 0.006\n",
            "Iteration 17600: Current success : 103 \t All success : 158 \t mean(l0) : 20.386076 \t Current Learning Rate: 0.006\n",
            "Iteration 17700: Current success : 104 \t All success : 158 \t mean(l0) : 20.386076 \t Current Learning Rate: 0.006\n",
            "Iteration 17800: Current success : 109 \t All success : 158 \t mean(l0) : 20.386076 \t Current Learning Rate: 0.006\n",
            "Iteration 17900: Current success : 107 \t All success : 158 \t mean(l0) : 20.386076 \t Current Learning Rate: 0.005\n",
            "Iteration 18000: Current success : 108 \t All success : 158 \t mean(l0) : 20.386076 \t Current Learning Rate: 0.005\n",
            "Iteration 18100: Current success : 108 \t All success : 158 \t mean(l0) : 20.386076 \t Current Learning Rate: 0.005\n",
            "Iteration 18200: Current success : 108 \t All success : 158 \t mean(l0) : 20.386076 \t Current Learning Rate: 0.005\n",
            "Iteration 18300: Current success : 108 \t All success : 158 \t mean(l0) : 20.386076 \t Current Learning Rate: 0.004\n",
            "Iteration 18400: Current success : 108 \t All success : 158 \t mean(l0) : 20.386076 \t Current Learning Rate: 0.004\n",
            "Iteration 18500: Current success : 108 \t All success : 158 \t mean(l0) : 20.386076 \t Current Learning Rate: 0.004\n",
            "Iteration 18600: Current success : 108 \t All success : 158 \t mean(l0) : 20.386076 \t Current Learning Rate: 0.004\n",
            "Iteration 18700: Current success : 108 \t All success : 158 \t mean(l0) : 20.386076 \t Current Learning Rate: 0.003\n",
            "Iteration 18800: Current success : 108 \t All success : 158 \t mean(l0) : 20.386076 \t Current Learning Rate: 0.003\n",
            "Iteration 18900: Current success : 108 \t All success : 158 \t mean(l0) : 20.386076 \t Current Learning Rate: 0.003\n",
            "Iteration 19000: Current success : 108 \t All success : 158 \t mean(l0) : 20.386076 \t Current Learning Rate: 0.003\n",
            "Iteration 19100: Current success : 108 \t All success : 158 \t mean(l0) : 20.386076 \t Current Learning Rate: 0.002\n",
            "Iteration 19200: Current success : 108 \t All success : 158 \t mean(l0) : 20.386076 \t Current Learning Rate: 0.002\n",
            "Iteration 19300: Current success : 108 \t All success : 158 \t mean(l0) : 20.386076 \t Current Learning Rate: 0.002\n",
            "Iteration 19400: Current success : 108 \t All success : 158 \t mean(l0) : 20.386076 \t Current Learning Rate: 0.002\n",
            "Iteration 19500: Current success : 108 \t All success : 158 \t mean(l0) : 20.386076 \t Current Learning Rate: 0.001\n",
            "Iteration 19600: Current success : 108 \t All success : 158 \t mean(l0) : 20.386076 \t Current Learning Rate: 0.001\n",
            "Iteration 19700: Current success : 108 \t All success : 158 \t mean(l0) : 20.386076 \t Current Learning Rate: 0.001\n",
            "Iteration 19800: Current success : 108 \t All success : 158 \t mean(l0) : 20.386076 \t Current Learning Rate: 0.001\n",
            "Iteration 19900: Current success : 108 \t All success : 158 \t mean(l0) : 20.386076 \t Current Learning Rate: 0.000\n",
            "Iteration 20000: Current success : 108 \t All success : 158 \t mean(l0) : 20.386076 \t Current Learning Rate: 0.000\n",
            "outer_step 4: all success : 251 \t mean(l0)(success) : 19.7291\n",
            "-------------------------------------------------------------\n",
            "tensor([   32.5000, 10000.0000, 10000.0000,  ..., 10000.0000, 10000.0000,\n",
            "        10000.0000], device='cuda:0')\n",
            "Iteration 100: Current success : 31 \t All success : 42 \t mean(l0) : 10.428572 \t Current Learning Rate: 0.098\n",
            "Iteration 200: Current success : 32 \t All success : 49 \t mean(l0) : 12.346938 \t Current Learning Rate: 0.096\n",
            "Iteration 300: Current success : 39 \t All success : 57 \t mean(l0) : 15.842105 \t Current Learning Rate: 0.094\n",
            "Iteration 400: Current success : 45 \t All success : 66 \t mean(l0) : 15.287879 \t Current Learning Rate: 0.092\n",
            "Iteration 500: Current success : 44 \t All success : 69 \t mean(l0) : 17.086956 \t Current Learning Rate: 0.090\n",
            "Iteration 600: Current success : 38 \t All success : 70 \t mean(l0) : 17.785715 \t Current Learning Rate: 0.088\n",
            "Iteration 700: Current success : 43 \t All success : 71 \t mean(l0) : 17.760563 \t Current Learning Rate: 0.086\n",
            "Iteration 800: Current success : 44 \t All success : 72 \t mean(l0) : 18.027779 \t Current Learning Rate: 0.084\n",
            "Iteration 900: Current success : 52 \t All success : 80 \t mean(l0) : 19.100000 \t Current Learning Rate: 0.082\n",
            "Iteration 1000: Current success : 55 \t All success : 87 \t mean(l0) : 19.298851 \t Current Learning Rate: 0.080\n",
            "Iteration 1100: Current success : 55 \t All success : 89 \t mean(l0) : 18.876404 \t Current Learning Rate: 0.078\n",
            "Iteration 1200: Current success : 59 \t All success : 89 \t mean(l0) : 18.876404 \t Current Learning Rate: 0.076\n",
            "Iteration 1300: Current success : 56 \t All success : 89 \t mean(l0) : 18.876404 \t Current Learning Rate: 0.074\n",
            "Iteration 1400: Current success : 56 \t All success : 89 \t mean(l0) : 18.876404 \t Current Learning Rate: 0.072\n",
            "Iteration 1500: Current success : 57 \t All success : 89 \t mean(l0) : 18.876404 \t Current Learning Rate: 0.070\n",
            "Iteration 1600: Current success : 57 \t All success : 89 \t mean(l0) : 18.876404 \t Current Learning Rate: 0.068\n",
            "Iteration 1700: Current success : 57 \t All success : 89 \t mean(l0) : 18.876404 \t Current Learning Rate: 0.066\n",
            "Iteration 1800: Current success : 57 \t All success : 89 \t mean(l0) : 18.876404 \t Current Learning Rate: 0.064\n",
            "Iteration 1900: Current success : 57 \t All success : 89 \t mean(l0) : 18.876404 \t Current Learning Rate: 0.062\n",
            "Iteration 2000: Current success : 57 \t All success : 89 \t mean(l0) : 18.853933 \t Current Learning Rate: 0.060\n",
            "Iteration 2100: Current success : 57 \t All success : 90 \t mean(l0) : 18.777779 \t Current Learning Rate: 0.058\n",
            "Iteration 2200: Current success : 57 \t All success : 90 \t mean(l0) : 18.777779 \t Current Learning Rate: 0.056\n",
            "Iteration 2300: Current success : 58 \t All success : 90 \t mean(l0) : 18.777779 \t Current Learning Rate: 0.054\n",
            "Iteration 2400: Current success : 57 \t All success : 90 \t mean(l0) : 18.777779 \t Current Learning Rate: 0.052\n",
            "Iteration 2500: Current success : 57 \t All success : 90 \t mean(l0) : 18.777779 \t Current Learning Rate: 0.050\n",
            "Iteration 2600: Current success : 57 \t All success : 90 \t mean(l0) : 18.777779 \t Current Learning Rate: 0.048\n",
            "Iteration 2700: Current success : 64 \t All success : 93 \t mean(l0) : 18.322580 \t Current Learning Rate: 0.046\n",
            "Iteration 2800: Current success : 66 \t All success : 93 \t mean(l0) : 18.322580 \t Current Learning Rate: 0.044\n",
            "Iteration 2900: Current success : 67 \t All success : 93 \t mean(l0) : 18.322580 \t Current Learning Rate: 0.042\n",
            "Iteration 3000: Current success : 67 \t All success : 93 \t mean(l0) : 18.322580 \t Current Learning Rate: 0.040\n",
            "Iteration 3100: Current success : 67 \t All success : 93 \t mean(l0) : 18.322580 \t Current Learning Rate: 0.038\n",
            "Iteration 3200: Current success : 67 \t All success : 93 \t mean(l0) : 18.322580 \t Current Learning Rate: 0.036\n",
            "Iteration 3300: Current success : 67 \t All success : 93 \t mean(l0) : 18.322580 \t Current Learning Rate: 0.034\n",
            "Iteration 3400: Current success : 67 \t All success : 93 \t mean(l0) : 18.322580 \t Current Learning Rate: 0.032\n",
            "Iteration 3500: Current success : 67 \t All success : 93 \t mean(l0) : 18.322580 \t Current Learning Rate: 0.030\n",
            "Iteration 3600: Current success : 67 \t All success : 93 \t mean(l0) : 18.322580 \t Current Learning Rate: 0.028\n",
            "Iteration 3700: Current success : 67 \t All success : 95 \t mean(l0) : 18.168423 \t Current Learning Rate: 0.026\n",
            "Iteration 3800: Current success : 69 \t All success : 95 \t mean(l0) : 18.168423 \t Current Learning Rate: 0.024\n",
            "Iteration 3900: Current success : 67 \t All success : 95 \t mean(l0) : 18.168423 \t Current Learning Rate: 0.022\n",
            "Iteration 4000: Current success : 67 \t All success : 95 \t mean(l0) : 18.168423 \t Current Learning Rate: 0.020\n",
            "Iteration 4100: Current success : 69 \t All success : 95 \t mean(l0) : 18.168423 \t Current Learning Rate: 0.018\n",
            "Iteration 4200: Current success : 69 \t All success : 95 \t mean(l0) : 18.168423 \t Current Learning Rate: 0.016\n",
            "Iteration 4300: Current success : 69 \t All success : 95 \t mean(l0) : 18.168423 \t Current Learning Rate: 0.014\n",
            "Iteration 4400: Current success : 69 \t All success : 95 \t mean(l0) : 18.168423 \t Current Learning Rate: 0.012\n",
            "Iteration 4500: Current success : 69 \t All success : 95 \t mean(l0) : 18.168423 \t Current Learning Rate: 0.010\n",
            "Iteration 4600: Current success : 69 \t All success : 95 \t mean(l0) : 18.168423 \t Current Learning Rate: 0.008\n",
            "Iteration 4700: Current success : 69 \t All success : 95 \t mean(l0) : 18.168423 \t Current Learning Rate: 0.006\n",
            "Iteration 4800: Current success : 69 \t All success : 95 \t mean(l0) : 18.168423 \t Current Learning Rate: 0.004\n",
            "Iteration 4900: Current success : 69 \t All success : 95 \t mean(l0) : 18.168423 \t Current Learning Rate: 0.002\n",
            "Iteration 5000: Current success : 69 \t All success : 95 \t mean(l0) : 18.168423 \t Current Learning Rate: 0.000\n",
            "Iteration 5100: Current success : 60 \t All success : 105 \t mean(l0) : 19.638096 \t Current Learning Rate: 0.049\n",
            "Iteration 5200: Current success : 75 \t All success : 118 \t mean(l0) : 20.169491 \t Current Learning Rate: 0.048\n",
            "Iteration 5300: Current success : 74 \t All success : 118 \t mean(l0) : 20.144068 \t Current Learning Rate: 0.047\n",
            "Iteration 5400: Current success : 75 \t All success : 118 \t mean(l0) : 20.135593 \t Current Learning Rate: 0.046\n",
            "Iteration 5500: Current success : 76 \t All success : 118 \t mean(l0) : 20.135593 \t Current Learning Rate: 0.045\n",
            "Iteration 5600: Current success : 76 \t All success : 118 \t mean(l0) : 20.135593 \t Current Learning Rate: 0.044\n",
            "Iteration 5700: Current success : 75 \t All success : 118 \t mean(l0) : 20.135593 \t Current Learning Rate: 0.043\n",
            "Iteration 5800: Current success : 76 \t All success : 118 \t mean(l0) : 20.135593 \t Current Learning Rate: 0.042\n",
            "Iteration 5900: Current success : 75 \t All success : 118 \t mean(l0) : 20.135593 \t Current Learning Rate: 0.041\n",
            "Iteration 6000: Current success : 75 \t All success : 118 \t mean(l0) : 20.135593 \t Current Learning Rate: 0.040\n",
            "Iteration 6100: Current success : 76 \t All success : 118 \t mean(l0) : 20.135593 \t Current Learning Rate: 0.039\n",
            "Iteration 6200: Current success : 75 \t All success : 118 \t mean(l0) : 20.135593 \t Current Learning Rate: 0.038\n",
            "Iteration 6300: Current success : 75 \t All success : 118 \t mean(l0) : 20.135593 \t Current Learning Rate: 0.037\n",
            "Iteration 6400: Current success : 75 \t All success : 118 \t mean(l0) : 20.135593 \t Current Learning Rate: 0.036\n",
            "Iteration 6500: Current success : 74 \t All success : 118 \t mean(l0) : 20.135593 \t Current Learning Rate: 0.035\n",
            "Iteration 6600: Current success : 76 \t All success : 118 \t mean(l0) : 20.135593 \t Current Learning Rate: 0.034\n",
            "Iteration 6700: Current success : 75 \t All success : 118 \t mean(l0) : 20.135593 \t Current Learning Rate: 0.033\n",
            "Iteration 6800: Current success : 74 \t All success : 118 \t mean(l0) : 20.135593 \t Current Learning Rate: 0.032\n",
            "Iteration 6900: Current success : 74 \t All success : 118 \t mean(l0) : 20.135593 \t Current Learning Rate: 0.031\n",
            "Iteration 7000: Current success : 74 \t All success : 118 \t mean(l0) : 20.135593 \t Current Learning Rate: 0.030\n",
            "Iteration 7100: Current success : 75 \t All success : 118 \t mean(l0) : 20.135593 \t Current Learning Rate: 0.029\n",
            "Iteration 7200: Current success : 74 \t All success : 118 \t mean(l0) : 20.118645 \t Current Learning Rate: 0.028\n",
            "Iteration 7300: Current success : 75 \t All success : 118 \t mean(l0) : 20.118645 \t Current Learning Rate: 0.027\n",
            "Iteration 7400: Current success : 75 \t All success : 118 \t mean(l0) : 20.118645 \t Current Learning Rate: 0.026\n",
            "Iteration 7500: Current success : 75 \t All success : 118 \t mean(l0) : 20.118645 \t Current Learning Rate: 0.025\n",
            "Iteration 7600: Current success : 76 \t All success : 118 \t mean(l0) : 20.118645 \t Current Learning Rate: 0.024\n",
            "Iteration 7700: Current success : 75 \t All success : 118 \t mean(l0) : 20.118645 \t Current Learning Rate: 0.023\n",
            "Iteration 7800: Current success : 76 \t All success : 118 \t mean(l0) : 20.118645 \t Current Learning Rate: 0.022\n",
            "Iteration 7900: Current success : 76 \t All success : 118 \t mean(l0) : 20.118645 \t Current Learning Rate: 0.021\n",
            "Iteration 8000: Current success : 76 \t All success : 118 \t mean(l0) : 20.118645 \t Current Learning Rate: 0.020\n",
            "Iteration 8100: Current success : 76 \t All success : 118 \t mean(l0) : 20.118645 \t Current Learning Rate: 0.019\n",
            "Iteration 8200: Current success : 76 \t All success : 118 \t mean(l0) : 20.118645 \t Current Learning Rate: 0.018\n",
            "Iteration 8300: Current success : 76 \t All success : 118 \t mean(l0) : 20.118645 \t Current Learning Rate: 0.017\n",
            "Iteration 8400: Current success : 76 \t All success : 118 \t mean(l0) : 20.118645 \t Current Learning Rate: 0.016\n",
            "Iteration 8500: Current success : 76 \t All success : 118 \t mean(l0) : 20.118645 \t Current Learning Rate: 0.015\n",
            "Iteration 8600: Current success : 76 \t All success : 118 \t mean(l0) : 20.118645 \t Current Learning Rate: 0.014\n",
            "Iteration 8700: Current success : 76 \t All success : 118 \t mean(l0) : 20.118645 \t Current Learning Rate: 0.013\n",
            "Iteration 8800: Current success : 76 \t All success : 118 \t mean(l0) : 20.118645 \t Current Learning Rate: 0.012\n",
            "Iteration 8900: Current success : 76 \t All success : 118 \t mean(l0) : 20.118645 \t Current Learning Rate: 0.011\n",
            "Iteration 9000: Current success : 76 \t All success : 118 \t mean(l0) : 20.118645 \t Current Learning Rate: 0.010\n",
            "Iteration 9100: Current success : 76 \t All success : 118 \t mean(l0) : 20.118645 \t Current Learning Rate: 0.009\n",
            "Iteration 9200: Current success : 76 \t All success : 118 \t mean(l0) : 20.118645 \t Current Learning Rate: 0.008\n",
            "Iteration 9300: Current success : 76 \t All success : 118 \t mean(l0) : 20.118645 \t Current Learning Rate: 0.007\n",
            "Iteration 9400: Current success : 76 \t All success : 118 \t mean(l0) : 20.118645 \t Current Learning Rate: 0.006\n",
            "Iteration 9500: Current success : 76 \t All success : 118 \t mean(l0) : 20.118645 \t Current Learning Rate: 0.005\n",
            "Iteration 9600: Current success : 76 \t All success : 118 \t mean(l0) : 20.118645 \t Current Learning Rate: 0.004\n",
            "Iteration 9700: Current success : 76 \t All success : 118 \t mean(l0) : 20.118645 \t Current Learning Rate: 0.003\n",
            "Iteration 9800: Current success : 76 \t All success : 118 \t mean(l0) : 20.118645 \t Current Learning Rate: 0.002\n",
            "Iteration 9900: Current success : 76 \t All success : 118 \t mean(l0) : 20.118645 \t Current Learning Rate: 0.001\n",
            "Iteration 10000: Current success : 76 \t All success : 118 \t mean(l0) : 20.118645 \t Current Learning Rate: 0.000\n",
            "Iteration 10100: Current success : 80 \t All success : 125 \t mean(l0) : 20.072001 \t Current Learning Rate: 0.025\n",
            "Iteration 10200: Current success : 82 \t All success : 128 \t mean(l0) : 20.242188 \t Current Learning Rate: 0.024\n",
            "Iteration 10300: Current success : 82 \t All success : 128 \t mean(l0) : 20.226562 \t Current Learning Rate: 0.024\n",
            "Iteration 10400: Current success : 80 \t All success : 128 \t mean(l0) : 20.179688 \t Current Learning Rate: 0.023\n",
            "Iteration 10500: Current success : 81 \t All success : 128 \t mean(l0) : 20.179688 \t Current Learning Rate: 0.023\n",
            "Iteration 10600: Current success : 80 \t All success : 128 \t mean(l0) : 20.179688 \t Current Learning Rate: 0.022\n",
            "Iteration 10700: Current success : 80 \t All success : 128 \t mean(l0) : 20.179688 \t Current Learning Rate: 0.022\n",
            "Iteration 10800: Current success : 80 \t All success : 128 \t mean(l0) : 20.179688 \t Current Learning Rate: 0.021\n",
            "Iteration 10900: Current success : 80 \t All success : 128 \t mean(l0) : 20.179688 \t Current Learning Rate: 0.021\n",
            "Iteration 11000: Current success : 80 \t All success : 128 \t mean(l0) : 20.171875 \t Current Learning Rate: 0.020\n",
            "Iteration 11100: Current success : 80 \t All success : 128 \t mean(l0) : 20.171875 \t Current Learning Rate: 0.020\n",
            "Iteration 11200: Current success : 80 \t All success : 128 \t mean(l0) : 20.171875 \t Current Learning Rate: 0.019\n",
            "Iteration 11300: Current success : 80 \t All success : 128 \t mean(l0) : 20.171875 \t Current Learning Rate: 0.019\n",
            "Iteration 11400: Current success : 80 \t All success : 128 \t mean(l0) : 20.171875 \t Current Learning Rate: 0.018\n",
            "Iteration 11500: Current success : 79 \t All success : 128 \t mean(l0) : 20.171875 \t Current Learning Rate: 0.018\n",
            "Iteration 11600: Current success : 80 \t All success : 128 \t mean(l0) : 20.171875 \t Current Learning Rate: 0.017\n",
            "Iteration 11700: Current success : 80 \t All success : 128 \t mean(l0) : 20.171875 \t Current Learning Rate: 0.017\n",
            "Iteration 11800: Current success : 80 \t All success : 128 \t mean(l0) : 20.171875 \t Current Learning Rate: 0.016\n",
            "Iteration 11900: Current success : 80 \t All success : 128 \t mean(l0) : 20.171875 \t Current Learning Rate: 0.016\n",
            "Iteration 12000: Current success : 80 \t All success : 128 \t mean(l0) : 20.171875 \t Current Learning Rate: 0.015\n",
            "Iteration 12100: Current success : 80 \t All success : 128 \t mean(l0) : 20.171875 \t Current Learning Rate: 0.015\n",
            "Iteration 12200: Current success : 80 \t All success : 128 \t mean(l0) : 20.171875 \t Current Learning Rate: 0.014\n",
            "Iteration 12300: Current success : 80 \t All success : 128 \t mean(l0) : 20.171875 \t Current Learning Rate: 0.014\n",
            "Iteration 12400: Current success : 80 \t All success : 128 \t mean(l0) : 20.171875 \t Current Learning Rate: 0.013\n",
            "Iteration 12500: Current success : 80 \t All success : 128 \t mean(l0) : 20.171875 \t Current Learning Rate: 0.013\n",
            "Iteration 12600: Current success : 80 \t All success : 128 \t mean(l0) : 20.171875 \t Current Learning Rate: 0.012\n",
            "Iteration 12700: Current success : 80 \t All success : 128 \t mean(l0) : 20.171875 \t Current Learning Rate: 0.012\n",
            "Iteration 12800: Current success : 79 \t All success : 128 \t mean(l0) : 20.171875 \t Current Learning Rate: 0.011\n",
            "Iteration 12900: Current success : 81 \t All success : 128 \t mean(l0) : 20.171875 \t Current Learning Rate: 0.011\n",
            "Iteration 13000: Current success : 90 \t All success : 128 \t mean(l0) : 20.171875 \t Current Learning Rate: 0.010\n",
            "Iteration 13100: Current success : 88 \t All success : 128 \t mean(l0) : 20.171875 \t Current Learning Rate: 0.010\n",
            "Iteration 13200: Current success : 88 \t All success : 128 \t mean(l0) : 20.171875 \t Current Learning Rate: 0.009\n",
            "Iteration 13300: Current success : 88 \t All success : 128 \t mean(l0) : 20.171875 \t Current Learning Rate: 0.009\n",
            "Iteration 13400: Current success : 88 \t All success : 128 \t mean(l0) : 20.171875 \t Current Learning Rate: 0.008\n",
            "Iteration 13500: Current success : 88 \t All success : 128 \t mean(l0) : 20.171875 \t Current Learning Rate: 0.008\n",
            "Iteration 13600: Current success : 88 \t All success : 128 \t mean(l0) : 20.171875 \t Current Learning Rate: 0.007\n",
            "Iteration 13700: Current success : 88 \t All success : 128 \t mean(l0) : 20.171875 \t Current Learning Rate: 0.007\n",
            "Iteration 13800: Current success : 88 \t All success : 128 \t mean(l0) : 20.171875 \t Current Learning Rate: 0.006\n",
            "Iteration 13900: Current success : 88 \t All success : 128 \t mean(l0) : 20.171875 \t Current Learning Rate: 0.006\n",
            "Iteration 14000: Current success : 88 \t All success : 128 \t mean(l0) : 20.171875 \t Current Learning Rate: 0.005\n",
            "Iteration 14100: Current success : 88 \t All success : 128 \t mean(l0) : 20.171875 \t Current Learning Rate: 0.005\n",
            "Iteration 14200: Current success : 88 \t All success : 128 \t mean(l0) : 20.171875 \t Current Learning Rate: 0.004\n",
            "Iteration 14300: Current success : 88 \t All success : 128 \t mean(l0) : 20.171875 \t Current Learning Rate: 0.004\n",
            "Iteration 14400: Current success : 88 \t All success : 128 \t mean(l0) : 20.171875 \t Current Learning Rate: 0.003\n",
            "Iteration 14500: Current success : 88 \t All success : 128 \t mean(l0) : 20.171875 \t Current Learning Rate: 0.003\n",
            "Iteration 14600: Current success : 88 \t All success : 128 \t mean(l0) : 20.171875 \t Current Learning Rate: 0.002\n",
            "Iteration 14700: Current success : 88 \t All success : 128 \t mean(l0) : 20.171875 \t Current Learning Rate: 0.002\n",
            "Iteration 14800: Current success : 88 \t All success : 128 \t mean(l0) : 20.171875 \t Current Learning Rate: 0.001\n",
            "Iteration 14900: Current success : 88 \t All success : 128 \t mean(l0) : 20.171875 \t Current Learning Rate: 0.001\n",
            "Iteration 15000: Current success : 88 \t All success : 128 \t mean(l0) : 20.171875 \t Current Learning Rate: 0.000\n",
            "Iteration 15100: Current success : 89 \t All success : 131 \t mean(l0) : 20.030535 \t Current Learning Rate: 0.012\n",
            "Iteration 15200: Current success : 92 \t All success : 131 \t mean(l0) : 20.007633 \t Current Learning Rate: 0.012\n",
            "Iteration 15300: Current success : 92 \t All success : 131 \t mean(l0) : 20.007633 \t Current Learning Rate: 0.012\n",
            "Iteration 15400: Current success : 92 \t All success : 131 \t mean(l0) : 20.007633 \t Current Learning Rate: 0.012\n",
            "Iteration 15500: Current success : 92 \t All success : 131 \t mean(l0) : 20.007633 \t Current Learning Rate: 0.011\n",
            "Iteration 15600: Current success : 93 \t All success : 131 \t mean(l0) : 20.007633 \t Current Learning Rate: 0.011\n",
            "Iteration 15700: Current success : 99 \t All success : 137 \t mean(l0) : 19.525547 \t Current Learning Rate: 0.011\n",
            "Iteration 15800: Current success : 93 \t All success : 137 \t mean(l0) : 19.525547 \t Current Learning Rate: 0.011\n",
            "Iteration 15900: Current success : 96 \t All success : 140 \t mean(l0) : 19.214285 \t Current Learning Rate: 0.010\n",
            "Iteration 16000: Current success : 93 \t All success : 140 \t mean(l0) : 19.214285 \t Current Learning Rate: 0.010\n",
            "Iteration 16100: Current success : 92 \t All success : 140 \t mean(l0) : 19.214285 \t Current Learning Rate: 0.010\n",
            "Iteration 16200: Current success : 91 \t All success : 140 \t mean(l0) : 19.214285 \t Current Learning Rate: 0.010\n",
            "Iteration 16300: Current success : 95 \t All success : 140 \t mean(l0) : 19.214285 \t Current Learning Rate: 0.009\n",
            "Iteration 16400: Current success : 92 \t All success : 140 \t mean(l0) : 19.214285 \t Current Learning Rate: 0.009\n",
            "Iteration 16500: Current success : 91 \t All success : 140 \t mean(l0) : 19.214285 \t Current Learning Rate: 0.009\n",
            "Iteration 16600: Current success : 95 \t All success : 140 \t mean(l0) : 19.214285 \t Current Learning Rate: 0.009\n",
            "Iteration 16700: Current success : 91 \t All success : 140 \t mean(l0) : 19.214285 \t Current Learning Rate: 0.008\n",
            "Iteration 16800: Current success : 92 \t All success : 140 \t mean(l0) : 19.214285 \t Current Learning Rate: 0.008\n",
            "Iteration 16900: Current success : 92 \t All success : 140 \t mean(l0) : 19.214285 \t Current Learning Rate: 0.008\n",
            "Iteration 17000: Current success : 94 \t All success : 140 \t mean(l0) : 19.214285 \t Current Learning Rate: 0.008\n",
            "Iteration 17100: Current success : 92 \t All success : 140 \t mean(l0) : 19.214285 \t Current Learning Rate: 0.007\n",
            "Iteration 17200: Current success : 96 \t All success : 143 \t mean(l0) : 19.146852 \t Current Learning Rate: 0.007\n",
            "Iteration 17300: Current success : 92 \t All success : 143 \t mean(l0) : 19.146852 \t Current Learning Rate: 0.007\n",
            "Iteration 17400: Current success : 95 \t All success : 144 \t mean(l0) : 19.326389 \t Current Learning Rate: 0.007\n",
            "Iteration 17500: Current success : 92 \t All success : 144 \t mean(l0) : 19.326389 \t Current Learning Rate: 0.006\n",
            "Iteration 17600: Current success : 91 \t All success : 144 \t mean(l0) : 19.326389 \t Current Learning Rate: 0.006\n",
            "Iteration 17700: Current success : 92 \t All success : 144 \t mean(l0) : 19.326389 \t Current Learning Rate: 0.006\n",
            "Iteration 17800: Current success : 91 \t All success : 144 \t mean(l0) : 19.326389 \t Current Learning Rate: 0.006\n",
            "Iteration 17900: Current success : 93 \t All success : 144 \t mean(l0) : 19.326389 \t Current Learning Rate: 0.005\n",
            "Iteration 18000: Current success : 93 \t All success : 144 \t mean(l0) : 19.326389 \t Current Learning Rate: 0.005\n",
            "Iteration 18100: Current success : 93 \t All success : 144 \t mean(l0) : 19.326389 \t Current Learning Rate: 0.005\n",
            "Iteration 18200: Current success : 93 \t All success : 144 \t mean(l0) : 19.326389 \t Current Learning Rate: 0.005\n",
            "Iteration 18300: Current success : 93 \t All success : 144 \t mean(l0) : 19.326389 \t Current Learning Rate: 0.004\n",
            "Iteration 18400: Current success : 93 \t All success : 144 \t mean(l0) : 19.326389 \t Current Learning Rate: 0.004\n",
            "Iteration 18500: Current success : 93 \t All success : 144 \t mean(l0) : 19.326389 \t Current Learning Rate: 0.004\n",
            "Iteration 18600: Current success : 93 \t All success : 144 \t mean(l0) : 19.326389 \t Current Learning Rate: 0.004\n",
            "Iteration 18700: Current success : 93 \t All success : 144 \t mean(l0) : 19.326389 \t Current Learning Rate: 0.003\n",
            "Iteration 18800: Current success : 93 \t All success : 144 \t mean(l0) : 19.326389 \t Current Learning Rate: 0.003\n",
            "Iteration 18900: Current success : 93 \t All success : 144 \t mean(l0) : 19.326389 \t Current Learning Rate: 0.003\n",
            "Iteration 19000: Current success : 93 \t All success : 144 \t mean(l0) : 19.326389 \t Current Learning Rate: 0.003\n",
            "Iteration 19100: Current success : 93 \t All success : 144 \t mean(l0) : 19.326389 \t Current Learning Rate: 0.002\n",
            "Iteration 19200: Current success : 93 \t All success : 144 \t mean(l0) : 19.326389 \t Current Learning Rate: 0.002\n",
            "Iteration 19300: Current success : 93 \t All success : 144 \t mean(l0) : 19.326389 \t Current Learning Rate: 0.002\n",
            "Iteration 19400: Current success : 93 \t All success : 144 \t mean(l0) : 19.326389 \t Current Learning Rate: 0.002\n",
            "Iteration 19500: Current success : 93 \t All success : 144 \t mean(l0) : 19.326389 \t Current Learning Rate: 0.001\n",
            "Iteration 19600: Current success : 93 \t All success : 144 \t mean(l0) : 19.326389 \t Current Learning Rate: 0.001\n",
            "Iteration 19700: Current success : 93 \t All success : 144 \t mean(l0) : 19.326389 \t Current Learning Rate: 0.001\n",
            "Iteration 19800: Current success : 93 \t All success : 144 \t mean(l0) : 19.326389 \t Current Learning Rate: 0.001\n",
            "Iteration 19900: Current success : 93 \t All success : 144 \t mean(l0) : 19.326389 \t Current Learning Rate: 0.000\n",
            "Iteration 20000: Current success : 93 \t All success : 144 \t mean(l0) : 19.326389 \t Current Learning Rate: 0.000\n",
            "outer_step 5: all success : 257 \t mean(l0)(success) : 20.0078\n",
            "-------------------------------------------------------------\n",
            "tensor([2.1250e+01, 1.0000e+05, 1.0000e+05,  ..., 1.0000e+05, 1.0000e+05,\n",
            "        1.0000e+05], device='cuda:0')\n",
            "Iteration 100: Current success : 27 \t All success : 48 \t mean(l0) : 8.541667 \t Current Learning Rate: 0.098\n",
            "Iteration 200: Current success : 40 \t All success : 64 \t mean(l0) : 10.875000 \t Current Learning Rate: 0.096\n",
            "Iteration 300: Current success : 45 \t All success : 74 \t mean(l0) : 13.905406 \t Current Learning Rate: 0.094\n",
            "Iteration 400: Current success : 47 \t All success : 77 \t mean(l0) : 14.571428 \t Current Learning Rate: 0.092\n",
            "Iteration 500: Current success : 49 \t All success : 80 \t mean(l0) : 14.925000 \t Current Learning Rate: 0.090\n",
            "Iteration 600: Current success : 49 \t All success : 81 \t mean(l0) : 14.790124 \t Current Learning Rate: 0.088\n",
            "Iteration 700: Current success : 48 \t All success : 81 \t mean(l0) : 14.790124 \t Current Learning Rate: 0.086\n",
            "Iteration 800: Current success : 48 \t All success : 81 \t mean(l0) : 14.777778 \t Current Learning Rate: 0.084\n",
            "Iteration 900: Current success : 49 \t All success : 81 \t mean(l0) : 14.777778 \t Current Learning Rate: 0.082\n",
            "Iteration 1000: Current success : 49 \t All success : 81 \t mean(l0) : 14.777778 \t Current Learning Rate: 0.080\n",
            "Iteration 1100: Current success : 49 \t All success : 81 \t mean(l0) : 14.777778 \t Current Learning Rate: 0.078\n",
            "Iteration 1200: Current success : 49 \t All success : 81 \t mean(l0) : 14.777778 \t Current Learning Rate: 0.076\n",
            "Iteration 1300: Current success : 50 \t All success : 82 \t mean(l0) : 14.634146 \t Current Learning Rate: 0.074\n",
            "Iteration 1400: Current success : 50 \t All success : 82 \t mean(l0) : 14.634146 \t Current Learning Rate: 0.072\n",
            "Iteration 1500: Current success : 51 \t All success : 84 \t mean(l0) : 15.571429 \t Current Learning Rate: 0.070\n",
            "Iteration 1600: Current success : 52 \t All success : 84 \t mean(l0) : 15.571429 \t Current Learning Rate: 0.068\n",
            "Iteration 1700: Current success : 52 \t All success : 84 \t mean(l0) : 15.571429 \t Current Learning Rate: 0.066\n",
            "Iteration 1800: Current success : 52 \t All success : 84 \t mean(l0) : 15.523809 \t Current Learning Rate: 0.064\n",
            "Iteration 1900: Current success : 57 \t All success : 88 \t mean(l0) : 16.465910 \t Current Learning Rate: 0.062\n",
            "Iteration 2000: Current success : 56 \t All success : 88 \t mean(l0) : 16.465910 \t Current Learning Rate: 0.060\n",
            "Iteration 2100: Current success : 56 \t All success : 88 \t mean(l0) : 16.465910 \t Current Learning Rate: 0.058\n",
            "Iteration 2200: Current success : 57 \t All success : 89 \t mean(l0) : 16.404495 \t Current Learning Rate: 0.056\n",
            "Iteration 2300: Current success : 58 \t All success : 89 \t mean(l0) : 16.404495 \t Current Learning Rate: 0.054\n",
            "Iteration 2400: Current success : 57 \t All success : 89 \t mean(l0) : 16.404495 \t Current Learning Rate: 0.052\n",
            "Iteration 2500: Current success : 58 \t All success : 91 \t mean(l0) : 16.109890 \t Current Learning Rate: 0.050\n",
            "Iteration 2600: Current success : 57 \t All success : 91 \t mean(l0) : 16.109890 \t Current Learning Rate: 0.048\n",
            "Iteration 2700: Current success : 64 \t All success : 97 \t mean(l0) : 15.618556 \t Current Learning Rate: 0.046\n",
            "Iteration 2800: Current success : 69 \t All success : 99 \t mean(l0) : 15.373737 \t Current Learning Rate: 0.044\n",
            "Iteration 2900: Current success : 69 \t All success : 99 \t mean(l0) : 15.373737 \t Current Learning Rate: 0.042\n",
            "Iteration 3000: Current success : 69 \t All success : 99 \t mean(l0) : 15.373737 \t Current Learning Rate: 0.040\n",
            "Iteration 3100: Current success : 69 \t All success : 99 \t mean(l0) : 15.373737 \t Current Learning Rate: 0.038\n",
            "Iteration 3200: Current success : 69 \t All success : 99 \t mean(l0) : 15.373737 \t Current Learning Rate: 0.036\n",
            "Iteration 3300: Current success : 69 \t All success : 99 \t mean(l0) : 15.373737 \t Current Learning Rate: 0.034\n",
            "Iteration 3400: Current success : 69 \t All success : 99 \t mean(l0) : 15.373737 \t Current Learning Rate: 0.032\n",
            "Iteration 3500: Current success : 69 \t All success : 99 \t mean(l0) : 15.373737 \t Current Learning Rate: 0.030\n",
            "Iteration 3600: Current success : 69 \t All success : 99 \t mean(l0) : 15.373737 \t Current Learning Rate: 0.028\n",
            "Iteration 3700: Current success : 69 \t All success : 99 \t mean(l0) : 15.373737 \t Current Learning Rate: 0.026\n",
            "Iteration 3800: Current success : 69 \t All success : 99 \t mean(l0) : 15.373737 \t Current Learning Rate: 0.024\n",
            "Iteration 3900: Current success : 69 \t All success : 99 \t mean(l0) : 15.373737 \t Current Learning Rate: 0.022\n",
            "Iteration 4000: Current success : 69 \t All success : 99 \t mean(l0) : 15.373737 \t Current Learning Rate: 0.020\n",
            "Iteration 4100: Current success : 69 \t All success : 99 \t mean(l0) : 15.373737 \t Current Learning Rate: 0.018\n",
            "Iteration 4200: Current success : 69 \t All success : 99 \t mean(l0) : 15.373737 \t Current Learning Rate: 0.016\n",
            "Iteration 4300: Current success : 69 \t All success : 99 \t mean(l0) : 15.373737 \t Current Learning Rate: 0.014\n",
            "Iteration 4400: Current success : 69 \t All success : 99 \t mean(l0) : 15.373737 \t Current Learning Rate: 0.012\n",
            "Iteration 4500: Current success : 69 \t All success : 99 \t mean(l0) : 15.373737 \t Current Learning Rate: 0.010\n",
            "Iteration 4600: Current success : 69 \t All success : 99 \t mean(l0) : 15.373737 \t Current Learning Rate: 0.008\n",
            "Iteration 4700: Current success : 69 \t All success : 99 \t mean(l0) : 15.373737 \t Current Learning Rate: 0.006\n",
            "Iteration 4800: Current success : 69 \t All success : 99 \t mean(l0) : 15.373737 \t Current Learning Rate: 0.004\n",
            "Iteration 4900: Current success : 69 \t All success : 99 \t mean(l0) : 15.373737 \t Current Learning Rate: 0.002\n",
            "Iteration 5000: Current success : 69 \t All success : 99 \t mean(l0) : 15.373737 \t Current Learning Rate: 0.000\n",
            "Iteration 5100: Current success : 77 \t All success : 123 \t mean(l0) : 19.097561 \t Current Learning Rate: 0.049\n",
            "Iteration 5200: Current success : 93 \t All success : 136 \t mean(l0) : 18.801470 \t Current Learning Rate: 0.048\n",
            "Iteration 5300: Current success : 98 \t All success : 141 \t mean(l0) : 20.014183 \t Current Learning Rate: 0.047\n",
            "Iteration 5400: Current success : 97 \t All success : 141 \t mean(l0) : 20.007092 \t Current Learning Rate: 0.046\n",
            "Iteration 5500: Current success : 96 \t All success : 141 \t mean(l0) : 20.000000 \t Current Learning Rate: 0.045\n",
            "Iteration 5600: Current success : 98 \t All success : 141 \t mean(l0) : 20.000000 \t Current Learning Rate: 0.044\n",
            "Iteration 5700: Current success : 97 \t All success : 141 \t mean(l0) : 20.000000 \t Current Learning Rate: 0.043\n",
            "Iteration 5800: Current success : 97 \t All success : 141 \t mean(l0) : 19.957447 \t Current Learning Rate: 0.042\n",
            "Iteration 5900: Current success : 97 \t All success : 141 \t mean(l0) : 19.957447 \t Current Learning Rate: 0.041\n",
            "Iteration 6000: Current success : 97 \t All success : 141 \t mean(l0) : 19.957447 \t Current Learning Rate: 0.040\n",
            "Iteration 6100: Current success : 96 \t All success : 141 \t mean(l0) : 19.957447 \t Current Learning Rate: 0.039\n",
            "Iteration 6200: Current success : 96 \t All success : 141 \t mean(l0) : 19.957447 \t Current Learning Rate: 0.038\n",
            "Iteration 6300: Current success : 97 \t All success : 142 \t mean(l0) : 20.274647 \t Current Learning Rate: 0.037\n",
            "Iteration 6400: Current success : 97 \t All success : 142 \t mean(l0) : 20.274647 \t Current Learning Rate: 0.036\n",
            "Iteration 6500: Current success : 97 \t All success : 142 \t mean(l0) : 20.267605 \t Current Learning Rate: 0.035\n",
            "Iteration 6600: Current success : 97 \t All success : 142 \t mean(l0) : 20.267605 \t Current Learning Rate: 0.034\n",
            "Iteration 6700: Current success : 97 \t All success : 142 \t mean(l0) : 20.267605 \t Current Learning Rate: 0.033\n",
            "Iteration 6800: Current success : 94 \t All success : 142 \t mean(l0) : 20.267605 \t Current Learning Rate: 0.032\n",
            "Iteration 6900: Current success : 97 \t All success : 142 \t mean(l0) : 20.267605 \t Current Learning Rate: 0.031\n",
            "Iteration 7000: Current success : 94 \t All success : 142 \t mean(l0) : 20.267605 \t Current Learning Rate: 0.030\n",
            "Iteration 7100: Current success : 95 \t All success : 142 \t mean(l0) : 20.267605 \t Current Learning Rate: 0.029\n",
            "Iteration 7200: Current success : 94 \t All success : 142 \t mean(l0) : 20.267605 \t Current Learning Rate: 0.028\n",
            "Iteration 7300: Current success : 94 \t All success : 142 \t mean(l0) : 20.267605 \t Current Learning Rate: 0.027\n",
            "Iteration 7400: Current success : 98 \t All success : 142 \t mean(l0) : 20.267605 \t Current Learning Rate: 0.026\n",
            "Iteration 7500: Current success : 94 \t All success : 142 \t mean(l0) : 20.267605 \t Current Learning Rate: 0.025\n",
            "Iteration 7600: Current success : 97 \t All success : 142 \t mean(l0) : 20.267605 \t Current Learning Rate: 0.024\n",
            "Iteration 7700: Current success : 98 \t All success : 142 \t mean(l0) : 20.267605 \t Current Learning Rate: 0.023\n",
            "Iteration 7800: Current success : 98 \t All success : 142 \t mean(l0) : 20.267605 \t Current Learning Rate: 0.022\n",
            "Iteration 7900: Current success : 98 \t All success : 142 \t mean(l0) : 20.267605 \t Current Learning Rate: 0.021\n",
            "Iteration 8000: Current success : 98 \t All success : 142 \t mean(l0) : 20.267605 \t Current Learning Rate: 0.020\n",
            "Iteration 8100: Current success : 98 \t All success : 142 \t mean(l0) : 20.267605 \t Current Learning Rate: 0.019\n",
            "Iteration 8200: Current success : 98 \t All success : 142 \t mean(l0) : 20.267605 \t Current Learning Rate: 0.018\n",
            "Iteration 8300: Current success : 98 \t All success : 142 \t mean(l0) : 20.267605 \t Current Learning Rate: 0.017\n",
            "Iteration 8400: Current success : 98 \t All success : 142 \t mean(l0) : 20.267605 \t Current Learning Rate: 0.016\n",
            "Iteration 8500: Current success : 98 \t All success : 142 \t mean(l0) : 20.267605 \t Current Learning Rate: 0.015\n",
            "Iteration 8600: Current success : 98 \t All success : 142 \t mean(l0) : 20.267605 \t Current Learning Rate: 0.014\n",
            "Iteration 8700: Current success : 98 \t All success : 142 \t mean(l0) : 20.267605 \t Current Learning Rate: 0.013\n",
            "Iteration 8800: Current success : 98 \t All success : 142 \t mean(l0) : 20.267605 \t Current Learning Rate: 0.012\n",
            "Iteration 8900: Current success : 98 \t All success : 142 \t mean(l0) : 20.267605 \t Current Learning Rate: 0.011\n",
            "Iteration 9000: Current success : 98 \t All success : 142 \t mean(l0) : 20.267605 \t Current Learning Rate: 0.010\n",
            "Iteration 9100: Current success : 98 \t All success : 142 \t mean(l0) : 20.267605 \t Current Learning Rate: 0.009\n",
            "Iteration 9200: Current success : 98 \t All success : 142 \t mean(l0) : 20.267605 \t Current Learning Rate: 0.008\n",
            "Iteration 9300: Current success : 98 \t All success : 142 \t mean(l0) : 20.267605 \t Current Learning Rate: 0.007\n",
            "Iteration 9400: Current success : 98 \t All success : 142 \t mean(l0) : 20.267605 \t Current Learning Rate: 0.006\n",
            "Iteration 9500: Current success : 98 \t All success : 142 \t mean(l0) : 20.267605 \t Current Learning Rate: 0.005\n",
            "Iteration 9600: Current success : 98 \t All success : 142 \t mean(l0) : 20.267605 \t Current Learning Rate: 0.004\n",
            "Iteration 9700: Current success : 98 \t All success : 142 \t mean(l0) : 20.267605 \t Current Learning Rate: 0.003\n",
            "Iteration 9800: Current success : 98 \t All success : 142 \t mean(l0) : 20.267605 \t Current Learning Rate: 0.002\n",
            "Iteration 9900: Current success : 98 \t All success : 142 \t mean(l0) : 20.267605 \t Current Learning Rate: 0.001\n",
            "Iteration 10000: Current success : 98 \t All success : 142 \t mean(l0) : 20.267605 \t Current Learning Rate: 0.000\n",
            "Iteration 10100: Current success : 86 \t All success : 145 \t mean(l0) : 19.979311 \t Current Learning Rate: 0.025\n",
            "Iteration 10200: Current success : 87 \t All success : 145 \t mean(l0) : 19.958620 \t Current Learning Rate: 0.024\n",
            "Iteration 10300: Current success : 87 \t All success : 145 \t mean(l0) : 19.958620 \t Current Learning Rate: 0.024\n",
            "Iteration 10400: Current success : 87 \t All success : 145 \t mean(l0) : 19.937931 \t Current Learning Rate: 0.023\n",
            "Iteration 10500: Current success : 87 \t All success : 145 \t mean(l0) : 19.937931 \t Current Learning Rate: 0.023\n",
            "Iteration 10600: Current success : 87 \t All success : 145 \t mean(l0) : 19.937931 \t Current Learning Rate: 0.022\n",
            "Iteration 10700: Current success : 87 \t All success : 145 \t mean(l0) : 19.937931 \t Current Learning Rate: 0.022\n",
            "Iteration 10800: Current success : 87 \t All success : 145 \t mean(l0) : 19.937931 \t Current Learning Rate: 0.021\n",
            "Iteration 10900: Current success : 87 \t All success : 145 \t mean(l0) : 19.937931 \t Current Learning Rate: 0.021\n",
            "Iteration 11000: Current success : 87 \t All success : 145 \t mean(l0) : 19.937931 \t Current Learning Rate: 0.020\n",
            "Iteration 11100: Current success : 87 \t All success : 145 \t mean(l0) : 19.937931 \t Current Learning Rate: 0.020\n",
            "Iteration 11200: Current success : 87 \t All success : 145 \t mean(l0) : 19.937931 \t Current Learning Rate: 0.019\n",
            "Iteration 11300: Current success : 87 \t All success : 145 \t mean(l0) : 19.937931 \t Current Learning Rate: 0.019\n",
            "Iteration 11400: Current success : 87 \t All success : 145 \t mean(l0) : 19.937931 \t Current Learning Rate: 0.018\n",
            "Iteration 11500: Current success : 87 \t All success : 145 \t mean(l0) : 19.937931 \t Current Learning Rate: 0.018\n",
            "Iteration 11600: Current success : 87 \t All success : 145 \t mean(l0) : 19.937931 \t Current Learning Rate: 0.017\n",
            "Iteration 11700: Current success : 87 \t All success : 145 \t mean(l0) : 19.937931 \t Current Learning Rate: 0.017\n",
            "Iteration 11800: Current success : 87 \t All success : 145 \t mean(l0) : 19.937931 \t Current Learning Rate: 0.016\n",
            "Iteration 11900: Current success : 87 \t All success : 145 \t mean(l0) : 19.937931 \t Current Learning Rate: 0.016\n",
            "Iteration 12000: Current success : 87 \t All success : 145 \t mean(l0) : 19.937931 \t Current Learning Rate: 0.015\n",
            "Iteration 12100: Current success : 87 \t All success : 145 \t mean(l0) : 19.937931 \t Current Learning Rate: 0.015\n",
            "Iteration 12200: Current success : 87 \t All success : 145 \t mean(l0) : 19.937931 \t Current Learning Rate: 0.014\n",
            "Iteration 12300: Current success : 87 \t All success : 145 \t mean(l0) : 19.937931 \t Current Learning Rate: 0.014\n",
            "Iteration 12400: Current success : 87 \t All success : 145 \t mean(l0) : 19.903448 \t Current Learning Rate: 0.013\n",
            "Iteration 12500: Current success : 87 \t All success : 145 \t mean(l0) : 19.868965 \t Current Learning Rate: 0.013\n",
            "Iteration 12600: Current success : 87 \t All success : 145 \t mean(l0) : 19.868965 \t Current Learning Rate: 0.012\n",
            "Iteration 12700: Current success : 87 \t All success : 145 \t mean(l0) : 19.868965 \t Current Learning Rate: 0.012\n",
            "Iteration 12800: Current success : 87 \t All success : 145 \t mean(l0) : 19.868965 \t Current Learning Rate: 0.011\n",
            "Iteration 12900: Current success : 87 \t All success : 145 \t mean(l0) : 19.868965 \t Current Learning Rate: 0.011\n",
            "Iteration 13000: Current success : 100 \t All success : 145 \t mean(l0) : 19.868965 \t Current Learning Rate: 0.010\n",
            "Iteration 13100: Current success : 100 \t All success : 145 \t mean(l0) : 19.868965 \t Current Learning Rate: 0.010\n",
            "Iteration 13200: Current success : 100 \t All success : 145 \t mean(l0) : 19.868965 \t Current Learning Rate: 0.009\n",
            "Iteration 13300: Current success : 100 \t All success : 145 \t mean(l0) : 19.868965 \t Current Learning Rate: 0.009\n",
            "Iteration 13400: Current success : 100 \t All success : 145 \t mean(l0) : 19.868965 \t Current Learning Rate: 0.008\n",
            "Iteration 13500: Current success : 100 \t All success : 145 \t mean(l0) : 19.868965 \t Current Learning Rate: 0.008\n",
            "Iteration 13600: Current success : 100 \t All success : 145 \t mean(l0) : 19.868965 \t Current Learning Rate: 0.007\n",
            "Iteration 13700: Current success : 100 \t All success : 145 \t mean(l0) : 19.868965 \t Current Learning Rate: 0.007\n",
            "Iteration 13800: Current success : 100 \t All success : 145 \t mean(l0) : 19.868965 \t Current Learning Rate: 0.006\n",
            "Iteration 13900: Current success : 100 \t All success : 145 \t mean(l0) : 19.868965 \t Current Learning Rate: 0.006\n",
            "Iteration 14000: Current success : 100 \t All success : 145 \t mean(l0) : 19.868965 \t Current Learning Rate: 0.005\n",
            "Iteration 14100: Current success : 100 \t All success : 145 \t mean(l0) : 19.868965 \t Current Learning Rate: 0.005\n",
            "Iteration 14200: Current success : 100 \t All success : 145 \t mean(l0) : 19.868965 \t Current Learning Rate: 0.004\n",
            "Iteration 14300: Current success : 100 \t All success : 145 \t mean(l0) : 19.868965 \t Current Learning Rate: 0.004\n",
            "Iteration 14400: Current success : 100 \t All success : 145 \t mean(l0) : 19.868965 \t Current Learning Rate: 0.003\n",
            "Iteration 14500: Current success : 100 \t All success : 145 \t mean(l0) : 19.868965 \t Current Learning Rate: 0.003\n",
            "Iteration 14600: Current success : 100 \t All success : 145 \t mean(l0) : 19.868965 \t Current Learning Rate: 0.002\n",
            "Iteration 14700: Current success : 100 \t All success : 145 \t mean(l0) : 19.868965 \t Current Learning Rate: 0.002\n",
            "Iteration 14800: Current success : 100 \t All success : 145 \t mean(l0) : 19.868965 \t Current Learning Rate: 0.001\n",
            "Iteration 14900: Current success : 100 \t All success : 145 \t mean(l0) : 19.868965 \t Current Learning Rate: 0.001\n",
            "Iteration 15000: Current success : 100 \t All success : 145 \t mean(l0) : 19.868965 \t Current Learning Rate: 0.000\n",
            "Iteration 15100: Current success : 98 \t All success : 152 \t mean(l0) : 19.210526 \t Current Learning Rate: 0.012\n",
            "Iteration 15200: Current success : 107 \t All success : 155 \t mean(l0) : 19.400000 \t Current Learning Rate: 0.012\n",
            "Iteration 15300: Current success : 107 \t All success : 155 \t mean(l0) : 19.341936 \t Current Learning Rate: 0.012\n",
            "Iteration 15400: Current success : 104 \t All success : 155 \t mean(l0) : 19.335484 \t Current Learning Rate: 0.012\n",
            "Iteration 15500: Current success : 107 \t All success : 155 \t mean(l0) : 19.329033 \t Current Learning Rate: 0.011\n",
            "Iteration 15600: Current success : 100 \t All success : 155 \t mean(l0) : 19.290321 \t Current Learning Rate: 0.011\n",
            "Iteration 15700: Current success : 103 \t All success : 155 \t mean(l0) : 19.251612 \t Current Learning Rate: 0.011\n",
            "Iteration 15800: Current success : 103 \t All success : 155 \t mean(l0) : 19.251612 \t Current Learning Rate: 0.011\n",
            "Iteration 15900: Current success : 104 \t All success : 155 \t mean(l0) : 19.245161 \t Current Learning Rate: 0.010\n",
            "Iteration 16000: Current success : 100 \t All success : 155 \t mean(l0) : 19.245161 \t Current Learning Rate: 0.010\n",
            "Iteration 16100: Current success : 104 \t All success : 155 \t mean(l0) : 19.238708 \t Current Learning Rate: 0.010\n",
            "Iteration 16200: Current success : 100 \t All success : 155 \t mean(l0) : 19.238708 \t Current Learning Rate: 0.010\n",
            "Iteration 16300: Current success : 107 \t All success : 155 \t mean(l0) : 19.238708 \t Current Learning Rate: 0.009\n",
            "Iteration 16400: Current success : 103 \t All success : 155 \t mean(l0) : 19.238708 \t Current Learning Rate: 0.009\n",
            "Iteration 16500: Current success : 107 \t All success : 155 \t mean(l0) : 19.238708 \t Current Learning Rate: 0.009\n",
            "Iteration 16600: Current success : 107 \t All success : 155 \t mean(l0) : 19.238708 \t Current Learning Rate: 0.009\n",
            "Iteration 16700: Current success : 104 \t All success : 155 \t mean(l0) : 19.199999 \t Current Learning Rate: 0.008\n",
            "Iteration 16800: Current success : 107 \t All success : 155 \t mean(l0) : 19.199999 \t Current Learning Rate: 0.008\n",
            "Iteration 16900: Current success : 99 \t All success : 155 \t mean(l0) : 19.199999 \t Current Learning Rate: 0.008\n",
            "Iteration 17000: Current success : 100 \t All success : 155 \t mean(l0) : 19.199999 \t Current Learning Rate: 0.008\n",
            "Iteration 17100: Current success : 103 \t All success : 155 \t mean(l0) : 19.199999 \t Current Learning Rate: 0.007\n",
            "Iteration 17200: Current success : 103 \t All success : 155 \t mean(l0) : 19.199999 \t Current Learning Rate: 0.007\n",
            "Iteration 17300: Current success : 102 \t All success : 155 \t mean(l0) : 19.199999 \t Current Learning Rate: 0.007\n",
            "Iteration 17400: Current success : 99 \t All success : 155 \t mean(l0) : 19.199999 \t Current Learning Rate: 0.007\n",
            "Iteration 17500: Current success : 104 \t All success : 156 \t mean(l0) : 19.365385 \t Current Learning Rate: 0.006\n",
            "Iteration 17600: Current success : 107 \t All success : 156 \t mean(l0) : 19.365385 \t Current Learning Rate: 0.006\n",
            "Iteration 17700: Current success : 107 \t All success : 156 \t mean(l0) : 19.365385 \t Current Learning Rate: 0.006\n",
            "Iteration 17800: Current success : 108 \t All success : 156 \t mean(l0) : 19.358974 \t Current Learning Rate: 0.006\n",
            "Iteration 17900: Current success : 107 \t All success : 156 \t mean(l0) : 19.358974 \t Current Learning Rate: 0.005\n",
            "Iteration 18000: Current success : 108 \t All success : 156 \t mean(l0) : 19.358974 \t Current Learning Rate: 0.005\n",
            "Iteration 18100: Current success : 108 \t All success : 156 \t mean(l0) : 19.358974 \t Current Learning Rate: 0.005\n",
            "Iteration 18200: Current success : 107 \t All success : 156 \t mean(l0) : 19.358974 \t Current Learning Rate: 0.005\n",
            "Iteration 18300: Current success : 107 \t All success : 156 \t mean(l0) : 19.358974 \t Current Learning Rate: 0.004\n",
            "Iteration 18400: Current success : 108 \t All success : 156 \t mean(l0) : 19.358974 \t Current Learning Rate: 0.004\n",
            "Iteration 18500: Current success : 107 \t All success : 156 \t mean(l0) : 19.358974 \t Current Learning Rate: 0.004\n",
            "Iteration 18600: Current success : 107 \t All success : 156 \t mean(l0) : 19.358974 \t Current Learning Rate: 0.004\n",
            "Iteration 18700: Current success : 107 \t All success : 156 \t mean(l0) : 19.358974 \t Current Learning Rate: 0.003\n",
            "Iteration 18800: Current success : 107 \t All success : 156 \t mean(l0) : 19.358974 \t Current Learning Rate: 0.003\n",
            "Iteration 18900: Current success : 107 \t All success : 156 \t mean(l0) : 19.358974 \t Current Learning Rate: 0.003\n",
            "Iteration 19000: Current success : 107 \t All success : 156 \t mean(l0) : 19.358974 \t Current Learning Rate: 0.003\n",
            "Iteration 19100: Current success : 107 \t All success : 156 \t mean(l0) : 19.358974 \t Current Learning Rate: 0.002\n",
            "Iteration 19200: Current success : 107 \t All success : 156 \t mean(l0) : 19.358974 \t Current Learning Rate: 0.002\n",
            "Iteration 19300: Current success : 107 \t All success : 156 \t mean(l0) : 19.358974 \t Current Learning Rate: 0.002\n",
            "Iteration 19400: Current success : 107 \t All success : 156 \t mean(l0) : 19.358974 \t Current Learning Rate: 0.002\n",
            "Iteration 19500: Current success : 107 \t All success : 156 \t mean(l0) : 19.358974 \t Current Learning Rate: 0.001\n",
            "Iteration 19600: Current success : 107 \t All success : 156 \t mean(l0) : 19.358974 \t Current Learning Rate: 0.001\n",
            "Iteration 19700: Current success : 107 \t All success : 156 \t mean(l0) : 19.358974 \t Current Learning Rate: 0.001\n",
            "Iteration 19800: Current success : 107 \t All success : 156 \t mean(l0) : 19.358974 \t Current Learning Rate: 0.001\n",
            "Iteration 19900: Current success : 107 \t All success : 156 \t mean(l0) : 19.358974 \t Current Learning Rate: 0.000\n",
            "Iteration 20000: Current success : 107 \t All success : 156 \t mean(l0) : 19.358974 \t Current Learning Rate: 0.000\n",
            "outer_step 6: all success : 262 \t mean(l0)(success) : 19.9275\n",
            "-------------------------------------------------------------\n",
            "tensor([1.5625e+01, 1.0000e+06, 1.0000e+06,  ..., 1.0000e+06, 1.0000e+06,\n",
            "        1.0000e+06], device='cuda:0')\n",
            "Iteration 100: Current success : 119 \t All success : 121 \t mean(l0) : 8.586777 \t Current Learning Rate: 0.098\n",
            "Iteration 200: Current success : 116 \t All success : 132 \t mean(l0) : 10.553031 \t Current Learning Rate: 0.096\n",
            "Iteration 300: Current success : 126 \t All success : 180 \t mean(l0) : 12.622223 \t Current Learning Rate: 0.094\n",
            "Iteration 400: Current success : 161 \t All success : 185 \t mean(l0) : 12.864865 \t Current Learning Rate: 0.092\n",
            "Iteration 500: Current success : 164 \t All success : 187 \t mean(l0) : 13.026738 \t Current Learning Rate: 0.090\n",
            "Iteration 600: Current success : 165 \t All success : 188 \t mean(l0) : 13.042553 \t Current Learning Rate: 0.088\n",
            "Iteration 700: Current success : 169 \t All success : 192 \t mean(l0) : 13.505209 \t Current Learning Rate: 0.086\n",
            "Iteration 800: Current success : 170 \t All success : 192 \t mean(l0) : 13.505209 \t Current Learning Rate: 0.084\n",
            "Iteration 900: Current success : 166 \t All success : 192 \t mean(l0) : 13.500000 \t Current Learning Rate: 0.082\n",
            "Iteration 1000: Current success : 166 \t All success : 192 \t mean(l0) : 13.500000 \t Current Learning Rate: 0.080\n",
            "Iteration 1100: Current success : 169 \t All success : 192 \t mean(l0) : 13.500000 \t Current Learning Rate: 0.078\n",
            "Iteration 1200: Current success : 169 \t All success : 192 \t mean(l0) : 13.500000 \t Current Learning Rate: 0.076\n",
            "Iteration 1300: Current success : 168 \t All success : 192 \t mean(l0) : 13.500000 \t Current Learning Rate: 0.074\n",
            "Iteration 1400: Current success : 169 \t All success : 193 \t mean(l0) : 13.772020 \t Current Learning Rate: 0.072\n",
            "Iteration 1500: Current success : 170 \t All success : 193 \t mean(l0) : 13.772020 \t Current Learning Rate: 0.070\n",
            "Iteration 1600: Current success : 170 \t All success : 193 \t mean(l0) : 13.772020 \t Current Learning Rate: 0.068\n",
            "Iteration 1700: Current success : 170 \t All success : 193 \t mean(l0) : 13.772020 \t Current Learning Rate: 0.066\n",
            "Iteration 1800: Current success : 172 \t All success : 195 \t mean(l0) : 14.194872 \t Current Learning Rate: 0.064\n",
            "Iteration 1900: Current success : 172 \t All success : 195 \t mean(l0) : 14.194872 \t Current Learning Rate: 0.062\n",
            "Iteration 2000: Current success : 172 \t All success : 195 \t mean(l0) : 14.194872 \t Current Learning Rate: 0.060\n",
            "Iteration 2100: Current success : 172 \t All success : 195 \t mean(l0) : 14.194872 \t Current Learning Rate: 0.058\n",
            "Iteration 2200: Current success : 172 \t All success : 195 \t mean(l0) : 14.194872 \t Current Learning Rate: 0.056\n",
            "Iteration 2300: Current success : 172 \t All success : 195 \t mean(l0) : 14.194872 \t Current Learning Rate: 0.054\n",
            "Iteration 2400: Current success : 174 \t All success : 197 \t mean(l0) : 14.340101 \t Current Learning Rate: 0.052\n",
            "Iteration 2500: Current success : 174 \t All success : 197 \t mean(l0) : 14.340101 \t Current Learning Rate: 0.050\n",
            "Iteration 2600: Current success : 174 \t All success : 197 \t mean(l0) : 14.340101 \t Current Learning Rate: 0.048\n",
            "Iteration 2700: Current success : 174 \t All success : 197 \t mean(l0) : 14.340101 \t Current Learning Rate: 0.046\n",
            "Iteration 2800: Current success : 174 \t All success : 197 \t mean(l0) : 14.340101 \t Current Learning Rate: 0.044\n",
            "Iteration 2900: Current success : 174 \t All success : 197 \t mean(l0) : 14.340101 \t Current Learning Rate: 0.042\n",
            "Iteration 3000: Current success : 174 \t All success : 197 \t mean(l0) : 14.340101 \t Current Learning Rate: 0.040\n",
            "Iteration 3100: Current success : 174 \t All success : 197 \t mean(l0) : 14.340101 \t Current Learning Rate: 0.038\n",
            "Iteration 3200: Current success : 175 \t All success : 198 \t mean(l0) : 14.439394 \t Current Learning Rate: 0.036\n",
            "Iteration 3300: Current success : 175 \t All success : 198 \t mean(l0) : 14.439394 \t Current Learning Rate: 0.034\n",
            "Iteration 3400: Current success : 174 \t All success : 198 \t mean(l0) : 14.439394 \t Current Learning Rate: 0.032\n",
            "Iteration 3500: Current success : 175 \t All success : 198 \t mean(l0) : 14.439394 \t Current Learning Rate: 0.030\n",
            "Iteration 3600: Current success : 175 \t All success : 198 \t mean(l0) : 14.439394 \t Current Learning Rate: 0.028\n",
            "Iteration 3700: Current success : 175 \t All success : 198 \t mean(l0) : 14.439394 \t Current Learning Rate: 0.026\n",
            "Iteration 3800: Current success : 175 \t All success : 198 \t mean(l0) : 14.439394 \t Current Learning Rate: 0.024\n",
            "Iteration 3900: Current success : 175 \t All success : 198 \t mean(l0) : 14.439394 \t Current Learning Rate: 0.022\n",
            "Iteration 4000: Current success : 175 \t All success : 198 \t mean(l0) : 14.439394 \t Current Learning Rate: 0.020\n",
            "Iteration 4100: Current success : 175 \t All success : 198 \t mean(l0) : 14.439394 \t Current Learning Rate: 0.018\n",
            "Iteration 4200: Current success : 175 \t All success : 198 \t mean(l0) : 14.439394 \t Current Learning Rate: 0.016\n",
            "Iteration 4300: Current success : 175 \t All success : 198 \t mean(l0) : 14.439394 \t Current Learning Rate: 0.014\n",
            "Iteration 4400: Current success : 175 \t All success : 198 \t mean(l0) : 14.439394 \t Current Learning Rate: 0.012\n",
            "Iteration 4500: Current success : 175 \t All success : 198 \t mean(l0) : 14.439394 \t Current Learning Rate: 0.010\n",
            "Iteration 4600: Current success : 175 \t All success : 198 \t mean(l0) : 14.439394 \t Current Learning Rate: 0.008\n",
            "Iteration 4700: Current success : 175 \t All success : 198 \t mean(l0) : 14.439394 \t Current Learning Rate: 0.006\n",
            "Iteration 4800: Current success : 175 \t All success : 198 \t mean(l0) : 14.439394 \t Current Learning Rate: 0.004\n",
            "Iteration 4900: Current success : 175 \t All success : 198 \t mean(l0) : 14.439394 \t Current Learning Rate: 0.002\n",
            "Iteration 5000: Current success : 175 \t All success : 198 \t mean(l0) : 14.439394 \t Current Learning Rate: 0.000\n",
            "Iteration 5100: Current success : 198 \t All success : 213 \t mean(l0) : 15.262911 \t Current Learning Rate: 0.049\n",
            "Iteration 5200: Current success : 198 \t All success : 219 \t mean(l0) : 16.159817 \t Current Learning Rate: 0.048\n",
            "Iteration 5300: Current success : 198 \t All success : 219 \t mean(l0) : 16.159817 \t Current Learning Rate: 0.047\n",
            "Iteration 5400: Current success : 201 \t All success : 220 \t mean(l0) : 16.449999 \t Current Learning Rate: 0.046\n",
            "Iteration 5500: Current success : 201 \t All success : 221 \t mean(l0) : 16.438915 \t Current Learning Rate: 0.045\n",
            "Iteration 5600: Current success : 200 \t All success : 221 \t mean(l0) : 16.434389 \t Current Learning Rate: 0.044\n",
            "Iteration 5700: Current success : 200 \t All success : 221 \t mean(l0) : 16.434389 \t Current Learning Rate: 0.043\n",
            "Iteration 5800: Current success : 201 \t All success : 221 \t mean(l0) : 16.420815 \t Current Learning Rate: 0.042\n",
            "Iteration 5900: Current success : 201 \t All success : 221 \t mean(l0) : 16.407240 \t Current Learning Rate: 0.041\n",
            "Iteration 6000: Current success : 201 \t All success : 221 \t mean(l0) : 16.407240 \t Current Learning Rate: 0.040\n",
            "Iteration 6100: Current success : 201 \t All success : 221 \t mean(l0) : 16.407240 \t Current Learning Rate: 0.039\n",
            "Iteration 6200: Current success : 201 \t All success : 221 \t mean(l0) : 16.407240 \t Current Learning Rate: 0.038\n",
            "Iteration 6300: Current success : 201 \t All success : 221 \t mean(l0) : 16.407240 \t Current Learning Rate: 0.037\n",
            "Iteration 6400: Current success : 201 \t All success : 221 \t mean(l0) : 16.407240 \t Current Learning Rate: 0.036\n",
            "Iteration 6500: Current success : 201 \t All success : 221 \t mean(l0) : 16.407240 \t Current Learning Rate: 0.035\n",
            "Iteration 6600: Current success : 201 \t All success : 221 \t mean(l0) : 16.407240 \t Current Learning Rate: 0.034\n",
            "Iteration 6700: Current success : 201 \t All success : 221 \t mean(l0) : 16.407240 \t Current Learning Rate: 0.033\n",
            "Iteration 6800: Current success : 201 \t All success : 221 \t mean(l0) : 16.407240 \t Current Learning Rate: 0.032\n",
            "Iteration 6900: Current success : 201 \t All success : 221 \t mean(l0) : 16.407240 \t Current Learning Rate: 0.031\n",
            "Iteration 7000: Current success : 201 \t All success : 221 \t mean(l0) : 16.407240 \t Current Learning Rate: 0.030\n",
            "Iteration 7100: Current success : 201 \t All success : 221 \t mean(l0) : 16.407240 \t Current Learning Rate: 0.029\n",
            "Iteration 7200: Current success : 201 \t All success : 221 \t mean(l0) : 16.407240 \t Current Learning Rate: 0.028\n",
            "Iteration 7300: Current success : 201 \t All success : 221 \t mean(l0) : 16.407240 \t Current Learning Rate: 0.027\n",
            "Iteration 7400: Current success : 201 \t All success : 221 \t mean(l0) : 16.407240 \t Current Learning Rate: 0.026\n",
            "Iteration 7500: Current success : 201 \t All success : 221 \t mean(l0) : 16.407240 \t Current Learning Rate: 0.025\n",
            "Iteration 7600: Current success : 201 \t All success : 221 \t mean(l0) : 16.407240 \t Current Learning Rate: 0.024\n",
            "Iteration 7700: Current success : 201 \t All success : 221 \t mean(l0) : 16.407240 \t Current Learning Rate: 0.023\n",
            "Iteration 7800: Current success : 201 \t All success : 221 \t mean(l0) : 16.407240 \t Current Learning Rate: 0.022\n",
            "Iteration 7900: Current success : 201 \t All success : 221 \t mean(l0) : 16.407240 \t Current Learning Rate: 0.021\n",
            "Iteration 8000: Current success : 201 \t All success : 221 \t mean(l0) : 16.407240 \t Current Learning Rate: 0.020\n",
            "Iteration 8100: Current success : 201 \t All success : 221 \t mean(l0) : 16.407240 \t Current Learning Rate: 0.019\n",
            "Iteration 8200: Current success : 201 \t All success : 221 \t mean(l0) : 16.407240 \t Current Learning Rate: 0.018\n",
            "Iteration 8300: Current success : 201 \t All success : 221 \t mean(l0) : 16.407240 \t Current Learning Rate: 0.017\n",
            "Iteration 8400: Current success : 201 \t All success : 221 \t mean(l0) : 16.407240 \t Current Learning Rate: 0.016\n",
            "Iteration 8500: Current success : 201 \t All success : 221 \t mean(l0) : 16.407240 \t Current Learning Rate: 0.015\n",
            "Iteration 8600: Current success : 201 \t All success : 221 \t mean(l0) : 16.407240 \t Current Learning Rate: 0.014\n",
            "Iteration 8700: Current success : 201 \t All success : 221 \t mean(l0) : 16.407240 \t Current Learning Rate: 0.013\n",
            "Iteration 8800: Current success : 201 \t All success : 221 \t mean(l0) : 16.407240 \t Current Learning Rate: 0.012\n",
            "Iteration 8900: Current success : 201 \t All success : 221 \t mean(l0) : 16.407240 \t Current Learning Rate: 0.011\n",
            "Iteration 9000: Current success : 201 \t All success : 221 \t mean(l0) : 16.407240 \t Current Learning Rate: 0.010\n",
            "Iteration 9100: Current success : 201 \t All success : 221 \t mean(l0) : 16.407240 \t Current Learning Rate: 0.009\n",
            "Iteration 9200: Current success : 201 \t All success : 221 \t mean(l0) : 16.407240 \t Current Learning Rate: 0.008\n",
            "Iteration 9300: Current success : 201 \t All success : 221 \t mean(l0) : 16.407240 \t Current Learning Rate: 0.007\n",
            "Iteration 9400: Current success : 201 \t All success : 221 \t mean(l0) : 16.407240 \t Current Learning Rate: 0.006\n",
            "Iteration 9500: Current success : 201 \t All success : 221 \t mean(l0) : 16.407240 \t Current Learning Rate: 0.005\n",
            "Iteration 9600: Current success : 201 \t All success : 221 \t mean(l0) : 16.407240 \t Current Learning Rate: 0.004\n",
            "Iteration 9700: Current success : 201 \t All success : 221 \t mean(l0) : 16.407240 \t Current Learning Rate: 0.003\n",
            "Iteration 9800: Current success : 201 \t All success : 221 \t mean(l0) : 16.407240 \t Current Learning Rate: 0.002\n",
            "Iteration 9900: Current success : 201 \t All success : 221 \t mean(l0) : 16.407240 \t Current Learning Rate: 0.001\n",
            "Iteration 10000: Current success : 201 \t All success : 221 \t mean(l0) : 16.407240 \t Current Learning Rate: 0.000\n",
            "Iteration 10100: Current success : 197 \t All success : 221 \t mean(l0) : 16.407240 \t Current Learning Rate: 0.025\n",
            "Iteration 10200: Current success : 198 \t All success : 221 \t mean(l0) : 16.407240 \t Current Learning Rate: 0.024\n",
            "Iteration 10300: Current success : 199 \t All success : 221 \t mean(l0) : 16.407240 \t Current Learning Rate: 0.024\n",
            "Iteration 10400: Current success : 199 \t All success : 221 \t mean(l0) : 16.407240 \t Current Learning Rate: 0.023\n",
            "Iteration 10500: Current success : 199 \t All success : 221 \t mean(l0) : 16.407240 \t Current Learning Rate: 0.023\n",
            "Iteration 10600: Current success : 199 \t All success : 221 \t mean(l0) : 16.407240 \t Current Learning Rate: 0.022\n",
            "Iteration 10700: Current success : 200 \t All success : 222 \t mean(l0) : 16.382883 \t Current Learning Rate: 0.022\n",
            "Iteration 10800: Current success : 200 \t All success : 222 \t mean(l0) : 16.382883 \t Current Learning Rate: 0.021\n",
            "Iteration 10900: Current success : 200 \t All success : 222 \t mean(l0) : 16.382883 \t Current Learning Rate: 0.021\n",
            "Iteration 11000: Current success : 200 \t All success : 222 \t mean(l0) : 16.382883 \t Current Learning Rate: 0.020\n",
            "Iteration 11100: Current success : 200 \t All success : 222 \t mean(l0) : 16.382883 \t Current Learning Rate: 0.020\n",
            "Iteration 11200: Current success : 200 \t All success : 222 \t mean(l0) : 16.382883 \t Current Learning Rate: 0.019\n",
            "Iteration 11300: Current success : 200 \t All success : 222 \t mean(l0) : 16.382883 \t Current Learning Rate: 0.019\n",
            "Iteration 11400: Current success : 200 \t All success : 223 \t mean(l0) : 16.349777 \t Current Learning Rate: 0.018\n",
            "Iteration 11500: Current success : 200 \t All success : 223 \t mean(l0) : 16.349777 \t Current Learning Rate: 0.018\n",
            "Iteration 11600: Current success : 200 \t All success : 223 \t mean(l0) : 16.349777 \t Current Learning Rate: 0.017\n",
            "Iteration 11700: Current success : 200 \t All success : 223 \t mean(l0) : 16.349777 \t Current Learning Rate: 0.017\n",
            "Iteration 11800: Current success : 200 \t All success : 223 \t mean(l0) : 16.349777 \t Current Learning Rate: 0.016\n",
            "Iteration 11900: Current success : 200 \t All success : 223 \t mean(l0) : 16.349777 \t Current Learning Rate: 0.016\n",
            "Iteration 12000: Current success : 200 \t All success : 223 \t mean(l0) : 16.349777 \t Current Learning Rate: 0.015\n",
            "Iteration 12100: Current success : 200 \t All success : 223 \t mean(l0) : 16.349777 \t Current Learning Rate: 0.015\n",
            "Iteration 12200: Current success : 201 \t All success : 223 \t mean(l0) : 16.349777 \t Current Learning Rate: 0.014\n",
            "Iteration 12300: Current success : 201 \t All success : 223 \t mean(l0) : 16.349777 \t Current Learning Rate: 0.014\n",
            "Iteration 12400: Current success : 200 \t All success : 223 \t mean(l0) : 16.349777 \t Current Learning Rate: 0.013\n",
            "Iteration 12500: Current success : 201 \t All success : 223 \t mean(l0) : 16.349777 \t Current Learning Rate: 0.013\n",
            "Iteration 12600: Current success : 201 \t All success : 223 \t mean(l0) : 16.349777 \t Current Learning Rate: 0.012\n",
            "Iteration 12700: Current success : 201 \t All success : 223 \t mean(l0) : 16.349777 \t Current Learning Rate: 0.012\n",
            "Iteration 12800: Current success : 201 \t All success : 223 \t mean(l0) : 16.349777 \t Current Learning Rate: 0.011\n",
            "Iteration 12900: Current success : 201 \t All success : 223 \t mean(l0) : 16.349777 \t Current Learning Rate: 0.011\n",
            "Iteration 13000: Current success : 200 \t All success : 223 \t mean(l0) : 16.349777 \t Current Learning Rate: 0.010\n",
            "Iteration 13100: Current success : 200 \t All success : 223 \t mean(l0) : 16.349777 \t Current Learning Rate: 0.010\n",
            "Iteration 13200: Current success : 200 \t All success : 223 \t mean(l0) : 16.349777 \t Current Learning Rate: 0.009\n",
            "Iteration 13300: Current success : 200 \t All success : 223 \t mean(l0) : 16.349777 \t Current Learning Rate: 0.009\n",
            "Iteration 13400: Current success : 200 \t All success : 223 \t mean(l0) : 16.349777 \t Current Learning Rate: 0.008\n",
            "Iteration 13500: Current success : 200 \t All success : 223 \t mean(l0) : 16.349777 \t Current Learning Rate: 0.008\n",
            "Iteration 13600: Current success : 200 \t All success : 223 \t mean(l0) : 16.349777 \t Current Learning Rate: 0.007\n",
            "Iteration 13700: Current success : 200 \t All success : 223 \t mean(l0) : 16.349777 \t Current Learning Rate: 0.007\n",
            "Iteration 13800: Current success : 200 \t All success : 223 \t mean(l0) : 16.349777 \t Current Learning Rate: 0.006\n",
            "Iteration 13900: Current success : 200 \t All success : 223 \t mean(l0) : 16.349777 \t Current Learning Rate: 0.006\n",
            "Iteration 14000: Current success : 200 \t All success : 223 \t mean(l0) : 16.349777 \t Current Learning Rate: 0.005\n",
            "Iteration 14100: Current success : 200 \t All success : 223 \t mean(l0) : 16.349777 \t Current Learning Rate: 0.005\n",
            "Iteration 14200: Current success : 200 \t All success : 223 \t mean(l0) : 16.349777 \t Current Learning Rate: 0.004\n",
            "Iteration 14300: Current success : 200 \t All success : 223 \t mean(l0) : 16.349777 \t Current Learning Rate: 0.004\n",
            "Iteration 14400: Current success : 200 \t All success : 223 \t mean(l0) : 16.349777 \t Current Learning Rate: 0.003\n",
            "Iteration 14500: Current success : 200 \t All success : 223 \t mean(l0) : 16.349777 \t Current Learning Rate: 0.003\n",
            "Iteration 14600: Current success : 200 \t All success : 223 \t mean(l0) : 16.349777 \t Current Learning Rate: 0.002\n",
            "Iteration 14700: Current success : 200 \t All success : 223 \t mean(l0) : 16.349777 \t Current Learning Rate: 0.002\n",
            "Iteration 14800: Current success : 200 \t All success : 223 \t mean(l0) : 16.349777 \t Current Learning Rate: 0.001\n",
            "Iteration 14900: Current success : 200 \t All success : 223 \t mean(l0) : 16.349777 \t Current Learning Rate: 0.001\n",
            "Iteration 15000: Current success : 200 \t All success : 223 \t mean(l0) : 16.349777 \t Current Learning Rate: 0.000\n",
            "Iteration 15100: Current success : 201 \t All success : 223 \t mean(l0) : 16.349777 \t Current Learning Rate: 0.012\n",
            "Iteration 15200: Current success : 202 \t All success : 223 \t mean(l0) : 16.349777 \t Current Learning Rate: 0.012\n",
            "Iteration 15300: Current success : 201 \t All success : 223 \t mean(l0) : 16.349777 \t Current Learning Rate: 0.012\n",
            "Iteration 15400: Current success : 201 \t All success : 224 \t mean(l0) : 16.580359 \t Current Learning Rate: 0.012\n",
            "Iteration 15500: Current success : 201 \t All success : 224 \t mean(l0) : 16.580359 \t Current Learning Rate: 0.011\n",
            "Iteration 15600: Current success : 201 \t All success : 224 \t mean(l0) : 16.580359 \t Current Learning Rate: 0.011\n",
            "Iteration 15700: Current success : 201 \t All success : 224 \t mean(l0) : 16.580359 \t Current Learning Rate: 0.011\n",
            "Iteration 15800: Current success : 202 \t All success : 224 \t mean(l0) : 16.580359 \t Current Learning Rate: 0.011\n",
            "Iteration 15900: Current success : 202 \t All success : 224 \t mean(l0) : 16.580359 \t Current Learning Rate: 0.010\n",
            "Iteration 16000: Current success : 203 \t All success : 224 \t mean(l0) : 16.580359 \t Current Learning Rate: 0.010\n",
            "Iteration 16100: Current success : 203 \t All success : 224 \t mean(l0) : 16.580359 \t Current Learning Rate: 0.010\n",
            "Iteration 16200: Current success : 202 \t All success : 224 \t mean(l0) : 16.580359 \t Current Learning Rate: 0.010\n",
            "Iteration 16300: Current success : 203 \t All success : 224 \t mean(l0) : 16.580359 \t Current Learning Rate: 0.009\n",
            "Iteration 16400: Current success : 203 \t All success : 224 \t mean(l0) : 16.580359 \t Current Learning Rate: 0.009\n",
            "Iteration 16500: Current success : 203 \t All success : 224 \t mean(l0) : 16.580359 \t Current Learning Rate: 0.009\n",
            "Iteration 16600: Current success : 203 \t All success : 224 \t mean(l0) : 16.580359 \t Current Learning Rate: 0.009\n",
            "Iteration 16700: Current success : 203 \t All success : 224 \t mean(l0) : 16.580359 \t Current Learning Rate: 0.008\n",
            "Iteration 16800: Current success : 203 \t All success : 224 \t mean(l0) : 16.580359 \t Current Learning Rate: 0.008\n",
            "Iteration 16900: Current success : 203 \t All success : 224 \t mean(l0) : 16.580359 \t Current Learning Rate: 0.008\n",
            "Iteration 17000: Current success : 203 \t All success : 224 \t mean(l0) : 16.580359 \t Current Learning Rate: 0.008\n",
            "Iteration 17100: Current success : 203 \t All success : 224 \t mean(l0) : 16.580359 \t Current Learning Rate: 0.007\n",
            "Iteration 17200: Current success : 203 \t All success : 224 \t mean(l0) : 16.580359 \t Current Learning Rate: 0.007\n",
            "Iteration 17300: Current success : 202 \t All success : 224 \t mean(l0) : 16.580359 \t Current Learning Rate: 0.007\n",
            "Iteration 17400: Current success : 202 \t All success : 224 \t mean(l0) : 16.580359 \t Current Learning Rate: 0.007\n",
            "Iteration 17500: Current success : 202 \t All success : 224 \t mean(l0) : 16.580359 \t Current Learning Rate: 0.006\n",
            "Iteration 17600: Current success : 202 \t All success : 224 \t mean(l0) : 16.580359 \t Current Learning Rate: 0.006\n",
            "Iteration 17700: Current success : 202 \t All success : 224 \t mean(l0) : 16.580359 \t Current Learning Rate: 0.006\n",
            "Iteration 17800: Current success : 202 \t All success : 224 \t mean(l0) : 16.580359 \t Current Learning Rate: 0.006\n",
            "Iteration 17900: Current success : 202 \t All success : 224 \t mean(l0) : 16.580359 \t Current Learning Rate: 0.005\n",
            "Iteration 18000: Current success : 202 \t All success : 224 \t mean(l0) : 16.580359 \t Current Learning Rate: 0.005\n",
            "Iteration 18100: Current success : 202 \t All success : 224 \t mean(l0) : 16.580359 \t Current Learning Rate: 0.005\n",
            "Iteration 18200: Current success : 203 \t All success : 224 \t mean(l0) : 16.580359 \t Current Learning Rate: 0.005\n",
            "Iteration 18300: Current success : 203 \t All success : 224 \t mean(l0) : 16.580359 \t Current Learning Rate: 0.004\n",
            "Iteration 18400: Current success : 203 \t All success : 224 \t mean(l0) : 16.580359 \t Current Learning Rate: 0.004\n",
            "Iteration 18500: Current success : 202 \t All success : 224 \t mean(l0) : 16.580359 \t Current Learning Rate: 0.004\n",
            "Iteration 18600: Current success : 202 \t All success : 224 \t mean(l0) : 16.580359 \t Current Learning Rate: 0.004\n",
            "Iteration 18700: Current success : 203 \t All success : 224 \t mean(l0) : 16.580359 \t Current Learning Rate: 0.003\n",
            "Iteration 18800: Current success : 202 \t All success : 224 \t mean(l0) : 16.580359 \t Current Learning Rate: 0.003\n",
            "Iteration 18900: Current success : 202 \t All success : 224 \t mean(l0) : 16.580359 \t Current Learning Rate: 0.003\n",
            "Iteration 19000: Current success : 202 \t All success : 224 \t mean(l0) : 16.580359 \t Current Learning Rate: 0.003\n",
            "Iteration 19100: Current success : 201 \t All success : 224 \t mean(l0) : 16.580359 \t Current Learning Rate: 0.002\n",
            "Iteration 19200: Current success : 201 \t All success : 224 \t mean(l0) : 16.580359 \t Current Learning Rate: 0.002\n",
            "Iteration 19300: Current success : 201 \t All success : 224 \t mean(l0) : 16.580359 \t Current Learning Rate: 0.002\n",
            "Iteration 19400: Current success : 201 \t All success : 224 \t mean(l0) : 16.580359 \t Current Learning Rate: 0.002\n",
            "Iteration 19500: Current success : 201 \t All success : 224 \t mean(l0) : 16.580359 \t Current Learning Rate: 0.001\n",
            "Iteration 19600: Current success : 201 \t All success : 224 \t mean(l0) : 16.580359 \t Current Learning Rate: 0.001\n",
            "Iteration 19700: Current success : 201 \t All success : 224 \t mean(l0) : 16.580359 \t Current Learning Rate: 0.001\n",
            "Iteration 19800: Current success : 201 \t All success : 224 \t mean(l0) : 16.580359 \t Current Learning Rate: 0.001\n",
            "Iteration 19900: Current success : 201 \t All success : 224 \t mean(l0) : 16.580359 \t Current Learning Rate: 0.000\n",
            "Iteration 20000: Current success : 201 \t All success : 224 \t mean(l0) : 16.580359 \t Current Learning Rate: 0.000\n",
            "1130\n",
            "264\n",
            "Accuracy of just malwares (without attack): 98.85% | Under attack: 76.64%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "W3bo5I5ciZLF"
      },
      "outputs": [],
      "source": [
        "for x_test, y_test in test_loader:\n",
        "    x_test, y_test = x_test.to(torch.float32).to(device), y_test.to(device)\n",
        "    mal_x_batch, mal_y_batch = x_test[y_test.squeeze() == 1], y_test[y_test.squeeze() == 1]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation on rounded adversarial examples\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "combined_model.eval()\n",
        "\n",
        "# Apply the final rounding solution to adversarial examples\n",
        "adv_rounded = final_rounding(mal_x_batch, adv, combined_model,removal_array)\n",
        "\n",
        "# Get the model's predictions on the rounded adversarial examples\n",
        "outputs = combined_model(adv_rounded)\n",
        "y_pred = outputs.argmax(dim=1)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "num_malwares = len(mal_y_batch)\n",
        "num_successful_attacks = (y_pred == 0).sum().item()\n",
        "total_difference = (torch.abs(adv_rounded[y_pred == 0] - mal_x_batch[y_pred == 0])).sum().item()\n",
        "accuracy = (y_pred == 1).sum().item() / num_malwares * 100\n",
        "\n",
        "# Display the results with clear formatting\n",
        "print(f\"Evaluation Results on Rounded Adversarial Examples\")\n",
        "print(f\"-----------------------------------------------\")\n",
        "print(f\"Number of malware samples: {num_malwares}\")\n",
        "print(f\"Number of successful attacks: {num_successful_attacks}\")\n",
        "print(f\"Total L1 norm difference (successful adversarial examples vs. original): {total_difference}\")\n",
        "print(f\"Accuracy of the model on malware under attack: {accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dce34718-b960-4afd-f6fb-6de60ad08c9f",
        "id": "rYQTOqMTiZLF"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Results on Rounded Adversarial Examples\n",
            "-----------------------------------------------\n",
            "Number of malware samples: 1130\n",
            "Number of successful attacks: 264\n",
            "Total L1 norm difference (successful adversarial examples vs. original): 5235.0\n",
            "Accuracy of the model on malware under attack: 76.64%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure that fixed elements are not changed\n",
        "\n",
        "# Ensure `removal_array` is expanded to match the batch size\n",
        "expanded_removal_array = removal_array.expand(mal_x_batch.shape[0], -1)\n",
        "\n",
        "# Create a mask where non-fixed features are set to 1\n",
        "non_fixed_features_mask = torch.bitwise_or(expanded_removal_array, 1 - mal_x_batch.to(torch.uint8))\n",
        "\n",
        "# Calculate indices where the mask is 0 (fixed) or 1 (non-fixed)\n",
        "mask_fixed = non_fixed_features_mask == 0\n",
        "mask_non_fixed = non_fixed_features_mask == 1\n",
        "\n",
        "# Calculate and print the differences\n",
        "diff_adv_fixed = (torch.abs(adv - mal_x_batch) * mask_fixed).sum().item()\n",
        "diff_adv_non_fixed = (torch.abs(adv - mal_x_batch) * mask_non_fixed).sum().item()\n",
        "diff_adv_rounded_fixed = (torch.abs(adv_rounded - mal_x_batch) * mask_fixed).sum().item()\n",
        "diff_adv_rounded_non_fixed = (torch.abs(adv_rounded - mal_x_batch) * mask_non_fixed).sum().item()\n",
        "\n",
        "# Display results with clear formatting\n",
        "print(f\"Differences when mask is 0 (fixed features):\")\n",
        "print(f\"  Adv vs. Original: {diff_adv_fixed}\")\n",
        "print(f\"  Rounded Adv vs. Original: {diff_adv_rounded_fixed}\")\n",
        "print(f\"Differences when mask is 1 (non-fixed features):\")\n",
        "print(f\"  Adv vs. Original: {diff_adv_non_fixed}\")\n",
        "print(f\"  Rounded Adv vs. Original: {diff_adv_rounded_non_fixed}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cd20d81-d507-4bea-e333-24b6692564bd",
        "id": "Qtd2P57GiZLG"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Differences when mask is 0 (fixed features):\n",
            "  Adv vs. Original: 0.0\n",
            "  Rounded Adv vs. Original: 0.0\n",
            "Differences when mask is 1 (non-fixed features):\n",
            "  Adv vs. Original: 59497.92578125\n",
            "  Rounded Adv vs. Original: 74941.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming differences is already computed as in your code\n",
        "differences = (torch.abs(adv_rounded[y_pred == 0] - mal_x_batch[y_pred == 0])).sum(1)\n",
        "\n",
        "# Convert differences to a numpy array for plotting\n",
        "differences_np = differences.cpu().numpy()\n",
        "\n",
        "# Set the style of the plot\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# Create a histogram with Seaborn\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(differences_np, bins=16, kde=False, color='skyblue')\n",
        "#sns.histplot(differences_np, bins=30, kde=True, color='skyblue')\n",
        "\n",
        "# Set the title and labels\n",
        "plt.title('Abundance of Differences', fontsize=16)\n",
        "plt.xlabel('Sum of Absolute Differences', fontsize=14)\n",
        "plt.ylabel('Frequency', fontsize=14)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "8VTp4lzDiZLG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 576
        },
        "outputId": "565846cc-4de9-47c1-d67b-6a85deeadf2a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "adv[y_pred==0][0][:200]- mal_x_batch[y_pred==0][0][:200]"
      ],
      "metadata": {
        "id": "4yZbq9OJiZLG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b76da6a0-a009-458c-c3f5-207e46659ff4"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00, -2.9002e-01,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  2.4158e-02,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  1.0000e+00,  1.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  1.5911e-04,  0.0000e+00,  0.0000e+00,  3.6955e-06,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         1.2219e-06,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  2.0862e-07,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  1.7136e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  3.2485e-06,\n",
              "         1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00, -9.9989e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  2.4582e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  2.1761e-03,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         6.5565e-07,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         2.5985e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "adv[y_pred==0][1][:200] - mal_x_batch[y_pred==0][1][:200]"
      ],
      "metadata": {
        "id": "efN26d9QiZLH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bb6d381-3746-4402-f9d2-be6392d1e168"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.0000e+00,  9.9999e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  9.9999e-01,  1.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  2.9981e-05,  0.0000e+00,\n",
              "         0.0000e+00,  2.7508e-05,  0.0000e+00,  0.0000e+00,  1.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  1.0000e+00, -9.9902e-01,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "        -3.5167e-06,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00, -1.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00, -1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  9.3350e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00,\n",
              "         0.0000e+00,  1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aadr6kWk3BrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y6_UW5DP4Ytu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# tanh(4 * perturbation_tensor)"
      ],
      "metadata": {
        "id": "cpKrIlvD4ZIG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "from torch.optim.lr_scheduler import CyclicLR\n",
        "\n",
        "def compare(output, target, confidence):\n",
        "    if not isinstance(output, (float, int, torch.LongTensor, torch.cuda.LongTensor)):\n",
        "        output = torch.clone(output).detach()\n",
        "        output[target] -= confidence  # Slightly reduce the confidence for targeted attacks\n",
        "        output = torch.argmax(output, dim=1)\n",
        "    return output == target\n",
        "\n",
        "def compare_round(mal_x_batch,newimg, model,removal_array):\n",
        "\n",
        "    adv_rounded = final_rounding(mal_x_batch, newimg, model,removal_array)\n",
        "    #l0dist = torch.sum((mal_x_batch - newimg)!=0, dim=1)\n",
        "    l0dist = torch.sum(torch.abs(adv_rounded - mal_x_batch), dim=1)\n",
        "    logits = model(adv_rounded)\n",
        "    output = torch.argmax(logits, dim=1)\n",
        "\n",
        "    return output == 0, l0dist\n",
        "\n",
        "def sin_func_torch(input_tensor):\n",
        "    \"\"\"\n",
        "    A sine-based transformation applied to the input tensor.\n",
        "\n",
        "    The function applies a sine transformation and rescales the output to [0, 1].\n",
        "    \"\"\"\n",
        "    return (torch.sin((input_tensor - 0.5) * torch.pi) + 1) / 2.0\n",
        "\n",
        "def CW_l1(imgs, y, model, removal_array, max_iterations=1000, max_learning_rate=0.1,\n",
        "       binary_search_steps=3, confidence=0.01, initial_const=0.1,\n",
        "       cons_increase_factor=2, device='cuda'):\n",
        "    \"\"\"\n",
        "    Implements the Carlini & Wagner (CW) L1 attack.\n",
        "\n",
        "    Parameters:\n",
        "    - imgs: The original images to be attacked.\n",
        "    - y: The true labels of the images.\n",
        "    - model: The target model to attack.\n",
        "    - removal_array: A mask array specifying which features are fixed.\n",
        "    - max_iterations: The maximum number of iterations for the attack.\n",
        "    - learning_rate: The learning rate for the optimizer.\n",
        "    - binary_search_steps: The number of steps for the binary search over the constant.\n",
        "    - confidence: Confidence value for the attack.\n",
        "    - initial_const: The initial constant for the attack loss.\n",
        "    - cons_increase_factor: The factor by which the constant is increased during binary search.\n",
        "    - device: The device on which to perform computations.\n",
        "\n",
        "    Returns:\n",
        "    - final_output: The perturbed images after the attack.\n",
        "    \"\"\"\n",
        "\n",
        "    labs = torch.zeros_like(y.squeeze(), dtype=torch.int64)\n",
        "    batch_size = imgs.shape[0]\n",
        "\n",
        "    # Expand removal array to match the batch size\n",
        "    expanded_removal_array = removal_array.expand(imgs.shape[0], -1)\n",
        "    non_fixed_features_mask = torch.bitwise_or(expanded_removal_array, 1 - imgs.to(torch.uint8))\n",
        "\n",
        "    mask_0 = non_fixed_features_mask == 0\n",
        "    mask_1 = non_fixed_features_mask == 1\n",
        "\n",
        "    imgs_tensor = imgs.clone().detach().to(dtype=torch.float32, device=device)\n",
        "    labs_tensor = labs.clone().detach().to(dtype=torch.int64, device=device)\n",
        "\n",
        "    lower_bound = torch.zeros(batch_size, dtype=torch.float32, device=device)\n",
        "    CONST = torch.full((batch_size,), initial_const, dtype=torch.float32, device=device)\n",
        "    upper_bound = torch.full((batch_size,), 1e10, dtype=torch.float32, device=device)\n",
        "\n",
        "    o_bestl0 = torch.full((batch_size,), 1e10, device=device)\n",
        "    o_bestattack = imgs.clone().detach()\n",
        "\n",
        "    o_best_loss1 = torch.full((batch_size,), float('inf'), device=device)\n",
        "    o_best_unsuccessfull_attack = imgs.clone().detach()\n",
        "    o_success_attack = torch.zeros(batch_size, dtype=torch.bool, device=device)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    for outer_step in range(binary_search_steps):\n",
        "        print(CONST)\n",
        "\n",
        "        modifier = torch.zeros_like(imgs_tensor, requires_grad=True).to(device)\n",
        "        optimizer = torch.optim.Adam([modifier], lr=max_learning_rate)\n",
        "\n",
        "        def grad_hook(grad):\n",
        "            return grad * non_fixed_features_mask\n",
        "        modifier.register_hook(grad_hook)\n",
        "\n",
        "        scheduler = CyclicLR(optimizer, base_lr=0.0001, max_lr=max_learning_rate, step_size_up=5,\n",
        "                             step_size_down=4995, mode='triangular2')\n",
        "\n",
        "        best_loss = torch.full((batch_size,), float('inf'), device=device)\n",
        "        no_improvement_count = torch.zeros(batch_size, dtype=torch.int, device=device)\n",
        "        active_mask = torch.ones(batch_size, dtype=torch.bool, device=device)\n",
        "\n",
        "        bestl0 = torch.full((batch_size,), 1e10, device=device)\n",
        "        bestscore = torch.full((batch_size,), -1, device=device)\n",
        "        success_mask = torch.zeros(batch_size, dtype=torch.bool, device=device)\n",
        "\n",
        "        for iteration in range(max_iterations):\n",
        "\n",
        "            active_indices = torch.where(active_mask)[0]\n",
        "            active_modifier = modifier[active_mask]\n",
        "            active_imgs_tensor = imgs_tensor[active_mask]\n",
        "            active_labs_tensor = labs_tensor[active_mask]\n",
        "            active_CONST = CONST[active_mask]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            modified_input = active_imgs_tensor + active_modifier\n",
        "            newimg = sin_func_torch(modified_input)\n",
        "            output = model(newimg)\n",
        "\n",
        "            #eps = 1e-2\n",
        "            perturbation = torch.where((active_imgs_tensor==1.), (active_imgs_tensor - newimg), (newimg - active_imgs_tensor))\n",
        "\n",
        "            #l2dist = torch.sum((1 - (perturbation - 1)**2 + eps)**0.5 - eps**0.5 , dim=1)\n",
        "            #l2dist = torch.sum((newimg - active_imgs_tensor) ** 2, dim=1)\n",
        "            l2dist = torch.sum(torch.tanh(4 * perturbation), dim=1)\n",
        "\n",
        "            real = output[torch.arange(active_indices.size(0)), active_labs_tensor]\n",
        "            other = output[torch.arange(active_indices.size(0)), active_labs_tensor + 1]\n",
        "\n",
        "            loss1 = torch.clamp((other - real + confidence), min=0.0)\n",
        "            #loss2 = l2dist\n",
        "\n",
        "            if outer_step != (binary_search_steps - 1):\n",
        "                loss = (active_CONST * loss1) + l2dist\n",
        "            else:\n",
        "                loss = loss1\n",
        "\n",
        "            total_loss = torch.sum(loss)\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            if (iteration + 1) % 10 == 0:\n",
        "                with torch.no_grad():\n",
        "                    success_mask_inloop, l0dist = compare_round(active_imgs_tensor,newimg, model,removal_array)\n",
        "                    success_mask  = torch.logical_or(success_mask_inloop, success_mask)\n",
        "                    current_lr = scheduler.get_last_lr()[0]\n",
        "\n",
        "                    improved_mask2 = (loss1 < o_best_loss1[active_mask])\n",
        "                    o_best_loss1[active_mask] = torch.where(improved_mask2, loss1, o_best_loss1[active_mask])\n",
        "                    o_best_unsuccessfull_attack[active_mask] = torch.where(improved_mask2.unsqueeze(-1), newimg, o_best_unsuccessfull_attack[active_mask])\n",
        "\n",
        "                    update_mask = (l0dist < bestl0[active_mask]) & success_mask_inloop\n",
        "                    bestl0[active_mask] = torch.where(update_mask, l0dist, bestl0[active_mask])\n",
        "                    bestscore[active_mask] = torch.where(update_mask, torch.argmax(output, dim=1), bestscore[active_mask])\n",
        "\n",
        "                    update_mask2 = (l0dist < o_bestl0[active_mask]) & success_mask_inloop\n",
        "                    o_bestl0[active_mask] = torch.where(update_mask2, l0dist, o_bestl0[active_mask])\n",
        "                    o_bestattack[active_mask] = torch.where(update_mask2.unsqueeze(-1), newimg, o_bestattack[active_mask])\n",
        "\n",
        "                    # Skip the rest of the loop for samples that haven't improved in 50 iterations\n",
        "                    #active_mask[active_mask.clone()] = no_improvement_count[active_mask.clone()] < 50\n",
        "\n",
        "                    if (iteration + 1) % 100 == 0:\n",
        "                        print(f\"Iteration {iteration + 1}: Current success : {success_mask_inloop.sum()} \\t All success : {success_mask.sum()} \\t mean(l0) : {o_bestl0[success_mask].mean():.6f} \\t Current Learning Rate: {current_lr:.3f}\")\n",
        "\n",
        "        if outer_step != (binary_search_steps - 1):\n",
        "            #o_success_attack  = torch.logical_or(o_success_attack, success_mask)\n",
        "            o_success_attack, o_l0dist = compare_round(imgs,o_bestattack, model,removal_array)\n",
        "            print(f\"outer_step {outer_step + 1}: all success : {o_success_attack.sum()} \\t mean(l0)(success) : {o_l0dist[o_success_attack].mean():.4f}\")\n",
        "            #print('o_success_attack ', o_success_attack.sum())\n",
        "            print('-------------------------------------------------------------')\n",
        "            failure_mask = ~success_mask\n",
        "            upper_bound[success_mask] = torch.minimum(upper_bound[success_mask], CONST[success_mask])\n",
        "            lower_bound[failure_mask] = torch.maximum(lower_bound[failure_mask], CONST[failure_mask])\n",
        "\n",
        "            upper_limit_mask = upper_bound < 1e10\n",
        "            CONST[upper_limit_mask] = (lower_bound[upper_limit_mask] + upper_bound[upper_limit_mask]) / 2\n",
        "            CONST[failure_mask & ~upper_limit_mask] *= cons_increase_factor\n",
        "\n",
        "    success_attack_mask = (o_bestl0 != 1e10)\n",
        "    final_output = torch.where(success_attack_mask.unsqueeze(-1), o_bestattack, o_best_unsuccessfull_attack)\n",
        "\n",
        "    return final_output\n"
      ],
      "metadata": {
        "id": "0UBE6RWm4ZIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#l2dist = torch.sum(perturbation, dim=1)\n",
        "attack_params =  {'device':device, 'confidence':0.5, 'removal_array':removal_array,'max_iterations': 20000, 'max_learning_rate': 0.1, 'initial_const': 1 , 'binary_search_steps' : 7, 'cons_increase_factor' : 10}\n",
        "adv = adv_predict(test_loader, combined_model, CW_l1, **attack_params)"
      ],
      "metadata": {
        "id": "DYUioTn44ZIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yApHDRx94ZIJ"
      },
      "outputs": [],
      "source": [
        "for x_test, y_test in test_loader:\n",
        "    x_test, y_test = x_test.to(torch.float32).to(device), y_test.to(device)\n",
        "    mal_x_batch, mal_y_batch = x_test[y_test.squeeze() == 1], y_test[y_test.squeeze() == 1]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation on rounded adversarial examples\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "combined_model.eval()\n",
        "\n",
        "# Apply the final rounding solution to adversarial examples\n",
        "adv_rounded = final_rounding(mal_x_batch, adv, combined_model,removal_array)\n",
        "\n",
        "# Get the model's predictions on the rounded adversarial examples\n",
        "outputs = combined_model(adv_rounded)\n",
        "y_pred = outputs.argmax(dim=1)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "num_malwares = len(mal_y_batch)\n",
        "num_successful_attacks = (y_pred == 0).sum().item()\n",
        "total_difference = (torch.abs(adv_rounded[y_pred == 0] - mal_x_batch[y_pred == 0])).sum().item()\n",
        "accuracy = (y_pred == 1).sum().item() / num_malwares * 100\n",
        "\n",
        "# Display the results with clear formatting\n",
        "print(f\"Evaluation Results on Rounded Adversarial Examples\")\n",
        "print(f\"-----------------------------------------------\")\n",
        "print(f\"Number of malware samples: {num_malwares}\")\n",
        "print(f\"Number of successful attacks: {num_successful_attacks}\")\n",
        "print(f\"Total L1 norm difference (successful adversarial examples vs. original): {total_difference}\")\n",
        "print(f\"Accuracy of the model on malware under attack: {accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "id": "Ft4YrvBB4ZIK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure that fixed elements are not changed\n",
        "\n",
        "# Ensure `removal_array` is expanded to match the batch size\n",
        "expanded_removal_array = removal_array.expand(mal_x_batch.shape[0], -1)\n",
        "\n",
        "# Create a mask where non-fixed features are set to 1\n",
        "non_fixed_features_mask = torch.bitwise_or(expanded_removal_array, 1 - mal_x_batch.to(torch.uint8))\n",
        "\n",
        "# Calculate indices where the mask is 0 (fixed) or 1 (non-fixed)\n",
        "mask_fixed = non_fixed_features_mask == 0\n",
        "mask_non_fixed = non_fixed_features_mask == 1\n",
        "\n",
        "# Calculate and print the differences\n",
        "diff_adv_fixed = (torch.abs(adv - mal_x_batch) * mask_fixed).sum().item()\n",
        "diff_adv_non_fixed = (torch.abs(adv - mal_x_batch) * mask_non_fixed).sum().item()\n",
        "diff_adv_rounded_fixed = (torch.abs(adv_rounded - mal_x_batch) * mask_fixed).sum().item()\n",
        "diff_adv_rounded_non_fixed = (torch.abs(adv_rounded - mal_x_batch) * mask_non_fixed).sum().item()\n",
        "\n",
        "# Display results with clear formatting\n",
        "print(f\"Differences when mask is 0 (fixed features):\")\n",
        "print(f\"  Adv vs. Original: {diff_adv_fixed}\")\n",
        "print(f\"  Rounded Adv vs. Original: {diff_adv_rounded_fixed}\")\n",
        "print(f\"Differences when mask is 1 (non-fixed features):\")\n",
        "print(f\"  Adv vs. Original: {diff_adv_non_fixed}\")\n",
        "print(f\"  Rounded Adv vs. Original: {diff_adv_rounded_non_fixed}\")\n"
      ],
      "metadata": {
        "id": "Vd59Sz-i4ZIK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming differences is already computed as in your code\n",
        "differences = (torch.abs(adv_rounded[y_pred == 0] - mal_x_batch[y_pred == 0])).sum(1)\n",
        "\n",
        "# Convert differences to a numpy array for plotting\n",
        "differences_np = differences.cpu().numpy()\n",
        "\n",
        "# Set the style of the plot\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# Create a histogram with Seaborn\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(differences_np, bins=16, kde=False, color='skyblue')\n",
        "#sns.histplot(differences_np, bins=30, kde=True, color='skyblue')\n",
        "\n",
        "# Set the title and labels\n",
        "plt.title('Abundance of Differences', fontsize=16)\n",
        "plt.xlabel('Sum of Absolute Differences', fontsize=14)\n",
        "plt.ylabel('Frequency', fontsize=14)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "LZaGNurm4ZIK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adv[y_pred==0][0][:200]- mal_x_batch[y_pred==0][0][:200]"
      ],
      "metadata": {
        "id": "MC4-X8E14ZIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adv[y_pred==0][1][:200] - mal_x_batch[y_pred==0][1][:200]"
      ],
      "metadata": {
        "id": "C39dDrQN4ZIL"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "FZ57x18g4b-h",
        "BTzMhU-t8Fvy",
        "ur8Xjp9qQdWp",
        "5RWC9JQRs6ZD",
        "GehIpqUrhVHo"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}