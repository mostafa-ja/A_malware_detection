{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOJtP6bCNPqZPgakMtqmWXf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mostafa-ja/A_malware_detection/blob/main/paralell_models2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "5rtOyp2MpkWZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "mnist_train = datasets.MNIST(\"./data\", train=True, download=True, transform=transforms.ToTensor())\n",
        "mnist_test = datasets.MNIST(\"./data\", train=False, download=True, transform=transforms.ToTensor())\n",
        "\n",
        "# Filter the training dataset to include only digits 0 and 1\n",
        "train_idx = (mnist_train.targets <= 1)\n",
        "mnist_train.data = mnist_train.data[train_idx]\n",
        "mnist_train.targets = mnist_train.targets[train_idx]\n",
        "\n",
        "# Filter the test dataset to include only digits 0 and 1\n",
        "test_idx = (mnist_test.targets <= 1)\n",
        "mnist_test.data = mnist_test.data[test_idx]\n",
        "mnist_test.targets = mnist_test.targets[test_idx]\n",
        "\n",
        "train_loader = DataLoader(mnist_train, batch_size=100, shuffle=True)\n",
        "test_loader = DataLoader(mnist_test, batch_size=100, shuffle=False)"
      ],
      "metadata": {
        "id": "M5hFYUtWsmzj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X,y = next(iter(train_loader))\n",
        "print(X.shape)\n",
        "print(X.view(X.shape[0], -1).shape)\n",
        "print(y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZp9zF3Usrdd",
        "outputId": "75581b63-0ed3-4b7d-a1bb-4e90dcfc1060"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([100, 1, 28, 28])\n",
            "torch.Size([100, 784])\n",
            "torch.Size([100])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = nn.Linear(784, 1)\n",
        "yp = model(X.view(X.shape[0], -1))\n",
        "print(yp.shape)\n",
        "print(yp[:,0].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5GIBOwzsz0K",
        "outputId": "3e14ff75-3c9e-4ee8-8414-df5321f8b625"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([100, 1])\n",
            "torch.Size([100])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# do a single pass over the data\n",
        "def epoch(loader, model, opt=None):\n",
        "    total_loss, total_err = 0.,0.\n",
        "    for X,y in loader:\n",
        "        yp = model(X.view(X.shape[0], -1))[:,0]\n",
        "        loss = nn.BCEWithLogitsLoss()(yp, y.float())\n",
        "        if opt:\n",
        "            opt.zero_grad()\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "        total_err += ((yp > 0) * (y==0) + (yp < 0) * (y==1)).sum().item()\n",
        "        total_loss += loss.item() * X.shape[0]\n",
        "    return total_err / len(loader.dataset), total_loss / len(loader.dataset)"
      ],
      "metadata": {
        "id": "ZFyg_tues6MW"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_model = nn.Linear(784, 1)\n",
        "opt = optim.SGD(original_model.parameters(), lr=1.)\n",
        "print(\"Train Err\", \"Train Loss\", \"Test Err\", \"Test Loss\", sep=\"\\t\")\n",
        "for i in range(10):\n",
        "    train_err, train_loss = epoch(train_loader, original_model, opt)\n",
        "    test_err, test_loss = epoch(test_loader, original_model)\n",
        "    print(*(\"{:.6f}\".format(i) for i in (train_err, train_loss, test_err, test_loss)), sep=\"\\t\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_0RhSTFtEMg",
        "outputId": "c0108d48-b0c2-4bf4-e3ef-87b37499c8bf"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Err\tTrain Loss\tTest Err\tTest Loss\n",
            "0.004580\t0.014935\t0.000473\t0.003424\n",
            "0.001579\t0.005480\t0.000473\t0.002695\n",
            "0.001184\t0.004368\t0.000473\t0.002442\n",
            "0.001105\t0.003858\t0.000473\t0.002268\n",
            "0.001105\t0.003535\t0.000946\t0.002099\n",
            "0.000869\t0.003230\t0.000473\t0.002061\n",
            "0.000790\t0.003049\t0.000946\t0.001958\n",
            "0.000947\t0.002786\t0.000473\t0.001930\n",
            "0.000790\t0.002657\t0.000473\t0.001878\n",
            "0.000711\t0.002490\t0.000946\t0.001829\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for param in original_model.parameters():\n",
        "  print(param.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRtWCII9tSv9",
        "outputId": "82bf76fa-1e6c-40b2-9125-28b3521ecb3a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 784])\n",
            "torch.Size([1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = [param.detach().flatten() for param in original_model.parameters()]"
      ],
      "metadata": {
        "id": "26NTCvyNtvTf"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-ngwteWzCiM",
        "outputId": "e017c9e7-1652-4ec8-d7ff-1529b4383883"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([784])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cat(a).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fkI6CZ81uwjk",
        "outputId": "0c01a494-1c96-4dcc-c456-6820a6dcb693"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([785])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nn.functional.cosine_similarity(torch.cat(a).view(1, -1), torch.cat(a).view(1, -1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5sGThyzCzSV2",
        "outputId": "1cceff71-319c-4686-8da3-697e7533da5d"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomLoss(nn.Module):\n",
        "    def __init__(self, existing_model, weight_lambda):\n",
        "        super(CustomLoss, self).__init__()\n",
        "        self.existing_model = existing_model\n",
        "        self.weight_lambda = weight_lambda\n",
        "\n",
        "    def forward(self, new_model, y_pred, y_true):\n",
        "        # Calculate the task-specific loss (e.g., mean squared error)\n",
        "        task_loss = nn.functional.binary_cross_entropy_with_logits(y_pred, y_true)\n",
        "\n",
        "        # Calculate parameter dissimilarity loss\n",
        "        a = [param.detach().flatten() for param in original_model.parameters()]\n",
        "        b = [param.flatten() for param in new_model.parameters()]\n",
        "        dissimilarity_loss = nn.functional.cosine_similarity(torch.cat(a).view(1, -1), torch.cat(b).view(1, -1))\n",
        "\n",
        "        # Combine the task-specific loss and parameter dissimilarity loss\n",
        "        total_loss = task_loss + self.weight_lambda * dissimilarity_loss\n",
        "\n",
        "        return total_loss"
      ],
      "metadata": {
        "id": "40LTL8ZByCe-"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_model = nn.Linear(784, 1)\n",
        "#new_model.load_state_dict(original_model.state_dict())\n",
        "\n",
        "weight_lambda = 0.1\n",
        "custom_loss_function = CustomLoss(original_model, weight_lambda=weight_lambda)\n",
        "optimizer = optim.SGD(new_model.parameters(), lr=0.1)\n",
        "num_epochs = 40\n",
        "\n",
        "print(\"Train Err\", \"Train Loss\", \"Test Err\", \"Test Loss\", sep=\"\\t\")\n",
        "\n",
        "for e in range(num_epochs):\n",
        "  total_loss, total_err = 0.,0.\n",
        "  for X,y in train_loader:\n",
        "    optimizer.zero_grad()\n",
        "    outputs = new_model(X.view(X.shape[0], -1))[:,0]\n",
        "    loss = custom_loss_function(new_model, outputs, y.float())\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    total_loss += loss.item() * X.shape[0]\n",
        "    total_err += ((outputs > 0) * (y==0) + (outputs < 0) * (y==1)).sum().item()\n",
        "\n",
        "  train_err = total_err / len(train_loader.dataset)\n",
        "  train_loss = total_loss / len(train_loader.dataset)\n",
        "  test_err, test_loss = epoch(test_loader, new_model)\n",
        "  print(*(\"{:.6f}\".format(i) for i in (train_err, train_loss, test_err, test_loss)), sep=\"\\t\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pfav_oqUwFcy",
        "outputId": "7b6230bd-e4a0-4464-db0f-9df3f58021b5"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Err\tTrain Loss\tTest Err\tTest Loss\n",
            "0.006711\t0.116806\t0.002364\t0.022266\n",
            "0.005369\t0.072336\t0.002364\t0.017831\n",
            "0.006553\t0.062206\t0.003310\t0.016906\n",
            "0.007106\t0.055700\t0.002837\t0.016579\n",
            "0.007185\t0.051044\t0.002837\t0.016374\n",
            "0.007501\t0.047563\t0.002837\t0.015993\n",
            "0.007343\t0.044786\t0.002837\t0.015835\n",
            "0.006711\t0.042563\t0.002837\t0.015516\n",
            "0.006869\t0.040671\t0.002837\t0.015335\n",
            "0.007027\t0.039094\t0.002364\t0.014860\n",
            "0.007185\t0.037700\t0.002364\t0.014676\n",
            "0.007027\t0.036520\t0.002364\t0.014460\n",
            "0.006948\t0.035421\t0.002364\t0.014327\n",
            "0.006948\t0.034500\t0.002364\t0.014082\n",
            "0.007027\t0.033598\t0.001891\t0.013964\n",
            "0.006948\t0.032782\t0.001891\t0.013860\n",
            "0.006790\t0.032022\t0.001891\t0.013695\n",
            "0.006869\t0.031399\t0.001891\t0.013525\n",
            "0.006790\t0.030776\t0.002364\t0.013322\n",
            "0.006632\t0.030132\t0.002837\t0.013156\n",
            "0.006317\t0.029608\t0.001891\t0.013260\n",
            "0.006711\t0.029161\t0.001891\t0.013022\n",
            "0.006632\t0.028632\t0.001418\t0.012987\n",
            "0.006475\t0.028230\t0.002364\t0.012741\n",
            "0.006159\t0.027787\t0.001418\t0.012660\n",
            "0.006080\t0.027412\t0.002364\t0.012541\n",
            "0.006080\t0.027037\t0.001418\t0.012537\n",
            "0.006001\t0.026650\t0.001891\t0.012390\n",
            "0.006080\t0.026322\t0.001891\t0.012268\n",
            "0.006080\t0.025998\t0.001891\t0.012259\n",
            "0.006159\t0.025691\t0.001891\t0.012200\n",
            "0.006080\t0.025391\t0.001891\t0.012101\n",
            "0.006159\t0.025092\t0.001891\t0.012134\n",
            "0.006080\t0.024819\t0.001891\t0.012028\n",
            "0.005922\t0.024567\t0.001891\t0.011965\n",
            "0.006001\t0.024266\t0.001891\t0.011833\n",
            "0.005606\t0.024028\t0.001418\t0.011884\n",
            "0.005843\t0.023780\t0.001418\t0.011940\n",
            "0.005606\t0.023646\t0.001418\t0.011818\n",
            "0.005685\t0.023370\t0.001418\t0.011780\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "original_model.weight"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6msHopV-7VP",
        "outputId": "8761b51c-6e77-41d7-a4a0-304015ade50c"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[ 3.8082e-03,  2.5526e-02, -9.3072e-03,  1.9966e-02,  1.5991e-03,\n",
              "          2.2240e-02, -1.7062e-02, -9.9386e-03, -2.4688e-02,  2.0577e-02,\n",
              "         -7.2293e-03,  1.3848e-02, -3.2568e-02, -3.3239e-02, -2.5869e-02,\n",
              "         -8.2383e-03,  3.9105e-03,  2.3276e-02,  2.1999e-02, -1.7862e-04,\n",
              "         -2.8451e-02, -2.3479e-02,  1.2209e-02,  1.7007e-02,  1.8712e-03,\n",
              "         -2.5416e-02,  3.5677e-02,  1.3594e-02, -2.2197e-02,  1.3516e-02,\n",
              "         -6.5574e-03, -3.5285e-02,  2.0137e-02, -3.2952e-02, -1.5436e-02,\n",
              "         -2.4242e-02,  1.8378e-02, -3.1672e-02, -2.1499e-02,  3.4478e-02,\n",
              "          1.3755e-02,  3.1069e-02,  1.8147e-02, -7.1200e-04, -1.3859e-02,\n",
              "          2.7157e-02,  2.7785e-02,  1.7844e-03, -7.9886e-03,  2.8365e-02,\n",
              "          1.1827e-02, -7.7374e-03, -9.1095e-03, -5.8481e-03,  2.6158e-02,\n",
              "         -8.5409e-03, -2.5151e-02, -2.6076e-02,  3.2913e-02, -1.4227e-02,\n",
              "         -1.3804e-02, -2.5062e-02,  3.1875e-02, -3.0031e-02, -1.3548e-02,\n",
              "          1.3789e-02,  1.6662e-02, -3.4114e-02, -2.9258e-02,  2.1115e-02,\n",
              "          3.6701e-02, -1.7034e-02,  6.3046e-02,  9.3666e-02, -1.5590e-02,\n",
              "          3.4591e-02,  2.6363e-02,  2.4738e-02, -3.3445e-03, -1.9746e-02,\n",
              "          1.5725e-02,  2.4820e-03,  8.8221e-03,  1.0060e-02,  3.2764e-02,\n",
              "         -2.8332e-02,  2.6157e-03,  3.3522e-02,  8.5424e-03, -1.2456e-02,\n",
              "          1.7014e-02,  1.1546e-02, -2.6745e-02,  3.7769e-02,  2.1528e-02,\n",
              "         -2.4439e-02,  6.7145e-03,  5.8188e-02,  1.2180e-01,  9.3842e-02,\n",
              "          1.4642e-01,  1.1867e-01,  5.6796e-02, -1.1584e-02, -1.5429e-02,\n",
              "          8.1443e-03, -1.1874e-02,  1.4827e-02,  1.7014e-02,  2.5585e-02,\n",
              "          1.8432e-03,  8.8561e-03,  1.5056e-02,  2.3059e-02,  2.4098e-02,\n",
              "          1.2869e-02,  3.4975e-02,  1.6733e-02,  1.4425e-02, -3.5021e-02,\n",
              "          7.6217e-02,  9.9346e-02, -3.6441e-03,  6.2959e-02,  7.8806e-02,\n",
              "          1.3105e-01,  7.7075e-02,  9.9875e-02,  6.5680e-02,  9.6109e-02,\n",
              "         -3.4440e-02, -2.0131e-02,  1.4738e-01,  7.2480e-02,  6.6838e-02,\n",
              "         -3.0016e-03, -3.1642e-03,  8.9209e-03,  6.9887e-03, -7.1119e-03,\n",
              "          3.2666e-02, -3.1700e-02,  2.4129e-02,  1.1291e-02, -3.0658e-02,\n",
              "         -9.4223e-03,  4.0182e-02,  3.6385e-03,  5.8665e-02,  7.7723e-02,\n",
              "          2.7167e-02,  6.6606e-02,  7.9042e-02,  1.4798e-01,  1.9996e-02,\n",
              "         -6.2006e-02, -6.4875e-02, -5.4831e-02, -1.1500e-01,  3.8005e-03,\n",
              "          1.2426e-01,  1.5089e-01,  1.2900e-01,  4.7411e-02,  4.1634e-02,\n",
              "         -1.6806e-02, -5.6119e-07,  1.8668e-03, -3.2408e-02,  1.5445e-02,\n",
              "          2.9557e-02, -1.2189e-02,  3.5914e-02,  2.5439e-02,  4.1885e-02,\n",
              "          4.7810e-02,  1.0495e-01,  1.1133e-02, -8.5767e-03, -6.1147e-02,\n",
              "          2.9789e-02,  1.2466e-02, -3.8184e-02, -7.3107e-02, -8.2733e-02,\n",
              "         -1.2519e-01, -7.0820e-02, -6.7834e-02,  1.4477e-01,  1.3901e-01,\n",
              "          3.5733e-02, -1.2873e-02,  5.0571e-02, -2.7543e-02,  1.5268e-02,\n",
              "         -1.0204e-02,  2.8055e-03,  3.8748e-03,  2.0893e-02,  1.3662e-02,\n",
              "          4.4590e-02,  6.8847e-02,  2.4313e-02,  8.8334e-02,  1.0686e-03,\n",
              "         -7.2291e-02, -1.5988e-01, -1.6280e-01, -6.4600e-02,  5.2695e-02,\n",
              "         -1.6946e-02, -2.9408e-02, -7.9033e-02, -1.6986e-01, -1.1455e-01,\n",
              "         -7.3929e-02,  4.5466e-02, -9.0260e-03, -7.4266e-02, -8.1408e-02,\n",
              "         -4.7953e-02, -3.3129e-02, -2.9064e-02, -2.3235e-04,  1.6999e-02,\n",
              "          1.3754e-02,  5.0725e-04,  1.7097e-02,  1.0128e-02,  8.6254e-02,\n",
              "          4.5105e-02,  1.0621e-01,  2.8960e-02, -1.1658e-01, -1.7982e-01,\n",
              "         -1.0250e-01,  3.8155e-02,  1.1114e-02,  1.5107e-01,  5.3621e-02,\n",
              "         -1.2765e-01, -1.8899e-01, -3.5943e-02, -8.0358e-02, -1.2961e-01,\n",
              "         -1.5196e-01, -1.2662e-01, -9.5037e-02, -1.7053e-02, -3.1426e-02,\n",
              "          2.3330e-03,  2.7282e-02,  1.2335e-03,  1.8952e-02, -3.5335e-02,\n",
              "          3.4516e-02,  1.2423e-02,  6.6269e-02,  6.7049e-03,  4.6536e-02,\n",
              "          2.5408e-02, -4.0162e-02, -7.4141e-02, -3.3361e-03,  5.9944e-02,\n",
              "          1.3453e-01,  1.6049e-01,  1.1556e-01, -4.5442e-02, -7.8125e-02,\n",
              "         -1.1884e-02, -2.5729e-02, -2.3131e-01, -2.8567e-01, -2.2454e-01,\n",
              "         -1.4275e-01, -4.0997e-03, -2.2630e-02, -1.4647e-02, -2.0689e-02,\n",
              "         -1.5381e-02, -3.0536e-02,  3.5441e-02,  2.6596e-02,  1.7214e-02,\n",
              "          1.3808e-02, -1.0313e-01, -6.6963e-02, -9.4659e-02, -5.7705e-02,\n",
              "         -2.1003e-03,  1.1925e-02,  4.6174e-02,  2.7015e-02,  1.3197e-01,\n",
              "          2.4926e-01,  1.4394e-01, -7.1850e-02, -7.0524e-02, -1.0662e-01,\n",
              "         -2.7870e-01, -3.1498e-01, -2.3128e-01, -1.5675e-01, -7.5377e-02,\n",
              "         -1.9169e-02,  2.9063e-02,  2.2325e-03, -3.2129e-02,  9.5844e-03,\n",
              "          3.5574e-02,  3.5090e-02,  5.5573e-02, -6.6611e-02, -1.2931e-01,\n",
              "         -1.3125e-01, -1.9541e-01, -1.3740e-01, -7.9302e-02, -1.0565e-01,\n",
              "         -6.4230e-02, -6.3532e-03,  2.0584e-01,  4.2330e-01,  2.9046e-01,\n",
              "         -1.5406e-02, -1.0283e-01, -2.4788e-01, -2.2529e-01, -2.3018e-01,\n",
              "         -2.7252e-01, -1.3785e-01, -9.1191e-02,  2.1913e-02, -1.2527e-02,\n",
              "          1.0933e-02, -3.0268e-03,  2.1124e-02,  2.9939e-02, -7.8071e-03,\n",
              "          1.7070e-02, -8.9511e-02, -1.2442e-01, -1.3801e-01, -2.2063e-01,\n",
              "         -1.9752e-01, -1.6281e-01, -2.1510e-01, -2.4145e-01,  1.2742e-01,\n",
              "          4.2034e-01,  5.5945e-01,  2.9693e-01, -4.8904e-02, -2.5286e-01,\n",
              "         -3.2690e-01, -2.0086e-01, -2.6817e-01, -2.4290e-01, -1.8446e-01,\n",
              "         -6.6360e-02,  2.3400e-02, -2.3484e-02, -1.5241e-02,  2.6452e-02,\n",
              "         -4.7877e-03,  2.3915e-02, -1.0529e-02, -7.6493e-03, -9.1308e-02,\n",
              "         -1.6564e-01, -2.4835e-01, -2.4150e-01, -2.4480e-01, -2.1113e-01,\n",
              "         -3.0086e-01, -1.7547e-01,  3.6251e-01,  6.4397e-01,  6.3057e-01,\n",
              "          2.5095e-01, -1.1052e-01, -4.1013e-01, -2.9182e-01, -1.5490e-01,\n",
              "         -3.0645e-01, -3.0661e-01, -2.1358e-01, -1.1893e-01,  1.4666e-02,\n",
              "          2.7659e-02,  2.4047e-02,  1.6506e-02,  3.2410e-02, -2.1542e-02,\n",
              "          3.8305e-03, -1.2223e-02, -1.5516e-01, -2.0215e-01, -3.0836e-01,\n",
              "         -2.5218e-01, -1.4471e-01, -3.5458e-01, -3.4071e-01, -5.1896e-02,\n",
              "          4.5954e-01,  8.1801e-01,  5.7360e-01,  1.9501e-01, -1.9155e-01,\n",
              "         -5.3760e-01, -2.3896e-01, -2.0727e-01, -2.5804e-01, -2.5769e-01,\n",
              "         -1.9920e-01, -1.1947e-01, -3.6731e-02, -2.7112e-02,  1.5915e-02,\n",
              "         -7.6646e-03,  3.2855e-02,  1.7851e-02, -1.0888e-03, -4.3636e-02,\n",
              "         -1.5049e-01, -2.2076e-01, -2.7470e-01, -2.3908e-01, -2.8627e-01,\n",
              "         -4.5008e-01, -2.4131e-01,  1.1448e-01,  5.7240e-01,  8.1181e-01,\n",
              "          3.9901e-01,  6.2035e-02, -3.6123e-01, -3.9700e-01, -1.8859e-01,\n",
              "         -2.3841e-01, -2.9717e-01, -2.5714e-01, -1.8502e-01, -7.6861e-02,\n",
              "          1.9559e-02, -1.1066e-02,  4.4765e-03,  6.0856e-03,  9.8279e-03,\n",
              "          3.0544e-02, -7.9398e-04, -6.2601e-02, -1.8914e-01, -2.6209e-01,\n",
              "         -2.7768e-01, -2.6497e-01, -4.1500e-01, -3.5325e-01, -8.2589e-02,\n",
              "          2.3311e-01,  5.9104e-01,  6.0137e-01,  3.1218e-01, -9.7033e-02,\n",
              "         -3.7171e-01, -2.2796e-01, -2.1373e-01, -2.7308e-01, -2.2119e-01,\n",
              "         -2.2561e-01, -1.7660e-01, -3.4853e-02,  4.2233e-02, -1.4243e-02,\n",
              "          3.2798e-02,  1.6048e-02,  2.4183e-02,  1.4303e-02,  5.7261e-03,\n",
              "         -6.9626e-02, -1.8938e-01, -2.1386e-01, -2.2702e-01, -2.8337e-01,\n",
              "         -3.9458e-01, -2.5292e-01, -1.6239e-02,  2.5631e-01,  4.2808e-01,\n",
              "          3.0472e-01,  1.8875e-01, -1.3113e-01, -2.8529e-01, -2.2569e-01,\n",
              "         -2.6126e-01, -2.4426e-01, -1.7638e-01, -1.6418e-01, -1.1864e-01,\n",
              "          9.7190e-03,  9.5411e-02,  6.7240e-03,  4.3888e-03, -1.9292e-02,\n",
              "         -8.4364e-03,  3.0876e-02, -2.5148e-02, -9.4683e-02, -1.7839e-01,\n",
              "         -1.7625e-01, -2.1711e-01, -2.9933e-01, -2.4908e-01, -1.1437e-01,\n",
              "         -6.5256e-02,  1.4031e-01,  2.7239e-01,  1.9102e-01,  6.7524e-02,\n",
              "         -7.9168e-02, -2.0231e-01, -1.5009e-01, -2.6446e-01, -1.4598e-01,\n",
              "         -9.3943e-02, -1.4062e-01, -7.4625e-02,  7.1719e-02,  9.2515e-02,\n",
              "         -2.9581e-02,  2.1455e-02, -1.1437e-02, -3.0203e-04, -2.5314e-02,\n",
              "          1.5677e-02, -8.3737e-02, -2.1680e-01, -1.5065e-01, -8.7440e-02,\n",
              "         -2.2668e-01, -1.0199e-01, -4.2600e-02, -6.9883e-02,  4.1469e-02,\n",
              "          1.5339e-01,  1.3421e-01,  1.0029e-01, -2.9088e-02, -8.3123e-02,\n",
              "         -8.1258e-02, -1.0033e-01, -4.8484e-02, -2.1313e-02,  3.6047e-03,\n",
              "          2.3155e-02,  7.9003e-02,  7.8826e-02,  2.4636e-02,  3.5247e-02,\n",
              "          8.5574e-03, -3.4490e-02, -1.6373e-02, -3.1068e-02, -4.6152e-02,\n",
              "         -7.2906e-02,  3.8201e-03, -6.1057e-02, -4.2416e-03, -1.0600e-02,\n",
              "         -3.7746e-02, -5.7157e-02, -9.6755e-02, -1.6145e-02,  1.1813e-01,\n",
              "          1.4909e-01,  1.1667e-01,  1.7180e-01,  1.8753e-01,  1.0560e-01,\n",
              "          1.3612e-01,  6.4438e-03,  1.0504e-01,  1.1962e-01,  9.3473e-02,\n",
              "          4.2376e-02, -2.4897e-02, -1.1197e-02,  1.3666e-02, -1.0038e-02,\n",
              "         -7.7984e-03, -1.6755e-02,  5.9311e-02,  1.3488e-01,  2.1488e-01,\n",
              "          1.6482e-01,  1.6748e-01,  1.0334e-01, -6.3111e-02, -2.0538e-01,\n",
              "         -1.9719e-01, -7.8000e-02,  6.5974e-02,  4.0893e-02,  1.1328e-01,\n",
              "          2.0002e-01,  2.0346e-01,  1.6111e-01,  2.7335e-01,  1.8823e-01,\n",
              "          7.0412e-02,  8.4932e-02,  1.1093e-01,  5.5386e-02,  2.6362e-02,\n",
              "          7.8899e-03,  3.1811e-02, -1.0326e-02,  1.7406e-02, -2.1494e-02,\n",
              "          1.1418e-01,  1.6002e-01,  1.1209e-01,  1.1853e-01,  1.5336e-01,\n",
              "          1.4910e-01, -9.5702e-02, -2.6758e-01, -3.5691e-01, -1.6047e-01,\n",
              "         -7.5960e-02,  1.4834e-04,  7.4328e-03,  7.4742e-02,  1.0725e-01,\n",
              "          6.4228e-02,  2.0417e-01,  9.6092e-02,  5.9815e-04,  3.5551e-02,\n",
              "          5.2450e-02,  3.8522e-02, -1.8852e-02, -5.6093e-03,  2.4662e-02,\n",
              "         -1.9616e-02, -2.9668e-02,  2.3938e-02,  2.6533e-02,  6.0933e-02,\n",
              "          6.9846e-02, -2.7737e-02,  2.4180e-03,  1.1475e-01, -9.1531e-02,\n",
              "         -2.6246e-01, -2.2923e-01, -1.2815e-01, -8.7513e-02, -6.3852e-03,\n",
              "          1.0131e-02,  3.3523e-02,  1.8769e-02,  1.9568e-02,  1.0844e-01,\n",
              "         -7.8266e-03, -9.0337e-03, -3.2097e-02,  2.9219e-02, -1.4278e-02,\n",
              "         -3.2990e-02, -1.4568e-02,  1.5296e-02, -2.6168e-03,  1.1437e-02,\n",
              "         -2.0134e-03, -4.0281e-04, -7.1957e-03, -7.6081e-05,  5.3319e-02,\n",
              "          2.8712e-04,  2.5029e-02,  4.2717e-02,  4.4637e-02, -8.3635e-02,\n",
              "         -5.7193e-02, -5.6362e-02,  6.9169e-03,  1.4760e-02,  7.0551e-02,\n",
              "          4.1525e-03,  3.1656e-02, -2.3321e-02,  3.2067e-02,  1.1977e-02,\n",
              "          4.6296e-03,  2.0185e-02, -1.3527e-03,  3.3445e-02,  1.6522e-02,\n",
              "          1.0382e-02, -5.2634e-04,  1.5171e-02,  3.5127e-02,  2.0866e-02,\n",
              "         -8.5647e-04, -1.2772e-02, -4.7446e-03,  3.2720e-02,  1.8574e-02,\n",
              "          1.7215e-02, -2.2800e-02,  7.9032e-02,  2.6247e-02,  3.4138e-02,\n",
              "          7.0959e-02,  2.4019e-02,  2.1323e-02, -8.2898e-05, -2.0665e-02,\n",
              "         -1.9406e-02, -2.4184e-02,  1.7369e-02, -1.0412e-02, -5.9012e-04,\n",
              "         -3.5303e-02,  1.7054e-02,  3.5022e-02, -9.1576e-03, -3.3824e-02,\n",
              "         -3.5502e-02, -3.0280e-02, -3.7431e-03,  4.8783e-03, -2.5486e-02,\n",
              "         -2.8102e-02, -5.9120e-03,  2.7717e-02, -2.0421e-02,  9.2302e-03,\n",
              "          3.5021e-02, -7.0589e-04, -3.0665e-03,  2.7785e-04,  2.2445e-03,\n",
              "         -1.1272e-03, -2.8635e-03, -1.5045e-02,  6.3627e-03,  6.4758e-03,\n",
              "         -1.9133e-02, -1.2225e-02,  7.6773e-03,  2.0387e-02,  2.0479e-02,\n",
              "         -2.5506e-02, -1.8203e-02, -3.4527e-02,  3.0822e-02, -3.4908e-02,\n",
              "         -2.0562e-02, -2.9285e-02,  1.6458e-02,  1.3618e-02,  2.3638e-03,\n",
              "         -1.4715e-02,  7.9751e-03,  1.3965e-02,  6.8124e-03,  6.6342e-03,\n",
              "          2.8520e-02, -1.7891e-02, -1.9508e-02, -7.3218e-03,  8.6837e-03,\n",
              "          9.7113e-03, -1.7707e-02,  8.4858e-04,  2.7474e-03,  1.7407e-02,\n",
              "         -3.1032e-03,  2.1098e-02, -6.6415e-03, -2.4368e-02]],\n",
              "       requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    }
  ]
}